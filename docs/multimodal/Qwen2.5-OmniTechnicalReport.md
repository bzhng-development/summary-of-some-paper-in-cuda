# Qwen2.5-Omni Technical Report

**ArXiv:** [2503.20215](https://arxiv.org/abs/2503.20215)

## ğŸ¯ Pitch

Qwen2.5-Omni introduces a unified, end-to-end multimodal model that can perceive text, images, audio, and video, while simultaneously generating real-time text and high-quality speech responses. Its novel TMRoPE (Time-aligned Multimodal RoPE) and Thinkerâ€“Talker architecture enable block-wise streaming and synchronized audio-video understanding, breaking the traditional latency and modality barriers of prior systems. This fusion empowers next-generation conversational agents with fluid, low-latency, human-like interactions across modalities, achieving state-of-the-art results on leading multimodal benchmarks and paving the way toward truly intelligent multi-sensory AI assistants.

---

## 1. Executive Summary
Qwen2.5â€‘Omni is a single endâ€‘toâ€‘end multimodal model that ingests text, images, audio, and video and can stream both text and naturalâ€‘speech responses at the same time. It introduces two core mechanismsâ€”`TMRoPE` for timeâ€‘aligned audioâ€‘video fusion and a `Thinkerâ€“Talker` architecture for concurrent, lowâ€‘latency text and speech generationâ€”yielding stateâ€‘ofâ€‘theâ€‘art performance on multimodal understanding benchmarks while maintaining competitive text and highâ€‘quality speech generation (Figures 1â€“4; Tables 1â€“10).

## 2. Context and Motivation
- Problem/gap addressed
  - Most systems handle only a subset of modalities (e.g., audio or vision) or require multiâ€‘stage pipelines (ASR â†’ LLM â†’ TTS) that add latency and error propagation. This work targets a unified, endâ€‘toâ€‘end model that:
    - streams multimodal inputs (audio/video) in real time,
    - aligns audio and video temporally,
    - and generates text and speech simultaneously without mutual interference (Section 1; Figure 2).
- Why it matters
  - Realâ€‘time assistants need to â€œseeâ€ and â€œhearâ€ while â€œtalking.â€ Lowâ€‘latency, synchronized understanding and response unlocks voice/video dialogue, live video reasoning, and natural conversational agents (Figure 1).
- Where prior approaches fall short
  - LVLMs (vision + language) and LALMs (audio + language) typically do not unify all modalities endâ€‘toâ€‘end or stream outputs; audioâ€‘video timing is often loosely aligned; text and speech decoding interfere when trained together (Section 1).
- Positioning relative to existing work
  - Qwen2.5â€‘Omni compares to Qwen2.5â€‘VL on vision and Qwen2â€‘Audio on audio, and challenges recent omni models (e.g., Gemini 1.5, MiniCPMâ€‘o, Baichuanâ€‘Omni). It aims to match or exceed modalityâ€‘specific models while adding synchronized streaming generation (Sections 1â€“2; Tables 1â€“8).

## 3. Technical Approach
Stepâ€‘byâ€‘step overview (Figure 2 and Section 2):

1) Inputs and encoders (â€œPerceivation,â€ Section 2.2; Figure 3)
- Text
  - Tokenized with the Qwen tokenizer (151,643 tokens).
- Audio
  - Resampled to 16 kHz; converted to a 128â€‘channel melâ€‘spectrogram (25 ms window, 10 ms hop). The audio encoder (from Qwen2â€‘Audio) outputs one frame â‰ˆ 40 ms of original audio.
- Vision (images and video)
  - Vision encoder (â‰ˆ675Mâ€‘param ViT from Qwen2.5â€‘VL) is trained on mixed image/video data. Video frames are sampled dynamically to preserve time, and a single image is treated as two identical frames for consistency with video.

2) Time alignment across modalities with `TMRoPE` (Section 2.2; Figure 3)
- What it is
  - `TMRoPE` (Timeâ€‘aligned Multimodal Rotary Position Embedding) decomposes rotary positional embedding into three components: `temporal`, `height`, and `width`.
- How it works
  - Text and audio use identical (1Dâ€‘like) position IDs; audio also gets an absolute temporal ID at a granularity of 40 ms per frame.
  - Images: a constant temporal ID per image; distinct `height`/`width` IDs across patches.
  - Video: temporal IDs increase per frame. Because FPS varies, the temporal ID step is dynamically set so that one temporal ID â‰ˆ 40 ms.
  - When mixing modalities, position IDs are offset so the sequence is globally ordered (modality Aâ€™s max ID + 1 starts modality B).
- Why it matters
  - It gives the shared attention mechanism a consistent, timeâ€‘aware coordinate system across audio and video, enabling fineâ€‘grained, timeâ€‘synced fusion (Figure 3).

3) Interleaved audioâ€“video packing (Section 2.2; Figure 3)
- The model segments streams into 2â€‘second chunks. Within each 2â€‘second chunk, it places visual representations first and audio representations after, then interleaves such chunks in time. This ensures the LLM receives temporally adjacent audioâ€“video content together.

4) The `Thinkerâ€“Talker` architecture (Section 2.1; Figure 2)
- `Thinker` (Transformer decoder)
  - Acts as the â€œbrain.â€ It consumes text/audio/video encodings and produces highâ€‘level hidden representations and autoregressive text tokens.
- `Talker` (dualâ€‘track autoregressive Transformer decoder)
  - Acts as the â€œmouth.â€ It takes: (a) `Thinker`â€™s highâ€‘level representations and (b) the embeddings of sampled `Thinker` text tokens. It autoregressively emits discrete audio tokens while having access to all `Thinker` historical context. Text and speech streams are generated concurrently.
- Rationale
  - Speech needs prosody and pragmatics before the entire text is known. Feeding `Thinker`â€™s hidden states gives `Talker` anticipation of tone/emotion while the discrete text tokens remove phonetic ambiguity (Section 2.3).

5) Speech tokenization and streaming vocoder (Sections 2.3â€“2.4; Figure 4)
- Discrete speech tokens: `qwenâ€‘ttsâ€‘tokenizer` encodes speech compactly and supports streaming decoding via a causal audio decoder.
- Tokenâ€‘toâ€‘waveform decoding:
  - A `Flowâ€‘Matching` DiT maps codes to melâ€‘spectrograms, then a modified `BigVGAN` reconstructs waveforms (Section 2.4).
  - Slidingâ€‘window block attention restricts DiTâ€™s receptive field to 4 blocks with 2â€‘block lookback and 1â€‘block lookahead, reducing initial latency while preserving local context (Figure 4).

6) Streaming inference and prefilling (Section 2.4)
- Encoders are revised for blockâ€‘wise streaming. Audio attention is confined to 2â€‘second blocks; the vision encoder uses FlashAttention and merges adjacent 2Ã—2 tokens to keep compute bounded. These changes enable `chunkedâ€‘prefill`â€”feeding chunks early to build key/value caches so generation can start quickly.

7) Training pipeline (Sections 3â€“4)
- Preâ€‘training: three stages
  - Stage 1: Initialize the LLM from `Qwen2.5`, vision from `Qwen2.5â€‘VL`, audio from `Whisperâ€‘largeâ€‘v3`. Freeze LLM; train encoders and adapters on large audioâ€‘text and imageâ€‘text data to align them to the LLM.
  - Stage 2: Unfreeze all; train on expansive mixed multimodal data: ~800B image/videoâ€‘related tokens, 300B audioâ€‘related, 100B videoâ€‘withâ€‘audio, plus text corpora. Max length 8,192 tokens.
  - Stage 3: Extend sequences and include long audio/video to 32,768 tokens for longâ€‘context understanding.
- Postâ€‘training (supervised instruction tuning; Section 4)
  - `Thinker` uses `ChatML` dialogs across text, image, audio, and mixed modalities (Section 4.1â€“4.2).
  - `Talker` threeâ€‘stage training (Section 4.3):
    1) Nextâ€‘token continuation on multimodal dialog + spoken responses (learn monotonic mapping from semantics to speech; timbre disentanglement to avoid overfitting specific voices).
    2) Stability optimization with DPOâ€‘style objective. Equation (1) (Section 4.3) shows an LDPO loss that prefers â€œgoodâ€ speech samples (`yw`) over â€œbadâ€ (`yl`) for the same context `x`, based on rewards tied to WER and punctuationâ€‘pause errors.
    3) Multiâ€‘speaker instruction fineâ€‘tuning for naturalness and controllability of voice.

## 4. Key Insights and Innovations
- `Thinkerâ€“Talker` for concurrent, lowâ€‘interference generation (Figure 2; Sections 2.1â€“2.3)
  - Novelty: Separates semantic reasoning (`Thinker`) from speech realization (`Talker`), while letting `Talker` directly consume `Thinker`â€™s hidden states and sampled text tokens. This preserves semantic coherence and reduces interference between text and speech decodingâ€”an issue in prior unified decoders.
  - Why it matters: Enables realâ€‘time voice responses that already reflect intended tone and content while text is still being formed.
- `TMRoPE` for timeâ€‘aligned multimodal fusion (Figure 3; Section 2.2)
  - Novelty: Decomposes rotary embeddings into `temporal/height/width` and injects absolute time (40 ms steps) to synchronize audio and video streamsâ€”even under variable video frame ratesâ€”while maintaining a single attention space for all modalities.
  - Why it matters: Creating a shared, aligned coordinate system is essential for coherent audioâ€‘video reasoning and streaming fusion.
- Interleaved 2â€‘second chunking + blockâ€‘wise encoders for streaming (Section 2.4)
  - Novelty: The encoders themselves are made streamingâ€‘aware (2â€‘second audio blocks; ViT token merging + FlashAttention), enabling `chunkedâ€‘prefill` across modalities. Interleaving keeps audio and visual for the same window adjacent in the sequence.
  - Why it matters: Reduces firstâ€‘token latency and avoids quadratic growth in compute/memory with long inputs.
- Slidingâ€‘window DiT with Flowâ€‘Matching for streaming vocoding (Figure 4; Section 2.4)
  - Novelty: Constrains receptive field (2â€‘block lookback, 1â€‘block lookahead; 4 total) when mapping tokens â†’ mel, then uses a streamingâ€‘friendly `BigVGAN` to produce waveforms chunk by chunk.
  - Why it matters: Improves robustness and reduces initial delay for speech output; the lookahead helps maintain continuity without waiting for long future context.

These are fundamental architectural choices rather than small tweaks; they reconfigure how a multimodal LLM handles time, streaming, and dualâ€‘output generation.

## 5. Experimental Analysis
Evaluation setup (Section 5; Tables 1â€“10):
- Modalities evaluated
  - Understanding: `X â†’ Text` where `X âˆˆ {text, audio, image, video, mixed}`.
  - Generation: `X â†’ Speech` focusing on zeroâ€‘shot and singleâ€‘speaker TTSâ€‘style metrics.
- Datasets and metrics
  - Text: MMLUâ€‘Pro, MMLUâ€‘redux, LiveBench, GPQA, GSM8K, MATH, HumanEval, MBPP, MultiPLâ€‘E, LiveCodeBench (Table 1).
  - Audioâ†’Text: ASR (LibriSpeech, Common Voice 15, FLEURS, WenetSpeech, VoxPopuli), S2TT (CoVoST2), SER, VSC, music tasks; reasoning on MMAU; voice interaction on VoiceBench (Tables 2â€“3).
  - Imageâ†’Text: MMMU, MMMUâ€‘Pro, MathVista, MathVision, MMBenchâ€‘V1.1, MMVet, MMStar, MME, MuirBench, CRPE, RealWorldQA, MMEâ€‘RealWorld, MMâ€‘MTâ€‘Bench; OCR tasks AI2D, TextVQA, DocVQA, ChartQA, OCRBench_v2; grounding (RefCOCO family, ODinW, point grounding) (Tables 5â€“6).
  - Videoâ†’Text: Videoâ€‘MME, MVBench, EgoSchema (Table 7).
  - Mixedâ€‘modality: OmniBench (Table 8).
  - Speech generation: SEEDâ€‘TTS for WER (content consistency) and speaker similarity; NMOS for subjective naturalness on a selfâ€‘created set (Tables 9â€“10).

Main quantitative results
- Textâ†’Text (Table 1)
  - Qwen2.5â€‘Omniâ€‘7B generally sits between Qwen2â€‘7B and Qwen2.5â€‘7B. Examples:
    - MMLUâ€‘redux: 71.0 vs Qwen2â€‘7B 67.3 and Qwen2.5â€‘7B 75.4.
    - MATH: 71.5 vs Qwen2â€‘7B 52.9 and Qwen2.5â€‘7B 75.5.
    - GSM8K: 88.7 vs Qwen2â€‘7B 85.7 and Qwen2.5â€‘7B 91.6.
  - Takeaway: multimodal additions do not collapse text ability; it stays competitive, though not equal to the strongest pureâ€‘text 7B baseline.

- Audioâ†’Text (Tables 2â€“3)
  - ASR examples:
    - LibriSpeech testâ€‘other: 3.4 WER (vs Qwen2â€‘Audio 3.6; Whisperâ€‘largeâ€‘v3 3.6).
    - Common Voice 15 en/zh: 7.6/5.2 WER (Qwen2â€‘Audio 8.6/6.9).
    - FLEURS zh/en: 3.0/4.1 (competitive with best models listed).
  - S2TT (CoVoST2): enâ€‘de 30.2 BLEU, deâ€‘en 37.7, enâ€‘zh 41.4, zhâ€‘en 29.4 (Table 2).
  - General audio understanding/reasoning:
    - VSC: 0.939 (ties Qwen2â€‘Audio best; Table 3).
    - MMAU avg: 65.60 vs Qwen2â€‘Audio 49.20; perâ€‘subset: Sound 67.87, Music 69.16, Speech 59.76 (Table 3).
  - Voice interaction:
    - VoiceBench average: 74.12, best among listed omni/audio models of similar size (Table 3).

- Voiceâ€‘chatting with speech instructions (Table 4)
  - On converted speech prompts, Qwen2.5â€‘Omni narrows the gap with a textâ€‘prompt LLM:
    - GSM8K (math word problems): 85.4 (voice) vs Qwen2â€‘7B text 82.3.
    - MMLU: 65.6 (voice) vs Qwen2â€‘7B text 69.3.
  - > Table 4: â€œQwen2.5â€‘Omniâ€‘7B â€¦ GSM8K* 85.4; MMLU* 65.6 â€¦ Qwen2â€‘7B (text) GSM8K* 82.3; MMLU* 69.3.â€

- Imageâ†’Text (Table 5) and grounding (Table 6)
  - Comparable to Qwen2.5â€‘VLâ€‘7B and often ahead of other openâ€‘source omni models:
    - MMMUval: 59.2 (vs Qwen2.5â€‘VL 58.6).
    - MMBenchâ€‘V1.1â€‘EN: 81.8 (vs 82.6).
    - TextVQA: 84.4; DocVQA: 95.2; ChartQA: 85.3.
  - Grounding:
    - RefCOCO(val): 90.5; RefCOCO+(testA): 91.0; ODinW mAP: 42.2 (Table 6).

- Videoâ†’Text (Table 7)
  - Competitive with the vision specialist:
    - Videoâ€‘MME with subtitles: 72.4 (vs Qwen2.5â€‘VL 71.6); MVBench: 70.3 (vs 69.6).

- Mixedâ€‘modality understanding (Table 8)
  - State of the art on OmniBench:
    - > Table 8: â€œQwen2.5â€‘Omniâ€‘7B: Speech 55.25%, Sound Event 60.00%, Music 52.83%, Avg 56.13%.â€

- Speech generation quality (Tables 9â€“10)
  - Zeroâ€‘shot on SEED:
    - > Table 9 (WER): â€œQwen2.5â€‘Omniâ€‘7B_RL: testâ€‘zh 1.42%, testâ€‘en 2.33%, testâ€‘hard 6.54%.â€
    - Similarity: 0.754/0.641/0.752 (zh/en/hard).
    - Competitive vs strong nonâ€‘streaming and streaming TTS (e.g., better WER than CosyVoice 2 on zh/en/hard in Table 9).
  - Singleâ€‘speaker (after speaker fineâ€‘tuning):
    - > Table 10 (NMOS): â€œSpeakers Aâ€“D â‰ˆ 4.46â€“4.62,â€ approaching human (zh 4.51).
    - WER remains low (e.g., Speaker A zh/en: 1.29/1.86).

Do the experiments support the claims?
- The breadth of evaluations (Tables 1â€“10) substantiates: (i) strong multiâ€‘modal understanding (especially audio reasoning and OmniBench), (ii) competitive text ability for a 7B model, and (iii) robust, streamingâ€‘ready speech synthesis.
- What is less quantified
  - The paper details several latencyâ€‘oriented design choices (blockâ€‘wise encoders, slidingâ€‘window vocoder; Section 2.4) but does not report endâ€‘toâ€‘end latency numbers or ablations isolating `TMRoPE` and `Thinkerâ€“Talker` contributions.

## 6. Limitations and Trade-offs
- Streaming design constraints (Sections 2.2, 2.4)
  - 2â€‘second chunking and 40 ms temporal discretization may limit ultraâ€‘fine timing resolution (e.g., subâ€‘phonetic cues) or rapid context shifts crossing chunk boundaries.
  - Slidingâ€‘window DiT restricts longâ€‘range acoustic dependencies; prosodic planning beyond the 2â€‘block lookback + 1â€‘block lookahead may be limited.
- Missing latency metrics
  - Although multiple mechanisms aim to reduce initial packet delay (Section 2.4), the paper does not report measured latency or throughput for different hardware/batch sizesâ€”a key practical metric for â€œstreamingâ€ claims.
- Data and compute intensity (Section 3)
  - Largeâ€‘scale pretraining (hundreds of billions of multimodal tokens) implies heavy compute and data requirements; reproducing the model may be costly.
- Scope of languages and domains
  - Many speech metrics emphasize zh/en; generalization to lowâ€‘resource languages, accents, or noisy environments is only indirectly addressed through aggregate benchmarks.
- Ablations and component isolation
  - No explicit ablation shows how much `TMRoPE`, interleaving, or `Thinkerâ€“Talker` individually contribute to the gains. This makes it hard to guide minimal implementations.
- Safety and reliability
  - The work mentions improving stability with DPO (Section 4.3), but broader safety, hallucination, and controllability analyses across modalities (e.g., video OCR errors highlighted in Conclusion) are not systematically quantified.

## 7. Implications and Future Directions
- How this changes the landscape
  - Demonstrates that a single 7Bâ€‘class model can unify audioâ€‘video understanding with synchronized, streaming text and speech output, moving assistants closer to natural humanâ€‘like interaction. Strong OmniBench and MMAU results suggest unified training can surpass specialist models on crossâ€‘modal reasoning (Tables 3 and 8).
- Enabled followâ€‘ups
  - Component ablations: quantify the independent effects of `TMRoPE`, interleaving, and `Thinkerâ€“Talker`.
  - Latency/efficiency studies: report endâ€‘toâ€‘end latency under varied hardware; explore adaptive chunk sizes and dynamic receptive fields.
  - Broader outputs: the paperâ€™s Conclusion points to generating images, videos, and musicâ€”natural next steps given the architectureâ€™s multiâ€‘output design.
  - Longâ€‘horizon streaming: extend sliding windows for prosody planning; hierarchical prosody tokens for utteranceâ€‘level coherence.
  - Robustness and coverage: evaluate many more languages, accents, spontaneous speech, and realâ€‘world video OCRâ€”an explicit challenge identified in Section 6 (Conclusion).
- Practical applications
  - Realâ€‘time voice and video assistants, live video tutoring/explanation with narrated responses, accessibility tools (describing scenes and conversing in speech), customer service kiosks, and onâ€‘device or edge deployments where low latency is crucial.

In short, Qwen2.5â€‘Omni combines timeâ€‘aligned multimodal perception with concurrent text/speech generation in a single model. Its architectural innovations (`TMRoPE`, `Thinkerâ€“Talker`, streaming encoders, slidingâ€‘window DiT) are the main enablers, and the empirical resultsâ€”especially on OmniBench and audio reasoningâ€”indicate clear benefits. The next steps are to quantify latency, provide ablations, and broaden outputs and language coverage.

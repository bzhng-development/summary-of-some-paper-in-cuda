# OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER

**ArXiv:** [1701.06538](https://arxiv.org/abs/1701.06538)

## ğŸ¯ Pitch

This paper introduces the Sparsely-Gated Mixture-of-Experts (MoE) layer, a novel neural network component that selectively activates a small subset of specialized 'expert' sub-networks for each input, instead of the entire model. By combining scalable algorithmic innovations with practical systems engineering, it enables models with over 1000Ã— more capacity without increasing computation proportionallyâ€”delivering state-of-the-art results on large-scale language modeling and machine translation tasks. This breakthrough makes it feasible to unlock unprecedented model expressiveness and accuracy at modest computational cost, paving the way for deep learning models to scale to new heights in data-hungry real-world applications.

---

## 1. Executive Summary
This paper introduces a scalable building block for deep networksâ€”the `Sparselyâ€‘Gated Mixtureâ€‘ofâ€‘Experts (MoE) layer`â€”that activates only a few specialized â€œexpertsâ€ per input instead of the entire model. By combining sparse, differentiable gating with practical systems techniques, it scales model capacity by 1000Ã—+ while keeping computation roughly constant, yielding stateâ€‘ofâ€‘theâ€‘art results in language modeling and machine translation at substantially lower compute (Sections 1.2â€“3; Figures 2â€“3; Tables 1â€“5).

## 2. Context and Motivation
- Problem addressed
  - Modern neural models improve with more parameters, but activating every parameter on every example makes training and inference compute scale linearly with model size; with growing datasets this creates a â€œquadraticâ€ cost pressure (Section 1.1).
  - â€œConditional computationâ€ aims to activate only parts of the network per example, but prior attempts struggled to deliver large real gains due to:
    - Hardware branching inefficiency on GPUs.
    - Shrinking effective batch size for conditionally active submodules.
    - Network bandwidth bottlenecks in distributed training.
    - Load imbalance across conditional branches (some experts get most traffic).
    - Evaluation on small datasets that do not justify very large capacity (Section 1.1, bullet list).
- Why this matters
  - Realâ€‘world language tasks have massive training corpora (â€œ100 billion wordsâ€ in Section 5.2) and benefit from large capacity to memorize and generalize patterns. Achieving far larger capacity without proportional compute unlocks better accuracy and economical scaling.
- Prior approaches and their gaps
  - Mixtureâ€‘ofâ€‘Experts (MoE) has a long history (Section 1.3), and deep MoE variants existed (e.g., two stacked dense MoEs in Eigen et al. 2013), but lacked:
    - Sparse, trainable gating that works efficiently on GPUs.
    - A concrete recipe for balanced routing at scale.
    - A systems design that preserves throughput on multiâ€‘GPU clusters.
- Positioning of this work
  - Provides a general, dropâ€‘in `MoE` layer with sparse, differentiable gating (Section 2).
  - Solves engineering bottlenecks: batch size, bandwidth, and load balancing (Sections 3â€“4).
  - Demonstrates largeâ€‘scale wins on strong benchmarks and productionâ€‘scale translation (Section 5).

## 3. Technical Approach
At a high level, the paper inserts a `MoE` layer between stacked recurrent layers (e.g., LSTMs) and ensures that, for each input token, only a few experts run. This yields huge parameter counts (experts replicate parameters) without executing all of them.

- Core `MoE` formulation (Section 2; Eq. 1)
  - A `MoE` has `n` experts `E1 â€¦ En` (each a small feedâ€‘forward network) and a gating network `G`.
  - For input `x`, compute expert outputs `Ei(x)` for a small subset and mix them with learned weights `G(x)i`:
    - `y = sum_i G(x)i * Ei(x)` (Eq. 1).
  - Only experts with nonzero `G(x)i` are executedâ€”this is the compute saving.

- Gating network design (Section 2.1)
  - Concept: produce a sparse probability distribution over experts.
  - Two gating variants:
    1) `Softmax gating` (dense; Eq. 2): `GÏƒ(x) = Softmax(x Â· Wg)`. Not sparse, thus no compute saving.
    2) `Noisy Topâ€‘K gating` (sparse; Eqs. 3â€“5): add learnable Gaussian noise per expert logit before softmax, then keep only the topâ€‘`k` logits and set others to `âˆ’âˆ`. After softmax, only `k` experts get nonzero weights.
       - Preâ€‘activation: `H(x)i = (xÂ·Wg)i + Normal(0,1) * Softplus((xÂ·Wnoise)i)` (Eq. 4).
       - Sparsification: `KeepTopK(H(x), k)` zeroes all but the top `k` elements (Eq. 5), then apply `Softmax` (Eq. 3).
       - Why noise? It promotes exploration and helps load balancing (Appendix A discussion).
  - Training: endâ€‘toâ€‘end backpropagation through the topâ€‘`k` softmax for the active experts; gradients flow to gating weights of selected experts and to the input of the gating network (Section 2.1).

- Balancing expert utilization (Section 4; Appendix A)
  - Challenge: without constraints, the gate collapses to a few experts, hurting both quality and system load balance (Section 4).
  - Two complementary regularizers:
    - `Limportance` (Eqs. 6â€“7): Define perâ€‘expert â€œimportanceâ€ as the batchwise sum of gate weights `Importance(X) = sum_{x in X} G(x)`. Penalize coefficient of variation (CV) across experts: `Limportance = wimportance * CV(Importance(X))^2`. This pushes the gate to distribute weight evenly.
    - `Lload` (Appendix A; Eqs. 8â€“11): Encourages equal counts of assigned examples (â€œloadâ€) per expert. Because the actual count is discrete, they derive a smooth estimator `Load(X)i = sum_{x in X} P(x,i)` where `P(x,i)` is the probability that expert `i` is in topâ€‘`k` for `x`, computed using the CDF `Î¦` of the normal distribution and the difference to the `k`â€‘th largest competitor (Eqs. 8â€“9). Penalize `CV(Load(X))^2` with weight `wload` (Eq. 11).
  - Initialization trick: set `Wg` and `Wnoise` to zeros to start in a noiseâ€‘only, balanced regime (Appendix A).

- Hierarchical MoE (Appendix B)
  - When `n` is huge, use two levels: a primary gate selects a few groups; each group contains a secondary MoE that then selects experts. The composite output multiplies primary and secondary gates (Eq. 12). Importance/load metrics are adapted accordingly (Eqs. 13â€“14).
  - This reduces branching factor per decision and maps nicely to device partitions.

- Systems techniques to keep throughput high (Section 3)
  - Shrinking batch problem (Section 3.1): As experts multiply, each expert sees fewer examples per batch (â‰ˆ `k*b/n`), reducing arithmetic efficiency. Solutions:
    - `Mix data and model parallelism`: run multiple dataâ€‘parallel replicas synchronously, but keep a single shared copy of each expert across devices; route microâ€‘batches from all replicas to the expertâ€™s host device, raising perâ€‘expert batch size by factor `d` (number of replicas). If each device processes batch `b`, expert batch becomes `â‰ˆ k*b*d/n` (Section 3.1).
    - `Exploit convolutionality over time`: in sequence models, call the same MoE at every time step; execute it jointly over all time steps of the unrolled window to enlarge its batch (Section 3.1).
    - Note on recurrent MoE: applying MoE recursively inside an RNN breaks the above trick; they point to recomputation strategies to reduce memory and enlarge batch (Section 3.1).
  - Network bandwidth (Section 3.2): Keep the computationâ€‘toâ€‘communication ratio high by using experts with large hidden layers. With input/output size `d` and hidden size `h`, compute scales with `~2*d*h`, while communication scales with `~2*d`. The ratio equals `h`, so using thousands of hidden units makes network transfer negligible relative to compute (Section 3.2).
  - Memory/optimizer tweaks for very large models (Appendix D):
    - Do not store expert hidden activations; recompute on the backward pass.
    - Modify Adam: set `Î²1=0` to drop first moment; use a factored secondâ€‘moment approximation (store row/column averages for matrices) to cut auxiliary memory.

- Where the `MoE` layer sits in endâ€‘toâ€‘end models (Figures 1; Sections 5.1, 5.3, 5.4; Appendices Câ€“E)
  - Language modeling: two stacked LSTMs with an MoE between them, called once per token position (Figure 1; Appendix C).
  - Machine translation (GNMTâ€‘style): reducedâ€‘depth encoder/decoder LSTMs with MoE layers inserted in both encoder and decoder; attention connects encoder and decoder (Section 5.3; Appendix E). For some MT runs they use a strictly balanced, batchwise gating variant with learned inference thresholds to ensure equal batch sizes per expert (Appendix F).

- Simple walkâ€‘through example
  - Suppose `n=256` experts, `k=4`. For token representation `x`:
    1) Compute `H(x)` via Eq. 4 (linear transform plus noise with learnable scale).
    2) Mask to the `k=4` largest entries (Eq. 5), set others to `âˆ’âˆ`, apply softmax (Eq. 3) to get `G(x)`.
    3) Dispatch `x` only to the four chosen experts; get `E_i(x)` from each.
    4) Return weighted sum `y = Î£ G(x)_i * E_i(x)` (Eq. 1).
  - Only 4/256 experts do work for this tokenâ€”a 98.4% sparsity in execution.

## 4. Key Insights and Innovations
- Sparse, differentiable gating that scales to thousands of experts
  - Distinctive features: `Noisy Topâ€‘K` gating (Eqs. 3â€“5) allows training with standard backprop through a sparse softmax, unlike boolean gates requiring REINFORCE. It keeps routing decisions differentiable for the selected experts and injects controlled noise to promote balanced exploration (Section 2.1; Appendix A).
  - Significance: Enables massive conditional computation without bespoke gradient estimators.

- Loadâ€‘balancing losses that make sparse MoE practical
  - `Limportance` and `Lload` explicitly balance both total attention weight and the number of routed examples per expert (Section 4; Appendix A). Experiments show quality collapses without them and becomes stable with them:
    > In Appendix A Table 6, removing both losses yields test perplexity 39.8 and extreme load imbalance (max load 17.8Ã— mean). Adding either loss reduces perplexity to â‰ˆ35.6â€“35.7 and keeps max load â‰ˆ1.1â€“1.5Ã— mean.
  - Significance: Prevents gate collapse, avoids outâ€‘ofâ€‘memory skew, and keeps hardware fully utilized.

- A systems recipe that keeps compute efficiency high at enormous scale
  - Combining synchronous data parallelism with modelâ€‘parallel expert placement preserves large perâ€‘expert batches (Section 3.1) and makes network transfers amortized and computeâ€‘dominated by using large expert hidden layers (Section 3.2).
  - Significance: Measured throughput remains a substantial fraction of GPU peak:
    > Table 7 reports 0.74â€“1.56 TFLOPS/GPU for MoE models on K40s; the largest highâ€‘compute model reaches 1.56 TFLOPS/GPU.

- Hierarchical MoE for extreme expert counts
  - Twoâ€‘level gating reduces branching pressure and maps well to device topology (Appendix B), enabling tens of thousands to over a hundred thousand experts (Appendix D; Figure 3).
  - Significance: Achieves up to 137B parameters in an `MoE` layer with retained efficiency:
    > Appendix D Table 8 shows `MoEâ€‘131072â€‘h` with 137.6B parameters in the MoE component.

- Evidence of emergent specialization
  - Experts learn interpretable roles (e.g., handling phrases like â€œplays a central â€¦â€ vs. â€œrapidly â€¦â€), illustrated in Appendix E Table 9 by sorting inputs with highest gate weights for each expert.
  - Significance: Supports the intended â€œdivideâ€‘andâ€‘conquerâ€ behavior.

## 5. Experimental Analysis
- Evaluation setup
  - Datasets and tasks
    - Language modeling: 1â€‘Billionâ€‘Word Benchmark (829M words; Section 5.1) and a 100â€‘Billionâ€‘Word Google News corpus (Section 5.2).
    - Machine translation: WMTâ€™14 Enâ†’Fr and Enâ†’De (36M and 5M sentence pairs) and a productionâ€‘scale Enâ†’Fr dataset (Section 5.3). Multilingual MT on 12 language pairs (Section 5.4).
  - Metrics
    - `Perplexity`: standard intrinsic LM/MT metric (lower is better).
    - `BLEU`: translation quality (higher is better).
    - `ops/timestep`: forward multiplyâ€‘adds per token/time step excluding softmax/embedding (Appendices Câ€“E).
    - `TFLOPS/GPU`: realized throughput (Sections 5.1â€“5.2; Tables 7â€“8).
  - Baselines
    - Strong LSTM language models from Jozefowicz et al. (Section 5.1; Figure 2â€‘right, top line).
    - GNMT and GNMT+RL translation systems (Tables 2â€“3).
  - Architectures
    - LM: 2 LSTMs with an MoE block between them; varied number of experts; typically `k=4` experts used per input; experts are 1â€‘hiddenâ€‘layer MLPs with thousands of hidden units (Appendix C).
    - MT: shallower GNMT with MoE in encoder and decoder (Appendix E). For multilingual, nonâ€‘hierarchical 512â€‘expert MoE with larger expert hidden size (Appendix E).

- Main quantitative findings
  - 1â€‘Billionâ€‘Word LM, fixed compute budget (~8M ops/timestep)
    - Progressively adding capacity via more experts reduces perplexity:
      > Table 7: `LSTMâ€‘2048â€‘512` (no MoE) = 44.7; `MoEâ€‘32` = 39.7; `MoEâ€‘256` = 35.7; `MoEâ€‘1024â€‘h` = 34.6; `MoEâ€‘4096â€‘h` = 34.1.
    - This is a â‰ˆ24% relative drop from 44.7 to 34.1 at roughly equal compute (Figure 2â€‘left).
  - 1â€‘Billionâ€‘Word LM, increasing compute with high capacity (all ~4B MoE params)
    - > Table 1: `Lowâ€‘Budget` MoE: 34.1; `Medium`: 31.3; `High`: 28.0 test perplexity after 10 epochs.
    - The â€œHighâ€‘Budget MoEâ€ at 142.7M ops/timestep (still modest) beats the previously best 10â€‘epoch perplexity 34.7 by a large margin and even undercuts the best published 100â€‘epoch result (30.6) when trained for only 10 epochs (Section 5.1; Table 1; Figure 2â€‘right).
  - 100â€‘Billionâ€‘Word LM (single pass)
    - With ~8â€“10M ops/timestep kept approximately constant, increasing experts continues to help up to 65,536 experts (â‰ˆ68B parameters):
      > Appendix D Table 8: `MoEâ€‘65536â€‘h` test perplexity = 28.9 vs LSTM baseline 47.0; `MoEâ€‘131072â€‘h` slightly degrades to 29.2.
    - Figure 3 shows the perplexity improvement curve after 10B and 100B words; improvements are larger with more data, supporting the â€œcapacity helps more when data is largeâ€ thesis (Section 5.2).
    - Efficiency remains reasonable even at extreme scale: > â€œ0.72 TFLOPS/GPUâ€ for 65,536 experts; efficiency drops for 131,072 experts partly because batch size wasnâ€™t scaled with GPU count (Appendix D, Table 8, and discussion).
  - Machine Translation, singleâ€‘pair
    - Enâ†’Fr (WMTâ€™14):
      > Table 2: `MoEâ€‘2048` achieves BLEU 40.35 (40.56 with longer training) vs GNMT 39.22 (GNMT+RL 39.92); perplexity 2.69 vs 2.79. Training used fewer/smaller GPUs and shorter time compared to GNMT.
    - Enâ†’De (WMTâ€™14):
      > Table 3: `MoEâ€‘2048` BLEU 26.03 vs GNMT 24.91; perplexity 4.64 vs 5.25.
    - Production Enâ†’Fr:
      > Table 4: Test BLEU 36.57 vs GNMT 35.56; perplexity 2.69 vs 2.87 after 1 day on 64 K40s (GNMT trained 6 days on 96 K80s).
  - Multilingual MT (12 directions)
    - > Table 5: The MoE model reduces dev perplexity by 19% vs the multilingual GNMT baseline (3.35 vs 4.14) and improves BLEU on 11/12 directions (e.g., Deâ†’En: 34.80 vs 31.17; Jaâ†’En: 25.91 vs 21.62). It even beats singleâ€‘pair GNMT on 8/12 directions.
    - One regression (Enâ†’Ko: âˆ’1.79 BLEU) is attributed to overtraining on rare pairs due to oversampling (Section 5.4).

- Do the experiments support the claims?
  - Yes, across multiple settings the same pattern appears: holding compute roughly fixed while increasing expert count consistently improves quality (Figures 2â€“3; Tables 7â€“8).
  - The regularizersâ€™ necessity is empirically established (Appendix A Table 6).
  - Throughput measurements indicate the systems recipe retains a substantial fraction of GPU peak (Tables 7â€“8), supporting the efficiency claim.

- Ablations/robustness
  - Explicit ablation of the two balancing losses shows their effect on perplexity and on load balance metrics (Appendix A Table 6).
  - Scaling study on data size (Figure 3) shows larger datasets unlock further gains from increased capacity.
  - Very extreme sparsity (131,072 experts) begins to hurt efficiency and slightly harms perplexity, illustrating a practical operating regime (Appendix D Table 8).

## 6. Limitations and Trade-offs
- Dependence on large data
  - The largest gains emerge on very large corpora (Figure 3). For small datasets, capacity may not be exploited and regularization/overfitting risks grow.
- Systems complexity and hardware assumptions
  - Requires multiâ€‘GPU clusters with highâ€‘bandwidth interconnects and careful synchronization to mix data/model parallelism (Section 3.1). Engineering the routing/allâ€‘toâ€‘all exchanges is nonâ€‘trivial.
- Communicationâ€‘compute balance
  - Efficiency assumes experts have large hidden layers so that compute dominates network transfer (Section 3.2). With small experts or slow networks, bandwidth can bottleneck.
- Gating discontinuities and gradient locality
  - Topâ€‘K gating creates discontinuities (Section 2.1 acknowledges â€œtheoretically scary discontinuitiesâ€), though no issues were observed. Gradients flow only to selected experts per example; training stability relies on noise and balancing losses.
- Potential underâ€‘utilization at extreme sparsity
  - Very large expert counts (e.g., 131k) reduced efficiency and slightly worsened perplexity compared to 65k experts (Appendix D Table 8), indicating diminishing returns and practical ceilings.
- Scope of experts
  - Experts are simple feedâ€‘forward MLPs; the work does not explore more expressive expert types or MoE inside recurrent weights (Section 3.1 notes this as future work).
- Inference considerations
  - A strictly balanced batchwise gate used in some MT experiments necessitated learning perâ€‘expert thresholds for inference (Appendix F), which adds complexity if batch sizes differ between train/test.

## 7. Implications and Future Directions
- How this changes the field
  - Establishes a practical path to dramatic capacity increases with modest compute increases via `conditional computation`. The MoE layer becomes a reusable component that can be placed between standard layers (Figure 1), allowing researchers to trade parameters for quality without linear compute growth.
- Followâ€‘up research enabled or suggested
  - Recurrent and attentionâ€‘internal MoEs: replacing RNN or attention subâ€‘matrices with MoE blocks (Section 3.1).
  - Improved routing: exploring alternative sparse gates (e.g., structured sparsity, learned temperature, or entropy regularization) and better loadâ€‘balancing objectives beyond CV penalties (Sections 2.1, 4, Appendix A).
  - Expert architectures: using deeper or specialized experts (convolutional, recurrent, or taskâ€‘specific modules), or heterogeneous experts per layer.
  - Distributed training advances: optimizing the routing/allâ€‘toâ€‘all communication pattern; adaptive device placement for experts; integrating with pipeline and tensor parallelism.
  - Data curriculum and multilingual sharing: leveraging expert specialization for lowâ€‘resource transfer, domain adaptation, and continual learning (Section 5.4 suggests experts naturally specialize).
- Practical applications
  - Largeâ€‘scale language modeling for search, dictation, and text generation (Sections 5.1â€“5.2).
  - Highâ€‘quality, costâ€‘effective machine translationâ€”including multilingual systems that cover many directions with one model (Sections 5.3â€“5.4).
  - Any setting where model capacity is the bottleneck but compute is constrainedâ€”e.g., serving large models with latency budgets by activating only a fraction of the network per request.

Overall, the paper delivers an endâ€‘toâ€‘end recipeâ€”sparse differentiable gating, balancing losses, hierarchical structuring, and distributed systems tricksâ€”that turns the longâ€‘standing idea of conditional computation into measurable gains on important NLP tasks, at scales up to 137 billion parameters in a single layer (Appendix D Table 8).

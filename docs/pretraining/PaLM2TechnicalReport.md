# PaLM 2 Technical Report

**ArXiv:** [2305.10403](https://arxiv.org/abs/2305.10403)

## ðŸŽ¯ Pitch

PaLM 2 introduces a new state-of-the-art family of Transformer-based language models that achieve dramatically better performance in multilingual tasks, reasoning, coding, and translationâ€”while being far more compute-efficient than its predecessor, PaLM. By validating and applying compute-optimal scaling laws at unprecedented scales, employing a diverse data mixture and multiple training objectives, and integrating practical safety and memorization controls, PaLM 2 delivers superior quality and safer, faster deployment in real-world applications. This leap not only cuts inference costs and broadens accessibility but also sets a new benchmark for scientifically grounded, responsible, and inclusive generative AI.

---

## 1. Executive Summary (2â€“3 sentences)
PaLM 2 is a family of Transformer language models trained with a carefully engineered data mixture and a mixture of training objectives, designed to be computeâ€‘efficient while markedly improving multilingual, reasoning, coding, and translation performance. By validating computeâ€‘optimal scaling at large scales and adding practical safety tooling (toxicity control tokens) and memorization measurement (multilingual canaries), PaLM 2 delivers higher quality than its predecessor PaLM despite being smaller at inference time, enabling faster and broader deployment (Abstract; Sections 1â€“3, 5).

## 2. Context and Motivation
- Problem addressed:
  - Recent large language models (LLMs) reached strong performance by scaling parameters, but were often computeâ€‘suboptimal, largely monolingual, trained with a single objective, and lacked robust safety/controllability and memorization analysis. This limited multilingual capabilities, reasoning strength, and safe deployment (Introduction; Section 3; Section 5).
- Why it matters:
  - Practical significance: smaller models with better quality and faster inference reduce serving cost and enable new products (Introduction).
  - Scientific significance: validating scaling laws at new regimes clarifies how to allocate compute between data and parameters (Section 2).
- Prior approaches and gaps:
  - Earlier scaling laws (Kaplan et al., 2020) favored growing parameters faster than data; later â€œChinchillaâ€ (Hoffmann et al., 2022) suggested data and parameters should grow roughly 1:1. Many LLMs still trained computeâ€‘suboptimally.
  - Most training mixtures were Englishâ€‘heavy and used a single causal LM objective; safety controls were added postâ€‘hoc rather than built into preâ€‘training (Sections 1, 3, 5).
- How this work positions itself:
  - Independently verifies computeâ€‘optimal scaling at much larger compute budgets (10^19â€“10^22 FLOPs) and derives optimal model/data ratios (Section 2, Figures 4â€“5, Table 1).
  - Introduces a more multilingual, deduplicated dataset with parallel data, code, math, and conversation; and a UL2â€‘style mixture of objectives for better generalization (Section 3).
  - Adds inferenceâ€‘time steerability through toxicity control tokens and evaluates memorization with multilingual â€œcanariesâ€ (Sections 4.7, 5.1).

## 3. Technical Approach
Stepâ€‘byâ€‘step view of what PaLM 2 is and how it was built and evaluated.

- Model family and training paradigm
  - Architecture: Transformer; significantly longer context window than PaLM to support long dialogue, longâ€‘range reading, and summarization (Section 3).
  - Objective: UL2â€‘style â€œmixture of objectivesâ€â€”instead of only predicting the next token (causal LM), training alternates among different denoising/LM subâ€‘tasks. This teaches the model complementary skills such as filling in missing text and reading with bidirectional context (Section 1).
  - Sizes and efficiency: Largest PaLM 2 (`PaLM 2â€‘L`) is significantly smaller than PaLM 540B yet trained with more compute and a larger, higherâ€‘quality dataset, emphasizing that better data/objectives can beat bruteâ€‘force parameter scaling for overall quality and inference speed (Introduction).

- Training data mixture and processing (Section 3; Appendix D.1)
  - Sources: Web documents, books, code, mathematics, conversational data.
  - Multilingual emphasis: Substantially higher share of nonâ€‘English data; includes parallel (bitext) pairs covering â€œhundreds of languages,â€ improving translation and crossâ€‘lingual reasoning.
  - Quality controls: Deduplication (to reduce memorization), PII removal, and filtering for quality. Table 21 lists the top 50 languages in the â€œmultilingual web documentsâ€ subâ€‘corpus (e.g., Spanish 11.5%, Chinese 10.2%, Russian 8.7%, Japanese 7.6%).
  - Special tokens: A small fraction of preâ€‘training data is tagged for toxicity levels using a fixed Perspective API signal, enabling inferenceâ€‘time control; multilingual â€œcanaryâ€ sequences injected to quantify memorization (Sections 3, 4.7, 5.1).

- Computeâ€‘optimal scaling experiments (Section 2)
  - Key terms:
    - `FLOPs` = floatingâ€‘point operations; a proxy for training compute.
    - `IsoFLOP curves` = for a fixed compute budget, train various (parameters, tokens) pairs and fit a curve of validation loss across model sizes at that fixed budget; the minimum identifies the best parameter count for that compute.
  - Procedure: Train many models at four compute scales (10^19, 10^20, 10^21, 10^22 FLOPs). Use the heuristic FLOPs â‰ˆ 6ND (N = parameters, D = training tokens) to allocate training tokens per model. Fit quadratic curves per compute band (Figure 4); extract optimal N and D (Figure 5; Table 1).
  - Finding: Optimal scaling follows ~1:1 growth in tokens and parameters, corroborating â€œChinchillaâ€ at larger scales (Figure 5; Table 1).

- Instruction tuning for some evaluations (Appendix A.2)
  - After preâ€‘training, some evaluations use `Flan` instruction tuning, a large mixture of 1,800+ tasks and prompts. This improves instruction following and reasoning (Table 16). When used, the paper marks it (e.g., Table 5).

- Safety and memorization mechanisms integrated into training and eval
  - `Control tokens` for toxicity: Preâ€‘training exposes the model to text tagged with low/med/high toxicity. At inference, prompting with the control token steers generation (Section 5.1; Table 14; Figure 10â€“11).
  - `Canaries` for memorization: Synthetic sequences created by shuffling/interleaving real documents in multiple languages and injecting them during training at controlled repetition counts. During eval, feed the prefix and check if the model reproduces the suffix verbatim to estimate memorization risk (Section 4.7; Table 13; Figure 9). Verbatim extraction is also measured on real training snippets (Figure 8).

- Taskâ€‘specific variants
  - `PaLM 2â€‘S*` for code: Continue training `PaLM 2â€‘S` on a codeâ€‘heavy, multilingual mix to boost coding tasks while keeping naturalâ€‘language quality (Section 4.4; Table 8; Figure 6).

## 4. Key Insights and Innovations
1) Validating computeâ€‘optimal scaling at unprecedented scale (Section 2)
- Whatâ€™s new: Independent derivation of scaling laws across 10^19â€“10^22 FLOPs shows that training tokens should grow roughly in proportion to model size (Figures 4â€“5; Table 1).
- Why it matters: Confirms that quality and efficiency come from allocating compute to both parameters and data, not just bigger models. This underwrites PaLM 2â€™s â€œsmaller but betterâ€ strategy.

2) Data and objective curation beats naive parameter scaling (Sections 1, 3; multiple eval sections)
- Whatâ€™s new: A more multilingual, deduplicated dataset plus a UL2â€‘style objective mix avoids English regression while boosting multilingual tasks; long context length enables longâ€‘form tasks. PaLM 2â€‘L, though smaller than PaLM 540B, outperforms it across many benchmarks (e.g., Table 2, Table 3, Table 5, Table 9, Table 11).
- Why it matters: Shifts emphasis from sheer size to â€œdata and objective design,â€ improving quality and inference latency together.

3) Builtâ€‘in, lowâ€‘overhead safety control via control tokens (Section 5.1)
- Whatâ€™s new: Tag a small fraction of training data with toxicity levels; expose `control tokens` so users can steer output toxicity at inference time without extra fineâ€‘tuning or classifiers in the loop (Table 14; Figure 10â€“11).
- Why it matters: Practical, generalâ€‘purpose steerability mechanism with minimal training overhead and no degradation on unrelated tasks (Section 5.1).

4) Multilingual memorization analysis with canaries (Section 4.7)
- Whatâ€™s new: Inject languageâ€‘specific outlier sequences (â€œinterleaveâ€ and â€œshuffleâ€ canaries) with controlled repetitions to quantify how often the model memorizes them; also compare to realâ€‘data extraction (Table 13; Figures 8â€“9).
- Why it matters: Nuanced understanding of privacy risk across languages: PaLM 2 memorizes less on average than PaLM, but repeated nâ€‘grams and tailâ€‘language repetitions increase risk (Figures 8â€“9).

5) A small, codeâ€‘specialized model that surpasses a much larger coder LM on several tasks (Section 4.4)
- Whatâ€™s new: `PaLM 2â€‘S*` (small) beats PaLMâ€‘Coderâ€‘540B on HumanEval@1, MBPP@1, and ARCADE@1 by large margins (Table 8) and across 12 programming languages (Figure 6; Table 18).
- Why it matters: Demonstrates that domainâ€‘focused continued preâ€‘training on a strong base can replace extreme parameter counts for developerâ€‘facing code assistants.

## 5. Experimental Analysis
How PaLM 2 is evaluated and what the numbers show.

- General English QA and classification (Section 4.2; Table 2)
  - Setup: 1â€‘shot inâ€‘context across 24+ datasets (openâ€‘domain QA, cloze, Winograd, reading comprehension, commonsense, SuperGLUE, ANLI).
  - Headline: Average score improves from 70.4 (PaLM 540B) to 76.9 (PaLM 2â€‘L).
  - Notable gains:
    - ANLI: R1 52.6â†’73.1, R2 48.7â†’63.4, R3 52.3â†’67.1.
    - RACEâ€‘H 52.1â†’62.3, RACEâ€‘M 69.3â†’77.0.
    - ReCoRD 92.8â†’93.8; SQuAD v2 EM 78.7â†’80.5.
  - Interpretation: Broad gains in robust reasoning and reading comprehension.

- Multilingual QA (TyDi QA; Section 4.2; Table 3)
  - `Gold Passage` F1 average: 69.8 (PaLM) â†’ 73.6 (PaLM 2â€‘L).
  - `Noâ€‘context` (closedâ€‘book) F1 average: 31.5 â†’ 40.3; largest gains in lowâ€‘resource/Nonâ€‘Latin languages (e.g., Swahili 39.7â†’50.3; Indonesian 35.5â†’46.4; Korean 35.0â†’46.9).

- Reasoning and BIGâ€‘Bench Hard (Section 4.3; Tables 5â€“6)
  - Fewâ€‘shot with CoT/SC where marked; some results use the instructionâ€‘tuned variant (Appendix A.2).
  - Selected results (Table 5):
    - `StrategyQA`: 81.6 (SOTA reported) vs 90.4 (`PaLM 2`).
    - `CSQA`: 91.2 SOTA vs 90.4 PaLM 2 (nearâ€‘SOTA).
    - `BB Hard` (23 tasks): PaLM 65.2 (CoT) â†’ PaLM 2 78.1 (CoT).
  - Taskâ€‘level jumps in BB Hard (Table 6):
    > `temporal_sequences`: 39.6/78.8 (PaLM direct/CoT) â†’ 96.4/100.0 (PaLM 2).  
    > `multistep_arithmetic_two`: 1.6/19.6 â†’ 0.8/75.6.  
    > `dyck_languages`: 28.4/28.0 â†’ 35.2/63.6.  
    > `logical_deduction`: 42.7/56.9 â†’ 64.5/69.1.
  - Takeaway: Chainâ€‘ofâ€‘thought (CoT) amplifies PaLM 2â€™s gains on multiâ€‘step reasoning.

- Mathematical reasoning (Section 4.3; Table 7)
  - `MATH`: 48.8 (PaLM 2 with SC) vs 50.3 (Minerva SOTA); far above PaLM 8.8.
  - `GSM8K`: 91.0 (PaLM 2 with SC) vs 92.0 (GPTâ€‘4 reported) and PaLM 74.4.
  - `MGSM` (multilingual GSM8K): 87.0 (PaLM 2 with SC), exceeding prior SOTA 72.0.
  - Implication: Strong quantitative reasoning in both English and multiple languages.

- Coding (Section 4.4; Table 8; Figure 6; Table 18)
  - `PaLM 2â€‘S*` vs PaLMâ€‘Coderâ€‘540B:  
    > HumanEval pass@1: 37.6 vs 35.9; pass@100: both 88.4.  
    > MBPP pass@1: 50.0 vs 47.0; pass@80: 86.6 vs 80.8.  
    > ARCADE pass@1: 16.2 vs 7.9; pass@30: 43.6 vs 33.6.
  - Multilingual HumanEval (BabelCode): higher pass@1 than PaLM(-Coder) on 10/12 languages; extreme gains on lowâ€‘resource languages (e.g., Haskell 8.7% vs 1.86%; Julia 16.8% vs 4.35%; Figure 6; Table 18).
  - Message: A small, codeâ€‘tuned PaLM 2 achieves or beats the much larger coder baseline, especially on notebook completion (ARCADE).

- Translation (Section 4.5; Tables 9â€“10)
  - WMT21 with human `MQM` (lower better):  
    > Chineseâ†’English: PaLM 3.7, Google Translate 3.1, PaLM 2 3.0.  
    > Englishâ†’German: 1.2, 1.0, 0.9.  
    > BLEURT also improves (Table 9).
  - Dialectâ€‘aware FRMT (Fewâ€‘shot): PaLM 2 beats both PaLM and Google Translate across Brazilian/European Portuguese and Mainland/Taiwanese Chinese (Table 10).

- Natural Language Generation (NLG) (Section 4.6; Tables 11â€“12; Appendix A.5)
  - Oneâ€‘shot summarization/headline generation across English and 10+ nonâ€‘English languages:
    > `XSum` (en ROUGEâ€‘2): 14.5 â†’ 23.2.  
    > `WikiLingua` (ar/ja/ko/ru/th/tr): 11.7 â†’ 23.5.  
    > `XLSum` (11 languages): 12.7 â†’ 21.3 average with PaLM 2â€‘L (Table 11).
  - Data contamination check: Filtering based on 15â€‘gram overlap changes scores minimally and positively (+0.3 to +0.6), arguing against memorizationâ€‘inflated metrics (Table 12).

- Responsibleâ€‘AI measurements (Sections 4.2, 4.6, 5; Appendix D)
  - Toxicity classification AUCâ€‘ROC improves in English and multilingual (Jigsaw/Civil Comments; Table 4).
  - Openâ€‘ended toxic degeneration: small improvement vs PaLM on RealToxicityPrompts; conversational LM shows slight regression vs PaLM (Appendix D.7, Table 30).
  - Dialog prompting dramatically lowers toxic responses relative to languageâ€‘modeling alone (Appendix D.3, Figure 30), but bias varies by language/identity terms (Figures 31â€“32).
  - Misgendering in translation: Into English is stable or slightly better worstâ€‘case vs PaLM (Table 24). Out of English (humanâ€‘rated), mixed: improvements in Spanish/Polish/Portuguese but regressions in Telugu, Hindi, Arabic (Table 26; Figure 33).

- Inferenceâ€‘time toxicity control (Section 5.1)
  - With control tokens on nonâ€‘toxic prompts, probability of toxic continuation drops from 0.075 to 0.033 (lowâ€‘toxicity setting), and can be increased when desired (0.203 high setting; Table 14; Figure 10).
  - In conversational LM (single sample), control tokens reduce toxic responses on standard dataset 30%â†’12% and adversarial 18%â†’7% (Section 5.1).
  - In dialog uses, dialogâ€‘prompting itself was even more effective than control tokens; specialized systems (LaMDA) remain best (Figure 11; Section 5.1).

- Memorization (Section 4.7; Figures 8â€“9; Table 13)
  - Average verbatim extraction on English shared data is lower for PaLM 2 than PaLM across model sizes (Figure 8a).
  - But as nâ€‘grams repeat more, PaLM 2 memorizes repeated sequences more readily (Figure 8b).
  - Tail languages: canaries need fewer repetitions to be memorized; real data extraction shows no strong correlation with language size, except higher risk when sequences are highly repeated (Figure 9).

- Scaling law downstream check (Appendix A.1; Table 15)
  - At fixed compute (10^22 FLOPs), 9.5B and 16.1B models perform similarly across 26 downstream tasks (avg 57.7 vs 58.3), while 9.5B has the lowest training loss. Training loss is not a perfect surrogate for downstream task quality.

Overall assessment: The breadth of benchmarks, the human evaluation for translation, contamination checks for generation, and the safety analyses together support the core claimsâ€”PaLM 2 is markedly more capable and efficient, with new safety/measurement toolingâ€”while also surfacing nuanced mixed results in certain safety and multilingual settings.

## 6. Limitations and Trade-offs
- Assumptions and constraints
  - Compute and infrastructure: Training at up to 10^22 FLOPs with TPUv4 and the Pathways stack is resourceâ€‘intensive (Model Card; Sections 2â€“3), out of reach for most labs.
  - Some architectural specifics and exact parameter counts are not publicly disclosed (Model Card), limiting reproducibility.
- Safety metrics and tools
  - Toxicity labeling relies on a fixed Perspective API signal during both preâ€‘training tagging and evaluation; this proxy has known limitations across languages and sociolects (Section 5.1; Appendix D.3â€“D.7).
  - Control tokens help in language modeling and conversational LM, but dialogâ€‘prompting and specialized safety systems still outperform them in dialog uses (Section 5.1; Figure 11).
- Mixed or conditional results
  - Conversational LM shows a slight toxicity regression vs PaLM (Appendix D.7, Table 30).
  - Misgendering when translating out of English regresses in some languages (e.g., Telugu, Hindi, Arabic; Table 26).
  - Some reasoning results depend on chainâ€‘ofâ€‘thought and selfâ€‘consistency sampling; direct prompting is weaker on certain tasks (Table 6).
- Data and evaluation caveats
  - Some instructionâ€‘tuning tasks overlap with training in the Flan mixture (Appendix A.2 and notes in Section 4.3), though test/dev splits are held out; realâ€‘world generalization is still the key question.
  - While memorization is lower on average, highly repeated sequencesâ€”especially in tail languagesâ€”remain a risk (Figures 8â€“9).

## 7. Implications and Future Directions
- How this work shifts the field
  - Confirms computeâ€‘optimal scaling at large regimes and demonstrates that data/objective design can deliver stepâ€‘change performance without maximal parameter counts (Sections 2â€“3). This encourages focusing on data quality, multilingual breadth, and objective mixturesâ€”not just bigger models.
  - Establishes builtâ€‘in steerability (control tokens) and multilingual memorization audits (canaries) as practical components of preâ€‘training pipelines (Sections 4.7, 5.1).
- Followâ€‘up research enabled/suggested
  - Steerability beyond toxicity: extend control tokens to attributes like helpfulness, formality, or disclosure of uncertainty; study how such conditional training interacts with instruction tuning (Section 5.1, final paragraphs).
  - Safer multilingual systems: develop evaluation sets and mitigation methods that go beyond English and better capture diverse sociocultural contexts and harms (Appendix D.2â€“D.6).
  - Memorization under distribution shift: explore how data repetition, dedup granularity, and objective mixtures influence memorization across languages and domains (Section 4.7).
  - Bridging training loss and task metrics: more systematic ablations to understand when computeâ€‘optimal training loss transfers to downstream optimality (Appendix A.1, Table 15).
- Practical applications
  - Multilingual assistants and tools: stronger translation (MQMâ€‘verified; Table 9), multilingual QA (Table 3), and MGSM math reasoning (Table 7) enable wider global deployment.
  - Coding copilots: `PaLM 2â€‘S*` shows that small, specialized models can surpass much larger coder LMs in practical settings such as notebooks (Table 8; Figure 6).
  - Summarization and content generation: large improvements across many languages (Table 11), with low contamination sensitivity (Table 12).
  - Safer deployments: integrated toxicity control and robust safety evaluations give developers more levers and guidance (Section 5; Appendix D).

> Bottom line: PaLM 2 demonstrates that smarter scaling (1:1 dataâ€“parameter growth), better data (multilingual, deduplicated, parallel), and richer objectives can yield a smaller, faster model that substantially advances multilingual reasoning, coding, and translationâ€”while beginning to address safety and memorization in ways that are measurable and controllable.

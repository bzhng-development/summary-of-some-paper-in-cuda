# Qwen2.5 Technical Report

**ArXiv:** [2412.15115](https://arxiv.org/abs/2412.15115)
**Authors:** Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu
**Institutions:** 

## ðŸŽ¯ Pitch

Qwen2.5 introduces groundbreaking large language models, scaling up to 18 trillion tokens and integrating a unique two-stage reinforcement learning pipeline to deliver exceptional performance in math, coding, and long-context processing. By bridging the gap between model size, cost, and capability, Qwen2.5 addresses real-world demands for diverse, high-quality LLM applications, setting new benchmarks for both small edge devices and large-scale deployments.

---

## 1. Executive Summary (2-3 sentences)
Qwen2.5 is a family of large language models (LLMs) that scales highâ€‘quality preâ€‘training to 18 trillion tokens and introduces a twoâ€‘stage reinforcement learning pipeline (offline DPO, online GRPO) on top of a millionâ€‘example supervised fineâ€‘tuning set. It delivers strong general, math, coding, and longâ€‘context performance across sizes from edge (0.5B) to large (72B) and API MoE variants, with `Qwen2.5-72B-Instruct` competing with much larger models and `Qwen2.5-Turbo` handling up to 1Mâ€‘token contexts (Sections 1â€“5; Tables 2â€“17; Figure 2).

## 2. Context and Motivation
- Problem gap:
  - Openâ€‘weight LLMs have improved rapidly, but users face tradeâ€‘offs among size, cost, longâ€‘context handling, and alignment (Section 1).
  - Prior Qwen2 used 7T tokens and had limited postâ€‘training breadth; generation length was short and structured I/O support was weaker (Abstract; Section â€œBetter in Useâ€).
- Importance:
  - Realâ€‘world applications need models that are costâ€‘effective at multiple sizes, follow instructions reliably, reason about math/code, and process very long inputs (Sections 1, 4.1, 4.4).
- Prior approaches and limitations:
  - Existing open series (Llama, Mistral, Gemma, etc.) offer strong baselines but often have narrower context windows, less emphasis on structured data and long responses, or weaker smallâ€‘model performance (Section 1; evaluation in Tables 3â€“5, 7â€“10).
  - Longâ€‘context handling typically relies on postâ€‘hoc extrapolation with quality drop on short tasks; reward model (RM) evaluation is often Goodhartâ€‘prone (Sections 3.3, 5.2.3; Tables 16â€“17).
- Positioning:
  - Qwen2.5 scales data and postâ€‘training breadth, introduces staged longâ€‘context training plus inferenceâ€‘time attention improvements, and provides a wide size range with open weights and API MoE options (Sections 2â€“4; Table 1).

## 3. Technical Approach
This section explains how Qwen2.5 is built and aligned.

- Model family and architecture (Section 2; Table 1):
  - Dense decoderâ€‘only Transformers at 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B parameters; APIâ€‘served MoE variants `Qwen2.5-Turbo` and `Qwen2.5-Plus`.
  - Key components:
    - `GQA` (Grouped Query Attention): reduces keyâ€‘value cache cost by sharing keys/values across groups of attention headsâ€”keeps attention efficient at long contexts.
    - `SwiGLU` activation and `RMSNorm` with preâ€‘norm: stable and efficient training.
    - `RoPE` (rotary positional embeddings) with a learned bias term in attention (`QKV bias`), which improves length extrapolation.
  - Tokenizer: byteâ€‘level BPE with 151,643 tokens and 22 control tokens, including new ones for tool use; a unified vocabulary across all models (Section 2).

- Preâ€‘training data and schedule (Section 3):
  - Scale and curation:
    - Expanded from 7T to 18T tokens, with better filtering and mixture balancing using Qwen2â€‘Instruct as a data quality filter; domains like science/technology are upâ€‘sampled (Section 3.1).
    - Stronger math/code via integrating datasets from `Qwen2.5-Math` and `Qwen2.5-Coder`; additional highâ€‘quality synthetic data filtered by general and math RMs (Section 3.1).
  - Hyperparameter scaling laws (Section 3.2):
    - Empirical laws relate optimal learning rate and batch size to model size (`N`) and data scale (`D`) for both dense and MoE models, targeting loss minimization across a grid of sizes (dense: 44Mâ€“14B; MoE: 44Mâ€“1B activated params).
    - Used to configure MoE to reach parity with selected dense models by tuning activated/total parameters.
  - Longâ€‘context preâ€‘training (Section 3.3):
    - Twoâ€‘phase for dense models: start at 4,096 tokens then extend to 32,768; `RoPE` base raised from 10,000 to 1,000,000 via ABF (Attention Base Frequency) to preserve position encoding quality at long lengths.
    - `Qwen2.5-Turbo` uses progressive stages up to 262,144 tokens with RoPE base 10,000,000; each stage mixes 40% maxâ€‘length and 60% shorter sequences for smooth adaptation.

- Inferenceâ€‘time longâ€‘context upgrades (Section 3.3):
  - `YARN` and `Dual Chunk Attention (DCA)`:
    - YARN: a trainingâ€‘free method to extend usable context windows of RoPEâ€‘based models by rescaling positional encodings.
    - DCA: splits sequences into chunks and structures attention to preserve longâ€‘range information with manageable compute.
    - Outcome: up to 4Ã— sequence length capacityâ€”Turbo handles up to 1M tokens; others up to 131,072 tokensâ€”while keeping shortâ€‘sequence quality (Tables 16â€“17).

- Postâ€‘training pipeline (Section 4):
  - Supervised Fineâ€‘Tuning (`SFT`, Section 4.1):
    - >1M examples, emphasizing long responses (up to 8,192 generated tokens), math chainâ€‘ofâ€‘thought, code with execution checks and unit tests, structured data reasoning (tables/JSON), logical reasoning (70k new queries), multilingual transfer, robust system prompts, and response filtering with critic and multiâ€‘agent scoring.
    - Training details: 2 epochs, sequence length 32,768; LR decays 7eâ€‘6 â†’ 7eâ€‘7; weight decay 0.1; gradient clipping 1.0.
  - Offline RL (`DPO`, Section 4.2):
    - DPO reframes preference learning as direct optimization over pairs (a â€œchosenâ€ vs â€œrejectedâ€ answer). Qwen2.5 builds ~150k pairs by resampling with the SFT model and applying executionâ€‘based checks or matching when objective scoring exists (math/coding/instruction logic).
    - Includes human + automated review; trained 1 epoch with Online Merging Optimizer at LR 7eâ€‘7.
  - Online RL (`GRPO`, Section 4.3):
    - `GRPO` (Group Relative Policy Optimization): samples 8 responses per query and updates the policy relative to groupâ€‘wise baselines, improving signals like truthfulness/helpfulness/conciseness/relevance/harmlessness/debiasing defined by the RMâ€™s labeling criteria.
    - Curriculum: prioritize queries with higher score variance under the RM to learn where quality differs most; global batch size 2048; each episode uses pairs of (query, response).
  - Longâ€‘context postâ€‘training (Section 4.4):
    - For `Qwen2.5-Turbo`, SFT is twoâ€‘stage: only short instructions (â‰¤32k) first, then a mix of short and long (â‰¤262k). RL is done on short instructions only, due to cost and lack of reliable longâ€‘context RMs, yet still improves longâ€‘context alignment.

- Implementation choices and why they matter:
  - Using verifiable domains (math, code) for offline RL ensures â€œlearnable and reliableâ€ signals before moving to harderâ€‘toâ€‘score human preference dimensions in online RL (Sections 4.2â€“4.3).
  - YARN + DCA avoids longâ€‘context training from scratch for all models while preserving shortâ€‘length behavior (Tables 16â€“17 show â€œw/o DCA+YARNâ€ ablation).
  - Size coverage (0.5Bâ†’72B + MoE) addresses practical deployment ranges and costâ€‘latency tradeâ€‘offs (Section 1; Table 1).

## 4. Key Insights and Innovations
- Scaling highâ€‘quality data with smarter mixture control (Section 3.1):
  - Novelty: combines LLMâ€‘assisted multiâ€‘dimensional data filtering, domain balancing (downâ€‘sample social/entertainment templates; upâ€‘sample science/tech), and expert synthetic data with RM filtering.
  - Significance: directly ties to large gains in math/coding and general benchmarks at fixed model sizes (Tables 2â€“5, 7â€“10). This is more than just â€œmore tokensâ€â€”it is â€œmore useful tokens.â€

- Twoâ€‘stage RL that separates verifiable skills from subjective alignment (Sections 4.2â€“4.3):
  - Novelty: a structured pipelineâ€”Offline DPO on objectively checkable tasks, then Online GRPO on humanâ€‘preference dimensions with a strong RM.
  - Significance: improves instruction following and preference alignment without sacrificing reasoning quality; evidenced by large jumps on IFEval, Arenaâ€‘Hard, and MTâ€‘Bench (Tables 6â€“8).

- Longâ€‘context capability without sacrificing shortâ€‘context quality (Sections 3.3, 4.4; Tables 16â€“17):
  - Novelty: progressive longâ€‘context training for Turbo plus YARN+DCA upgrades for all models; ablations show YARN+DCA preserves â‰¤32k behavior and substantially boosts â‰¥64k performance.
  - Significance: `Qwen2.5-Turbo` reaches 1M tokens and achieves 100% in a 1Mâ€‘token passkey retrieval test (Figure 2); `Qwen2.5-72B-Instruct` leads openâ€‘weight longâ€‘context scores (Tables 16â€“17).

- Rewardâ€‘model evaluation skepticism substantiated with multiâ€‘benchmark evidence (Section 5.2.3; Table 15):
  - Insight: optimizing an RM for one benchmark (e.g., RewardBench) risks Goodhartâ€™s lawâ€”improvements there may degrade on others and not predict downstream RL model quality.
  - Significance: pushes the community to adopt broader RM evaluation and to seek RM metrics that better predict RL outcomes.

## 5. Experimental Analysis
- Evaluation methodology (Section 5):
  - Contamination control: nâ€‘gram deâ€‘duplication with LCS thresholds â‰¥13 and â‰¥60% of the shorter sequence length to remove training samples overlapping with test items (Section 5).
  - Base model evaluation (Section 5.1; Tables 2â€“5): general (MMLU, MMLUâ€‘Pro, MMLUâ€‘redux, BBH, ARCâ€‘C, TruthfulQA, Winogrande, HellaSwag), math/science (GPQA, TheoremQA, GSM8K, MATH), coding (HumanEval, HumanEval+, MBPP, MBPP+, MultiPLâ€‘E), multilingual (exam, understanding, math, translation).
  - Instructionâ€‘tuned evaluation (Section 5.2; Tables 6â€“10): general (MMLUâ€‘Pro, MMLUâ€‘redux, LiveBench 0831), math/science (GPQA, GSM8K, MATH), coding (HumanEval, MBPP, MultiPLâ€‘E, LiveCodeBench 2305â€“2409), alignment (IFEval, MTâ€‘Bench, Arenaâ€‘Hard).
  - Inâ€‘house automatic evaluations in English/Chinese and multilingual extensions (Tables 11â€“14).
  - Longâ€‘context tests: RULER, LVâ€‘Eval with keyword recall, LongBenchâ€‘Chat; passkey retrieval to 1M tokens for Turbo (Tables 16â€“17; Figure 2).
  - Longâ€‘context speed: sparse attention based on Minference to accelerate prefill; TTFT speedups 3.2â€“4.3Ã— at 1M tokens (Figure 3).

- Representative quantitative highlights (instructionâ€‘tuned):
  - Large scale (Table 6):
    - â€œ`Qwen2.5-72B-Instruct` achieves MATH 83.1, GSM8K 95.8, LiveCodeBench 55.5, Arenaâ€‘Hard 81.2, MTâ€‘Bench 9.35.â€
    - â€œ`Qwen2.5-Plus` further improves MATH to 84.7, MultiPLâ€‘E to 77.0, and Arenaâ€‘Hard to 81.4.â€
    - On MMLUâ€‘redux: 86.8 (`Qwen2.5-72B-Instruct`) vs 86.2 (Llamaâ€‘3.1â€‘405Bâ€‘Instruct).
  - Mid scale (Table 7):
    - â€œ`Qwen2.5-32B-Instruct`: MMLUâ€‘Pro 69.0, MATH 83.1, LiveCodeBench 51.2, Arenaâ€‘Hard 74.5.â€
    - â€œ`Qwen2.5-Turbo` (MoE): MATH 81.1, GSM8K 93.8, MultiPLâ€‘E 73.7, IFEval 76.3â€”often matching or beating `Qwen2.5-14B-Instruct` despite lower cost.â€
  - 7B scale (Table 8):
    - â€œ`Qwen2.5-7B-Instruct` improves MATH to 75.5 and HumanEval to 84.8, beating Gemma2â€‘9Bâ€‘IT and Llama3.1â€‘8Bâ€‘Instruct on most metrics.â€
  - Edge models (Table 9â€“10):
    - â€œ`Qwen2.5-3B-Instruct` reaches MATH 65.9 and MultiPLâ€‘E 60.2, competitive with larger 3.5â€“4B models.â€
    - â€œ`Qwen2.5-1.5B-Instruct` jumps to MATH 55.2 and HumanEval 61.6; `Qwen2.5-0.5B-Instruct` reaches MATH 34.4.â€

- Longâ€‘context results and ablations:
  - RULER (Table 16): at 128k tokens average, `Qwen2.5-72B-Instruct` scores 95.1 overall and 88.4 at 128k; without YARN+DCA the 128k score drops to 67.0.
  - LVâ€‘Eval (Table 17): `Qwen2.5-72B-Instruct` averages 60.4 at 16k and remains 50.9 at 128k (45.2 at 256k); removing YARN+DCA substantially degrades â‰¥64k.
  - Passkey retrieval (Figure 2): Turbo achieves â€œ100% accuracy at 1M tokens,â€ demonstrating precise recall across extreme lengths.

- Base models (Tables 2â€“5):
  - `Qwen2.5-72B` base surpasses 70B peers and is competitive with Llamaâ€‘3â€‘405B base on several tasks (e.g., MMLU 86.1 vs 85.2; GSM8K 91.5 vs 89.0; MATH 62.1 vs 53.8; Table 2).
  - `Qwen2.5-32B` base is particularly strong for math/coding (MATH 57.7, MBPP 84.5; Table 3).
  - 7B base improves markedly over Qwen2â€‘7B (MATH 49.8 vs 43.5; HumanEval 57.9 vs 51.2; Table 4).
  - Small bases (Table 5) show outsized gains; `Qwen2.5-0.5B` outperforms Gemma2â€‘2.6B on several math/coding tasks.

- RM evaluation (Table 15):
  - `Qwen2.5-RM-72B` is competitive across RewardBench, RMB, PPE, and a Chinese preference set; it leads on PPEâ€™s objective average (69.85) and on the Chinese set (Accuracy 61.27), but it is not strictly dominant everywhereâ€”underscoring the multiâ€‘benchmark perspective.

- Do the experiments support the claims?
  - Breadth and depth: results cover multiple scales, domains, and public + internal benchmarks, with decontamination (Section 5). Longâ€‘context ablations quantify the impact of YARN+DCA (Tables 16â€“17).
  - Alignment claims are supported by IFEval, MTâ€‘Bench, Arenaâ€‘Hard, and inâ€‘house preference results (Tables 6â€“8, 11â€“12). The RM section explicitly cautions against singleâ€‘metric overâ€‘optimization (Section 5.2.3), which adds credibility.

## 6. Limitations and Trade-offs
- Training and compute:
  - 18T tokens and longâ€‘context staging imply very high compute; reproducing training is out of reach for most groups (Sections 3.1â€“3.3).
- Openness and licensing:
  - Some openâ€‘weight sizes carry nonâ€‘Apache licenses (`3B` is â€œQwen Researchâ€; `72B` is â€œQwenâ€; Table 1). MoE variants with 1M context (`Turbo`, `Plus`) are proprietary API models (Abstract, Section 2).
- Generation length vs context length:
  - Openâ€‘weight models support long context (up to 128k) but generation length is capped at 8,192 tokens (Table 1). Very long reasoning chains may still be truncated.
- RL and reward models:
  - Longâ€‘context RL is avoided due to cost and lack of reliable longâ€‘context RMs (Section 4.4). Alignment for ultraâ€‘long inputs thus relies mainly on SFT.
  - RM quality does not straightforwardly predict RL model quality; this remains an open problem (Section 5.2.3).
- Data and synthetic content:
  - Heavy use of synthetic data and LLMâ€‘based filtering could propagate upstream biases or errors despite multiâ€‘stage filtering (Section 3.1, 4.1). The paper does not detail public release of datasets for external audit.
- Multilingual and cultural nuance:
  - While multilingual scores are strong, cultural nuance understanding (BLEnD) leaves room for improvement even at large scales (Tables 13â€“14).

## 7. Implications and Future Directions
- How this changes the field:
  - Demonstrates that careful data scaling and staged postâ€‘training can let a 72B model compete with much larger ones on hard tasks (Table 6). It also shows small, wellâ€‘trained models (0.5Bâ€“3B) can be surprisingly capable, useful for onâ€‘device or edge deployments (Tables 9â€“10).
  - Establishes a practical recipe for longâ€‘context capability that preserves shortâ€‘context quality via YARN+DCA, plus progressive longâ€‘context training for API models (Tables 16â€“17; Figure 2).
  - Encourages the community to move beyond single RM benchmarks to multiâ€‘metric, predictive RM evaluations (Section 5.2.3; Table 15).

- Followâ€‘up research enabled/suggested:
  - Reward modeling:
    - Develop RM benchmarks that better predict downstream RL outcomes; investigate multiâ€‘objective RMs and uncertaintyâ€‘aware RMs.
  - Longâ€‘context alignment:
    - Build reliable longâ€‘context reward signals and efficient longâ€‘context RL training to go beyond SFTâ€‘only alignment (Section 4.4).
  - Data governance and transparency:
    - Public audits of mixture composition, synthetic data filtering pipelines, and crossâ€‘lingual quality checks.
  - Inferenceâ€‘time scaling:
    - Combine longâ€‘context with inferenceâ€‘time reasoning methods (e.g., toolâ€‘augmented reflection) under strict latency budgets.

- Practical applications:
  - Enterprise assistants needing long documents, contracts, or codebases; analytics over semiâ€‘structured/structured data (tables/JSON) with verifiable outputs (Section 4.1).
  - Education and scientific domains requiring math/theorem problem solving (Tables 6â€“8).
  - Edge scenarios where `0.5Bâ€“3B` models deliver useful accuracy under tight resource constraints (Tables 9â€“10).
  - API deployments where `Qwen2.5-Turbo` offers 1Mâ€‘token context with improved TTFT using sparse attention (Figure 3).

> In short, Qwen2.5â€™s technical recipeâ€”highâ€‘quality 18T preâ€‘training, verifiableâ€‘first offline RL + preferenceâ€‘focused online RL, and trainingâ€‘plusâ€‘inference methods for long contextsâ€”yields strong, scalable performance across sizes and tasks, while surfacing open challenges in reward modeling and longâ€‘context alignment that invite further work.

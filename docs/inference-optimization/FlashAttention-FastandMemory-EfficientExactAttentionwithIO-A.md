# FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

**ArXiv:** [2205.14135](https://arxiv.org/abs/2205.14135)

## ğŸ¯ Pitch

FlashAttention introduces an IO-aware, exact attention algorithm that dramatically speeds up and reduces the memory footprint of Transformer models by minimizing costly reads and writes to GPU high-bandwidth memory. By carefully tiling and recomputing attention blocks using fast on-chip memory, FlashAttention achieves up to 7.6Ã— kernel speedups and linear memory scaling with sequence length, all while preserving exact model accuracy. This breakthrough enables practical, efficient training and inference on long sequences, addressing a core limitation of existing Transformers and unlocking new capabilities for deep learning on large contexts.

---

## 1. Executive Summary (2-3 sentences)
FlashAttention is an exact (non-approximate) attention algorithm that makes the attention step in Transformers much faster and more memoryâ€‘efficient by being IOâ€‘awareâ€”i.e., by minimizing slow reads/writes to GPU highâ€‘bandwidth memory (HBM) and using fast onâ€‘chip memory (SRAM) effectively. It tiles the attention computation to avoid ever materializing the NÃ—N attention matrix, and recomputes small pieces onâ€‘chip during backpropagation, yielding up to 7.6Ã— kernel speedups (Figure 1 right), 2â€“3Ã— endâ€‘toâ€‘end training speedups on common models, and linear memory growth in sequence length (Figure 3 right), while preserving exact attention outputs.

## 2. Context and Motivation
- Problem addressed
  - Selfâ€‘attention in Transformers has quadratic time and memory in sequence length N because it forms an NÃ—N matrix S=QKáµ€ and often stores both S and the softmax P (Section 2.2, Algorithm 0). This is the main barrier to longâ€‘context training and inference.
- Why this matters
  - Practical: Long contexts are increasingly important (e.g., long documents, code, multiâ€‘modal sequences). Yet common GPUs run out of memory and become slow on long sequences.
  - Architectural: Modern GPUs are often bottlenecked by memory movement, not floatingâ€‘point compute. On GPUs like the A100, onâ€‘chip SRAM bandwidth (~19 TB/s) is an order of magnitude faster than HBM (~1.5â€“2.0 TB/s), but SRAM is tiny (â‰ˆ20 MB aggregated) (Figure 1 left; Section 2.1).
- Shortcomings of prior approaches
  - Approximate attentions (sparse, lowâ€‘rank) reduce FLOPs but often donâ€™t speed up wallâ€‘clock time because they ignore memory access costs, incur overheads, or hurt accuracy (Section 1; Table 3 shows mixed accuracy/speed tradeâ€‘offs on LRA).
  - NaÃ¯ve kernel fusion in deepâ€‘learning frameworks canâ€™t avoid writing large intermediates (S or P) to HBM for backward (Section 2.1 â€œKernel fusionâ€).
- Positioning
  - This work reframes attention optimization around IO (reads/writes between HBM and SRAM) rather than FLOPs, introduces an IOâ€‘optimal exact attention algorithm (FlashAttention), provides an IOâ€‘complexity analysis and lower bound (Theorem 2; Proposition 3), and shows a blockâ€‘sparse extension that further reduces IO (Proposition 4).

Definitions used once:
- HBM: GPU offâ€‘chip High Bandwidth Memory; large, relatively slow.
- SRAM: GPU onâ€‘chip memory (registers/shared memory/L1), tiny but very fast.
- IO complexity: Number of transfers between memory levels; here, HBM accesses dominate runtime.
- Tiling: Process input in blocks that fit onâ€‘chip, reusing data to reduce HBM traffic.

## 3. Technical Approach
The core idea is to never form or store the full NÃ—N attention matrix in HBM. Instead, the algorithm streams small blocks through SRAM and incrementally maintains the quantities needed to compute the correct softmax and output.

Stepâ€‘byâ€‘step (Algorithm 1; detailed forward Algorithm 2; backward Algorithm 4):

1) Block the inputs to fit onâ€‘chip
- Split Q (queries), K (keys), and V (values) into rectangular blocks Qáµ¢ (size Báµ£Ã—d) and Kâ±¼, Vâ±¼ (size BğšŒÃ—d), where d is head dimension.
- Choose block sizes from SRAM capacity M so that all onâ€‘chip temporaries fit:
  - BğšŒ â‰ˆ M/(4d), Báµ£ â‰ˆ min(M/(4d), d) (Algorithm 1 lines 1â€“4; constraints justified in proof of Theorem 2).

2) Twoâ€‘level loop that keeps K/V onâ€‘chip and streams Q
- Outer loop over Kâ±¼,Vâ±¼ blocks: load one Kâ±¼ and Vâ±¼ from HBM to SRAM once (Algorithm 1 lines 5â€“6).
- Inner loop over Qáµ¢ blocks: for each Qáµ¢, load Qáµ¢ and the running perâ€‘row statistics (explained next), compute the attention contributions for the (i,j) block, update the running output Oáµ¢, and write the updated Oáµ¢ and stats back (lines 7â€“13).

3) Compute softmax one block at a time via incremental statistics
- Problem: softmax along each row needs the sum over all N keys; with tiling you only see BğšŒ keys at a time.
- Solution (Section 3.1 â€œTilingâ€): maintain two perâ€‘row statistics for each query row:
  - m (rowâ€‘wise running max of the logits) for numerical stability, and
  - â„“ (rowâ€‘wise running sum of exp(logits âˆ’ m)).
- When a new (i,j) tile Sáµ¢â±¼ = Qáµ¢Kâ±¼áµ€ is computed onâ€‘chip (line 9), compute its temporary rowâ€‘max mÌƒ and temporary expâ€‘sum â„“Ìƒ (line 10). Update the running m and â„“ using:
  - m_new = max(m, mÌƒ); â„“_new = exp(mâˆ’m_new)Â·â„“ + exp(mÌƒâˆ’m_new)Â·â„“Ìƒ (line 11).
- Update the partial output Oáµ¢ to incorporate the contribution of block j using the same normalization (line 12):
  - Oáµ¢ â† (1/â„“_new) Â· [exp(mâˆ’m_new)Â·â„“Â·Oáµ¢ + exp(mÌƒâˆ’m_new)Â·(exp(Sáµ¢â±¼âˆ’mÌƒ)Â·Vâ±¼)].
- Intuition: this is the softmax of a concatenation trickâ€”sum and max of exponentials can be merged across chunks by rescaling (Section 3.1; equations right above Algorithm 1).

4) Handle masks and dropout while staying onâ€‘chip
- Apply masking to Sáµ¢â±¼ before softmax (Algorithm 2 line 11), and dropout to the blockâ€‘level probabilities onâ€‘chip (line 14), then proceed as above.
- To avoid storing a huge dropout mask for backward, save the RNG state once in forward and regenerate the same mask blocks during backward (Algorithm 2 line 1; Algorithm 4 lines 1, 14).

5) Backward pass without storing NÃ—N intermediates
- Standard backward needs P and S; storing them costs O(NÂ²). Instead, recompute blockâ€‘wise onâ€‘chip using Q,K,V and the saved perâ€‘row (â„“,m) and the RNG state (Algorithm 4 lines 11â€“20).
- A key simplification: the scalar Dáµ¢ = Î£â±¼ Páµ¢â±¼Â·dPáµ¢â±¼ equals the dot product dOáµ¢Â·Oáµ¢ (Eq. (4)), which uses vectors of length d, avoiding reductions over length N (Algorithm 4 line 19).
- Then compute dS = P âˆ˜ (dP âˆ’ D) blockâ€‘wise and accumulate dQ and dK via small GEMMs, while dV accumulates via (Páµ€ dO) (Algorithm 4 lines 20â€“24). All done tileâ€‘byâ€‘tile in SRAM.

6) Fused implementation
- The entire forward pipelineâ€”QKáµ€, masking, softmax stats, dropout, and PVâ€”runs inside a single CUDA kernel per head per batch, with one write of O to HBM (Section 3.1 â€œImplementation detailsâ€).
- The backward is likewise an onâ€‘chip tiled recomputation fused into a single kernel loop over blocks (Algorithm 4).

7) IOâ€‘complexity analysis and nearâ€‘optimality
- Standard attention performs Î˜(Nd + NÂ²) HBM accesses (Theorem 2; Algorithm 0).
- FlashAttention reduces this to Î˜(NÂ² d / (2Mâˆ’1)) â‰ˆ Î˜(NÂ² d / M) (Theorem 2), by reusing onâ€‘chip Kâ±¼,Vâ±¼ and streaming Q multiple times. The same bound holds for backward (Theorem 5).
- Lower bound: no exact attention algorithm can asymptotically beat o(NÂ² d / (2Mâˆ’1)) HBM transfers uniformly over Mâˆˆ[d, Nd] (Proposition 3). Intuition: when M=Î˜(Nd) you must at least read inputs/outputs once, already Î©(Nd).

8) Blockâ€‘sparse FlashAttention (Algorithm 5)
- If the attention mask is blockâ€‘sparse (only a fraction s of blocks are nonzero), skip zero blocks in both loops. IO complexity improves to Î˜(Nd + (NÂ² d / (2Mâˆ’1))Â·s) (Proposition 4).
- In experiments, a fixed â€œbutterflyâ€ pattern is used (Section 3.3), which empirically offers good coverage with small s.

Why these design choices?
- Tiling leverages the large speed gap between SRAM and HBM (Figure 1 left) to cut down memory trafficâ€”the dominant bottleneck for attention (Section 2.1; arithmetic intensity discussion).
- Recomputing onâ€‘chip in backward costs extra FLOPs but avoids NÂ² reads/writes (Figure 2 left shows more FLOPs but far fewer HBM GB, resulting in 5.7Ã— runtime reduction on GPTâ€‘2 medium).
- Fusing all steps prevents writing intermediate S or P back to HBM and reloading them later.

## 4. Key Insights and Innovations
- IOâ€‘aware exact attention as the primary optimization target
  - Novelty: Shifts the optimization objective from FLOP reduction to minimizing HBM accesses, matching the true bottleneck on GPUs (Section 1; 2.1).
  - Evidence: For GPTâ€‘2 medium at seq=1024, FlashAttention uses 4.4 GB vs 40.3 GB HBM R/W and runs 7.3 ms vs 41.7 ms, despite slightly more FLOPs (75.2 vs 66.6 GFLOPs) (Figure 2 left).
- Incremental softmax with perâ€‘row statistics (m, â„“) enabling tiling
  - Novelty: An exact, numerically stable decomposition of softmax across blocks (Section 3.1; Algorithm 1 lines 10â€“12) so the full NÃ—N matrix never exists in HBM.
  - Significance: Reduces memory footprint from quadratic to linear in N (Theorem 1) and enables singleâ€‘kernel fusion.
- Recomputationâ€‘based backward with lightweight saved state
  - Novelty: Store only O(N) vectors (O, m, â„“) and RNG state; recompute S and P blockâ€‘wise onâ€‘chip in backward (Algorithm 4), using dOÂ·O to avoid Nâ€‘length reductions (Eq. (4)).
  - Significance: Further cuts HBM traffic in backward to Î˜(NÂ² d / M) (Theorem 5), turning memoryâ€‘bound steps into onâ€‘chip compute even when dropout is used.
- IOâ€‘complexity optimality characterization
  - Novelty: Formal upper bound for FlashAttention and a matching parameterâ€‘range lower bound (Theorem 2; Proposition 3), arguing no exact algorithm can asymptotically do better (for all M).
  - Significance: Provides a principled baseline for future IOâ€‘aware attention designs.
- Blockâ€‘sparse FlashAttention as a fast approximate primitive
  - Novelty: Same tiling engine but skip zero blocks; IO scales with sparsity fraction s (Proposition 4).
  - Significance: 2â€“4Ã— faster than dense FlashAttention and faster than other approximate attentions tested (Figure 2 right; Table 3 speedups).

## 5. Experimental Analysis
Evaluation setup (Sections 4, E; Figures 1â€“3; Tables 1â€“7, 8â€“21):
- Models and datasets
  - BERTâ€‘large pretraining on Wikipedia; metric: time to a target maskedâ€‘LM accuracy (Table 1).
  - GPTâ€‘2 small/medium on OpenWebText; metric: validation perplexity and wallâ€‘clock training time (Table 2).
  - Long Range Arena (LRA) with sequence lengths 1Kâ€“4K; metrics: accuracy, throughput, training time (Table 3).
  - Longâ€‘document classification on MIMICâ€‘III and ECtHR; metric: microâ€‘F1 vs sequence length (Table 5).
  - Pathâ€‘X (seq 16K) and Pathâ€‘256 (seq 64K) pathâ€‘finding tasks; metric: accuracy (Table 6).
  - System microbenchmarks: runtime and HBM traffic (Figures 1â€“3), hardware variations (Appendix E.5), memory footprint (Figure 3 right; Table 21).
- Baselines
  - Exact attention in PyTorch and Megatronâ€‘LM; NVIDIA Apex FMHA for short sequences (Table 7).
  - Approximate/sparse attentions: Linformer, Linear/Performer, Local, Reformer, BigBird/Longformer, SMYRF, LSFormer, OpenAI Blockâ€‘Sparse (Tables 9â€“20).

Main quantitative findings (representative numbers as block quotes):
- Kernelâ€‘level speed and IO
  - Figure 1 right (GPTâ€‘2 attention microbenchmark): 
    > â€œFlashAttention â€¦ results in a 7.6Ã— speedup on the attention computation.â€
  - Figure 2 left (GPTâ€‘2 medium, seq=1024, A100):
    > FLOPs 66.6â†’75.2 GFLOPs; HBM R/W 40.3â†’4.4 GB; runtime 41.7â†’7.3 ms.
  - Figure 2 middle (ablating block size): 
    > larger tiles reduce HBM accesses and runtime until other bottlenecks dominate (beyond block size 256).
- Endâ€‘toâ€‘end training speedups
  - Table 1 (BERTâ€‘large, 8Ã—A100):
    > 20.0Â±1.5 min â†’ 17.4Â±1.4 min to target accuracy (â‰ˆ15% faster).
  - Table 2 (GPTâ€‘2 small/medium, 8Ã—A100):
    > small: 9.5 d (HF) â†’ 2.7 d (3.5Ã—); 4.7 d (Megatron) â†’ 2.7 d (1.7Ã—).  
    > medium: 21.0 d (HF) â†’ 6.9 d (3.0Ã—); 11.5 d (Megatron) â†’ 6.9 d (1.8Ã—).  
    > Perplexity matches baselines (18.2/14.3).
- Quality and longâ€‘context capability
  - Table 4 (GPTâ€‘2 small with longer context):
    > At 4K context, training is still 1.3Ã— faster than Megatron at 1K and perplexity improves from 18.2â†’17.5 (âˆ’0.7).
  - Table 5 (Longâ€‘document classification with RoBERTa+FlashAttention):
    > MIMICâ€‘III: 52.8â†’57.1 microâ€‘F1 from 512â†’16K tokens (+4.3).  
    > ECtHR: 72.2â†’80.7 at 8K (+8.5), slight drop at 16K (79.2).
  - Table 6 (Path tasks):
    > First betterâ€‘thanâ€‘chance Transformer results on Pathâ€‘X: 61.4% (FlashAttention).  
    > Pathâ€‘256 at seq=64K: 63.1% with blockâ€‘sparse FlashAttention; all other Transformer baselines fail.
- Benchmarking vs. approximate methods
  - Figure 3 left:
    > FlashAttention beats exact baselines by up to 3Ã— across 128â€“2048 tokens; approximate methods cross over around 512â€“1024 tokens; blockâ€‘sparse FlashAttention is faster than all tested methods across lengths.
- Memory footprint
  - Figure 3 right; Table 21:
    > Memory scales linearly with N and is up to 20Ã— lower than exact baselines; only Linformer reaches 64K among baselines, but FlashAttention is still ~2Ã— more memoryâ€‘efficient there.
- Against NVIDIA Apex FMHA (short sequences; Table 7):
  > At seq=512, forward 1.14â†’0.81 ms; backward 1.81â†’2.00 ms; net 2.95â†’2.81 ms (slightly faster overall).

Ablations/robustness
- Blockâ€‘size sensitivity (Figure 2 middle): performance tracks HBM accesses down to tile size limits.
- Hardware sensitivity (Appendix E.5): larger speedups on GPUs with lower HBM bandwidth (RTX 3090), smaller speedups on GPUs with smaller SRAM (T4).
- Head dimension sensitivity: with d=128, speedups decrease but remain significant, especially under causal masks (Appendix E.5).

Assessment
- The experiments convincingly support the central claim: reducing HBM accesses yields large realâ€‘world speedups even with more FLOPs (Figure 2 left).
- Endâ€‘toâ€‘end training wins on BERT/GPTâ€‘2 and the LRA suite (Tables 1â€“3) corroborate system microbenchmarks.
- Quality is maintained (GPTâ€‘2 perplexity) or improved via longer context (Tables 4â€“6).
- Results are broad (multiple tasks, models, hardware), though training comparisons are against strong but not exhaustive baselines.

## 6. Limitations and Trade-offs
- Still quadratic compute for dense attention
  - FlashAttention does not reduce arithmetic complexity; if compute becomes the bottleneck (e.g., extremely large d or very fast HBM), speedups shrink (Figure 2 middle shows a regime where runtime becomes computeâ€‘bound).
- Engineering effort and portability
  - Requires bespoke CUDA kernels and careful onâ€‘chip memory management; porting across GPU generations and supporting new attention variants demands engineering (Section 5 â€œCompiling to CUDAâ€ limitation).
- Dependence on SRAM size and GPU characteristics
  - Smaller SRAM reduces tile sizes and speedups (Appendix E.5, T4 results). Benefits depend on the hardware memory hierarchy.
- Backward recomputation adds FLOPs
  - Extra compute is usually masked by memory savings (Figure 2 left), but in computeâ€‘bound regimes could be a tradeâ€‘off.
- Multiâ€‘GPU communication not optimized
  - Analysis and kernels target singleâ€‘GPU IO; crossâ€‘GPU communication patterns for very long sequences are not addressed (Section 5 â€œMultiâ€‘GPU IOâ€‘Aware Methodsâ€).
- Blockâ€‘sparse quality depends on sparsity pattern
  - The butterfly mask is fixed in experiments (Section 3.3); while fast, it is not learned/adaptive and may not suit all tasks.

## 7. Implications and Future Directions
- Broader shift toward IOâ€‘aware deep learning
  - This work demonstrates that careful control of HBM traffic can unlock large, practical speedups even for â€œoptimizedâ€ operations. The analysis and methodology provide a template for other memoryâ€‘bound layers (Section 5 â€œIOâ€‘Aware Deep Learningâ€).
- Enabling longer contexts in standard Transformers
  - Linear memory scaling allows training and inference at 16Kâ€“64K tokens on a single A100 (Figure 3; Path tasks), opening applications in longâ€‘document NLP, longâ€‘horizon RL, genomics, and code modeling.
- Compiler support for IOâ€‘aware kernels
  - A natural next step is a highâ€‘level compiler that emits IOâ€‘efficient fused kernels from PyTorchâ€‘level code (Section 5 â€œCompiling to CUDAâ€), akin to Halide for image pipelines.
- Multiâ€‘GPU IOâ€‘aware attention
  - Extending the IO analysis to interâ€‘GPU communication and designing tiled, overlapâ€‘computeâ€‘withâ€‘communication kernels could scale longâ€‘sequence attention beyond singleâ€‘GPU limits (Section 5; Appendix D.2).
- Beyond attention
  - The same principles could accelerate sparse MLPs (memoryâ€‘bound in practice) and kernel methods that form implicit NÃ—N matrices (Appendix D.2), as well as other sequence operations with large intermediate tensors.
- Better blockâ€‘sparse design
  - Learning taskâ€‘specific block patterns or combining blockâ€‘sparsity with lowâ€‘rank compression (as in Scatterbrain/BigBird) on top of the FlashAttention primitive could preserve quality while achieving nearâ€‘linear IO.

In short, FlashAttention reframes attention optimization around memory movement, provides an exact, IOâ€‘optimal algorithm with a fused implementation, and validates that IOâ€‘awarenessâ€”not just FLOP countingâ€”drives real speedups and unlocks longer contexts.

# Liger Kernel: Efficient Triton Kernels for LLM Training

**ArXiv:** [2410.10989](https://arxiv.org/abs/2410.10989)

## üéØ Pitch

Liger Kernel delivers a significant leap in Large Language Model (LLM) training efficiency by providing a suite of custom Triton GPU kernels that fuse and optimize core operations like normalization, gated MLPs, rotary embeddings, and cross-entropy loss‚Äîincluding a fused linear+loss kernel that eliminates bottleneck memory usage. By sharply reducing memory overhead (up to 60%) and boosting training throughput (by ~20%) compared to standard methods, Liger Kernel empowers researchers and practitioners to scale LLMs more cost-effectively, unlock larger batch sizes or longer sequences, and accelerate AI development even on modest hardware.

---

## 1. Executive Summary
Liger-Kernel is an open-source collection of GPU kernels written in Triton that fuses and restructures common Large Language Model (LLM) training operations to reduce memory traffic and kernel-launch overhead. The library delivers substantial end-to-end gains‚Äîtypically around 20% higher training throughput and up to 60% lower GPU memory usage versus standard Hugging Face implementations‚Äîthrough targeted kernels for normalization, gated MLPs, rotary embeddings, and the cross-entropy loss, including a fused linear+loss variant that avoids materializing massive logits.

## 2. Context and Motivation
- The specific gap:
  - PyTorch‚Äôs eager execution runs operations one-by-one, paying dispatch and kernel-launch overhead each time, and materializing intermediate tensors to support backpropagation (Section 2; ‚ÄúEager mode execution‚Ä¶ entails extra computational overheads‚Äù).
  - Memory movement, not just compute, dominates many transformer workloads: data shuttles between slow but large HBM (high-bandwidth memory) and fast but tiny on-chip SRAM. Repeated reads/writes leave GPU cores idle waiting for data (Section 2.2).
  - A major training bottleneck appears in the last layer: materializing logits for very large vocabularies (100k‚Äì250k tokens) can consume tens of GB on its own (Section 3.2, FLCE; Gemma example).
- Why it matters:
  - Improving kernel-level efficiency multiplies across the GPU‚Äôs massively parallel execution. Small per-op savings amplify into large end-to-end gains (Section 1; Section 2.2).
  - Lower peak memory enables larger batch sizes, longer sequences, or training on smaller/cheaper GPUs (Sections 4.2 and figures 4‚Äì8).
- Prior approaches and their limits:
  - Model compilers (torch.compile, TVM, XLA, nvFuser; Section 2.1) perform general-purpose fusion and code generation but may miss domain-specific opportunities like block-structured algorithms (e.g., FlashAttention) tuned to transformer math and memory patterns.
  - Algorithmic fusions (FlashAttention; Section 2.2) show the value of bespoke kernels that explicitly exploit SRAM/HBM hierarchy. However, many other bottlenecks (norms, activations, embeddings, loss) remain unoptimized in common stacks.
- Positioning:
  - Liger-Kernel focuses on a curated set of last-mile, Triton-based fusions designed specifically for training LLMs. It targets ease of adoption (drop-in patching) and thorough testing (correctness, performance, convergence), while integrating with distributed training frameworks (Section 1, 3.1, 3.4).

## 3. Technical Approach
Liger-Kernel supplies Triton implementations that fuse multi-op sequences, avoid redundant reads/writes, and compute gradients in-place where safe. Kernels standardize inputs as 2D matrices of shape `(B√óT, H)`‚Äîbatch size B, sequence length T, hidden size H‚Äîand Triton parallelizes work row-wise (Section 3.2). Below is how each kernel works and why it helps.

- Terminology clarified:
  - Triton: a Python-like domain-specific language for writing high-performance GPU kernels (Section 2.3).
  - Operation fusion: combining multiple GPU ops into a single kernel to avoid kernel-launch overhead and repeated memory traffic (Section 2.2).
  - Online softmax: computing softmax-related quantities incrementally without storing the entire vector at once, improving numerical stability and reducing memory.

A) Normalization kernels (Section 3.2)
- RMSNorm
  - Forward (Eq. 1): normalize each row by its RMS and scale with a learned vector `Œ≥` in one kernel; cache per-row RMS for reuse.
  - Backward (Eq. 2): compute gradients w.r.t. inputs and `Œ≥`; aggregate `Œ≥`‚Äôs gradient across rows because the same parameters apply to all rows in the batch.
  - Why it helps: one-pass normalization+scaling minimizes reads/writes and reuses cached statistics.
  - Implementation detail: efficient parameter-gradient aggregation uses a two-stage reduction (Section 3.2, footnote 8) rather than na√Øve PyTorch summation.
- LayerNorm
  - Forward (Eq. 3): center, normalize, then apply `Œ≥` and `Œ≤` in one kernel, caching inverse RMS.
  - Backward (Eq. 4): compute gradients w.r.t. inputs, `Œ≥`, and `Œ≤`; sum parameter gradients across rows.
  - Design choice: similar fusion rationale as RMSNorm, with extra centering step.

B) Rotary Position Embedding, RoPE (Section 3.2, Eq. 5‚Äì6)
- Background: RoPE rotates query/key vectors by a position-dependent 2D rotation per feature pair to encode absolute positions through relative phases.
- Implementation:
  - Fuses the rotations for both queries and keys into one kernel to reduce overhead.
  - Uses the Hugging Face-style block-diagonal rotation structure (a sequence of 2√ó2 rotation blocks; details in the long matrix in Section 3.2) and its sparsity to compute rotations efficiently.
  - Backprop uses the transpose rotation (Eq. 6).
  - Practical note: inputs must be contiguous; a production bug traced to non-contiguous tensors underscores this requirement (Section 3.3.4).

C) Gated MLP fusions: SwiGLU and GeGLU (Section 3.2)
- Background: These replace standard MLP activations with a ‚Äúgated‚Äù product that often boosts model quality at similar or lower compute.
- SwiGLU (Eq. 7‚Äì9)
  - Computes `y = SiLU(Wx + b) ‚äô (Vx + c)` in one kernel.
  - Backward: uses derivatives of SiLU and multiplies by the other gate.
  - Memory-saving strategy: recompute activation values during backward instead of caching them to reduce peak memory (discussed again in results).
- GeGLU (Eq. 10‚Äì14)
  - Computes `y = GELU(Wx + b) ‚äô (Vx + c)` using the tanh approximation for GELU (Eq. 11) and its derivative (Eq. 14).
  - Same recomputation strategy for memory.

D) Cross-Entropy (CE) loss and logit handling (Section 3.2)
- CE kernel:
  - Uses online softmax in the forward to compute probabilities `y = softmax(x)` and immediately form the gradient `‚àáxL = y ‚àí t` (Eq. 15‚Äì16) without keeping both logits and gradients simultaneously.
  - Performs in-place replacement of the logits with their gradients to avoid double materialization, and uses a ‚Äúsafe log‚Äù for stability.
- Why this matters: the CE loss is often the single largest activation by size because its logits are `(B√óT)√óV`. Reducing its footprint directly reduces peak memory.

E) Fused Linear + Cross-Entropy (FLCE) with chunking (Section 3.2; Figure 1; Eq. 17)
- Problem: Materializing the full logits `(B√óT)√óV` can be prohibitive. For example, with `V=256k`, `B=8`, `T=4096`, and bfloat16, the logits alone can take ~16.8 GB (Section 3.2, FLCE paragraph).
- Approach:
  - Flatten hidden states to matrix `H ‚àà R^{(B√óT)√óH}`.
  - Split `H` into ‚Äúinput chunks‚Äù along the `(B√óT)` dimension.
  - For each chunk: project logits `x = W^T h`, compute CE loss and its gradient using the CE kernel, then immediately backpropagate into `h` and accumulate gradients for the shared `W`:
    - `‚àáhL = W ‚àáxL`, `‚àáW L = h (‚àáxL)^T` (Eq. 17).
  - This pipeline is depicted in Figure 1, showing:
    - Logit gradients produced chunk-wise by the CE kernel,
    - Immediate in-place storage of these gradients,
    - Accumulation of projection-head gradients without materializing full logits.
- Chunk-size heuristic:
  - ‚ÄúSet the chunk size to be `2^{ceil(log2( ceil( (B√óT) / ceil(V/H) ) ))}` to balance memory and GPU utilization,‚Äù targeting block sizes closer to hidden dimension `H` (Section 3.2, FLCE).
- Gradient scaling:
  - When CE reduction is a mean over all tokens, chunk-wise gradients need a corrective scaling factor (ratio of chunk size to `B√óT`) to match the global average (Section 3.2, Remark).

F) API and integration (Section 3.1, 3.4)
- Three adoption paths:
  - Drop-in: `AutoLigerKernelForCausalLM.from_pretrained(...)` autoprovisions kernels for supported models.
  - Model-specific patching: e.g., `apply_liger_kernel_to_llama()` lets you use HF trainers for tasks beyond causal LM.
  - Compose your own: import building blocks like `LigerLayerNorm` and `LigerCrossEntropyLoss` in custom modules.
- Works with common frameworks: Hugging Face Trainer and TRL‚Äôs `SFTTrainer` (one-flag enablement), Axolotl, LLaMA-Factory (Section 3.4).
- Distributed training compatibility: FSDP, DeepSpeed ZeRO, ZeRO++ (Section 1).

G) Testing and engineering practices (Section 3.3)
- Correctness:
  - Compare to pure PyTorch/Hugging Face references across diverse shapes and dtypes.
  - Tolerances: fp32 `atol=1e-7, rtol=1e-5`; bf16 `atol=1e-3, rtol=1e-2`. Tighten or relax if needed but validate via convergence tests (Section 3.3.1).
  - Avoid index overflow: promote program indices to int64 when `program_id * stride` risks exceeding 2,147,483,647 (Section 3.3.1).
- Performance:
  - Use real training shapes (e.g., `B=4`, `H=2048`, long `T`) to ensure benchmark relevance (Section 3.3.2).
- Convergence testing:
  - End-to-end, small-scale training runs to confirm identical losses and model evolution, not just unit-test parity (Section 3.3.3).
- Contiguity:
  - Require contiguous tensors to avoid illegal memory access‚Äîthis surfaced as a real bug during RoPE deployment (Section 3.3.4).

## 4. Key Insights and Innovations
- Fused Linear Cross-Entropy (FLCE) with chunking is the standout contribution.
  - What‚Äôs new: a practical, memory-safe way to compute the last-layer projection and CE loss without ever materializing full logits. The chunked pipeline (Figure 1, Eq. 17) integrates an online-softmax CE kernel that returns gradients in-place, plus immediate accumulation of `‚àáW` and `‚àáh`.
  - Why it matters: the logits tensor is often the single biggest memory consumer in LLM training; eliminating it unlocks larger batch sizes and vocabularies (Section 3.2, FLCE; Gemma example).
- Targeted, Triton-based fusions beyond attention:
  - RMSNorm, LayerNorm, RoPE, SwiGLU, GeGLU are optimized with caching, reduced memory traffic, and recomputation strategies. These are not generic compiler fusions; they exploit the structure of each op (Section 3.2).
- Engineering practices that surface real-world failure modes:
  - Explicit checks for contiguity and 32-bit index overflow (Section 3.3.1, 3.3.4) are practical contributions often missing from academic repos. These reduce integration risk in production training.
- Usability-first API:
  - Auto-patching (`AutoLigerKernelForCausalLM`) and single-flag integration in HF/TRL (Section 3.1, 3.4) lower adoption friction. While not algorithmic, this is pivotal for impact.

Overall, FLCE is a fundamental innovation for memory-bound training regimes; the other kernels are strong incremental yet well-engineered optimizations that cumulatively improve stability and throughput.

## 5. Experimental Analysis
Evaluation methodology
- Kernel microbenchmarks (Section 4.1):
  - Hardware: single NVIDIA A100 80GB.
  - Shapes: `T, H ‚àà {4096, 8192, 12288, 16384}`; CE vocab sizes `{40960, 81920, 122880, 163840}`.
  - Repeats: 10 runs; report median with [0.2, 0.8] quantiles.
  - Metrics: execution time and peak allocated memory.
- End-to-end training (Section 4.2):
  - Hardware: 4√ó A100 80GB.
  - Models/dataset: LLaMA 3-8B, Qwen2, Gemma 7B, Mistral 7B, Phi-3; Alpaca dataset.
  - Setup: bfloat16 precision, AdamW, cosine LR scheduler, `T=512`, throughput and peak memory measured after 20 steps, with 5 repetitions (standard errors reported in figures).
- Medusa multi-token prediction (Section ‚ÄúMedusa‚Äù):
  - Two regimes: stage-1 (train only Medusa heads) and stage-2 (tune backbone + heads).
  - Tested with 3 and 5 heads, variable sequence lengths; 8√ó A100s for these experiments.

Main quantitative results (all figures cited from Section 4)
- Kernel-level speed and memory:
  - CrossEntropy:
    - ‚Äúapproximately 3√ó faster execution (Figure 2a) and approximately 5√ó less memory (Figure 3a) for a vocab size of 163840.‚Äù
  - RMSNorm:
    - ‚Äúapproximately 7√ó reduction in execution time and roughly 3√ó reduction in peak memory for hidden size 16384‚Äù (Figures 2d, 3d).
  - LayerNorm:
    - ‚Äúapproximately 30% reduction in execution time with minimal memory overheads‚Äù (Figures 2e, 3e).
  - RoPE:
    - ‚Äúapproximately 8√ó speedup and approximately 3√ó lower memory for hidden size 16384‚Äù (Figures 2f, 3f).
  - GeGLU and SwiGLU:
    - ‚Äúspeed parity with baseline‚Äù but ‚Äú~1.6√ó lower peak memory at T=16384‚Äù thanks to recomputation (Figures 2b, 2c, 3b, 3c).
- End-to-end training throughput and memory (Figures 4‚Äì8):
  - LLaMA 3-8B @ batch 64:
    - Throughput: +42.8%
    - Peak memory: ‚àí54.8% (Figure 4).
  - Qwen2 @ batch 48:
    - Throughput: +25.5%
    - Peak memory: ‚àí56.8% (Figure 5).
  - Gemma 7B @ batch 48:
    - Throughput: +11.9%
    - Peak memory: ‚àí51.8% (Figure 6).
  - Mistral 7B @ batch 128:
    - Throughput: +27%
    - Peak memory: ‚àí21% (Figure 7).
  - Phi-3 @ batch 128:
    - Throughput: +17%
    - Peak memory: ‚àí13% (Figure 8).
- Medusa multi-token training (Figures 9‚Äì12):
  - The baseline frequently runs OOM at longer sequences with 3 or 5 heads; Liger avoids OOM, reduces memory, and increases throughput in both stage-1 and stage-2 regimes.
  - Quote:
    > ‚ÄúWithout the Liger kernel, experiments are highly prone to out of memory issues. ‚Ä¶ the Liger kernel has demonstrated reduced memory usage and improved throughput.‚Äù (Medusa section)

Do the experiments support the claims?
- Yes, the microbenchmarks isolate per-kernel gains and explain where benefits come from (e.g., recomputation vs. caching vs. fusion). The end-to-end experiments across five popular LLMs, uniform training settings, and replicated runs substantiate the average ‚Äú~20% throughput, ~60% memory‚Äù headline (Abstract; Section 4.2 figures).
- Robustness checks:
  - Convergence tests are described (Section 3.3.3), indicating parity in training outcomes under realistic scenarios.
  - Correctness tolerances and index-width safeguards address numerical and memory-access edge cases (Section 3.3.1).
- Caveats:
  - Gains are operation- and workload-dependent. GeGLU/SwiGLU prioritize memory over speed; CE and RoPE deliver the most dramatic improvements at large vocabularies or hidden sizes (Figures 2‚Äì3).
  - Results are on A100 GPUs; portability is discussed (CI across vendors), but end-to-end numbers are not reported for other accelerators.

## 6. Limitations and Trade-offs
- Precision and numerical tolerances (Section 3.3.1):
  - Even exact kernels may require relaxed tolerances (bf16), and the library relies on convergence tests to validate equivalence when unit-test tolerances are loose.
- Contiguity constraint (Section 3.3.4):
  - Non-contiguous tensors can cause illegal memory access; users must ensure `.contiguous()` before kernels, which adds occasional copies.
- Index-size issues for very large tensors (Section 3.3.1):
  - Programmers must promote indices to int64 in giant workloads; Liger does this, but other custom kernels in a pipeline might not.
- Chunking trade-offs in FLCE (Section 3.2):
  - Chunk size is a heuristic balancing GPU utilization and memory; suboptimal choices can reduce speedups.
  - Requires careful gradient scaling when reduction is a mean across all tokens to avoid bias (Remark under FLCE).
- Scope:
  - Focuses on training kernels; inference optimizations are ‚Äúseamlessly adapted,‚Äù but no inference benchmarks are reported (Conclusions).
  - Attention kernels like FlashAttention are referenced rather than reimplemented here; overall training speed still depends on attention performance in the chosen stack.
- Hardware and framework coverage:
  - While CI includes AMD and Intel GPUs (Acknowledgements), quantitative end-to-end results are reported on NVIDIA A100 only; performance portability is plausible but not empirically documented in this paper.
- Mixed results for some ops:
  - GeGLU/SwiGLU show memory gains but not speedups versus strong baselines (Figures 2b‚Äìc), indicating diminishing returns where the baseline is already near-optimal.

## 7. Implications and Future Directions
- Field impact:
  - By removing the longstanding ‚Äúlogits materialization‚Äù bottleneck and fusing non-attention hotspots, Liger-Kernel shifts the Pareto frontier for training efficiency‚Äîespecially at large vocabularies and hidden sizes.
  - The library demonstrates that practical, operation-specific Triton kernels can coexist with and complement compiler-based optimizations (torch.compile, nvFuser).
- Practical applications:
  - Fine-tuning larger models on fewer GPUs, training at longer context lengths, or using larger vocabularies without OOM.
  - Multi-token prediction methods (e.g., Medusa) become feasible at higher sequence lengths and head counts due to the FLCE kernel (Figures 9‚Äì12).
- Research and engineering directions:
  - Extend fused-kernel coverage: e.g., embedding layers (including tied weights), optimizer updates, residual add + norm patterns, block-sparse or mixture-of-experts routing fusions.
  - Integrate kernel selection/tuning with model compilers: auto-choose between Liger kernels and compiler fusions per workload.
  - Formalize chunk-size selection for FLCE: analytical or learned models to optimize throughput under memory constraints.
  - Broaden hardware validation: systematic benchmarks on AMD/Intel accelerators and across interconnect topologies.
  - Inference kernels: adapt the same fusion ideas (e.g., fused linear+softmax for decoding steps; memory-aware RoPE) and quantify latency/throughput gains.

Selected grounded quotations and references
- CE and memory:
  > ‚Äúsingle logit tensor size is still prohibitive‚Ä¶ The main idea of FLCE is shown in Figure 1.‚Äù (Section 3.2, FLCE)
- Kernel-level gains:
  > ‚Äúapproximately 3√ó faster execution (Figure 2a) and consumes approximately 5√ó less memory (Figure 3a) for a vocab size of 163840.‚Äù (Section 4.1, CE)
  > ‚Äúapproximately 7√ó reduction in execution time and roughly 3√ó reduction in peak memory‚Ä¶ for a hidden dimension of 16384.‚Äù (RMSNorm; Figures 2d, 3d)
  > ‚Äúapproximately 30% reduction in execution time‚Ä¶ minimal memory overheads.‚Äù (LayerNorm; Figures 2e, 3e)
  > ‚Äúapproximately 8√ó speedup‚Ä¶ approximately 3√ó lower memory‚Ä¶ hidden size of 16384.‚Äù (RoPE; Figures 2f, 3f)
- End-to-end gains:
  > LLaMA 3-8B: ‚Äú42.8% increase in throughput‚Ä¶ 54.8% reduction in GPU memory‚Äù at batch 64 (Figure 4).
  > Qwen2: ‚Äú25.5%‚Ä¶ 56.8% reduction‚Äù at batch 48 (Figure 5).
  > Gemma 7B: ‚Äú11.9%‚Ä¶ 51.8% reduction‚Äù at batch 48 (Figure 6).
  > Mistral 7B: ‚Äú27%‚Ä¶ 21% drop‚Äù at batch 128 (Figure 7).
  > Phi-3: ‚Äú17%‚Ä¶ 13%‚Äù at batch 128 (Figure 8).
- Testing best practices:
  > ‚ÄúIf program id * Y stride > 2,147,483,647, the value becomes negative, resulting in illegal memory access‚Ä¶ convert to int64.‚Äù (Section 3.3.1)
  > ‚Äúit‚Äôs best practice to ensure tensors are contiguous before passing them to the kernel.‚Äù (Section 3.3.4)

In sum, Liger-Kernel is a well-engineered, practically impactful set of Triton kernels with a particularly strong contribution in the fused linear+cross-entropy loss. The work combines algorithm-aware fusion, thoughtful memory handling, and an accessible API, yielding measurable, reproducible gains in real LLM training scenarios.

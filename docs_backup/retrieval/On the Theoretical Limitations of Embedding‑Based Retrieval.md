# On the Theoretical Limitations of Embeddingâ€‘Based Retrieval

**ArXiv:** [2508.21038](https://arxiv.org/abs/2508.21038)
**Authors:** Orion Weller, Michael Boratko, Iftekhar Naim, Jinhyuk Lee
**Institutions:** Google DeepMind

## ğŸ¯ Pitch

This paper establishes a fundamental limit of single-vector embedding retrieval systems by linking embedding dimension constraints to sign-rank, proving that certain query-document relevance patterns can't be realized within fixed dimensions. This insight is crucial as it highlights the hidden capacity limitations affecting instruction-following systems, challenging the scalability of dense retrieval models and prompting a reevaluation of current embedding architectures in handling complex queries.

---

## 1. Executive Summary
This paper proves and demonstrates a fundamental limit of singleâ€‘vector embedding retrieval: with a fixed embedding dimension `d`, there exist legitimate queryâ€“document relevance patterns that no dotâ€‘product embedding model can realize. It connects retrieval to the mathematical notion of signâ€‘rank to bound the minimum dimension needed, validates the bound in bestâ€‘case optimization, and releases a simple naturalâ€‘language dataset (LIMIT) where stateâ€‘ofâ€‘theâ€‘art embedders fail despite the taskâ€™s trivial semantics.

## 2. Context and Motivation
- Problem addressed
  - Modern retrieval systems often use single vectors for queries and documents and select topâ€‘k documents by dotâ€‘product similarity (â€œdense retrievalâ€). Benchmarks now require handling arbitrary instructions, reasoning, and logical combinations, implicitly asking embedders to represent any â€œset of relevant documentsâ€ a query might define (Â§1).
  - The question: Can a finiteâ€‘dimensional singleâ€‘vector embedding represent all possible topâ€‘k relevance combinations users might request?

- Why it matters
  - Practical: Instructionâ€‘following and agentic search can synthesize complex, hyperâ€‘specific queries that implicitly pick arbitrary combinations of documents. If embeddings cannot represent certain combinations, some user intents are unretrievable no matter how we train (Â§1, Â§5.1).
  - Theoretical: The paper ties representational limits of vector spaces to classic results in communication complexity via signâ€‘rank, making limits precise rather than heuristic (Â§3).

- Prior approaches and gaps
  - Empirical studies have hinted at dimensionality issues (e.g., higher false positives with lowerâ€‘dimensional embeddings in large corpora; Â§2.2).
  - Geometric bounds from orderâ€‘k Voronoi regions conceptually relate to topâ€‘k retrieval but are hard to compute tightly in high dimensions and offer little actionable guidance for IR (Â§2.3, Appendix Â§8).
  - No prior work links the exact realizability of topâ€‘k sets in embedding retrieval to a formal lower bound on dimension via signâ€‘rank and then shows the effect empirically.

- Positioning
  - The work (1) formalizes retrieval as a matrix order/threshold preservation problem, (2) proves tight dimension bounds using signâ€‘rank (within Â±1), (3) verifies bestâ€‘case realizability by directly optimizing free embeddings on the test qrels, and (4) provides a naturalâ€‘language dataset (LIMIT) that operationalizes the theory and exposes failures of current embedders (Â§Â§3â€“5).

## 3. Technical Approach
The paper proceeds in three layers: a formal theory, a bestâ€‘case optimization test, and a realistic dataset that instantiates the theoretical difficulty.

- Formalization of retrieval as matrix ordering (Section Â§3.1)
  - Setup:
    - `m` queries, `n` documents, and a binary relevance matrix `A âˆˆ {0,1}^{mÃ—n}` where `A[i,j]=1` iff document `j` is relevant to query `i`.
    - An embedding model maps queries and documents to `d`â€‘dimensional vectors. Scores are dot products. Let `U âˆˆ R^{dÃ—m}`, `V âˆˆ R^{dÃ—n}`, and `B = U^T V` be the score matrix.
  - Goal: For each row (query), ensure all relevant docs score above all irrelevant docs (or at least appear before them in order).

- Paperâ€‘specific notions (defined and used to tie retrieval to linear algebra; Â§3.1)
  - `rowâ€‘wise orderâ€‘preserving rank` (`rank_rop A`): smallest rank of a score matrix `B` that preserves the withinâ€‘row ordering given by `A`.
  - `rowâ€‘wise thresholdable rank` (`rank_rt A`): smallest rank enabling a perâ€‘row threshold `Ï„_i` that separates relevant from irrelevant entries in row `i`.
  - `globally thresholdable rank` (`rank_gt A`): smallest rank enabling a single global threshold `Ï„` that separates all 1s from 0s in `A`.

- Key equivalence and bridge to signâ€‘rank (Section Â§3.2)
  - Equivalence for binary matrices:
    - Proposition 1 shows `rank_rop A = rank_rt A`. Intuition: if in each row all relevant scores are above all irrelevant scores (orderâ€‘preserving), there exists a separating threshold per row, and viceâ€‘versa.
  - Connection to signâ€‘rank:
    - Signâ€‘rank (Definition 3) of a Â±1 matrix `M` is the minimum rank of a real matrix whose entrywise signs match `M`.
    - Construct `M = 2A âˆ’ 1_{mÃ—n} âˆˆ {âˆ’1, 1}^{mÃ—n}` (map relevant to +1, irrelevant to âˆ’1).
    - Proposition 2 provides the core chain of inequalities:
      > rankÂ±(2Aâˆ’1) âˆ’ 1 â‰¤ rank_rt(A) = rank_rop(A) â‰¤ rank_gt(A) â‰¤ rankÂ±(2Aâˆ’1)
    - Plainâ€‘language meaning:
      - The minimum dimension needed by a dotâ€‘product embedder to realize the relevance constraints (with perâ€‘row thresholds or order preservation) is sandwiched within one of the signâ€‘rank of `2Aâˆ’1`.
      - Therefore, if you know the signâ€‘rank, you know the minimum embedding dimension up to Â±1. Conversely, if you can realize `A` in `d` dimensions (e.g., by gradientâ€‘descent optimizing free vectors), you bound the signâ€‘rank to either `d` or `d+1` (Â§3.3).

- Consequences (Section Â§3.3)
  - Lower bound on required dimensionality:
    > â€œwe need at least rankÂ±(2A âˆ’ 1) âˆ’ 1 dimensions to capture the relationships in A exactlyâ€ (Â§3.3).
  - Practical mechanism:
    - If free embeddings can realize `A` in `d` dims, then the signâ€‘rank is â‰¤ `d+1`. This yields a constructive, optimizationâ€‘based upper bound on signâ€‘rankâ€”useful because signâ€‘rank is hard to compute exactly.

- Bestâ€‘case â€œfree embeddingâ€ optimization (Section Â§4)
  - Idea: Remove all languageâ€‘modeling constraints. Directly treat each query/document as its own learnable vector (â€œfree embeddingsâ€) and optimize them against the target qrels with fullâ€‘batch contrastive loss.
  - Setup:
    - Build a toy world where for `n` documents and `k=2`, include all â€œchooseâ€‘2â€ query sets, i.e., `m = C(n,2)` queries, each requiring two specific documents to be topâ€‘2.
    - Optimize query/document vectors (unitâ€‘normalized after each update) using Adam and full inâ€‘batch negatives with InfoNCE (Â§4; footnote 7). Early stop if no loss improvement for 1,000 steps.
    - Increase `n` gradually for a fixed dimension `d` until the optimizer can no longer reach 100% accuracy. The largest solvable `n` at that `d` is the â€œcriticalâ€‘nâ€ point.
  - Why this matters: If even this unconstrained, testâ€‘setâ€‘optimized procedure fails beyond a certain `n` for dimension `d`, then real embedders (which must encode language, generalize, and use finite data) cannot hope to realize those qrels at that dimension.

- Resulting empirical law (Figure 2; Table 6)
  - The criticalâ€‘n vs `d` curve fits a cubic polynomial:
    > â€œy = âˆ’10.5322 + 4.0309d + 0.0520d^2 + 0.0037d^3 (r^2=0.999)â€ (Figure 2).
  - Extrapolation yields bestâ€‘case limits for k=2:
    > â€œcriticalâ€‘n values (for embedding size): 500k (512), 1.7m (768), 4m (1024), 107m (3072), 250m (4096).â€ (Figure 2 caption text and Â§4 Results)
  - Interpretation: Even in the friendliest setting, finite `d` caps how many distinct topâ€‘2 combinations can be realized.

- LIMIT dataset: a naturalâ€‘language instantiation (Section Â§5.2; Figure 1)
  - Mapping idea:
    - Queries: â€œWho likes X?â€ where `X` is an attribute (e.g., â€œApplesâ€, â€œQuokkasâ€), keeping the query language trivial.
    - Documents: Short profiles like â€œJon Durben likes Quokkas and Apples.â€ Each query has exactly two relevant documents (k=2).
  - Construction details:
    - 1,850 attribute types curated by iterative deâ€‘duplication and overlap checks (Â§5.2).
    - Choose the largest `n` such that â€œn choose 2â€ slightly exceeds 1,000; that is `n=46` (since `C(46,2)=1035`) so every pair of these 46 documents appears as a relevant set to one query (Â§5.2).
    - Two settings:
      - LIMITâ€‘small: just these 46 documents and the 1,000 queries built from all chooseâ€‘2 pairs.
      - LIMITâ€‘full: a 50kâ€‘document corpus where only those 46 are ever relevant; the rest are realistic distractors (Â§5.2).
  - Why this is hard (and realistic):
    - The difficulty stems from how many distinct â€œpairsâ€ across the same 46 items must be representableâ€”high â€œqrel graph density.â€ Table 1 shows LIMITâ€™s Graph Density is 0.085 and Average Query Strength is 28.47, orders of magnitude higher than common IR test sets.

## 4. Key Insights and Innovations
- A tight theoretical bound linking embedding dimension and signâ€‘rank (Fundamental)
  - Proposition 2 pins the minimum dimension to realize a binary relevance pattern to the signâ€‘rank of a derived Â±1 matrix, within Â±1. This is stronger and more operational than prior geometric analogies (orderâ€‘k Voronoi) that lack tight, computable bounds in high dimensions (Â§Â§3.2â€“3.3, Appendix Â§8).

- â€œFree embeddingsâ€ as a constructive tool to probe realizability (Methodological)
  - By directly optimizing vectors on the target qrels, the study offers a practical way to estimate whether a pattern is representable in `d` dimensions and to bound its signâ€‘rank from above (to `d+1`). This bypasses the intractability of exact signâ€‘rank computation (Â§3.3, Â§4).

- An empirical law for capacity collapse: the criticalâ€‘n curve (Empirical insight)
  - The cubic fit between capacity (`n`) and dimension (`d`) in the k=2 setting quantifies where even bestâ€‘case embeddings will start failing (Figure 2; Table 6). This turns a qualitative intuition into a usable predictive model.

- LIMIT: a deceptively simple, naturalâ€‘language benchmark that stressâ€‘tests combination capacity (Benchmark contribution)
  - LIMIT shows that instructionâ€‘following embedders struggle not with reasoning or linguistic nuance, but with the sheer number of topâ€‘k combinations required for even simple â€œWho likes X?â€ queries. Results reveal large gaps versus sparse and multiâ€‘vector systems (Â§5.2â€“Â§5.6; Figures 3â€“6; Tables 3â€“5).

- Diagnosing the root cause: qrel density matters more than domain shift (Diagnostic)
  - Training on inâ€‘domain LIMITâ€‘train barely helps, while training on LIMITâ€‘test allows overfitting to nearâ€‘perfect scores (Table 2; Figure 5). Ablations across qrel patterns show that the â€œdenseâ€ pattern (maximizing combinations) is uniquely hard (Figure 6; Table 3).

## 5. Experimental Analysis
- Evaluation setup (Sections Â§4â€“Â§5)
  - Metrics: Recall@k (e.g., Recall@2, @10, @20, @100) using the MTEB evaluation framework (Â§9).
  - Baselines and systems (Figure 3; Table 5):
    - Singleâ€‘vector embedders spanning 1,024â€“4,096 dims: `GritLM 7B`, `Qwen3 Embedding`, `Promptriever Llama3 8B`, `Gemini Embedding`, `Snowflake Arctic 2.0`, `E5â€‘Mistral`.
    - Alternatives: `BM25` (sparse lexical), `GTEâ€‘ModernColBERT` (multiâ€‘vector).
    - Embedding dimension truncation is tested, including via Matryoshka Representation Learning (MRL) where available; stars in figures denote models trained with MRL (Â§5.2, Figure 3 caption).
  - LIMIT datasets:
    - LIMITâ€‘full: 50k documents, 1k queries, k=2 (Â§5.2).
    - LIMITâ€‘small: 46 documents (every pair queried), 1k queries, k=2 (Â§5.2).
  - Freeâ€‘embedding optimization:
    - Allâ€‘combinations setup with k=2; increment `n` until failure at 100% accuracy; InfoNCE loss; fullâ€‘batch negatives; normalized vectors; Adam; early stopping (Â§4).

- Main quantitative findings
  - Capacity limit law (Figure 2; Table 6)
    > â€œy = âˆ’10.5322 + 4.0309d + 0.0520d^2 + 0.0037d^3 (r^2=0.999).â€  
    > â€œcriticalâ€‘n â€¦ 500k (512), 1.7m (768), 4m (1024), 107m (3072), 250m (4096).â€
    - This is the bestâ€‘case capacity when the model can directly optimize query and document vectors for the test qrels.

  - LIMITâ€‘full results (Figure 3; Table 5)
    - Singleâ€‘vector embedders struggle to even surface the two relevant docs among 50k:
      > `Promptriever Llama3 8B` at 4096 dims: Recall@100 = 18.9; `GritLM 7B` at 4096 dims: Recall@100 = 12.9; `E5â€‘Mistral 7B` at 4096 dims: Recall@100 = 8.3 (Table 5).
      > The caption emphasizes â€œmodels perform poorly, scoring less than 20 recall@100â€ (Figure 1).
    - Alternatives fare better:
      > `GTEâ€‘ModernColBERT`: Recall@100 = 54.8; `BM25`: Recall@100 = 93.6 (Table 5).
    - Trend: Performance improves with dimension but remains far from acceptable at scale (Figure 3 shows monotonic gains with dimension for most embedders).

  - LIMITâ€‘small results (46 docs; Figure 4; Table 4)
    - Even with tiny `n=46`, singleâ€‘vector embedders cannot perfectly realize all 1,035 chooseâ€‘2 combinations:
      > `Promptriever Llama3 8B` at 4096 dims: Recall@2 = 54.3; Recall@20 = 97.7 (Table 4).
      > `GTEâ€‘ModernColBERT`: Recall@2 = 83.5; Recall@20 = 99.1 (Table 4).
      > `BM25`: achieves 100.0 Recall@10 and Recall@20 (Table 4).
    - Takeaway: The difficulty is not only massive corpus size; even small, dense combination spaces stress singleâ€‘vector capacity.

  - Domainâ€‘shift check via finetuning (Figure 5; Table 2)
    - Training a modern embedder on LIMITâ€‘train hardly helps:
      > Best Recall@10 on `Train` split is 2.8 (1024 dims), while most settings are <1.0 (Table 2).
    - Overfitting on LIMITâ€‘test succeeds (as expected if capacity exists for that exact matrix):
      > On `Test` split, Recall@10 > 98 for embedding dims as small as 32 (Table 2).
    - Interpretation: The failure is not due to vocabulary/domain mismatch; it is the combination density that matters (Â§5.3).

  - Qrelâ€‘pattern ablations (Figure 6; Table 3)
    - When the 1k queries are sampled to form â€œRandomâ€, â€œCycleâ€, or â€œDisjointâ€ patterns, scores rise substantially vs â€œDenseâ€ (maximizing unique pairs):
      > `E5â€‘Mistral 7B` (4096 dims): Recall@100 = 40.4 (Random) vs 4.8 (Dense).  
      > `GritLM 7B` (4096 dims): 61.8 (Random) vs 10.4 (Dense).  
      > `Promptriever` (4096 dims): 62.0 (Random) vs 19.4 (Dense). (Table 3)
    - Conclusion: The dominant difficulty is the number of distinct topâ€‘k combinations that must be realized, not linguistic complexity.

  - Dataset density metrics (Appendix Â§10; Table 1)
    > LIMIT Graph Density = 0.0855 and Average Query Strength = 28.47, while standard IR sets have nearâ€‘zero density/strength (e.g., HotpotQA density 0.000037; average strength 0.1104).
    - This quantifies why LIMIT stresses embedding capacity: many queries share and recombine document pairs.

  - Crossâ€‘encoder sanity check (Section Â§5.6)
    - A longâ€‘context reranker (`Geminiâ€‘2.5â€‘Pro`) can solve LIMITâ€‘small perfectly when given all 46 docs and all 1,000 queries in a single pass:
      > â€œsuccessfully solve (100%) all 1000 queries in one forward passâ€ (Â§5.6).
    - This supports that the task itself is trivial semantically; it is the singleâ€‘vector constraint that bites.

  - BEIR vs LIMIT (Figure 7; Table 7)
    > `Qwen3 Embedding`: BEIR = 62.76 vs LIMIT R@100 = 4.8;  
    > `Promptriever`: BEIR = 56.40 vs LIMIT R@100 = 18.9.  
    - There is no clear correlation, indicating that standard IR benchmarks do not expose this capacity limit (Â§5.5).

- Do the experiments support the claims?
  - Yes. The theory predicts capacity limits governed by dimension; freeâ€‘embedding runs expose the limit as a tight empirical curve; LIMIT demonstrates the functional consequence with natural language; density ablations and domainâ€‘shift tests isolate the cause to combination density, not language.

## 6. Limitations and Trade-offs
- Scope of theory
  - The proofs target singleâ€‘vector models with dotâ€‘product scoring and binary relevance; they do not cover multiâ€‘vector architectures (e.g., ColBERTâ€™s MaxSim) or crossâ€‘encoders (Â§Limitations).
  - The results address exact separability/ordering; approximate retrieval with tolerated errors is not bounded here (cited as future work; see also Benâ€‘David et al. 2002).

- Computing signâ€‘rank
  - Exact signâ€‘rank is difficult to compute; the paper instead brackets it via Proposition 2 and a constructive upper bound from freeâ€‘embedding realizability (Â§3.3). This gives guidance but not closedâ€‘form answers for arbitrary qrels.

- Dataset construction choices
  - LIMIT is synthetic in structure (likesâ€‘attributes mapping) though expressed in fluent natural language (Â§5.2). While this isolates the combination phenomenon, it does not measure other retrieval skills (e.g., multiâ€‘hop reasoning beyond set membership).

- Computational tradeâ€‘offs
  - Crossâ€‘encoders solve LIMITâ€‘small but are too expensive for firstâ€‘stage retrieval at web scale (Â§5.6). Multiâ€‘vector and sparse models perform better but come with indexing and storage costs, and unclear transfer to instructionâ€‘following reasoning tasks (Â§5.6).

- What is not addressed
  - Which specific combinations fail for a given model/dimension remains uncharacterized; the theory certifies the existence of failures but does not enumerate them (Â§Limitations).
  - Triangle inequalityâ€“based arguments do not apply for cosine similarity (nonâ€‘metric), so they cannot provide alternative bounds (Â§7).

## 7. Implications and Future Directions
- How this changes the landscape
  - Embedding dimension is not merely a performance knob; it is a hard capacity bound on the set of topâ€‘k combinations a singleâ€‘vector retriever can realize. As benchmarks move toward instructionâ€‘following and compositional querying, these bounds will be hit unless architectures evolve (Â§1, Â§5.6).

- Practical guidance
  - For systems that must handle many recombinations of a small set (e.g., catalog filters, attribute queries, facet combinations), singleâ€‘vector embedders may be insufficient even with large `d`. Consider:
    - Multiâ€‘vector retrievers (e.g., ColBERT variants), which showed large gains on LIMIT (Table 5).
    - Sparse retrieval or hybrid dense+sparse; `BM25` nearly solved LIMITâ€‘full (Table 5).
    - Crossâ€‘encoder reranking as a second stage, which trivially solved LIMITâ€‘small (Â§5.6).
  - When designing evaluations, include dense combination patterns; metrics like qrel Graph Density and Average Query Strength (Appendix Â§10; Table 1) help quantify stress on embedding capacity.

- Research directions
  - Theory: Extend signâ€‘rankâ€“style bounds to multiâ€‘vector architectures and to approximate retrieval where some errors are allowed (Â§Limitations).
  - Learning: Develop training strategies that better allocate dimensional capacity across many independent combination axes, or adaptive perâ€‘query subspaces.
  - Indexing: Explore hybrid and learned sparse methods that retain high effective dimensionality while supporting instruction following.
  - Benchmarking: Build more datasets in the style of LIMIT that vary qrel density systematically (Figure 6), to chart capability frontiers and drive architectural innovation.

> Core takeaway: A single dotâ€‘product vector has finite â€œcombination bandwidth.â€ Proposition 2 makes that precise via signâ€‘rank; the freeâ€‘embedding curve shows where capacity collapses; and LIMIT proves that even todayâ€™s strongest embedders fail on a conceptually trivialâ€”but combinationâ€‘denseâ€”task.

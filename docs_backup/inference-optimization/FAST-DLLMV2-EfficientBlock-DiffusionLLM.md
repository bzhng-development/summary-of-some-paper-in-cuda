# FAST-DLLM V2: Efficient Block-Diffusion LLM

**ArXiv:** [2509.26328](https://arxiv.org/abs/2509.26328)

## ðŸŽ¯ Pitch

FAST-DLLM v2 introduces a novel framework that efficiently adapts any pretrained autoregressive (AR) large language model into a block-diffusion generator, enabling multiple tokens to be generated in parallel per stepâ€”dramatically accelerating inference. By leveraging an innovative block-wise attention and hierarchical caching scheme, the method achieves up to 2.5Ã— faster decoding with virtually no loss in quality, all while requiring only around 1 billion fine-tuning tokensâ€”transforming the practicality of deploying fast, accurate LLMs in real-world, latency-sensitive applications.

---

## 1. Executive Summary (2-3 sentences)
Fast-dLLM v2 converts any pretrained autoregressive (AR) large language model into a block-diffusion generator that can finalize multiple tokens in parallel, while keeping AR-like quality. By combining a block-wise attention/masking design with a hierarchical cache (across blocks and within partially decoded blocks), it achieves up to 2.5Ã— faster decoding without retraining from scratch, using roughly 1â€“3B fine-tuning tokens (Figures 1 and 4; Appendix A.1).

## 2. Context and Motivation
- Problem addressed
  - AR LLMs generate one token at a time; this strictly sequential loop limits inference speed and underutilizes parallel hardware (Introduction).
  - Diffusion LLMs (dLLMs) can update many tokens at once, but typical bidirectional attention breaks standard KV caching, increases latency, and often imposes fixed or awkward sequence-length constraints (Introduction; Related Work Â§2.1â€“2.3).
- Why it matters
  - Real-time and high-throughput applications (chat, agents, coding assistants) need lower latency and better parallel scaling. A method that preserves AR modelsâ€™ quality while accelerating decoding is practically significant.
- Prior approaches and gaps
  - Full-attention dLLMs such as Dream require very large adaptation datasets (e.g., ~500B tokens) and still struggle to beat AR speed due to cache incompatibilities (Abstract; Â§2.1).
  - Training-free accelerations (e.g., Fast-dLLMâ€™s DualCache) improve cache reuse but do not fully resolve dLLM/KV-cache incompatibility (Introduction; Â§2.3).
  - Block-diffusion ideas (e.g., BD3-LM) show promise but were not demonstrated at modern LLM scale or on comprehensive benchmarks (Introduction; Related Work Â§2.2).
- Positioning
  - Fast-dLLM v2 operates between AR and diffusion: it generates in blocks (AR across blocks, diffusion within a block). It is explicitly designed to be AR-friendly so a strong AR model can be adapted with limited fine-tuning while enabling true cache reuse (Abstract; Â§3.2â€“3.3; Appendix A.2).

## 3. Technical Approach
Step-by-step overview of how Fast-dLLM v2 makes diffusion-style parallel decoding compatible with AR models.

- Core concepts (defined on first use)
  - Block diffusion: split a sequence into fixed-size blocks of D tokens; decode left-to-right across blocks (causal), while within the current block allow bidirectional refinement of masked tokens in parallel (Â§3.2; Figure 2).
  - Complementary masking: for each training sample, create two masked â€œviewsâ€ using a random mask m and its complement 1âˆ’m so every token is masked in exactly one view and unmasked in the other (Â§3.2; Figure 2).
  - Shifted-label prediction (â€œtoken shiftâ€): predict a masked token at position i using the hidden state at iâˆ’1, preserving the AR next-token prediction interface (Â§3.2, â€œToken shift for predictionâ€).
  - KV cache: the keyâ€“value representations stored per token to avoid recomputing attention over previously processed tokens. Standard in AR LLMs; often incompatible with bidirectional attention.
  - DualCache: a cache design that keeps both prefix and suffix KV for partially decoded spans so iterative refinement can reuse earlier computation (Â§3.3; Figure 3; Related Work Â§2.3).
  - Sub-block: a smaller slice within a block used during inference to refine tokens in parallel while controlling granularity and cache behavior (Â§3.3; Figure 3).

- Data pipeline and block packing (how training batches are built)
  - Pad each sequence so its length is a multiple of block size `D` using `[MASK]` tokens; these padding tokens are excluded from the loss (Â§3.2 â€œBlock-wise organizationâ€). This alignment prevents cross-sample attention leakage when sequences are packed and split into fixed-length training windows of size `L` (also Appendix A.1).
  - Each packed sequence has `B = L / D` non-overlapping blocks (Figure 2).

- Training-time masking and prediction (how diffusion-like learning is injected)
  - For each block, randomly mask some positions (mask vector `m âˆˆ {0,1}^D`), and duplicate the sample with the complementary mask `1âˆ’m` so that across the two views every token is masked exactly once (Figure 2; Â§3.2 â€œMasked token prediction with complementary viewsâ€).
  - Loss is computed only on masked tokens; a masked position `i` is predicted using the hidden state at `iâˆ’1` (shifted-label), keeping the AR-style predictor but feeding it a bidirectionally refined within-block context (Â§3.2 â€œToken shift for predictionâ€).
  - Training objective (masked-token cross-entropy). In Appendix A.3, the formulation drops the usual normalization by â€œnumber of masked tokensâ€ because complementary masks guarantee that, when both views are included, exactly `L` tokens per sample contribute to the loss. Intuitively, both halves together cover the full sequence.

- Attention pattern that bridges diffusion and AR (why cache works and quality is preserved)
  - During training, concatenate the noised sequence `x_t` and the clean targets `x_0` into a length-`2L` input and apply a structured attention mask `M_full` (Appendix A.2; Figure 7a):
    - `Block Diagonal` on `x_t`: full bidirectional attention inside each block for within-block denoising.
    - `Offset Block Causal` from `x_t` to `x_0`: each noised block attends to all clean tokens in earlier blocks (causal across blocks).
    - `Block Causal` on `x_0`: standard left-to-right attention for targets, preserving AR semantics.
  - At inference, only the current noised block is computed; all previously decoded blocks of `x_0` are cached and used as read-only causal prefix (Appendix A.2; Figure 7b). This yields exact, AR-style KV caching across blocks.

- Inference pipeline (how speedup is realized)
  - Decode block-by-block with KV cache reuse for previous blocks (Â§3.3 â€œBlock-wise autoregressive decoding with cachingâ€; Figure 3).
  - Parallel refinement within the current block:
    - Iteratively unmask multiple positions at once if their confidence exceeds a thresholdâ€”this is the â€œconfidence-aware parallel decodingâ€ borrowed from Fast-dLLM (Â§3.3 â€œParallel refinement within each blockâ€).
    - The threshold controls how aggressively tokens are finalized: lower thresholds mean more tokens per step (faster) but can risk quality (Figure 4).
  - Sub-block DualCache:
    - Within a block, maintain KV for both already-decoded (prefix) and still-masked (suffix) parts so that partial reveals do not trigger full recomputation (Â§3.3 â€œDualCache for sub-block reuseâ€; Figure 3).
    - Sub-block size is an inference hyperparameter; it governs the granularity of refinement and cache reuse (Table 3; Figure 6).

- Implementation specifics
  - Adaptation targets: Qwen2.5 Instruct (1.5B and 7B) (Â§4.1).
  - Training budget: context length 2048, batch 256, 64Ã—A100 GPUs; 1.5B trained 6k steps (â‰ˆ3.15B tokens), 7B trained 2.5k steps (â‰ˆ1.31B tokens) (Appendix A.1).
  - Default inference unless noted: block size 32, sub-block size 8, parallel decoding disabled (threshold=1) (Appendix A.4).

## 4. Key Insights and Innovations
- AR-compatible block diffusion without full retraining
  - Whatâ€™s new: a training recipe that keeps AR next-token alignment (shifted-label), adds complementary masks (to ensure per-token supervision), and uses a hybrid attention mask that is causal across blocks but bidirectional within a block (Figure 2; Appendix A.2).
  - Why it matters: enables post-training adaptation from a strong AR LLM using â‰ˆ1â€“3B tokens rather than hundreds of billions (Abstract; Appendix A.1), preserving much of the original modelâ€™s representation quality.

- Hierarchical caching that works with diffusion-style refinement
  - Whatâ€™s new: exact KV caching across blocks (because cross-block attention remains causal) plus a sub-block DualCache to avoid recomputation within the current block (Figure 3; Â§3.3).
  - Why it matters: this addresses a core reason dLLMs are slowâ€”their bidirectional attention usually breaks cache reuseâ€”so practical speedups materialize on real hardware (Figures 1, 4, 5; Figure 6b).

- Confidence-aware parallel decoding integrated with block diffusion
  - Whatâ€™s new: finalize multiple masked tokens per refinement step based on confidence, trading a small accuracy change for large throughput gains (Figure 4; Â§3.3).
  - Why it matters: achieves up to 2.6Ã— speedup at threshold 0.9 on GSM8K with minimal accuracy drop (Figure 4), and scales well with batch size and newer GPUs (Figure 5).

- Sub-block strategy that decouples training-time block size from inference-time granularity
  - Whatâ€™s new: keep the training block size fixed (for compatibility) but vary sub-block size at inference to tune speed/quality. This avoids the accuracy loss seen when changing the actual block size at inference (Table 3 vs. Table 4).
  - Why it matters: gives deployment flexibility (latency vs. accuracy) without violating the modelâ€™s learned attention structure.

## 5. Experimental Analysis
- Evaluation setup
  - Models: adapted Qwen2.5-Instruct at 1.5B and 7B parameters (Â§4.1).
  - Datasets/tasks: HumanEval, MBPP (code via EvalPlus), GSM8K, MATH (math), MMLU, GPQA (knowledge), IFEval (instruction following) (Â§4.1).
  - Protocol: mostly zero-shot, GPQA 5-shot; greedy decoding; default inference settings noted in Appendix A.4.

- Main quantitative results
  - End-to-end speed vs. quality
    - Throughput vs GSM8K accuracy (A100): Figure 1a shows Fast-dLLM v2 (7B) provides â€œ2.54Ã— Fasterâ€ throughput than `Qwen2.5-7B-Instruct` at comparable GSM8K accuracy. It also improves GSM8K accuracy by +5.2% over a prior Fast-dLLM variant based on LLaDA.
    - Batch-size scaling: Figure 1b (A100) shows throughput at batch=4 reaching 217.5 tokens/s for Fast-dLLM v2 vs. 102.5 for `Qwen2.5-7B-Instruct`.
    - GPU scaling: Figure 5 shows diffusion decoding outperforms AR across batch sizes on both A100 and H100, peaking at ~1.5Ã— speedup (A100) and ~1.8Ã— (H100) at large batch sizes.
    - Confidence threshold trade-off: Figure 4 (GSM8K) shows throughput rising from 39.1 to 101.7 tokens/s (â‰ˆ2.6Ã—) when lowering the threshold to 0.9, with only a small accuracy change (plot shows a mild drop from the non-parallel baseline).
  - Accuracy across benchmarks (Table 1)
    - 1.5B scale: Fast-dLLM v2 averages 45.0, slightly higher than `Qwen2.5-1.5B` (44.3) and `Qwen2.5-1.5B-Nemo-FT` (44.3). Gains appear on GSM8K (62.0 vs 57.0) and HumanEval/HumanEval+ (43.9/40.2 vs 42.1/37.2).
    - 7B scale: Fast-dLLM v2 averages 60.3, higher than `Qwen2.5-7B` (58.2), `Qwen2.5-7B-Nemo-FT` (59.6), and `Dream 7B` (57.6). Notable per-task patterns:
      - Stronger on code: HumanEval Base/Plus 63.4/58.5 vs `Qwen2.5-7B` 51.2/47.6.
      - Stronger on GSM8K: 83.7 vs 71.4.
      - Weaker on MMLU and MATH: 61.4 MMLU vs 70.8; 61.6 MATH vs 73.3.
    - Takeaway: the â€œAvg.â€ metric favors Fast-dLLM v2, but performance is task-dependent: large gains on reasoning/code, drops on some knowledge-heavy or formal math tasks.

- Ablations and robustness
  - Training recipe ablation (Table 2):
    - â€œNaive token shiftâ€ â†’ Avg. 41.3.
    - Adding padding to align blocks (â€œ+ padâ€) avoids cross-sample leakage and lifts Avg. to 42.2.
    - Adding complementary masks (â€œ+ pad + CMâ€) yields the best Avg. 45.0, improving GSM8K to 62.0 and code metrics as well.
  - Inference granularity (Tables 3 and 4; Figure 6):
    - Sub-block size: best average around 8 (Table 3). Larger sub-block sizes increase throughput (Figure 6b) but slightly reduce accuracy (Figure 6a).
    - Do not change the trained block size at inference: mismatching block sizes hurts accuracy (Table 4), e.g., GSM8K drops from 62.0 (with sub-block 8) to 58.5 when using block size 8 at inference.
  - Caching effects (Figure 6b):
    - Sub-block cache gives negligible gains at very small batch sizes (bandwidth underutilized) but significant speedups under compute-bound settings (e.g., batch=32). Accuracy is unaffected (Figure 6a).

- Do the experiments support the claims?
  - Speed claims are convincingly supported across figures (1, 4, 5, 6b), with consistent gains and clear knobs (threshold, sub-block size/cache) to trade accuracy vs. speed.
  - Quality claims are mixed by task: overall average is strong (Table 1), but MMLU and MATH degrade compared to the AR baseline at 7B. The paperâ€™s narrative emphasizes average parity/superiority; the task-level breakdown reveals where diffusion-style decoding helps most (code, GSM8K) and where it lags (knowledge exams, formal math).

## 6. Limitations and Trade-offs
- Task-dependent accuracy
  - Clear regressions on MMLU and MATH at 7B (Table 1: âˆ’9.4 and âˆ’11.7 points vs `Qwen2.5-7B`). Gains in code and GSM8K dominate the average but may not suit knowledge-intensive deployments.
- Block structure constraints
  - Training and inference rely on fixed block size alignment; mismatching block size at inference hurts performance (Table 4). Sequences must be padded to multiples of `D`, which can add compute/memory overhead (Â§3.2; Â§3.3).
- Training memory/computation
  - The training-time attention concatenates `x_t` and `x_0` (length 2L) with a specialized mask (Appendix A.2; Figure 7a). This doubles the effective sequence length per pass, requiring efficient implementations (they use flex-attention) and substantial GPU memory.
- Data requirement nuance
  - While the headline is â€œ~1B tokens,â€ Appendix A.1 reports â‰ˆ1.31B tokens for the 7B model and â‰ˆ3.15B for the 1.5B model. This is far less than Dreamâ€™s ~500B but is still a non-trivial post-training cost.
- Tuning knobs influence outcomes
  - Speed/accuracy depends on the confidence threshold and sub-block size. Aggressive settings (low threshold, large sub-blocks) increase throughput but can degrade accuracy (Figure 4; Figure 6a).
- Scope
  - Demonstrated on Qwen2.5-Instruct (1.5B, 7B) with context length 2048 (Appendix A.1). Behavior on much longer contexts, other architectures, or multilingual settings is not reported.

## 7. Implications and Future Directions
- How this changes the landscape
  - Fast-dLLM v2 shows a practical bridge between AR and diffusion generation: keep AR semantics where they help (cross-block causality, exact KV cache) and use diffusion-style parallel refinement where it speeds decoding. This makes dLLMs genuinely competitive on end-to-end latency.
- Follow-up research enabled or suggested
  - Task-specific tuning of sub-block size and confidence thresholds to balance speed and accuracy.
  - Extending the attention-mask design to longer contexts and retrieval-augmented setups; studying how block boundaries interact with retrieval chunks.
  - Improving knowledge-heavy performance (MMLU/MATH) via hybrid training curricula (e.g., mixing NTP and masked losses), or selective AR vs. diffusion decoding based on uncertainty.
  - Theoretical analysis of when complementary masking plus shifted-label best preserves AR representations.
  - Hardware-aware schedulers that adapt sub-block sizes at runtime based on batch size and GPU utilization (building on Figure 5 and Figure 6b trends).
- Practical applications
  - Latency-sensitive assistants and agents that need high throughput responses (chat, customer support, coding co-pilots).
  - Batch serving scenarios where H100-class GPUs can realize 1.5â€“1.8Ã— higher throughput at scale (Figure 5).
  - Edge or cost-constrained deployments where reducing decoding steps translates directly into lower inference cost.

> Headline results to remember:
> - â€œ2.54Ã— Fasterâ€ throughput at comparable GSM8K accuracy vs. `Qwen2.5-7B-Instruct` on A100 (Figure 1a).
> - Up to â€œ2.6Ã— Speedupâ€ on GSM8K at threshold 0.9 with minimal accuracy change (Figure 4).
> - 7B model averages 60.3 across tasks, outperforming `Qwen2.5-7B` (58.2) and `Dream 7B` (57.6), while trading off some MMLU/MATH for stronger code and GSM8K (Table 1).

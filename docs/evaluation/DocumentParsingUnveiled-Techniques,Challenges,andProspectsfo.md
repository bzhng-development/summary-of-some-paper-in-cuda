# Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Data Extraction

**ArXiv:** [2410.21169](https://arxiv.org/abs/2410.21169)

## ğŸ¯ Pitch

This comprehensive survey unifies the fragmented landscape of document parsing by systematically comparing modular pipeline systems and end-to-end vision-language models, spanning layout analysis, OCR, mathematical expression understanding, table and chart extraction, evaluation, and tools. By critically examining both methodologies and outlining their strengths, weaknesses, and future research frontiers, the paper delivers an essential and up-to-date roadmap for researchers and practitioners driving automation in knowledge extraction, search, and generative AI from complex real-world documents.

---

## 1. Executive Summary
This survey organizes the entire field of document parsing into two endâ€‘toâ€‘end strategiesâ€”modular pipelines and large visionâ€‘language models (VLMs)â€”and then walks through every major component: layout analysis, OCR, math expression understanding, table and chart parsing, evaluation metrics, and tools. Its primary significance is a unifying, upâ€‘toâ€‘date map of methods, datasets, and metrics, plus a careful discussion of where pipeline systems and VLMs succeed or fail in complex, realâ€‘world documents (Sections 2â€“9; Figures 1â€“7; Tables 1â€“10; Appendix 11).

## 2. Context and Motivation
- Problem/gap addressed
  - Extracting structured, machineâ€‘readable data (e.g., Markdown, JSON, LaTeX) from unstructured or semiâ€‘structured documents (PDFs, scans) is brittle in practice because documents include dense text, tables, formulas, charts, and complex reading orders. Section 1 frames this as â€œdocument parsing (DP),â€ which converts visual content into structured representations while preserving relationships among elements like text, equations, tables, images, and reading order.
  - Existing surveys are either dated or limited to subareas (layout, math, tables, charts). This survey aims to be holistic and current (Section 1: â€œlimitationsâ€¦ highâ€‘quality reviews often focus on specific subâ€‘technologiesâ€).
- Why it matters
  - Realâ€‘world impact: Parsed content fuels search and retrieval, RAG (retrievalâ€‘augmented generation), knowledge base construction, and training/evaluation of multiâ€‘modal models (Section 1; Section 2.1.2; Section 9).
  - Theoretical significance: Document parsing combines layout structure, language semantics, and visual signalsâ€”an ideal setting to study multiâ€‘modal representation learning (Sections 3 and 7).
- Prior approaches and their shortfalls
  - Modular pipelines (layout â†’ OCR â†’ tables/math/charts â†’ relation integration) are precise but fragile: error propagation, complex module orchestration, ruleâ€‘heavy readingâ€‘order logic, and poor generalization to varied layouts (Section 9, â€œChallengesâ€¦ Pipelineâ€‘Based Systemsâ€).
  - Generalâ€‘purpose VLMs understand images but struggle with dense, highâ€‘resolution, textâ€‘heavy pages and long multiâ€‘page documents (Section 7.1; Section 9).
- Positioning relative to existing work
  - The paper proposes a taxonomy (Figures 1â€“2) spanning:
    - Modular systems: layout analysis (physical and logical), OCR (detection/recognition/spotting), math detection/recognition, table detection/structure recognition, chart tasks, and relation integration (Sections 3â€“6; Figure 2).
    - Endâ€‘toâ€‘end VLMs and specialized document parsers (Nougat, Donut, Fox, Vary, OmniParser, GOT) with their training strategies and tradeâ€‘offs (Section 7).
  - It consolidates datasets and metrics across all components (Appendix 11; Tables 2â€“10), and catalogs openâ€‘source tools (Section 8; Table 1).

## 3. Technical Approach
This is a survey, so the â€œapproachâ€ is a structured decomposition of how document parsing systems work, not a single new algorithm. The paperâ€™s framework (Figures 1â€“2) divides the space into modular pipelines and endâ€‘toâ€‘end VLMs, then drills into how each module or model class operates.

A) Modular pipeline systems (Figure 2; Sections 2.1, 3â€“6)
1) `Document Layout Analysis (DLA)` (Section 3; Figure 3)
   - Goal: Identify elements (paragraphs, images, tables, formulas, headers/footers) with coordinates and reading order; also distinguish â€œphysicalâ€ layout (bounding boxes) from â€œlogicalâ€ semantic roles (title, caption).
   - Approaches and mechanics:
     - CNNâ€‘based detectors (Section 3.1.1): Adapt object detectors like Râ€‘CNN/Mask Râ€‘CNN/YOLO to page objects; FCNs for segmentation of regions. Example: `DocLayoutâ€‘YOLO` augments YOLOv10 with a globalâ€‘toâ€‘local receptive module to catch elements at multiple scales.
     - Transformerâ€‘based models (Section 3.1.2): `BEiT`/`DiT` split pages into patches and use selfâ€‘attention to capture global structure; strong features but computationally heavy.
     - Graphâ€‘based models (Section 3.1.3): Build a graph where nodes are detected regions and edges encode spatial/semantic proximity; run GCNs to refine types and relationships (`Docâ€‘GCN`, `GLAM`).
     - Gridâ€‘based models (Section 3.1.4): Rasterize the page into a 2D token grid (e.g., `BERTGrid`, `VGT`) to keep spatial structure explicit; useful but often large/slow.
     - Integrating semantics (Section 3.2): Pretrained multiâ€‘modal Transformers like `LayoutLM/v2/v3` fuse text, 2D positions, and image features via masking and crossâ€‘modal attention; `UniDoc` aligns ResNet visual features with Transformer text features via gated crossâ€‘attention. These target â€œlogicalâ€ layout (roles, hierarchy), not just boxes.

2) `Optical Character Recognition (OCR)` (Section 4; Figure 4)
   - Text detection (Section 4.1): Four families
     - Singleâ€‘stage regression: predict oriented boxes directly (e.g., `TextBoxes++`, `SegLink`, `DRRG`).
     - Twoâ€‘stage proposal: adapt Faster Râ€‘CNNâ€‘like pipelines to text proposals for arbitrarily oriented text.
     - Segmentationâ€‘based: perâ€‘pixel text masks (`CRAFT`, `PixelLink`, SPCNet).
     - Hybrids: combine regression with segmentation and improved NMS or attention (`EAST`, `CentripetalText`).
   - Text recognition (Section 4.2):
     - Visionâ€‘only encoders (CNN or ViT) with either `CTC` decoding (align free) or `seq2seq` decoders with attention.
     - Handling irregular/curved text: input rectification (`STN`, `MORAN`, `ESIR`) or 2D attention decoders (`SATRN`, `ViTSTR`, `TrOCR`).
     - Injecting semantics (Section 4.2.2): 
       - Characterâ€‘level hints (e.g., counting auxiliary tasks in `RFâ€‘L`, sorting/counting in `CDDP`).
       - Dedicated semantic modules (`SRN`â€™s global reasoning; `SEED` between encoder/decoder; `ABINet` iterative language refinement).
       - Pretraining with masked objectives (`VisionLAN`) or degradation robustness (`Textâ€‘DIAE`) to teach context even when pixels are noisy.
   - Text spotting (Section 4.3): Joint detection+recognition
     - Twoâ€‘stage: share a backbone; detect regions, then ROIâ€‘based recognition (Mask TextSpotter v1â€“v3, RoIRotate/BezierAlign variants, GLASS).
     - Oneâ€‘stage: avoid ROIs; directly model characters/words endâ€‘toâ€‘end (CRAFTS; Transformer decoders like `PGNet`, `SPTS`, `TESTR`).

3) `Mathematical expressions` (Section 5; Figure 5)
   - Detection (Section 5.1): 
     - Uâ€‘Netâ€‘style segmentation for inline vs display formulas; object detection variants (`DSâ€‘YOLOv5`, SSD, Faster/Mask Râ€‘CNN); or treat it as entityâ€‘relation extraction (`FormulaDet`) to leverage context.
   - Recognition (Section 5.2):
     - Encoderâ€‘decoder to produce LaTeX/MathML from images: CNN or Transformer encoders; RNN/Transformer decoders with attention (manage nesting and 2D structure).
     - Enhancements: multiâ€‘scale encoders (DenseNet/ResNet), global dependency via Swin Transformer; character/length hints; stroke orders for online handwriting; heavy augmentation.

4) `Tables` (Section 6; Figure 6)
   - Detection (Section 6.1): Fineâ€‘tune generic detectors (YOLO, Faster Râ€‘CNN, Deformable ConvNets) with tableâ€‘specific anchors/features; handle sparsity with modified training (e.g., SparseRâ€‘CNN variants).
   - Structure recognition (Section 6.2):
     - Row/column segmentation: use Transformers (`DETR`, `DQâ€‘DETR`) or Biâ€‘GRU scanning to find separators; then merge cells, sometimes with global attention to predict spanning relations.
     - Cellâ€‘based (bottomâ€‘up): detect individual cells or their keypoints; construct a graph and merge via GNNs; robust to irregular structures.
     - Imageâ€‘toâ€‘sequence: encode the whole table image; decode to HTML/LaTeX/Markdown (dual decoders for structure+content; e.g., `MASTER`, `VAST`).

5) `Charts` (Section 6.3â€“6.5; Figure 7)
   - Tasks: classification of chart types; split multiâ€‘panel composites; detect elements (axes, bars, legends, labels); link text to visual marks; extract data series; parse structures like flowcharts/org charts.
   - Methods:
     - Classification: CNNs, then stronger Vision Transformers; Swin Transformer fineâ€‘tuning leads current accuracy (Section 6.4).
     - Detection and textâ€‘element linking: Fasterâ€‘Râ€‘CNN/YOLO for visual elements; OCR for text; transformerâ€‘based methods for correlating labels to marks.
     - Structure extraction: DETRâ€‘style models for nodes+edges (`FRâ€‘DETR`) to parse diagrams with linking lines.

6) `Relation integration` (Section 2.1.3)
   - Combine outputs from all modules into a single document representation (Markdown/JSON/LaTeX), preserving spatial order and semantics. Reading order may use rules or specialized models. Figure 2 shows an explicit â€œIntegrate Allâ€ step using bounding boxes and types to rebuild the page.

B) Endâ€‘toâ€‘end, VLMâ€‘driven systems (Section 2.2 and Section 7)
- Early general VLMs (`Qwenâ€‘VL`, `InternVL`) handle image+text but miss fineâ€‘grained OCR and complex structures (Section 7.1).
- Specialized models:
  - `Donut`/`Nougat` (Section 7.2): Swin encoder + seq2seq decoder trained to emit Markdown with embedded LaTeX; excels on scientific PDFs without modular OCR, but slower and weaker on nonâ€‘Latin scripts.
  - `Vary` (Section 7.2): Enlarged visual vocabulary and SAMâ€‘style tokens to better handle charts and OCR on highâ€‘res pages without splitting.
  - `Fox` (Section 7.3): Multiâ€‘page understanding with multiple pretrained visual vocabularies (CLIPâ€‘ViT + SAMâ€‘ViT) for fineâ€‘grained crossâ€‘page tasks.
  - `Detectâ€‘Orderâ€‘Construct` (Section 7.3): Treeâ€‘construction for document hierarchyâ€”detect page objects, assign roles, predict reading order, then construct the hierarchical tree.
  - Unified frameworks (Section 7.4): 
    - `OmniParser`: decouple OCR from structural decoding with a twoâ€‘stage decoder; handles text spotting, KIE, and tables in one system.
    - `GOT` (â€œGeneral OCR Theoryâ€): treat all textâ€‘like entities (text, formulas, tables, scores) as objects and train endâ€‘toâ€‘end across scene and document OCR.

C) Evaluation foundations, benchmarks, and tools
- Datasets and metrics across all subâ€‘tasks are consolidated in Appendix 11 (Tables 2â€“10; Section 11.2).
- Openâ€‘source tools summarized in Table 1 (Section 8), spanning OCR engines (Tesseract, PaddleOCR), PDF parsers (PyMuPDF, pdfplumber), conversion systems (MinerU, PDFâ€‘Extractâ€‘Kit), and modelâ€‘based parsers (OmniParser).

## 4. Key Insights and Innovations
- A unifying twoâ€‘track taxonomy with explicit module interlocks (Figures 1â€“2; Sections 2â€“6).
  - Whatâ€™s new: A single map that spans physical layout and logical semantics, from detection all the way to reconstruction, while also covering VLM alternatives sideâ€‘byâ€‘side.
  - Why it matters: It helps practitioners decide when to prefer a robust module versus an endâ€‘toâ€‘end model, and how to connect modules cleanly (e.g., reading order and relation integration in Figure 2).
- Clear separation of physical vs logical layoutâ€”and methods that bridge them (Section 3; Figure 3).
  - Different from many older surveys that stop at bounding boxes, this review centers methods that fuse text, vision, and positions (`LayoutLM` family, `UniDoc`, `VSR`), i.e., the â€œlogical layoutâ€ layer needed for titles, captions, and hierarchical roles.
- Endâ€‘toâ€‘end document parsers are not universal replacements yet (Section 7; Section 9).
  - The paper distills the practical limits of OCRâ€‘free or VLMâ€‘only approaches: dense text, formatting fidelity, highâ€‘resolution images, and multiâ€‘page flow still challenge general VLMs. This is a corrective to â€œone model fits all.â€
- Comprehensive, crossâ€‘task evaluation toolbox (Appendix 11).
  - Beyond collecting datasets, the survey foregrounds taskâ€‘specific metrics that practitioners often overlook:
    - `TEDS` and `CAR` for table structure (Appendix 11.2.4).
    - `CDM` for math recognition fairness across LaTeX variants (Appendix 11.2.3).
    - Chartâ€‘specific scoring (strict/relaxed OKS, dataâ€‘series scores) (Appendix 11.2.5).
  - Significance: Better metrics change what models optimize for, especially in structureâ€‘heavy tasks where tokenâ€‘level accuracy alone is misleading.

## 5. Experimental Analysis
This survey does not run new experiments; instead it codifies how the community evaluates document parsing and what data is available. The most â€œquantitativeâ€ contributions are the dataset and metric inventories, with sizes and task coverage.

- Evaluation methodology (Appendix 11.2)
  - Layout: IoU, mAP, F1 for element localization and classification (Table 7).
  - OCR: character error rate (CER), edit distance, BLEU/METEOR/ROUGE for long text fidelity (Table 8).
  - Math: exact expression rate, imageâ€‘based similarity (SSIM/MSE), and `CDM` to avoid penalizing equivalent LaTeX renderings (Table 9).
  - Tables: detection purity/completeness; structure `CAR`, `TEDS`, simplified `Sâ€‘TEDS`, `A_all`, `F_Î²`, and `WAF` for adjacency across IoU thresholds (Table 10).
  - Charts: detection IoU/precision/recall; chartâ€‘type classification; taskâ€‘specific seriesâ€‘matching scores and keypoint similarity (Appendix 11.2.5).

- Datasets (Appendix 11.1)
  - Layout analysis (Table 2): from large synthetic corpora to modern diverse sets.
    - Example numbers: `DocLayNet` 80,863 docs with 11 classes; `M6Doc` 9,080 docs with 74 classes; `DocSynthâ€‘300K` 300,000 synthetic pages.
  - OCR (Table 3): classic sceneâ€‘text to documentâ€‘centric sets.
    - Example numbers: `LOCR` lists 7,000,000 instances across TD/TR/TS; `ICDAR2019â€‘ReCTS` has 25,000 Chinese pages with detection/recognition/structure labels.
  - Math (Table 4): `ArxivFormula` 700,000 pages; `FormulaNet` 46,672 images with ~1,000,000 expressions; `UniMERâ€‘1M` 1,061,791 printed/handwritten images for recognition.
  - Tables (Table 5): endâ€‘toâ€‘end structure benchmarks.
    - `PubTablesâ€‘1M` 1M scientific tables; `TableBank` 417,234; `Wired Table in the Wild` contains deformed and occluded images; `TableGraphâ€‘350K` 358,767; `TabRecSet` 38,100.
  - Charts (Table 6): classification/extraction sets.
    - `UBâ€‘PMC 2019/2020/2022` for classification and extraction; `LINEEX430K` for line charts; `ExcelChart400K` for pie/bar extraction.
  - Multiâ€‘task document sets (Appendix 11.1.6):
    - `Readoc` (2,233 PDFâ€“Markdown pairs) for endâ€‘toâ€‘end structure extraction.
    - `OmniDocBench`: â€œ981 PDF pages and 100,000 annotationsâ€¦ 9 document types, 19 layout tags, and 14 attribute tagsâ€ to compare modular and VLM methods.
- Tools (Section 8; Table 1)
  - From OCR engines (Tesseract, PaddleOCR) to highâ€‘level pipelines (`MinerU`, `PDFâ€‘Extractâ€‘Kit`, `OmniParser`), with brief capabilities summarized.

Do the curated evaluations support the claims?
- Yes, for two reasons:
  - The metrics are taskâ€‘appropriate and highlight nuances: e.g., using `TEDS`/`CAR` for tables (structure, not just detection), and `CDM` for math (Appendix 11.2.3â€“11.2.4).
  - The datasets are broad enough to expose known weaknesses: multiâ€‘page, diverse layouts, borderless/nested tables, and both printed/handwritten math (Appendix 11.1).

Whatâ€™s missing (and acknowledged in Section 9)?
- Crossâ€‘model, likeâ€‘forâ€‘like benchmarks on a single protocol are not run here; instead, the paper equips readers to perform them. Failure cases are discussed qualitatively (Section 9), not through ablation studies.

> Section 9 explicitly contrasts pipeline vs VLM failure modes: pipelines suffer from â€œmodular coordination, standardization of outputs, and handling irregular reading orders,â€ while VLMs struggle with â€œhighâ€‘density text, intricate table structures,â€ and â€œrepeated outputs or formatting errorsâ€ in long generations.

## 6. Limitations and Trade-offs
- Assumptions and boundaries of current practice (Section 9)
  - Pipelines assume module interfaces are stable and that reading order can be ruleâ€‘based or heuristically predicted; this breaks in multiâ€‘column, nested, or heavily styled pages.
  - Many OCR modules assume moderate text density and common fonts; complex fonts (bold/italics) and tiny characters degrade accuracy.
  - Math recognition pipelines often assume clean, printed expressions; inline, noisy, screenâ€‘captured, and handwritten expressions remain problematic.
  - Table structure methods often assume clear separators; borderless/nested/multiâ€‘page tables and multiâ€‘line cells remain hard.
  - Chart extraction lacks unified problem definitions and standard evaluation; many systems are partially manual or chartâ€‘typeâ€‘specific.
- Computational and data constraints (Section 7 and Section 9)
  - VLMs:
    - Highâ€‘res documents require tiling or special encoders; naive downscaling erases small text.
    - Many systems freeze LLM backbones; this hampers fineâ€‘grained OCR and leads to repetition/formatting drift on long outputs.
    - Resource heavy: training/inference on dense pages is computationally wasteful without architectural compression or sparse sampling.
  - Pipelines:
    - Error propagation: misâ€‘detections early on (e.g., missing a table region) corrupt downstream structure and final serialization.
    - Domain adaptation: each module may need reâ€‘tuning for new document styles or languages.
- Scenarios not fully addressed (Sections 7, 9)
  - Longâ€‘document, multiâ€‘page coherence and crossâ€‘page entity linking are still emerging (even in `Fox`, Section 7.3).
  - Multilingual and multiâ€‘script robustness is uneven (e.g., `Nougat` struggles on nonâ€‘Latin scripts, Section 7.2).
  - Diagramâ€‘rich documents (posters, manuals, newspapers) are underrepresented in training and evaluation (Section 9).

## 7. Implications and Future Directions
- How this work changes the landscape
  - Provides a complete â€œwiring diagramâ€ for document parsing systems that connects lowâ€‘level vision modules, multiâ€‘modal pretraining, and endâ€‘toâ€‘end VLMs (Figures 1â€“2). Practitioners can make principled buildâ€‘vsâ€‘buy decisions and identify exactly where to insert semantics (Section 3.2) or switch to OCRâ€‘free parsing (Section 7).
- Research enabled or suggested (Sections 7 and 9; Appendix 11)
  - Highâ€‘resolution, longâ€‘document VLMs: compression strategies (e.g., DocOwl2â€‘style highâ€‘res compression), sparse sampling (`WuKong` in refs), and multiâ€‘page memory mechanisms.
  - Reading order learning: move beyond rules to learned sequence prediction that aligns visual, positional, and linguistic signals reliably in complex layouts (Section 2.1.3; Section 9).
  - Structureâ€‘aware decoding: decoders that emit hierarchical representations (trees/graphs) with guaranteed wellâ€‘formedness, not just token streamsâ€”building on `Detectâ€‘Orderâ€‘Construct` (Section 7.3) and table/diagram graph models (Sections 6.2.2 and 6.5.4).
  - Unified, fair metrics: wider adoption and refinement of `TEDS`, `CAR`, `CDM`, and chart OKSâ€‘style measures; new math metrics that reflect semantic equivalence beyond LaTeX strings (Appendix 11.2).
  - Dataset diversification: beyond scientific papers/textbooks to manuals, posters, newspapers; multiâ€‘script and lowâ€‘resource languages (Appendix 11.1; Section 9).
  - Hybrid systems: tight coupling of strong detectors/recognizers with VLM decoders that enforce global coherence and formatting constraintsâ€”e.g., `OmniParser`â€‘style decoupling or `GOT`â€™s objectâ€‘asâ€‘token approach (Section 7.4).
- Practical applications
  - RAG pipelines that rely on faithful structure preservation for chunking and retrieval (Section 1; Section 9).
  - Enterprise ingestion: contracts, invoices, receipts, financial reports with tableâ€‘heavy and mathâ€‘heavy content.
  - Scientific workflows: Markdown/LaTeX export (`Nougat`), chart data extraction for metaâ€‘analysis, and equation databases for math search.
  - Training data engines: automatic generation of structured corpora to train or fineâ€‘tune domainâ€‘specific VLMs (Section 1; Section 7).

In short, this survey is a field guide: it shows exactly how each piece of a document parser works, where VLMs do and donâ€™t help, what data and metrics to use for each subtask, and what hard problems remainâ€”especially reading order, dense text, structure fidelity, and longâ€‘document reasoning.

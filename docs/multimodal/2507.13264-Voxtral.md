# Voxtral

**ArXiv:** [2507.13264](https://arxiv.org/abs/2507.13264)

## üéØ Pitch

Voxtral introduces two open-weight multimodal audio-chat models (Voxtral Mini and Small) that natively ingest spoken audio and text to perform ASR, translation, QA, summarization and long-context reasoning using a Whisper-based audio encoder plus a downsampling adapter and a 32K context window. By adding strong speech understanding and transcription without degrading text capabilities‚Äîand releasing checkpoints and new speech-understanding benchmarks‚ÄîVoxtral makes versatile, privacy-friendly, and long-form audio-aware LLMs practical for real-world multimodal applications.

---

## 1. Executive Summary (2-3 sentences)

Voxtral Mini and Voxtral Small are open-weight multimodal ‚Äúaudio chat‚Äù models that can take **spoken audio and text** as input and produce text outputs, targeting both **speech tasks (ASR/translation/QA/summarization)** and **text-only tasks** within one model (Abstract; Section 1). The paper‚Äôs central significance is showing that strong speech understanding and transcription can be added to an LLM‚Äîusing a Whisper-based audio front-end plus a downsampling adapter‚Äî**without materially degrading** the underlying text model‚Äôs performance, while supporting **long audio up to ~40 minutes** via a `32K` context window and temporal downsampling (Section 1; Section 2.2; Figure 1; Figure 6).

## 2. Context and Motivation

- **Problem / gap addressed**
  - The work targets a practical gap: models that can **converse about audio** (answer questions, summarize, translate, do reasoning) rather than only **transcribe** it, while still remaining strong on **text-only** tasks (Section 1).
  - The paper also identifies an evaluation gap: speech model evaluation is often dominated by transcription/translation benchmarks, with less standardized coverage of **speech reasoning and long-context QA** (Section 1; Section 3.4).

- **Why it matters**
  - Many real audio use cases require more than transcription: e.g., answering questions about long recordings, extracting ‚Äúneedle-in-haystack‚Äù details, summarizing meetings/podcasts, and multilingual comprehension (Section 3.2; Section 3.4).
  - Long recordings are common, so the model must scale to long audio contexts; Voxtral explicitly targets **audio up to ~40 minutes** with a `32K` context (Abstract; Section 1; Section 2.2).

- **Prior approaches and shortcomings (as positioned in the paper)**
  - Prior and contemporary systems include ASR models (e.g., Whisper large-v3) and closed multimodal assistants (e.g., GPT‚Äë4o mini Audio, Gemini 2.5 Flash) used as evaluation comparators (Section 4).
  - The paper‚Äôs critique is less about a specific algorithmic flaw and more about:
    - **Narrow evaluation focus** (transcription/translation-heavy) (Section 1; Section 3.4).
    - The need for models that combine speech understanding with **strong text capabilities** in one model (Section 1; Figure 6).

- **How Voxtral positions itself**
  - As an **open-weights** (Apache 2.0) multimodal audio+text system (Abstract; Section 1; Conclusion).
  - As competitive with selected closed models on speech understanding and strong (and sometimes best among compared models) on speech recognition/translation benchmarks (Section 4.1‚Äì4.3; Figure 3; Figure 4; Table 3; Table 7; Table 8).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- Voxtral is a **speech-and-text understanding assistant**: you can give it an audio file (plus optional text) and it generates a **text response** (Section 1; Figure 1).
- It solves long-audio understanding by combining a **Whisper-based audio encoder**, a **downsampling adapter** to keep sequences short, and an **LLM decoder** with a `32K` context window to handle long audio and multi-turn chat (Section 2; Section 2.2; Figure 1).

### 3.2 Big-picture architecture (diagram in words)

- **Input(s):** raw audio waveform (optionally with text instructions/prompts) (Section 2; Figure 1).
- **Audio encoder (Whisper large‚Äëv3 based):** converts audio ‚Üí high-rate audio embeddings (Section 2.1).
- **Audio-language adapter (MLP downsampler):** reduces embedding sequence length by **4√ó** (chosen via ablation) (Section 2.2; Section 5.2).
- **Language decoder (LLM backbone):** autoregressively predicts text tokens conditioned on (downsampled) audio embeddings + any text context (Section 2.3; Figure 1).
- **Output:** text (transcript, translation, QA answer, summary, etc.) (Sections 3‚Äì4).

### 3.3 Roadmap for the deep dive

- Explain the **audio encoder** first, because it determines what information from raw audio is available (Section 2.1).
- Then explain the **adapter/downsampling**, because it is key to scaling to long audio within `32K` context (Section 2.2; Section 5.2).
- Next cover the **LLM decoder variants** (Mini vs Small) to clarify capacity differences (Section 2.3; Table 1).
- Then walk through the **3-stage training recipe** (pretraining ‚Üí SFT ‚Üí preference alignment) because behavior depends strongly on training patterns and alignment (Section 3.1‚Äì3.3).
- Finally cover **evaluation design**, including the new benchmarks and the internal SU judging scheme, because results depend on these protocols (Section 3.4; Tables 2, 8).

### 3.4 Detailed, sentence-based technical breakdown

This is primarily an **empirical system and training-method paper**: it builds a multimodal architecture (Whisper encoder + adapter + LLM decoder) and demonstrates that specific pretraining patterns, downsampling, and alignment choices yield strong speech transcription and speech-understanding performance (Sections 2‚Äì5).

#### 3.4.1 System/data pipeline diagram in words (what happens first, second, third)

1. **First, audio is featurized.** The model takes a raw waveform and maps it to a `log-Mel spectrogram` with **128 Mel bins** and **160 hop-length** (Section 2.1).  
2. **Second, the Whisper-based encoder embeds the audio.** The spectrogram passes through a convolutional stem that downsamples time by **2√ó**, then through a stack of bidirectional self-attention layers, producing audio embeddings at **50 Hz** (Section 2.1).  
3. **Third, long audio is handled by chunked encoding.** Whisper has a fixed receptive field of **30 seconds**, so Voxtral processes each 30-second chunk **independently**, resets absolute positional encodings per chunk, and batches chunks on the batch axis; chunk embeddings are concatenated afterward (Section 2.1; Figure 1).  
4. **Fourth, embeddings are downsampled for the decoder.** A separate MLP ‚Äúadapter‚Äù downsamples the concatenated audio embeddings by **4√ó**, yielding an effective **12.5 Hz** representation (Section 2.2; Section 5.2).  
5. **Fifth, the LLM decoder generates text.** The language decoder conditions on the downsampled audio embeddings plus any text prompt and autoregressively predicts text tokens (Figure 1; Section 2.3).  
6. **Sixth, special tokens steer behavior.** During pretraining and inference, the model can be prompted with task-indicator tokens like `<repeat>` or `<next>` to choose between transcription-like behavior or cross-modal continuation behavior (Section 3.1; Figure 2), and a dedicated ‚Äútranscribe mode‚Äù token is introduced during SFT to make transcription requests unambiguous without a text prompt (Section 3.2).

#### 3.4.2 Architecture details and design choices

- **Audio encoder choice: Whisper large‚Äëv3 backbone.**
  - Voxtral‚Äôs encoder is ‚Äúbased on Whisper large‚Äëv3‚Äù and keeps its **30-second** receptive field (Section 2.1).
  - The paper emphasizes a chunking strategy for >30s audio: process chunks independently and concatenate embeddings (Section 2.1). It notes this is functionally equivalent to `chunk-wise attention` and is intended to reduce compute overhead and improve length generalization (Section 2.1).

- **Padding policy (short audio):**
  - Whisper pads short audio to 30 seconds; Voxtral investigates removing this constraint but observes a performance decline, so it keeps padding to the **next multiple of 30 seconds** (Section 2.1; Section 5.1; Figure 7).

- **Adapter/downsampling: why it exists and what it does.**
  - Without downsampling, a 30-minute audio at 50 Hz would yield ~90k audio ‚Äútime steps,‚Äù which would be too long for the decoder in memory and latency terms (Section 2.2).
  - Voxtral inserts an **MLP layer** that downsamples embeddings along time (Section 2.2). The selected setting is **4√ó downsampling ‚Üí 12.5 Hz**, which is presented as the best trade-off in ablations (Section 2.2; Section 5.2; Figure 8).
  - The paper‚Äôs length claim ties this to context: 12.5 Hz enables audio up to **~40 minutes** within a `32K` token context window (Section 2.2; Abstract).

- **Language decoder variants (Mini vs Small):**
  - `Voxtral Mini` uses a `Ministral 3B` backbone (Section 2.3) and totals **4.7B** parameters when including the 640M audio encoder, adapter, text embeddings, and decoder (Table 1).
  - `Voxtral Small` uses `Mistral Small 3.1 24B` (Section 2.3) and totals **24.3B** parameters (Table 1).
  - Table 1 breaks out parameters by submodule (Audio Encoder / Audio Adapter / Text Embeddings / Language Decoder) (Table 1).

#### 3.4.3 Training methodology: three phases

The paper trains Voxtral in three phases: **pretraining**, **supervised finetuning (SFT)**, and **preference alignment** (Section 3).

##### (A) Pretraining: two cross-modal patterns + text mixture

- **Data formation step:** audio is chunked into segments with transcripts, producing aligned pairs `(A1, T1), ‚Ä¶, (AN, TN)` where boundaries come from upstream VAD and diarization; missing transcripts are pseudo-labeled with an ASR model (Section 3.1).
- **Pattern 1 ‚Äî audio-to-text repetition (`<repeat>`):**
  - The input contains audio segment `An` followed by its transcript `Tn`, explicitly teaching speech-to-text alignment (Section 3.1; Figure 2).
- **Pattern 2 ‚Äî cross-modal continuation (`<next>`):**
  - The model sees sequences that interleave audio and text such that each audio `An` is followed by the subsequent text segment `T(n+1)`, e.g., `(A1, T2, A3, T4, ‚Ä¶)` (Section 3.1; Figure 2).
  - The intended effect is to build a ‚Äúmodality-invariant context modeling‚Äù behavior that supports discourse continuity and reasoning (Section 3.1).
- **Disambiguation tokens:**
  - Because both ‚Äúrepeat‚Äù and ‚Äúcontinue‚Äù could be valid next-text targets, Voxtral introduces `<repeat>` and `<next>` to specify which pattern is expected (Section 3.1; Figure 2).
- **Pattern balancing:**
  - Voxtral samples the two patterns with **equal probability** during pretraining, motivated by ablations showing each pattern supports different capabilities (Section 3.1; Section 5.3; Figure 9).
- **Warm-up stage (freeze encoder + decoder):**
  - In the first pass over the mixture, the audio encoder and language decoder are frozen and only the adapter is trained; the paper reports this warm-up helps speech understanding evaluations (Section 3.1).
- **Preserving text capability:**
  - Text pretraining data is included in the mixture to preserve text-only performance (Section 3.1).

##### (B) Supervised finetuning (SFT): instruction following for audio and text

- **Goal:** keep or slightly improve transcription established in pretraining while expanding performance on speech understanding tasks, and ensure instruction following whether the user input is audio or text (Section 3.2).
- **Category 1 ‚Äî Audio context + text query (largely synthetic):**
  - Long-form audio up to ~40 minutes with transcripts and language ID is used; transcripts are paired with prompts and fed to an LLM (`Mistral Large`) to generate QA pairs that are framed as ‚Äúlistening-based‚Äù rather than ‚Äúreading-based‚Äù (Section 3.2).
  - The generated QA types include factual questions, ‚Äúneedle-in-haystack‚Äù retrieval, and reasoning-heavy questions; multiple candidates are generated and one is sampled to reduce repetitive styles (Section 3.2).
  - The pipeline occasionally generates QA in a different language than the audio to enable cross-lingual QA (Section 3.2).
  - Additional synthetic data is created for summarization and translation, with translation target languages chosen using language ID metadata and user requests sampled from a curated set (Section 3.2).
- **Category 2 ‚Äî Audio-only user input (speech as the message):**
  - Existing text SFT data (including function calling datasets) is converted to audio by using TTS on user messages (Section 3.2).
  - The paper notes a generalization issue: TTS-only training leads to poor performance on real human speech (especially accents), often via transcription errors of conversational prompts (Section 3.2).
  - To mitigate this, Voxtral extracts real spoken questions from long-form ASR data that can be answered with general knowledge (no additional audio context needed) and generates text answers using `Mistral Large` (Section 3.2).
- **Transcribe mode token:**
  - Since ASR is unambiguous and doesn‚Äôt need a prompt, Voxtral introduces a dedicated ‚Äútranscribe mode‚Äù special token to request transcription directly (Section 3.2).

##### (C) Preference alignment: DPO and Online DPO using transcript-based reward modeling

- Voxtral applies `Direct Preference Optimization (DPO)` and an `online` variant (Section 3.3).
- **Online DPO mechanics as specified:**
  - For each example, two candidate responses are sampled from the current policy at temperature **T = 0.5** (Section 3.3).
  - To rank the responses, the evaluation context replaces audio with its transcription and uses a **text-based reward model** (Section 3.3).
  - The paper‚Äôs rationale is that semantics, style, and coherence transfer even if the reward model only sees transcription rather than raw audio (Section 3.3).
- The infrastructure is stated to be shared with the one powering the `Magistral` series (Section 3.3), but the paper does not provide additional implementation specifics in the provided excerpt.

#### 3.4.4 Core configurations and hyperparameters (what is specified vs missing)

- **Specified in the provided paper excerpt:**
  - Context window: `32K` tokens (Abstract; Section 1; Section 2.2).
  - Audio features: log-Mel spectrogram with **128 Mel bins**, **160 hop-length** (Section 2.1).
  - Encoder chunk size: **30 seconds**, independent processing with reset positional encodings (Section 2.1).
  - Encoder embedding rate: **50 Hz** (Section 2.1).
  - Adapter downsampling: selected **4√ó** ‚Üí **12.5 Hz** effective rate (Section 2.2; Section 5.2).
  - Online DPO sampling temperature: **0.5** (Section 3.3).
  - Parameter counts by component for Mini and Small (Table 1).

- **Not specified in the provided excerpt (cannot be filled without guessing):**
  - Optimizer and its settings (e.g., AdamW betas/eps, weight decay).
  - Learning rate schedule.
  - Batch size / global batch size.
  - Exact tokenizer and vocabulary details.
  - Decoder architecture hyperparameters (layers, hidden size, attention heads) for Voxtral variants.
  - Total training tokens, training steps, compute budget, or hardware used.
  - Precise dataset composition/weights for pretraining mixture beyond ‚Äúaudio + text pretraining data‚Äù (Section 3.1).
  - Exact function calling interface details for ‚Äúnative function calling support with audio‚Äù beyond being listed as a contribution (Section 1).

#### 3.4.5 Worked micro-example (single input ‚Üí output walk-through)

A minimal inference scenario consistent with the paper‚Äôs described mechanisms:

1. **User input:** a 5-minute audio recording of a meeting, plus the text question: ‚ÄúWhat decision did we make about the launch date?‚Äù  
2. **Preprocessing:** the waveform is converted to a log-Mel spectrogram (128 Mel bins, 160 hop-length) (Section 2.1).  
3. **Encoding:** the audio is split into 30-second chunks; each chunk is encoded independently by the Whisper-based encoder (Section 2.1).  
4. **Concatenation + downsampling:** chunk embeddings are concatenated and downsampled 4√ó by the MLP adapter (Section 2.2; Figure 1).  
5. **Decoding:** the LLM decoder conditions on the downsampled embeddings and the text question, generating a text answer that references the meeting content (Figure 1).  
6. **Optional behavior control:** if the user instead wants verbatim transcription, they would use ‚Äútranscribe mode‚Äù (special token) so the model outputs a transcript without needing a natural-language prompt (Section 3.2).

## 4. Key Insights and Innovations

- **(1) Two-pattern pretraining (`<repeat>` + `<next>`) to jointly support ASR and reasoning**
  - Novelty: Voxtral explicitly trains with two complementary audio-text sequencing patterns‚Äîone transcription-like and one discourse/continuation-like‚Äîwhile using special tokens to disambiguate the target behavior (Section 3.1; Figure 2).
  - Significance: Ablations show that training only one pattern can severely damage the other capability (ASR vs QA), while balancing them recovers both (Section 5.3; Figure 9).

- **(2) Scalable long-audio handling via chunked Whisper encoding + learned downsampling**
  - Novelty: It keeps a Whisper-style 30-second encoder receptive field but composes long audio by independently encoding chunks and concatenating them, then applies an MLP downsampler to fit within the decoder context (Section 2.1‚Äì2.2; Figure 1).
  - Significance: The chosen 4√ó downsampling (12.5 Hz) is argued and empirically supported as a sweet spot, enabling ~40-minute audio within `32K` context and improving Llama QA accuracy relative to a 50 Hz baseline in ablations (Section 2.2; Section 5.2; Figure 8).

- **(3) Post-training recipe emphasizing instruction-following across audio and text inputs**
  - Novelty: The SFT pipeline explicitly covers (i) audio-context + text-query tasks generated from transcripts via an LLM with ‚Äúlistening-based‚Äù prompting, and (ii) audio-only user messages via TTS conversion plus augmentation with real human speech questions for generalization (Section 3.2).
  - Significance: This is aimed at making the model behave like an assistant rather than a pure transcriber, and addresses a stated weakness of TTS-only audio (accent/generalization errors) (Section 3.2).

- **(4) Speech understanding evaluations broadened beyond ASR/translation**
  - Novelty: The paper creates speech-synthesized versions of `GSM8K`, `TriviaQA`, and `MMLU` via a filtering-and-rewrite process plus TTS, and also defines an internal longish-audio SU benchmark with LLM-judged helpfulness/quality metrics (Section 3.4; Appendix A.3; Appendix A.4).
  - Significance: This directly targets the evaluation breadth gap the paper identifies (Section 1; Section 3.4).

## 5. Experimental Analysis

### 5.1 Evaluation methodology (datasets, metrics, baselines)

- **Speech recognition (ASR):**
  - Metric: `WER` (word error rate), including macro-averages (Section 4.1; Figure 3).
  - Benchmarks include short-form English tasks such as LibriSpeech, GigaSpeech, VoxPopuli, SwitchBoard, CHiME-4, SPGISpeech, and long-form Earnings-21/22 segmented into 10-minute chunks for closed-provider payload constraints (Appendix A.1; Table 3).
  - Multilingual ASR benchmarks: `FLEURS`, `Mozilla Common Voice 15.1 (MCV)`, and `Multilingual LibriSpeech (MLS)` (Appendix A.1; Tables 4‚Äì6).
  - Compared systems include `Whisper large-v3`, `GPT-4o mini Transcribe`, `Gemini 2.5 Flash`, and `ElevenLabs Scribe` (Section 4.1; Table 3).

- **Speech translation:**
  - Metric: `BLEU` on `FLEURS` speech translation, with specific language directions reported (Section 4.2; Figure 4; Appendix A.2 Table 7).

- **Speech understanding:**
  - Public speech QA benchmarks: `Llama QA`, `Openbook QA` (Section 4.3; Appendix A.2 Table 8).
  - Speech-synthesized text benchmarks: `MMLU*`, `MMAU*` (as written in Table 8), `Trivia QA*`, `GSM8k*` (Section 3.4; Appendix A.2 Table 8).
  - Internal benchmarks:
    - `SU Benchmark`: audio up to **19 minutes**, judged by an LLM with access to **audio transcription**, question, reference, and candidate answer; metrics are binary helpfulness (`LLM_JUDGE_SCORE`) and a 0‚Äì5 grade (`GRADE_LLM_JUDGE_SCORE`) (Section 3.4; Appendix A.4).
    - The paper also reports an `AU Bench` score in Table 8; the provided excerpt does not define AU Bench beyond being part of speech understanding evaluation (Table 8).

- **Text-only benchmarks:**
  - Figure 6 compares Voxtral Mini/Small with `Mistral Small 3.1` across five text understanding benchmarks, reporting accuracy (Section 4.4; Figure 6).

### 5.2 Main quantitative results (with specific numbers)

#### (A) Speech recognition (ASR)

From **Table 3** (English ASR task breakdown), Voxtral Small and Mini Transcribe are competitive and often best among open-weight comparisons:

> **Table 3 highlights (WER; lower is better):**  
> - `Voxtral Small`: LS-C **1.53**, LS-O **3.14**, SPGI **1.89**, E21 10m **9.55**, E22 10m **12.48**.  
> - `Voxtral Mini Transcribe`: LS-C **1.57**, LS-O **3.21**, SPGI **2.04**, E21 10m **9.52**, E22 10m **12.18**.  
> - For reference, `Whisper large-v3`: LS-C **1.84**, LS-O **3.66**, SPGI **3.15** (Table 3).

Multilingual ASR examples:

- On **FLEURS (Table 4)**, `Voxtral Small` reports e.g. English **3.35**, French **4.03**, German **3.38**, Hindi **7.69** WER (Table 4).
- On **MCV (Table 5)**, `Voxtral Small` reports e.g. English **8.58**, French **6.18**, German **3.74**, Spanish **3.31** WER, while Arabic is very high for all models and is excluded from a macro-average in Figure 3 (Table 5 note).

The paper also makes a ‚Äúbest among compared models‚Äù claim for some macro-averages in **Figure 3**, but the excerpt does not provide the numeric macro-average values; the safest grounded summary is that the plotted macro-averages favor Voxtral Small and Voxtral Mini Transcribe relative to the compared systems (Section 4.1; Figure 3).

#### (B) Speech translation (FLEURS)

From **Table 7** (BLEU; higher is better), `Voxtral Small` is consistently highest across the listed directions:

> **Table 7 examples (BLEU):**  
> - en‚Üífr: `Voxtral Small` **57.3** vs `GPT‚Äë4o mini Audio` **52.7** vs `Gemini 2.5 Flash` **53.9**.  
> - de‚Üíen: `Voxtral Small` **56.6** vs `GPT‚Äë4o mini Audio` **51.8** vs `Gemini 2.5 Flash` **39.4**.  
> - it‚Üíen: `Voxtral Small` **46.8** vs `GPT‚Äë4o mini Audio` **41.5** vs `Gemini 2.5 Flash` **31.8** (Table 7).

This supports the paper‚Äôs Figure 4 claim within the reported subset: Voxtral Small leads all compared models on those language pairs (Section 4.2; Figure 4; Table 7).

#### (C) Speech understanding (accuracy)

From **Table 8** (accuracy; higher is better):

> - `Voxtral Small`: Llama QA **71.7**, Openbook QA **88.4**, MMLU* **74.3**, MMAU* **62.2**, TriviaQA* **79.4**, GSM8k* **89.7**, AU Bench **86.6**.  
> - `GPT‚Äë4o mini Audio`: Llama QA **74.3**, Openbook QA **83.7**, MMLU* **72.6**, GSM8k* **90.8**, AU Bench **80.0**.  
> - `Gemini 2.5 Flash`: Openbook QA **94.7**, MMLU* **84.8**, GSM8k* **94.2**, AU Bench **88.6** (Table 8).

These numbers align with the qualitative summary in Figure 5 that Voxtral Small is competitive and can outperform GPT‚Äë4o mini Audio on some tasks, though it does not dominate Gemini 2.5 Flash on the synthesized knowledge/reasoning tasks listed (Section 4.3; Figure 5; Table 8).

#### (D) Preference alignment improvements (internal SU benchmark)

From **Table 2**, Online DPO improves SU response quality metrics, with a small ASR regression for Voxtral Small:

> **Voxtral Mini (Table 2):**  
> - SFT: LLM Judge **83.47 ¬± 2.17**, Grade **3.92 ¬± 0.04**, En Short WER **6.77**.  
> - Online DPO: LLM Judge **85.59 ¬± 3.77**, Grade **4.08 ¬± 0.07**, En Short WER **6.79**.  
>
> **Voxtral Small (Table 2):**  
> - SFT: LLM Judge **86.61 ¬± 0.96**, Grade **4.16 ¬± 0.03**, En Short WER **6.31**.  
> - Online DPO: LLM Judge **88.31 ¬± 2.03**, Grade **4.38 ¬± 0.06**, En Short WER **6.50**.

The paper interprets this as a quality‚ÄìASR trade-off for Small (release SFT as default) and chooses the Online DPO checkpoint for Mini as the public release due to perceived grounding/hallucination improvements (Section 5.4; Table 2).

### 5.3 Do experiments support the claims?

- **Long-audio feasibility claim** is mechanistically supported by the downsampling analysis and the context-length arithmetic described (Section 2.2), but the excerpt does not show an explicit ‚Äú40-minute‚Äù benchmark result; it is mainly an architectural capacity statement tied to `32K` context and 12.5 Hz embeddings (Abstract; Section 2.2).
- **‚ÄúNo sacrifice in text performance‚Äù** is supported qualitatively by Figure 6: Voxtral Small performs comparably to the text-only Mistral Small 3.1 baseline across five text benchmarks (Section 4.4; Figure 6). Exact numbers are not shown in the excerpt, so the support is visual/relative rather than numerically verifiable here.
- **Pretraining pattern necessity** is directly supported by the ablation in Figure 9, where single-pattern training collapses the other capability (near-zero Llama QA with repetition-only; ~60% WER with continuation-only) and the balanced mix recovers both (Section 5.3; Figure 9).
- **Adapter downsampling choice** is supported by Figure 8: 12.5 Hz avoids major ASR degradation versus higher rates and improves Llama QA accuracy relative to the 50 Hz baseline (Section 5.2; Figure 8).
- **Preference alignment benefit** is quantitatively supported on the internal SU metrics in Table 2, with clearly reported mean ¬± SD over 10 judge samples per answer (Section 5.4; Table 2).

## 6. Limitations and Trade-offs

- **Missing training/compute details limit reproducibility.**
  - The provided excerpt does not specify optimizer, LR schedule, batch sizes, training tokens, compute, or hardware, which are typically critical for reproducing scaling-sensitive results (Section 3 describes phases but not these hyperparameters).

- **Chunked 30-second audio encoding may limit cross-chunk acoustic modeling.**
  - Voxtral resets positional encodings and runs attention independently per 30-second chunk (Section 2.1). This reduces compute, but it also implies the audio encoder itself does not attend across chunk boundaries; any cross-chunk integration must occur later in the decoder over concatenated embeddings.

- **Padding policy wastes compute on short clips.**
  - All audio inputs are padded to the next multiple of 30 seconds (Section 2.1), which can add overhead for short utterances. The paper keeps padding because removing it harms some ASR performance (e.g., ~0.5% WER degradation on FLEURS French) (Section 5.1; Figure 7).

- **Downsampling is a lossy compression with task-dependent trade-offs.**
  - Aggressive downsampling (e.g., 8√ó ‚Üí 6.25 Hz) increases WER penalties (over 1% on FLEURS French) (Section 5.2; Figure 8). The chosen 4√ó is a compromise, but it still discards temporal detail relative to 50 Hz.

- **Heavy reliance on synthetic data can bias behavior.**
  - Speech understanding SFT is described as relying ‚Äúsignificantly on synthetic data,‚Äù including LLM-generated QA and TTS-rendered user prompts (Section 3.2). The paper itself notes TTS-only audio leads to poor generalization to real speech/accents, requiring additional mitigation with real human speech questions (Section 3.2).

- **Evaluation via LLM judge introduces judge dependence.**
  - The SU benchmark uses an LLM judge that sees the audio transcript, question, reference, and candidate answer (Section 3.4; Appendix A.4). Even with multiple independent judgments to measure variability, the metric is sensitive to judge prompt/model choices, and the judge does not see raw audio.

- **Preference alignment reward model only sees transcripts.**
  - In Online DPO, ranking replaces audio with transcription for a text reward model (Section 3.3). This may fail to reward behaviors that depend on non-textual audio cues (prosody, speaker identity, background sounds), though the paper argues semantics/style transfer.

- **Function calling with audio is claimed but not detailed in the excerpt.**
  - ‚ÄúNative function calling support with audio‚Äù is listed as a primary contribution (Section 1), but the provided content does not explain the interface, schema, or evaluation results specific to audio function calling.

## 7. Implications and Future Directions

- **Field impact: a concrete recipe for ‚Äúaudio-capable LLMs‚Äù that keep strong text performance.**
  - Voxtral suggests a practical architecture pattern: leverage a strong pretrained speech encoder (Whisper large‚Äëv3), add a learned temporal adapter, and keep an LLM decoder backbone, then use targeted pretraining patterns to avoid trading off ASR and reasoning (Sections 2‚Äì3; Figure 2; Figure 9).

- **Evaluation impact: speech-synthesized knowledge/reasoning benchmarks could standardize SU testing.**
  - The paper‚Äôs pipeline for filtering and rewriting text benchmarks into speech-friendly prompts, then synthesizing diverse speaker audio, provides a reusable evaluation recipe (Section 3.4; Appendix A.3). This can broaden evaluation beyond transcription/translation toward reasoning and knowledge under speech input.

- **Practical applications enabled (as implied by tasks and evaluations)**
  - Long-form audio QA and summarization (SFT data generation explicitly targets long audio up to ~40 minutes and ‚Äúneedle-in-haystack‚Äù questions) (Section 3.2).
  - Multilingual speech transcription and translation (Section 4.1‚Äì4.2; Tables 3‚Äì7).
  - Audio-input assistants that can accept spoken user requests (audio-only input SFT, plus transcribe mode) (Section 3.2).

- **Repro/Integration Guidance (based on what the paper supports)**
  - Prefer `Voxtral Mini` when you need a smaller checkpoint (Table 1) and want the released version that the paper selects as Online DPO due to SU quality improvements (Section 5.4).
  - Prefer `Voxtral Small` when you need stronger accuracy on many understanding tasks and want text performance comparable to its text-only backbone (Section 2.3; Figure 6; Table 8).
  - If your application is pure transcription, the paper defines a `Voxtral Mini Transcribe` variant trained with repetition-only pretraining for ASR comparisons (Section 3.1; Section 4.1; Table 3); this indicates a ‚Äúspecialize for ASR‚Äù path exists, at the cost of SU performance per Figure 9.

- **Concrete future directions explicitly suggested by the paper**
  - Release an Online DPO version of Voxtral Small that does not regress on English short-form ASR metrics (Section 5.4).
  - Improve breadth and standardization of speech understanding evaluations, building on the released synthesized suites and judge protocols (Section 3.4).

- **Additional follow-ups suggested by the ablations (grounded in observed trade-offs)**
  - Explore better ways to remove or reduce 30-second padding overhead without ASR regression (Section 5.1; Figure 7).
  - Investigate alternative adapter/downsampling mechanisms that preserve ASR while improving SU, since frame-rate affects both and the best point (12.5 Hz) appears non-trivial (Section 5.2; Figure 8).

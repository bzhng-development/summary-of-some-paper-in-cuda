# Compact Language Models via Pruning and Knowledge Distillation

**ArXiv:** [2407.14679](https://arxiv.org/abs/2407.14679)

## ğŸ¯ Pitch

This paper introduces a practical framework for deriving an entire family of compact, high-performance language models from a single large pretrained LLM by unifying structured pruning across multiple axes with highly data-efficient knowledge distillation. By applying this to the 15B Nemotron-4 model, the authors produce smaller MINITRON models (8B and 4B) using up to 40Ã— fewer training tokens than training from scratch, with comparable or superior performance to similar community models. This work significantly lowers the cost and barrier to producing versatile LLM families, accelerating real-world deployment and democratizing access to powerful language models.

---

## 1. Executive Summary
This paper presents a practical recipe to turn one well-trained large language model (LLM) into a family of smaller, competitive models by combining structured pruning with dataâ€‘efficient knowledge distillation. Applied to the 15B-parameter `Nemotronâ€‘4` teacher, the method produces the `MINITRON` 8B and 4B models using up to 40Ã— fewer additional training tokens than training those sizes from scratch, while matching or beating similarly sized community models on standard benchmarks (Figure 1, Tables 2â€“4).

## 2. Context and Motivation
- Problem addressed
  - Building an LLM â€œfamilyâ€ (e.g., 15B, 8B, 4B) typically requires training each size from scratchâ€”expensive in data and compute. The paper asks whether one can train a single large model once and derive smaller variants via structured pruning plus light retraining (Section 1).
- Why it matters
  - Real deployments need various model sizes for latency and cost. Reducing the marginal cost of each additional size makes model families practical for more organizations. The paper quantifies the savings: producing 8B and 4B models from a 15B teacher uses up to 40Ã— fewer tokens per additional model and yields a 1.8Ã— reduction in total compute to train a 15B/8B/4B family (Section â€œCost Savings for Training a Model Familyâ€).
- Prior approaches and limitations
  - Structured pruning has been widely studied, but:
    - Many LLM pruning works focus on a single axis (depth or width) and/or require gradients, large memory, and substantial fineâ€‘tuning data (e.g., LLM-Pruner [33], SliceGPT [4], Sheared LLaMA [53]; see Related Work, Section 5).
    - Existing works rarely explore dataâ€‘efficient retraining via knowledge distillation (KD) from the unpruned teacher after structural pruning.
    - Users lack clear, empirically validated â€œbest practicesâ€ that cover how to rank/prune across axes, combine pruning axes, and retrain effectively.
- Positioning relative to existing work
  - This paper unifies depth and width pruning with an activation-only importance metric computed via forward passes on a tiny calibration set (1024 samples), plus a KDâ€‘based retraining regime that minimizes data use (Sections 2â€“3). It distills empirically grounded best practices and demonstrates stateâ€‘ofâ€‘theâ€‘art results against modern pruning baselines (Table 4).

## 3. Technical Approach
The pipeline (Figure 2) has four stages: compute importance, rank and prune, lightweight architecture search, and retraining via distillation.

1) Importance estimation (Section 2.2)
- Goal: quantify the â€œimportanceâ€ of each layer (depth), attention head, MLP neuron, and embedding channel (width) using only forward passes on a small calibration dataset `D` (1024 samples drawn from the pretraining mix).
- Width importance (for each Transformer block):
  - Heads: sum the squared L2 norm of each headâ€™s output across batch and sequence:
    - â€œF_head^(i) = Î£_{B,S} ||Attn(X W_Q,i, X W_K,i, X W_V,i)||Â²â€ (Section 2.2).
  - MLP neurons: aggregate the preâ€‘activation magnitude of each neuron, computed from the iâ€‘th row of the first MLP weight matrix `W1`:
    - â€œF_neuron^(i) = Î£_{B,S} X Â· (W1_i)^Tâ€ (Section 2.2; `W1_i` is the iâ€‘th row).
  - Embedding channels: sum the LayerNorm output of channel i:
    - â€œF_emb^(i) = Î£_{B,S} LN(X)_iâ€ (Section 2.2).
  - Aggregation choice matters. The best-performing reduction of these activations across batch and sequence is â€œbatch=L2, sequence=meanâ€; see Table 13 (zeroâ€‘shot) and Figure 5 (postâ€‘retraining).
- Depth importance:
  - Perplexity (â€œPPLâ€) drop: remove one layer at a time and measure the perplexity increase (Shortened LLaMAâ€“style; Section 2.2).
  - Block Importance (â€œBIâ€): compute 1 âˆ’ cosine similarity between a layerâ€™s input and output in one forward pass (Equation for `BI_i` in Section 2.2). BI is faster and can extend to multiple contiguous layers.

2) Rank and prune (Section 2.3)
- Rank units in each axis by importance and trim the corresponding weights to the target sizes.
- Special handling for attention heads:
  - After pruning from L heads to K heads, the method â€œmergesâ€ information by adding residuals from pruned heads into kept heads, inspired by â€œLayer Collapseâ€ for depth (Section 2.3). This preserves useful information and boosts accuracy.
  - For grouped-query attention (GQA), this merging is applied to query heads only (Section 2.3).
- After pruning, all affected matrices in MLP, MHA, LayerNorm, and embeddings are reshaped to match the new dimensions.

3) Lightweight architecture search (Section 2.3, Figure 3; Table 12)
- Enumerate feasible architectures within a narrow parameter range around the target (Â±5%), varying:
  - Number of layers (depth), attention heads, MLP expansion factor, and embedding size.
- Each candidate undergoes short retraining (about 1.8B tokens; â€œlightweight RTâ€) to stabilize rankings; then the best candidate is selected for full retraining (Sections 2.3 and 4.3).
- Figure 9 shows rankings stabilize after â‰ˆ300 steps (about 1.35B tokens), justifying lightweight retraining in the search loop.

4) Retraining via knowledge distillation (Section 3; Figure 4)
- Teacherâ€“student setup: teacher is the original `Nemotronâ€‘4 15B`, the student is the pruned model.
- Losses (with plain-language meaning first):
  - Logit distillation `L_logit`: make the studentâ€™s nextâ€‘token distribution match the teacherâ€™s using soft targets and KL divergence at temperature Ï„:
    - â€œL_logit = (1/l) Î£_{k=1..l} Loss(p_t^k(x,Ï„), p_s^k(x,Ï„))â€ (Section 3).
    - `p(x,Ï„)` is the softmax at temperature Ï„ (definition in Section 3).
  - Optional intermediateâ€‘state losses `L_is`: align hidden states between teacher and student at selected layers; the studentâ€™s hidden states are linearly upscaled to teacher dimensionality (Figure 4 and Section 3).
    - `L_is` can include: embedding output loss `L_emb`, encoder block output loss `L_o`, input loss `L_i`, and attention relation loss `L_att` (Appendix A.4).
  - Crossâ€‘entropy `L_CLM` with groundâ€‘truth labels (standard language modeling loss).
- Total loss:
  - `L = L_CLM + L_logit + Î±Â·L_is`, with a dynamic weight `Î± = L_logit / L_is` to balance magnitudes (Section 3).
- Empirical choices:
  - KLD works best for `L_logit` compared to MSE, cosine, or reverse KLD (Tables 15â€“16).
  - Temperature Ï„ = 1.0 performs best; topâ€‘K logits truncation hurts unless K is very large, and gives no benefit over not truncating (Appendix A.3).
  - When depth is not heavily reduced, using `L_logit` alone works best; when depth is significantly reduced, add selected `L_is` terms (Best Practices #6â€“#7, Section 4.1; Tables 17â€“18).

5) Oneâ€‘shot vs. iterative pruning
- For importance estimation: iterative reâ€‘ranking/pruning brings no benefit after retraining; singleâ€‘shot is sufficient (Table 14).
- Across axes: pruning width alone outperforms depth pruning after some retraining, and a simple depth+width combination can be helpful depending on target (Table 10, Figure 6; also Table 1).
- For aggressive compression to 4B, a twoâ€‘step path (15Bâ†’8Bâ†’4B) with retraining at each step yields substantially better final accuracy than 15Bâ†’4B in one shot (Table 11, last two rows).

6) Where to prune in multiâ€‘phase training
- If the teacher was trained with multiâ€‘phase pretraining (web-heavy phase followed by cleaner data phase), prune the finalâ€‘phase checkpoint and retrain on a portion of that cleaner phase data (Table 20).

Implementation details:
- All pruning/distillation is implemented in NVIDIA Megatronâ€‘LM (Section 4). Final model architectures are in Table 5.

## 4. Key Insights and Innovations
1) Activation-only, unified importance scoring across depth and width
- Whatâ€™s new: a single, forwardâ€‘passâ€‘only method to score layers, heads, MLP neurons, and embedding channels with a tiny calibration set (1024 samples) (Section 2.2). This avoids gradient computation and large memory overhead common in prior pruning methods.
- Why it matters: enables practical, lowâ€‘overhead pruning at LLM scale. The study shows aggregation choice is crucial; â€œbatch=L2, seq=meanâ€ ranks best (Table 13, Figure 5).

2) Empirical best practices that change pruning decisions
- Ten best practices (Section 4.1) synthesize extensive ablations across axes, losses, pruning order, and retraining. Two that significantly affect design:
  - Width pruning beats depth pruningâ€”but only after some retraining (Table 10; Figure 6; also Table 1).
  - Distillation with `L_logit` (KLD) alone usually beats using groundâ€‘truth `L_CLM` or adding many intermediate-state losses unless depth is heavily reduced (Tables 15â€“18).

3) Residual merging for pruned attention heads
- A simple â€œhead residual carryoverâ€ adds pruned headsâ€™ information back into kept heads, analogous to layer collapse for depth (Section 2.3). This improves accuracy with negligible overhead, and is tailored for groupedâ€‘query attention.

4) Lightweight architecture search with short retraining to stabilize rankings
- Rather than complex Bayesian/genetic search, the paper enumerates a small, practical search space and relies on ~1.8Bâ€‘token retraining to reveal the best candidate (Figure 9; Table 12). Rankings change materially in the first ~300 steps and then stabilize.

5) Costâ€‘effective model family creation
- Quantified saving: training the additional 8B and 4B models via prune+distill uses ~40Ã— fewer tokens each; total family compute is 1.8Ã— lower than training all sizes from scratch (Section â€œCost Savings for Training a Model Familyâ€).

## 5. Experimental Analysis
Evaluation setup (Section 4)
- Data and training
  - Teacher: `Nemotronâ€‘4 15B` trained on an 8T token curated dataset; continued training data (â€œCTâ€) available (Section 4).
  - Lightweight retraining for search: ~1.8B tokens (400 steps). Final retraining budgets vary; e.g., Table 18 shows 18.9B vs 94B tokens for 8B ablations.
  - Importance estimation uses a calibration set `D` of 1024 samples (Section 4).
- Benchmarks and metrics (Section 4)
  - Knowledge/logic: `MMLU`, `ARCâ€‘Challenge`, `HellaSwag`, `Winogrande`, `TruthfulQA`, `GSM8K`.
  - Coding: `HumanEval`, `MBPP` (pass@1; T=0.2, topâ€‘p=0.95).
  - Summarization: `XLâ€‘Sum (en)`.
  - Instruction-tuned evaluations: `MTâ€‘Bench`, `IFEval`, `ChatRAGâ€‘Bench`, `Berkeley Function Calling Leaderboard (BFCL)`.

Main results: quality vs. similarly sized models (Tables 2â€“3)
- `MINITRON 8B` (derived from 15B):
  - Beats previous generation `Nemotronâ€‘3 8B` while using 40Ã— fewer tokens, and is competitive with community baselines:
  - Selected numbers from Table 2:
    - MMLU (5â€‘shot): 63.8 vs `Nemotronâ€‘3 8B` 54.7; comparable to `Mistral 7B` 64.1 and `Gemma 7B` 64, near `Llamaâ€‘3 8B` 65.3.
    - HellaSwag (10â€‘shot, acc_norm): 80.7 vs `Nemotronâ€‘3 8B` 78.5; still below 84.6 of the 15B teacher.
    - GSM8K (5â€‘shot): 51.3 vs `Nemotronâ€‘3 8B` 24.0; close to `Gemma 7B` 50.
    - HumanEval (pass@1): 31.6 vs `Nemotronâ€‘3 8B` 20.7; close to `Gemma 7B` 32.
  - Quote:
    > Table 2: MINITRON 8B uses â€œ40Ã— fewer training tokens than Nemotronâ€‘3 8Bâ€ and shows improved MMLU (+9.1 points) and coding performance (HumanEval +10.9 points).
- `MINITRON 4B`:
  - Outperforms `Gemmaâ€‘2B` and is competitive with `Phiâ€‘2`/`Qwen2â€‘1.5B` on many tasks, despite using far fewer tokens than those modelsâ€™ pretraining budgets (Table 3):
    - MMLU: 58.6 (vs `Phiâ€‘2` 57.5, `Gemmaâ€‘2B` 42.0).
    - HellaSwag: 75.0 (vs `Phiâ€‘2` 75.2).
    - HumanEval: 23.3 (vs `Phiâ€‘2` 50.0; coding remains challenging for small base models without codeâ€‘heavy pretraining).
    - GSM8K: 24.1 (well below `Qwen2â€‘1.5B` 58.5; math reasoning remains a gap).

Against pruning baselines (Table 4)
- Quote:
  > Table 4 (8B range): `MINITRON` reaches MMLU 63.8 and HellaSwag 80.7, vs LLM-Pruner 25.2/67.8, SliceGPT 37.1/55.7, LaCo 45.9/64.4, ShortGPT 54.7/66.6â€”substantial gains despite fewer nonâ€‘embedding parameters.
  > Table 4 (4B range): `MINITRON 4B` achieves MMLU 58.6 and HellaSwag 75.0, far above ShortGPTâ€™s 43.96/53.02 and Sheared LLaMAâ€™s 26.4/70.8.

Instructionâ€‘tuned evaluation (Tables 6â€“9)
- `MINITRON 4Bâ€‘instruct` (SFT using Nemotronâ€‘4â€‘340B instruction data) shows strong downstream capability:
  - MTâ€‘Bench: 6.46, outperforming `Gemmaâ€‘2Bâ€‘IT` (5.19) and `StableLMâ€‘2 Chat 1.6B` (5.42) (Table 6).
  - IFEval: 68.76% strict promptâ€‘level accuracy; `Gemmaâ€‘2Bâ€‘IT` reports 28.70% (loose) (Table 7).
  - ChatRAGâ€‘Bench: 41.11 average vs `Gemmaâ€‘2Bâ€‘IT` 33.31 (Table 8).
  - BFCL v2: 53.09 average, beating `Gemmaâ€‘2Bâ€‘IT` 41.63 and even `Llamaâ€‘3â€‘8Bâ€‘instruct` 50.51 (Table 9).

Ablations and supporting studies
- Width vs. depth pruning:
  - After ~200 retraining steps, widthâ€‘only beats depthâ€‘only and depth+width for the same target size (Table 10 and Figure 6). Table 1 similarly shows width pruning becomes superior after distillation.
- Distillation vs. conventional training (isoâ€‘compute) (Table 11):
  - Quote:
    > Distilling a pruned 4B student (100B tokens) reaches HellaSwag 52.04 and MMLU 42.45, vs randomâ€‘init 4B trained with the same compute (150B tokens) at 46.22/24.36 (or even 400B tokens at 48.23/26.24).
- Loss selection for KD:
  - `L_logit` with KLD outperforms MSE, cosine, reverse KLD (Tables 15â€“16). Using `L_logit` alone generally works best unless depth is reduced a lot (Tables 17â€“18).
- Aggregation metric for importance:
  - â€œbatch=L2, seq=meanâ€ ranks best both before and after retraining (Table 13, Figure 5).
- Singleâ€‘ vs multiâ€‘phase retraining:
  - Pruning the phaseâ€‘2 (cleaner) checkpoint and retraining on phaseâ€‘2 data yields better results than mixing phaseâ€‘1+2 after pruning (Table 20).
- Architecture search:
  - Practical enumerations around target parameter budgets (Table 12) plus 1.8Bâ€‘token retraining yields stable rankings (Figure 9).
  
Do the experiments support the claims?
- Yes, for the scales tested (â‰¤15B teacher) and the chosen tasks. The study is methodical: it contrasts pruning axes, validates importance aggregation choices, and probes distillation loss design. It shows large, consistent margins over pruning baselines (Table 4) and competitive performance versus fromâ€‘scratch peers for similar sizes (Tables 2â€“3), all with a clear computeâ€‘saving story (Section â€œCost Savingsâ€¦â€).

## 6. Limitations and Trade-offs
- Reliance on a strong teacher
  - The approach assumes access to a highâ€‘quality, fully trained large model (here, `Nemotronâ€‘4 15B` with 8T tokens). Compute savings apply to the â€œadditional sizes,â€ not to producing the first large model.
- Data representativeness for importance estimation
  - Importance is computed from a tiny calibration set (1024 samples). If this set poorly represents deployment domains, importance rankings could mislead pruning (Section 2.2).
- Scale tested
  - Best practices (e.g., â€œwidth > depth after retrainingâ€) are established up to 15B parameters. It is unclear whether they hold at much larger scales or with very different architectures (Section 4.1, Table 10).
- Task coverage and mixed results
  - While `MINITRON 8B` is broadly strong, `MINITRON 4B` shows weaker math/coding compared to specialized or heavily codeâ€‘trained models (Table 3: GSM8K and HumanEval).
- Distillation cost and teacher inference overhead
  - KD adds a full teacher forward pass, which is nonâ€‘trivial. Even â€œlightweightâ€ retraining uses up to tens of billions of tokens for final models (e.g., 94B in Table 18), although still far less than fromâ€‘scratch pretraining.
- Structured changes can affect specialized capabilities
  - The paper does not evaluate longâ€‘context behavior or retrieval-augmented settings postâ€‘pruning. Pruning embeddings/heads could interact with such capabilities.

## 7. Implications and Future Directions
- Field impact
  - A practical path to economical model families: train one strong model once, then derive smaller variants with minimal extra data. The detailed best practices (Section 4.1) make pruning+KD actionable for teams beyond major labs.
- Applications
  - Deployable small/medium LLMs for edge or latencyâ€‘constrained environments.
  - Rapid â€œspinâ€‘offsâ€ of taskâ€‘ or domainâ€‘specific models by pruning a generalist teacher and retraining on small domain data.
  - As shown with `MINITRON 4Bâ€‘instruct`, instructionâ€‘tuned compact models can be strong for function calling and RAG (Tables 8â€“9).
- Research directions
  - Multiâ€‘teacher or mixtureâ€‘ofâ€‘experts distillation for pruned students.
  - Automatic calibration set selection to improve importance estimates for target domains.
  - Extending best practices to larger teachers and to architectures with longâ€‘context mechanisms.
  - Joint compression with quantization and lowâ€‘rank adaptation; the paper notes LoRA could be used during the lightweight search phase (Section 2.3).
  - Theoretical understanding of why width pruning overtakes depth pruning after short retraining (Table 10, Figure 6).

In sum, this paper contributes a carefully tested, endâ€‘toâ€‘end procedure to compress LLMsâ€”spanning importance scoring, pruning across multiple axes, efficient retraining by KD, and practical architecture searchâ€”backed by extensive ablations and competitive downstream results.

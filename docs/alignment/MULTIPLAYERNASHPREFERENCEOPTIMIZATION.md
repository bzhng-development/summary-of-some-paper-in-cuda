# MULTIPLAYER NASH PREFERENCE OPTIMIZATION

**ArXiv:** [2509.23102](https://arxiv.org/abs/2509.23102)

## üéØ Pitch

This paper introduces Multiplayer Nash Preference Optimization (MNPO), a breakthrough framework that extends Nash learning from human feedback (NLHF) to the multiplayer setting, capturing the true diversity and complexity of human preferences. By formulating large language model alignment as an n-player game, MNPO enables more robust policy optimization that accounts for heterogeneous, possibly non-transitive preferences‚Äîdemonstrating both theoretical rigor and empirical gains over state-of-the-art two-player baselines. This approach addresses the critical limitations of single-opponent algorithms and moves the field closer to genuinely human-centric, scalable, and reliable AI alignment.

---

## 1. Executive Summary
This paper introduces Multiplayer Nash Preference Optimization (MNPO), a framework that trains language models by posing alignment as an n‚Äëplayer game rather than a two‚Äëplayer duel. It provides an algorithmic recipe, theoretical grounding, and empirical evidence that multiplayer competition better captures heterogeneous, possibly non‚Äëtransitive human preferences, leading to consistent improvements over state‚Äëof‚Äëthe‚Äëart two‚Äëplayer methods.

## 2. Context and Motivation
- Problem addressed
  - Standard RLHF pipelines typically learn a scalar reward model from pairwise preferences using the Bradley‚ÄìTerry assumption (Section 2; Eq. 1). This implicitly assumes transitive preferences.
  - Real preferences can be non‚Äëtransitive and heterogeneous across annotators, contexts, and evaluation criteria (Introduction).
  - Recent ‚ÄúNash Learning from Human Feedback‚Äù (NLHF) reframes alignment as a two‚Äëplayer game (Section 2; Eqs. 2‚Äì4), but all current algorithms pit a single policy against a single opponent. This creates a single‚Äëopponent bias that cannot represent diverse populations of judges, model checkpoints, or evaluation policies (Introduction).

- Why it matters
  - Practically: Modern systems are judged by diverse user populations and ensembles of evaluators; single‚Äëopponent training can overfit to one preference slice, hurting robustness.
  - Theoretically: Two‚Äëplayer games cannot express rich multiplayer dynamics (e.g., cycles and coalitions) that arise with heterogeneous preferences.

- Prior approaches and shortcomings
  - Reward‚Äëmodel RLHF (Eq. 1) assumes transitivity and reduces preferences to a scalar reward; this is fragile under reward hacking and heterogeneity.
  - Two‚Äëplayer NLHF (Eq. 2) avoids explicit rewards but remains limited to a single opponent and a single best response trajectory.
  - Algorithmic advances such as iterative no‚Äëregret/self‚Äëplay (INPO), optimistic mirror descent (ONPO), and extragradient updates (EGPO) bring stability and convergence but remain two‚Äëplayer (Introduction; Related Work).

- Positioning
  - MNPO generalizes the NLHF game from two players to n players (Section 3.1). It defines a multiplayer objective, multiplayer Nash equilibria (Eq. 9), and a multiplayer duality gap (Eq. 10), then supplies practical updates (Eq. 11) and training losses (Eqs. 15‚Äì16). It also unifies many preference optimization algorithms as special cases of a time‚Äëdependent multiplayer formulation (Table 1).

## 3. Technical Approach
The core idea: treat alignment as an n‚Äëplayer game in which each policy competes against a population of opponents while being regularized toward a trusted reference model.

Step 1 ‚Äî Background: two standard formulations
- Reward‚Äëmodel RLHF (Section 2; Eq. 1)
  - Learn a reward R(x, y); then optimize the KL‚Äëregularized objective J(œÄ)=E[R]‚àíœÑ¬∑KL(œÄ||œÄ_ref). This presumes a Bradley‚ÄìTerry generative model of pairwise preferences (Section 2; ‚ÄúBradley‚ÄìTerry Model Assumption‚Äù).
- Two‚Äëplayer NLHF with a general preference oracle (Section 2; Eqs. 2‚Äì4)
  - Preference oracle `P(x, y1, y2)` returns the probability y1 beats y2 given prompt x.
  - Define a two‚Äëplayer zero‚Äësum‚Äëlike objective J(œÄ1, œÄ2) (Eq. 2) with KL regularization to `œÄ_ref`.
  - Nash policy `œÄ*` is a fixed point where neither player can improve unilaterally (Eq. 3). The duality gap (Eq. 4) measures how far a policy is from `œÄ*`.

Key terms defined
- Preference oracle `P`: a function that returns the win probability between two responses for a prompt, enabling training without learning a scalar reward.
- Nash equilibrium: a strategy profile where no player can gain by changing policy alone.
- Duality gap: the advantage available by best‚Äëresponding to a policy minus the policy‚Äôs worst loss when others best‚Äërespond, zero only at equilibrium.

Step 2 ‚Äî Extending to multiplayer (Section 3.1)
Two ways to generalize beyond pairwise preferences:

A) Plackett‚ÄëLuce ranking for listwise comparisons (Eqs. 6‚Äì7)
- The Bradley‚ÄìTerry model extends to multiple items using the Plackett‚ÄëLuce probability, which models the chance that one item is preferred among a set. When the set size k=2, it reduces back to Bradley‚ÄìTerry.
- The resulting reward‚Äëlearning objective increases the reward of the chosen response relative to a log‚Äësum‚Äëexp over alternatives (Eq. 7), encouraging dominance over the entire competitor pool.

B) Multiplayer general preference game (Eq. 8)
- With n policies {œÄ1,‚Ä¶,œÄn}, each player i maximizes its expected win probability against the set of other players {œÄj}j‚â†i while paying a KL penalty to the reference `œÄ_ref`:
  - J(œÄi, {œÄj}j‚â†i)=Ex[Ey‚àºœÄi,{yj‚àºœÄj}j‚â†i[P(y ‚âª {yj}j‚â†i | x)] ‚àí œÑ¬∑KL(œÄi||œÄ_ref)] (Eq. 8).
- Symmetry: all players are treated equally; in equilibrium, optimal policies coincide (Section 3.1).
- Multiplayer Nash equilibrium condition (Eq. 9) and equilibrium win rate: with no KL, a policy has average win 1/n when facing n‚àí1 copies of itself.
- Multiplayer duality gap (Eq. 10): extends the two‚Äëplayer definition to quantify how much a single player can gain by deviating against worst‚Äëcase opponents.

Step 3 ‚Äî Algorithmic update with multiplicative weights (Section ‚ÄúMultiplayer Nash Preference Optimization‚Äù and Appendix E)
- Idealized per‚Äëiteration update (Eq. 11)
  - For player i, update `œÄ_i^{(t+1)}` proportional to the geometric mean of opponents‚Äô policies times an exponential of its average advantage (the win probability against each opponent):
    - Intuition: ‚Äúlean toward what the population does‚Äù (geometric mean term) and ‚Äúamplify actions that consistently win‚Äù (exponential advantage term).
- Avoiding intractable normalization
  - Define the pairwise log‚Äëratio function h_t(œÄ, y, y‚Ä≤) that compares œÄ‚Äôs probability ratio to the opponents‚Äô geometric mean ratio (Eq. 13).
  - Show that the ideal update enforces a linear relationship between these log‚Äëratios and average preference margins (Eq. 14).
- Trainable loss without computing partition functions
  - Minimize a squared error loss L_t(œÄ) that matches h_t to the target margins (Eq. 15). Lemma 1: the minimizer is unique within the policy class.
  - Replace the unknown margin term with a learnable scale Œ∑ and sampled pairwise preferences to get L‚Ä≤_t(œÄ) (Eq. 16). Proposition 1: L‚Ä≤_t(œÄ) equals L_t(œÄ) up to a œÄ‚Äëindependent constant, so minimizing L‚Ä≤_t is equivalent.

Step 4 ‚Äî Time‚ÄëDependent MNPO (TD‚ÄëMNPO): building opponent populations from policy history (Section 3.2)
- Construct the opponent set at step t as a weighted mixture of recent policies `{œÄ_{t‚àíj}}`, with weights `{Œª_j}`.
- Minimize a distance D between
  - your current log‚Äëratio log œÄ(y_w)/œÄ(y_l) and
  - the weighted sum of historical opponents‚Äô log‚Äëratios, aiming at a target reward gap Œ¥‚ãÜ scaled by Œ∑ (TD‚ÄëMNPO loss following Eq. 18; notation compressed in the text).
- Why this matters
  - Stabilizes training (less overfitting to the most recent iterate), smooths policy evolution, and unifies many objectives.

Step 5 ‚Äî Unifying prior methods (Table 1)
- By choosing the number of players n, the opponent set, the distance D, and the target gap Œ¥‚ãÜ, MNPO recovers:
  - DPO, SimPO, SPPO, IPO, DNO, SPIN, INPO, etc.
- Example: DPO is recovered when n=2, opponent is `œÄ_ref`, unit weights, and a backward‚ÄëKL‚Äëlike distance with an infinite target gap (Table 1).

Step 6 ‚Äî Optional reward‚Äëaware variant (Section 3.2; Eq. 17)
- Reward‚Äëaware Preference Optimization (RPO): match the model‚Äôs implicit reward difference to a target reward difference from an external reward model using a distance D (Eq. 17).
- MNPO‚Äôs squared‚Äëloss variant L‚Ä≤_t(œÄ) is shown to be a special case with a particular choice of implicit reward and target gap.

Step 7 ‚Äî External‚Äëopponent MNPO (Appendix F.1; Eqs. 20‚Äì21)
- Replace historical opponents with a set of external LLMs {œÄ_j}. Optimize a weighted sum of log‚Äëratio matches (Eq. 20).
- Connection to multi‚Äëteacher knowledge distillation: maximizing expected reward with KL penalties to multiple teachers yields the same functional form (Eq. 21; Proposition 2).

Theoretical lens (Appendix E.3)
- The multiplicative update (Eq. 19, same as Eq. 11) can be derived as online mirror descent with KL regularization against the geometric mean of opponents. This yields no‚Äëregret guarantees with average regret O(1/‚àöT), implying convergence of empirical play to equilibrium.

## 4. Key Insights and Innovations
- Generalizing preference alignment to n‚Äëplayer games (Section 3.1)
  - What‚Äôs new: A formal multiplayer objective (Eq. 8), equilibrium concept (Eq. 9), and duality gap (Eq. 10).
  - Why it matters: Captures heterogeneous, non‚Äëtransitive preferences by training against a population, not a single opponent. The equilibrium interpretability extends the ‚Äúbalanced 50% win rate‚Äù idea from two players to ‚Äú1/n average win rate.‚Äù

- Practical multiplicative‚Äëweights update with a tractable loss (Eqs. 11, 13‚Äì16; Lemma 1; Proposition 1)
  - What‚Äôs new: A trainable, normalization‚Äëfree loss that uniquely targets the multiplicative update‚Äôs fixed point.
  - Why it matters: Turns an elegant but intractable population update into a stable supervised loss you can optimize with standard tools.

- Time‚Äëdependent opponent sets unify and improve prior methods (Section 3.2; Table 1)
  - What‚Äôs new: Build the opponent population from a mixture of past policies (and optionally others), controlled by weights `{Œª_j}` and a distance D.
  - Why it matters: Provides a single lens through which DPO/SimPO/SPPO/DNO/INPO and others arise as special cases; offers smoother learning and robustness to instability in any single iteration.

- Reward‚Äëaware integration without reverting to scalar rewards (Section 3.2; Eq. 17)
  - What‚Äôs new: A principled way to incorporate graded reward signals as auxiliary targets while keeping the game‚Äëtheoretic structure.
  - Why it matters: Bridges qualitative preference games and quantitative reward supervision for better stability and interpretability.

Overall, the multiplayer framing and the unifying TD‚ÄëMNPO formulation are fundamental conceptual advances; the tractable loss and reward‚Äëaware connection are practical innovations that enable training at scale.

## 5. Experimental Analysis
Evaluation methodology (Section 4)
- Base model and training
  - `Gemma-2-9B-it` as the initial policy; three online iterations (T=3).
  - Preference signals are generated by `ArmoRM-Llama3-8B-v0.1` reward model (no human labels in this study).
  - Important hyperparameters: Œ≤ tuned in [0.01, 10]; Œ≤ increases across iterations to mitigate degradation.

- Benchmarks and metrics
  - Instruction‚Äëfollowing and preference alignment:
    - AlpacaEval 2.0 (length‚Äëcontrolled win rate), Arena‚ÄëHard (win rate), MT‚ÄëBench (score/10). Judged by `GPT-5-mini` (Tables 2; Appendix D notes judge settings).
  - Broader abilities:
    - Instruction following (IFEval), knowledge (GPQA, MMLU, ARC), commonsense (HellaSwag, TruthfulQA, Winogrande) (Table 3).
  - Math and coding:
    - GSM8K, Minerva‚ÄëMath, AIME‚Äë24, HumanEval (Table 4).

- Baselines
  - Preference optimization: DPO, SimPO, SPPO, INPO.
  - External models: LLaMA‚Äë3.1‚Äë8B‚Äëit, Tulu‚Äë2‚ÄëDPO‚Äë70B, LLaMA‚Äë3.3‚Äë70B‚Äëit, Mixtral‚Äë8x22B‚Äëit, Qwen3‚Äë235B‚Äëit, plus closed models (Table 2).

Main quantitative results
- Instruction‚Äëfollowing and preference alignment (Table 2)
  - AlpacaEval 2.0:
    > MNPO 57.27 vs DPO 54.35, SimPO 55.16, SPPO 55.97, INPO 56.09.
  - Arena‚ÄëHard:
    > MNPO 52.26 vs INPO 48.03 (next best); others below 46.
  - MT‚ÄëBench:
    > MNPO 7.03 vs INPO 6.95; others ‚â§ 6.87.
  - Notably, MNPO‚Äôs Arena‚ÄëHard score (52.26) exceeds several larger open‚Äësource models and one listed closed model in Table 2.

- Knowledge and commonsense (Table 3)
  - Average across seven tasks:
    > MNPO 71.08 vs SFT 70.28; DPO 70.68; SPPO 70.19; INPO 70.25.
  - Highlights:
    > GPQA: MNPO 33.33 (best in table).  
    > IFEval: MNPO 73.94 (near best).  
    > TruthfulQA: MNPO 70.26 (SimPO drops to 63.40).

- Math and coding (Table 4)
  - Average across GSM8K, Minerva‚ÄëMath, AIME‚Äë24, HumanEval:
    > MNPO 48.10 (best), vs SPPO 47.33; INPO 47.10.
  - Highlights:
    > AIME‚Äë24: MNPO 3.33 while all baselines are 0.  
    > HumanEval: MNPO 61.59 (best).

Do experiments support the claims?
- The instruction‚Äëfollowing gains are consistent across three benchmarks (Table 2), lending credence to the claim that multiplayer competition improves alignment under heterogeneous preferences.
- The broader benchmark suite shows MNPO avoids the degradation seen in some baselines (e.g., TruthfulQA for SimPO in Table 3), suggesting better robustness.
- Math/coding results (Table 4) indicate MNPO helps on harder reasoning tasks (AIME‚Äë24, HumanEval), consistent with the idea that multiplayer dynamics encourage coverage of diverse strategies.

Caveats and missing pieces
- Preference supervision relies on a reward model, not human judgments, so improvements reflect alignment with that model‚Äôs preferences (Section 4).
- The study lacks ablations isolating the effect of the number of players n, the historical weight schedule `{Œª_j}`, or the reward‚Äëaware term; Table 1 shows recoveries of prior methods but does not provide per‚Äëcomponent ablations.
- No human evaluation or safety/robustness stress tests are reported.

## 6. Limitations and Trade-offs
- Reliance on preference oracle quality (Appendix F)
  - As the policy improves, distinguishing ‚Äúchosen‚Äù vs ‚Äúrejected‚Äù responses becomes harder for the oracle, constraining further gains.
  - Binary preference signals become less informative when both responses are high quality, causing diminishing returns.

- Computational and systems considerations
  - Multiplayer training maintains and samples from multiple opponent policies (historical or external). While tractable in this paper (3 iterations; 9B model), scaling to many large opponents may be costly in memory and sampling.

- Assumptions and scope
  - Theoretical results assume policies share support with the reference model and use KL‚Äëbased regularization (Section 2; Section 3.1). Practical deviations (e.g., truncated sampling, decoding constraints) are not analyzed.
  - The no‚Äëregret analysis (Appendix E.3) guarantees convergence in average play, not necessarily last‚Äëiterate convergence in noisy settings.

- Evaluation constraints
  - Automatic judging by `GPT-5-mini` stands in for human evaluation; judge bias and domain mismatch can affect scores.
  - The reward‚Äëaware connection (Eq. 17) is presented conceptually; the paper does not report separate experiments quantifying the benefit of this component.

## 7. Implications and Future Directions
- Field impact
  - Recasting preference alignment as a multiplayer game broadens the alignment toolkit beyond two‚Äëplayer dynamics. It encourages modeling populations of preferences‚Äîannotators, domains, or teacher models‚Äîas explicit opponents.
  - The TD‚ÄëMNPO lens provides a unifying view of the preference‚Äëoptimization landscape (Table 1), likely simplifying comparison, transfer of techniques, and hybrid designs.

- What this enables
  - Multi‚Äëannotator alignment: Simultaneously align to diverse preference clusters by treating each as an opponent population.
  - Multi‚Äëdomain or multi‚Äëskill training: Use external domain experts (Appendix F.1; Eq. 20) as opponents to distill strengths from multiple specialized models (Eq. 21).
  - Stability improvements: Historical‚Äëmixture opponents can yield smoother and safer online preference optimization.

- Practical applications
  - Instruction‚Äëtuned assistants evaluated by diverse users (robustness to style/length preferences).
  - Systems requiring balanced performance across reasoning, knowledge, and coding, where single‚Äëopponent tuning can overfit.

- Research directions
  - Opponent selection and weighting: Learn `{Œª_j}` and the opponent set adaptively (who to play, how often).
  - Human‚Äëin‚Äëthe‚Äëloop MNPO: Replace or complement reward models with stratified human preference panels to better capture heterogeneity.
  - Convergence and stability: Extend theory toward last‚Äëiterate convergence in stochastic multiplayer settings, and analyze the effect of KL strength and support constraints.
  - Safety and value pluralism: Encode safety reviewers and value groups as dedicated opponents to shape safer, more pluralistic behaviors.

In sum, MNPO contributes a principled, extensible framework for aligning large language models under complex, heterogeneous preferences, with both theoretical foundations (Eqs. 8‚Äì11; Lemma 1; Proposition 1; Appendix E.3) and empirical gains across instruction following and reasoning tasks (Tables 2‚Äì4).

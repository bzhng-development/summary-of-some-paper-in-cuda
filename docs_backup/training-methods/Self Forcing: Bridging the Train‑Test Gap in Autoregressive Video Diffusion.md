# Self Forcing: Bridging the Train‚ÄëTest Gap in Autoregressive Video Diffusion

**ArXiv:** [2506.08009](https://arxiv.org/abs/2506.08009)
**Authors:** Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Eli Shechtman
**Institutions:** Adobe Research, The University of Texas at Austin

## üéØ Pitch

This paper introduces "Self Forcing," a novel autoregressive training approach for video diffusion models, aligning training with inference by using the model's own predictions, and optimizing a holistic, video-level objective. By addressing exposure bias and incorporating an efficient rolling cache, it enables real-time, streaming video generation with superior visual quality, revolutionizing low-latency applications in content creation, gaming, and robotics.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces Self Forcing, a training paradigm for autoregressive (AR) video diffusion that explicitly rolls out the model on its own predictions during training and optimizes a holistic, video-level objective. By aligning training with inference and adding an efficient rolling key‚Äìvalue (KV) cache, it mitigates exposure bias and enables real-time, streaming video generation with sub-second latency while matching or surpassing the visual quality of slower, non-causal diffusion models (Table 1, Figure 4).

## 2. Context and Motivation
- Problem the paper addresses
  - AR video generators must produce frames sequentially, conditioning each new frame on previously generated frames. Traditional training regimes (e.g., teacher forcing) condition on ground-truth context, creating a train‚Äìtest mismatch: at inference time the model conditions on its own, imperfect outputs. This mismatch leads to error accumulation over time‚Äîknown as exposure bias (Section 1; Figure 1; citations [60, 71]).
- Importance
  - Practical: Low-latency streaming, live interactive content creation, games, and robotics require causal, frame-by-frame generation where the future is unknown and latency budgets are strict (Section 1). 
  - Scientific: Demonstrates how to bridge a fundamental distribution gap in sequential generative modeling without sacrificing efficiency, and shows how AR, diffusion, and GAN-style distribution matching can be combined coherently (Section 5).
- Prior approaches and their shortfalls
  - Teacher Forcing (TF): Each frame is denoised using clean ground-truth context frames (Figure 1a). It trains a next-frame predictor, but inference conditions on self-generated frames, creating exposure bias (Section 1).
  - Diffusion Forcing (DF): Each frame is denoised given noisy context frames with varying noise levels (Figure 1b), covering the inference case where context is clean and the current frame is noisy. However, DF still suffers from error accumulation and complicates caching and latency, and it does not fundamentally remove exposure bias (Section 1).
  - Bidirectional diffusion: High quality but non-causal, requiring access to future frames; unsuitable for real-time streaming (Section 1).
  - Closest related work, CausVid [100]: Uses DF inputs and distribution matching (DMD). The paper highlights a flaw: ‚Äúthe distribution it optimizes during training (using Diffusion Forcing outputs) deviates from the actual inference-time distribution‚Äù (Section 3.3), so the loss matches the wrong distribution.
- Positioning of this work
  - The method unrolls the actual AR inference process at training time (‚Äúself-rollout‚Äù) with KV caching, and trains using holistic video-level distribution matching losses (DMD, SiD, or GAN). This directly aligns the training and inference distributions, explicitly addressing exposure bias (Figures 1c, 2c; Section 3.2‚Äì3.3).

## 3. Technical Approach
The model is an AR video diffusion system: it factors a video `x1:N` into conditionals `p(xi | x<i)` and realizes each conditional via a short diffusion chain (‚Äúfew-step diffusion‚Äù). The key change is training it by autoregressively generating its own context during training itself, then optimizing a video-level loss that matches the distribution of generated videos to real data.

Step-by-step:

1) Background: Autoregressive video diffusion
- Factorization: The video distribution is written as a product of conditionals, `p(x1:N) = ‚àèi p(xi | x<i)` (Section 3.1).
- Conditional generation via diffusion: Each frame (or chunk of frames) is generated by iteratively denoising from Gaussian noise through a small number of timesteps T (few-step diffusion; T=4 in this paper) (Section 3.2; Appendix A). A ‚Äúchunk‚Äù is a small group of latent frames; this paper uses chunk size 3 for the chunk-wise variant (Section 4).

2) What ‚ÄúSelf Forcing‚Äù changes
- Self-rollout during training: Instead of conditioning on ground-truth or noisy ground-truth context (TF/DF), the model conditions on the frames it just generated itself, exactly mirroring inference (Figures 1c, 2c; Algorithm 1; Section 3.2).
- KV caching during training: The model caches attention keys/values for past frames so each new frame can attend to previous generated content efficiently‚Äîjust as in inference (Figure 2c; Algorithm 1).
- Holistic, video-level objective: After autoregressively generating the full video, the training signal compares the distribution of generated videos to the data distribution using one of several distribution-matching objectives applied at the whole-video level (Section 3.3).

3) How the few-step diffusion is implemented efficiently
- Few-step schedule: Use only 4 denoising steps with a uniform schedule `[t4, t3, t2, t1] = [1000, 750, 500, 250]` (Appendix A), based on the Flow Matching formulation of the Wan2.1 base model (Appendix A).
- Gradient truncation: To save memory, backpropagate only through the final denoising step used for the current training sample. Concretely, a timestep `s ‚àà {1,...,T}` is sampled per iteration, and only the `s`-th step receives gradient; earlier steps are run without gradient (Algorithm 1 lines 8‚Äì21; Section 3.2).
- Detaching past context: Gradients are stopped from flowing into the KV cache of past frames; this prevents backpropagating through the entire generated history (Algorithm 1 lines 12‚Äì14; Section 3.2).
- Rationale: This keeps training feasible while still providing supervision to all steps over time because `s` is sampled uniformly each iteration (Section 3.2).

4) The video-level distribution matching losses (Section 3.3; Appendix A)
- Motivation: Instead of MSE on a per-frame noise prediction (the TF/DF denoising loss), the model sees complete samples from its own inference-time distribution and receives a holistic signal that pushes the entire generated video distribution toward the true data distribution.
- Noise-injection trick for stability: Both generated and real videos are passed through the same diffusion forward process before computing the matching loss; this stabilizes training and leverages pretrained score networks (Section 3.3).
- Three alternatives are supported:
  - DMD (Distribution Matching Distillation): Minimizes reverse KL `DKL(pŒ∏,t || pdata,t)` via a score-difference update. The practical loss is Equation (3): 
    > LDMD(Œ∏) = E[ 1/2 || xÃÇ ‚àí sg[xÃÇ ‚àí (fœà(xÃÇt, t) ‚àí fœï(xÃÇt, t))] ||¬≤ ]
    where `fœï` is a pretrained ‚Äúreal‚Äù score network, `fœà` is a learned ‚Äúfake‚Äù score network used to estimate the model‚Äôs score, and `sg` is stop-gradient (Appendix A).
  - SiD (Score Identity Distillation): Matches scores via Fisher divergence; the training objective (Equation (4)) reduces to penalizing discrepancies between `fœï` and `fœà` with a specific weighting that yields stable training (`Œ±=1` in this paper; Appendix A).
  - GAN objective: Uses a relativistic discriminator with R1+R2 regularization implemented via finite-difference perturbations (Equations (5)‚Äì(7); Appendix A). The discriminator compares noisy real and noisy generated videos.

5) Rolling KV cache for long videos (Section 3.4; Algorithm 2; Figure 3)
- Problem: Extending generation to arbitrarily long videos via a sliding window is expensive. Bidirectional models cannot cache and have `O(T¬∑L¬≤)` complexity per new frame (Figure 3a). Prior causal models that use sliding windows still recompute KV for overlapping frames, leading to `O(L¬≤ + T¬∑L)` (Figure 3b).
- Approach: Maintain a fixed-size rolling KV cache of the last `L` frames; when a new frame is generated, evict the oldest KV and append the new one‚Äîno recomputation (Figure 3c; Algorithm 2). Complexity becomes `O(T¬∑L)`.
- Artifact mitigation for rolling cache: The first latent frame has different statistics (no temporal compression). If the cache rolls past the first frame, naive training causes flicker. The paper adds a training constraint: when denoising the final chunk, the attention window is restricted so the first chunk is invisible, simulating the rolling-cache condition (Section 3.4; Appendix B Figure 7).

6) Why these design choices
- Self-rollout vs TF/DF: Only self-rollout makes training and inference distributions match. Section 3.3 emphasizes: 
  > ‚Äúcontext frames {x<i} are sampled from the model‚Äôs own distribution pŒ∏ rather than from the data distribution.‚Äù
- Few-step diffusion and gradient truncation: Enables sequential training to be practical and fast, while still giving all steps supervision stochastically (Section 3.2).
- Holistic losses (DMD/SiD/GAN): Framewise denoising losses cannot assess a complete video from the model‚Äôs own distribution; holistic losses can (Section 3.3).
- Rolling KV cache: Required to sustain low latency over long sequences; the training-time attention restriction ensures that long-range extrapolation does not degrade (Section 3.4; Appendix B).

## 4. Key Insights and Innovations
- Training with self-generated context (core innovation)
  - What‚Äôs new: Train the AR video diffusion model by autoregressively rolling out on its own outputs during training with KV caching (Figures 1c, 2c; Algorithm 1).
  - Why it matters: It eliminates the train‚Äìtest mismatch (exposure bias) inherent in TF/DF and gives the model direct feedback on how to correct its own errors as they accumulate (Section 3.3).
- Holistic video-level distribution matching on the true inference distribution
  - What‚Äôs new: Apply DMD, SiD, or GAN to whole generated videos drawn from `pŒ∏` (the inference-time distribution), rather than applying per-frame denoising losses on TF/DF-generated training distributions (Section 3.3).
  - Why it matters: Aligns the entire sequence distribution and avoids CausVid‚Äôs issue of matching the wrong (DF) distribution (Section 3.3).
- Efficient sequential training despite transformers
  - What‚Äôs new: Combine few-step diffusion, gradient truncation to the final step, and detaching gradients through cached context to contain memory and compute (Algorithm 1; Section 3.2).
  - Why it matters: Training becomes fast enough to be practical; Figure 6 shows comparable per-iteration time to TF/DF and faster quality improvement vs wall-clock time.
- Rolling KV cache for long videos without recomputation
  - What‚Äôs new: A practical sliding-window method that never recomputes KVs and an associated training trick (local attention at the end of the context) to prevent flicker (Section 3.4; Appendix B Figure 7).
  - Why it matters: Improves throughput for extrapolation‚Äîe.g., 16.1 FPS vs 4.6 FPS when recomputing KVs for 10-second videos‚Äîwithout artifacts (Section 4, ‚ÄúRolling KV cache‚Äù).
- Real-time performance while preserving quality
  - What‚Äôs new: 17 FPS throughput and 0.69s latency (chunk-wise), and 0.45s latency (frame-wise), with VBench scores on par with or above much slower models (Table 1).
  - Why it matters: Low-latency, streaming-capable video generation with competitive quality enables interactive applications (Section 4).

## 5. Experimental Analysis
- Evaluation methodology
  - Base model and setup: Wan2.1-T2V-1.3B Flow Matching model, 832√ó480 at 16 FPS, 5s clips (Section 4; Appendix A). Both frame-wise and chunk-wise AR variants are built with 4-step diffusion; chunk size = 3 latent frames (Section 4).
  - Training data/pipeline:
    - For GAN and many-step TF/DF baselines, the paper generates 70k videos using a 14B base model as the ‚Äúreal‚Äù dataset (Section 4).
    - DMD and SiD forms of Self Forcing can be data-free, using only pre-trained score networks and prompts (Section 4).
    - Prompts come from a filtered and LLM-augmented VidProM set (Appendix A).
  - Metrics: VBench (16 sub-scores and overall), throughput (FPS), and first-frame latency (seconds) on a single NVIDIA H100 (Section 4; Figure 8 for sub-scores).
  - Baselines: Bidirectional diffusion (Wan2.1; LTX-Video), AR diffusion (SkyReels-V2, MAGI-1, CausVid), and other AR models (NOVA, Pyramid Flow) (Table 1).
- Main quantitative results
  - Real-time and quality (Table 1):
    > Self Forcing (chunk-wise): 17.0 FPS, 0.69s latency, VBench Total 84.31, Quality 85.07, Semantic 81.28  
    > Wan2.1: 0.78 FPS, 103s latency, Total 84.26  
    > CausVid: 17.0 FPS, 0.69s latency, Total 81.20  
    > Self Forcing (frame-wise): 8.9 FPS, 0.45s latency, Total 84.26
    The chunk-wise Self Forcing matches or slightly exceeds the Wan2.1 total score (84.31 vs 84.26) while being ~150√ó lower latency (0.69s vs 103s; Section 4; Table 1).
  - Human preference (Figure 4):
    > ‚ÄúSelf Forcing is preferred 66.1% vs 33.9% over CausVid; 62.7% vs 37.3% over Wan2.1; 57.9% vs 42.1% over SkyReels-V2; 54.2% vs 45.8% over MAGI-1.‚Äù
  - Qualitative comparisons (Figure 5): CausVid accumulates saturation artifacts over time; Self Forcing maintains consistent color/contrast across frames.
- Ablation studies (Table 2; Section 4)
  - Training paradigm comparison under identical setups:
    - Many-step (50√ó2) TF/DF vs few-step DF/TF+ DMD vs Self Forcing:
      > Chunk-wise: Self Forcing (DMD) 84.31 vs TF 83.58 vs DF 82.95; Self Forcing (SiD 84.07; GAN 83.88)  
      > Frame-wise: Self Forcing (DMD) 84.26 vs TF 80.34 vs DF 77.24
    - Key pattern: Baselines degrade when moving from chunk-wise to frame-wise (more AR steps = more error accumulation), while Self Forcing remains stable. This supports the claim that exposure bias is mitigated.
- Rolling KV cache results (Section 4; Appendix B)
  - Throughput: Recomputing KV in sliding windows yields 4.6 FPS for 10s videos; rolling cache achieves 16.1 FPS (Section 4).
  - Visual robustness: Naive rolling cache causes flicker; training with a restricted attention window during the last chunk removes these artifacts (Appendix B Figure 7).
- Training efficiency (Figure 6)
  - Per-iteration times are comparable to Teacher/Diffusion Forcing despite sequential rollout, and Self Forcing reaches higher VBench scores under the same wall-clock budget (Figure 6). The paper attributes this to: 
    - full attention with FlashAttention-3 (no complex masks; Section 4), 
    - and high GPU utilization within each frame/chunk even though frames are processed sequentially.
- Do the experiments support the claims?
  - Real-time: Yes‚Äîboth throughput and first-frame latency are reported and meet real-time constraints for certain applications (Table 1; Section 4).
  - Quality: Across VBench, user studies, and ablations, Self Forcing consistently equals or outperforms baselines while correcting exposure bias symptoms (Table 1, Table 2, Figures 4‚Äì5).
  - Robustness: Demonstrated across three different holistic objectives (DMD, SiD, GAN) and both frame-wise and chunk-wise variants (Table 2), suggesting the central idea (self-rollout + holistic matching) is objective-agnostic.

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Few-step diffusion is assumed sufficient to approximate each conditional `p(xi|x<i)`; very few steps keep the training/inference practical but may limit ultimate quality compared with many-step diffusion in some regimes (Sections 3.2, 4).
  - The approach presumes a capable pretrained video diffusion backbone (Wan2.1) and relies on a pretrained score network for DMD/SiD (Appendix A Table 3).
- Long-horizon generation
  - While rolling KV cache enables efficient extrapolation, quality still degrades on videos substantially longer than training context; the paper attributes this partly to gradient truncation limiting learning of very long-range dependencies (Section 5).
- Compute and reproducibility
  - Training examples: DMD converges in ~1.5 hours on 64 H100 GPUs; SiD/GAN take 2‚Äì3 hours (Section 4; Figure 6). This is efficient for large-scale labs but still heavy for many practitioners.
- Data considerations
  - For GAN training and strong baselines, the paper uses 70k ‚Äúreal‚Äù videos generated by a 14B model (Section 4). While standard in distillation/post-training work, this may bias results toward the teacher‚Äôs distribution and limits conclusions about performance on truly out-of-distribution content.
- Evaluation design
  - User study: one annotator per prompt over 1003 prompts (Appendix E). While the preference margins are sizable (Figure 4), single-judge evaluations introduce variance; more raters per prompt would tighten confidence.
- Engineering constraints
  - The method‚Äôs sequential nature sets a ceiling on throughput compared to fully parallel bidirectional denoisers; Self Forcing compensates with KV caching and few-step diffusion, but some applications may still desire more FPS than reported.

## 7. Implications and Future Directions
- How this changes the field
  - It reframes training for sequential generators: rather than optimizing local, per-step losses on ground-truth contexts, train by sampling from the model‚Äôs own inference distribution and optimize holistic sequence-level alignment. This is a principled remedy for exposure bias in continuous media (Sections 3.3, 5).
  - It demonstrates that AR, diffusion, and GAN-style distribution matching can be composed effectively: chain-rule factorization for time, few-step diffusion for spatial realism, and distribution matching for holistic supervision (Section 5).
- Follow-up research directions
  - Longer contexts with better recurrence: Explore state-space models or hybrid architectures to maintain long-range memory without large caches (Section 5; citations [19, 63]).
  - Improved video-level objectives: Alternative divergences, better discriminators for temporal coherence, or teacher-free score estimators to reduce reliance on large pretrained ‚Äúreal‚Äù score networks.
  - Curriculum and robustness: Training curricula that gradually increase AR unroll length, noise distributions, or prompt complexity to stress-test and harden the rollout policy.
  - Cross-domain generalization: Apply Self Forcing to other continuous sequence domains‚Äîspeech synthesis, motion capture, robotic trajectories‚Äîwhere exposure bias similarly hurts long rollouts (Section 5; citations [53, 90]).
- Practical applications
  - Low-latency, interactive generation: live streaming video effects, real-time content creation, game engines, online video editing, and robotics simulation, all benefiting from sub-second first-frame latency and steady throughput (Sections 1, 4; Table 1).
  - Long-video creation: Rolling KV cache supports scalable extrapolation with high temporal consistency when trained with the proposed local-attention constraint (Section 3.4; Appendix B).

> Bottom line: By training the model to practice on its own outputs and scoring the whole video it produces‚Äîexactly as it will run at test time‚ÄîSelf Forcing both fixes exposure bias and unlocks real-time, streaming video generation. The method‚Äôs modular nature (DMD/SiD/GAN; frame-wise/chunk-wise; rolling cache) makes it a practical template for sequential generative modeling beyond video.

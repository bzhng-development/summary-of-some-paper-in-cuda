# Qwen2.5-1M Technical Report

**ArXiv:** [2501.15383](https://arxiv.org/abs/2501.15383)

## üéØ Pitch

Introduces Qwen2.5-1M ‚Äî a family of open-weight and API models plus an open-source inference stack that reliably processes up to 1,000,000 tokens by combining cost-aware long-context pre/post-training with training-free length extrapolation, sparse attention, and engine-level optimizations. This makes practical, deployable million-token applications (repository-scale coding, multi-document research, large-context retrieval) possible without sacrificing short-context performance or prohibitive inference costs.

---

## 1. Executive Summary (2-3 sentences)

Qwen2.5-1M is a set of Qwen2.5-based language models and an accompanying open-sourced inference stack designed to reliably handle **up to 1,000,000 tokens of context** while keeping short-context performance largely intact. The work combines (i) **cost-aware long-context training** (synthetic long-dependency data + progressive context expansion + staged post-training) with (ii) **training-free inference extensions and accelerations** (length extrapolation + sparse attention + engine optimizations) that make 1M-token deployment practical. The key significance is enabling repository-scale coding, multi-document research, and other ‚Äúvery large input‚Äù applications with open(-weight) models and a deployable serving framework.

## 2. Context and Motivation

- **Problem / gap addressed**
  - Standard LLMs have limited `context length` (the number of input tokens processed at once), which blocks tasks requiring whole-repository or many-document reasoning (Introduction).
  - Extending context to hundreds of thousands or millions of tokens is expensive because attention compute and memory scale poorly with sequence length (Section 5.2).

- **Why it matters**
  - Real applications often require *global* context: code generation/debugging with full repositories, large-scale document research, and long-form synthesis (Introduction).
  - Even when models ‚Äúclaim‚Äù long context, performance often degrades when inference length exceeds training length due to positional encoding/extrapolation issues (Section 5.1).

- **Prior approaches and shortcomings (as positioned here)**
  - Industry trend: context windows grew from 4k/8k to ~128k, with some attempts at ~1M (Introduction).
  - Two practical blockers highlighted:
    1. **Training cost** of long-context pre-training (Section 3).
    2. **Inference cost and memory** at very long lengths, where dense attention can dominate runtime (Section 5.2 notes attention can be >90% of forward time at 1M tokens).

- **How this work positions itself**
  - Provides both:
    - **Model-side improvements**: long-context pre-training + post-training recipe to reach a robust 1M setting (Sections 3‚Äì4).
    - **Deployment-side improvements**: open-sourced inference framework integrated into `vLLM` that supports **length extrapolation** and **sparse attention acceleration** with additional kernel/system optimizations (Section 5).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **1M-context LLM family plus an inference/deployment framework** that can run those models efficiently.
- It solves long-input processing by combining **(a) training that teaches long-range use of context up to 262,144 tokens** and **(b) inference tricks that safely extrapolate and accelerate computation up to 1,000,000 tokens** (Sections 3‚Äì5).

### 3.2 Big-picture architecture (diagram in words)

- **Base architecture (models)** ‚Üí Qwen2.5 Transformer with `GQA`, `RoPE`, `RMSNorm`, `SwiGLU`, and attention `QKV bias` (Section 2).
- **Long-context pre-training pipeline**:
  1. Build corpus (natural long data + synthetic long-dependency tasks) (Section 3).
  2. Train progressively with longer sequence lengths and adjusted RoPE base frequencies (Section 3, Table 2).
- **Post-training pipeline**:
  1. Synthesize long instruction data using model + agent framework (Section 4).
  2. Two-stage `SFT` (short-only then mixed short+long) (Section 4).
  3. Offline `RL` (DPO-like) using short preference pairs that generalize to long tasks (Section 4, Table 3).
- **Inference/deployment pipeline**:
  1. Training-free **length extrapolation** (`DCA` + `YaRN` scaling) (Section 5.1, Figure 2, Eq. (1)).
  2. **Sparse attention** based on `MInference`, combined with chunked prefill + refinements for compatibility and accuracy at 1M length (Section 5.2, Figures 4‚Äì6, Eqs. (2)‚Äì(4), Algorithm 1).
  3. **Inference engine optimizations** (kernels, pipeline parallelism, scheduling) in BladeLLM; some integrated into vLLM (Section 5.3, Figures 7‚Äì10).

### 3.3 Roadmap for the deep dive

- Explain (1) **what the released models are** and their architecture constraints (Section 2, Table 1).
- Then (2) **how training is made long-context-capable without prohibitive cost** (Section 3, Table 2).
- Next (3) **how post-training preserves short-task skill while adding long-task alignment** (Section 4, Table 3).
- Then (4) **how inference reaches 1M tokens without retraining** via length extrapolation (Section 5.1, Figure 2, Eq. (1)).
- Finally (5) **how deployment becomes fast enough** using sparse attention and systems optimizations, including accuracy fixes (Section 5.2‚Äì5.3, Algorithm 1, Figures 6‚Äì11).

### 3.4 Detailed, sentence-based technical breakdown

This is an **empirical systems + methods** report: it combines a training recipe for long-context instruction models with an inference stack (length extrapolation + sparse attention + engine optimizations) to make **1M-token** contexts usable in practice (Abstract; Sections 3‚Äì6).

#### 3.4.1 Models and architecture (what is built)

- The open-weight releases are `Qwen2.5-7B-Instruct-1M` and `Qwen2.5-14B-Instruct-1M`, plus an API-served `Qwen2.5-Turbo` MoE model (Abstract; Section 2).
- The Transformer backbone stays compatible with Qwen2.5 inference and uses:
  - `Grouped Query Attention (GQA)` to reduce KV-cache cost (Section 2).
  - `RoPE` for position encoding and `RMSNorm` with pre-normalization for training stability (Section 2).
  - `SwiGLU` activation and attention `QKV bias` (Section 2).
- Table 1 specifies the key architecture-level counts for the open-weight models:
  - `7B`: **28 layers**, **28 attention heads (Q) / 4 (KV)**, embeddings not tied, **context 1M / generation 8K**, Apache 2.0 license (Table 1).
  - `14B`: **48 layers**, **40 heads (Q) / 8 (KV)**, embeddings not tied, **context 1M / generation 8K**, Apache 2.0 license (Table 1).
- Important missing details: the provided excerpt does **not** include optimizer settings, learning rate schedule, batch size, tokenizer, hidden size, head dimension, total training tokens, or training hardware for pre-training/SFT/RL; those cannot be filled in without guessing.

#### 3.4.2 Long-context pre-training (how the base model learns long-range use)

**System / data pipeline diagram in words (pre-training):**
1. First, the training corpus is assembled from **natural long-text sources** (Common Crawl, arXiv, books, code repositories) to cover domains and formats (Section 3).
2. Second, because natural long texts often have weak long-distance dependencies, **synthetic tasks** are generated and mixed in to force the model to connect far-apart parts of the sequence (Section 3).
3. Third, training proceeds with a **progressive context expansion schedule** across five stages, increasing sequence length while adjusting RoPE base frequency (Section 3).
4. Fourth, performance is monitored at the end of each stage using the `RULER` benchmark to track how capability changes with training length (Section 3, Table 2).

**Synthetic data tasks used to induce long-range dependencies (Section 3):**
- `Fill in the Middle (FIM)`: the model predicts missing spans inside a sequence, which forces it to use both left and right distant context (Section 3).
- `Keyword-based and position-based retrieval`: the model is trained to retrieve paragraphs by keyword or relative position (before/after a specified location), encouraging cross-document-span association (Section 3).
- `Paragraph reordering`: shuffled paragraphs must be put back into logical order, encouraging global structure modeling (Section 3).

**Progressive training strategy and RoPE adjustments (Section 3):**
- The schedule uses five stages, starting from Qwen2.5 base training regimes and then extending:
  - Start at **4,096 tokens**, then transfer to **32,768 tokens** (Section 3).
  - Use `ABF` to adjust RoPE base frequency from **10,000 ‚Üí 1,000,000** in the early transfer (Section 3).
  - Continue expanding context length to **65,536**, **131,072**, and **262,144** tokens with RoPE base frequencies **1,000,000**, **5,000,000**, and **10,000,000** respectively (Section 3).
- Data mixing at long stages uses **75% sequences at the current maximum length and 25% shorter sequences**, which is intended to both adapt to the new maximum and preserve multi-length generalization (Section 3).

**Evidence that progressive length training helps (Table 2):**
- Table 2 evaluates `Qwen2.5-14B-1M` on RULER after each stage. The most salient pattern is that as training length increases, scores at longer evaluation lengths improve substantially.
  - Example: RULER at **128K** rises from **37.6** (after 32,768-token stage) ‚Üí **56.0** (65,536) ‚Üí **83.8** (131,072) ‚Üí **87.6** (262,144) (Table 2).
  - The average score increases from **82.3** ‚Üí **86.8** ‚Üí **92.5** ‚Üí **92.7** across those stages (Table 2).
- The report highlights that training at 262,144 tokens still improves performance at 128K evaluation length, consistent with ‚Äútraining longer helps shorter long-context too‚Äù (Section 3 discussion around Table 2).

#### 3.4.3 Post-training (how instruction-following and alignment extend to long contexts)

**System / data pipeline diagram in words (post-training):**
1. First, long documents are sampled from the pre-training corpus (Section 4).
2. Second, `Qwen2.5` is prompted to generate **queries** based on randomly extracted document segments, spanning task types like summarization, retrieval, multi-hop QA, reasoning, and coding (Section 4).
3. Third, `Qwen-Agent` generates **high-quality responses** using mechanisms like retrieval-augmented generation, chunk-by-chunk reading, and step-by-step reasoning to incorporate the full document (Section 4).
4. Fourth, these (document, query, response) triples become synthetic long instruction tuning data (Section 4).
5. Fifth, training uses a **two-stage SFT** to protect short-task skills, and an **offline RL** stage for preference alignment (Section 4).

**Two-stage supervised fine-tuning (Section 4):**
- Stage 1: train exclusively on *short* instruction data up to **32,768 tokens** with the same number of steps as other Qwen2.5 models, to preserve short-task competence (Section 4).
- Stage 2: train on a *mixed* dataset with lengths spanning up to **32,768** and up to **262,144** tokens, balancing the short/long ratio to avoid catastrophic forgetting (Section 4).

**Offline reinforcement learning for alignment (Section 4, Table 3):**
- The RL method is described as similar to `DPO` (Direct Preference Optimization) and uses **short** preference pairs up to **8,192 tokens** (Section 4).
- The key claim is generalization: short-sample RL improves long-context alignment, evaluated on `Longbench-Chat`.
  - Table 3 shows improvements ‚Äúbefore vs after RL‚Äù:
    - `Qwen2.5-7B-Instruct-1M`: **7.32 ‚Üí 8.08** (**+0.75**) (Table 3).
    - `Qwen2.5-14B-Instruct-1M`: **8.56 ‚Üí 8.76** (**+0.20**) (Table 3).
    - `Qwen2.5-Turbo`: **7.60 ‚Üí 8.34** (**+0.74**) (Table 3).

#### 3.4.4 Inference: reaching 1M context efficiently

At 1M tokens, two distinct inference problems appear: (i) **extrapolation** (maintaining accuracy beyond trained length) and (ii) **cost** (reducing the huge attention compute/memory).

##### A) Training-free length extrapolation: DCA + YaRN attention scaling (Section 5.1)

**Why extrapolation is needed (Section 5.1):**
- RoPE-based models degrade when inference sequences exceed training sequences because attention must compare tokens at relative distances not seen during training.

**Dual Chunk Attention (`DCA`) mechanism (Section 5.1, Figure 2):**
- DCA splits the long sequence into chunks and **remaps relative positions** so that the maximum relative distance used in attention does not exceed the pre-training length.
- It uses three attention patterns (Section 5.1):
  - `Intra-Chunk Attention`: within-chunk attention keeps original short relative positions.
  - `Inter-Chunk Attention`: across non-adjacent chunks uses repeated/limited relative position patterns to keep distances bounded.
  - `Successive-Chunk Attention`: for adjacent chunks, it keeps original relative positions within a local window, and otherwise falls back to the inter-chunk remapping strategy.
- Figure 2 illustrates the intuition: remapping avoids ‚Äúgray areas‚Äù of large relative positions not encountered during training (Figure 2 caption).

**YaRN attention logit scaling (Section 5.1, Eq. (1)):**
- Adds a temperature `t` to the attention logits to keep attention ‚Äúfocused‚Äù at long lengths:
  - The modified attention uses `softmax((q^T k) / (t * sqrt(D)))`.
  - The temperature depends on scaling factor `s` (inference length / training length) via `1/t = 0.1 ln(s) + 1` (Eq. (1)).
  - Symbols defined in the report: `q` query, `k` key, `D` attention head dimension, `s` length ratio (Section 5.1).

**Combined use and short-length invariance:**
- The report uses `DCA + YaRN` together in experiments and notes these methods ‚Äúdo not alter behavior‚Äù for sequences within training length (Section 5.1; also reiterated in Tables 4‚Äì5 notes).

**Evidence of benefit (Section 5.1, Figure 3):**
- Figure 3 evaluates 1M-token inference on selected RULER tasks (Passkey Retrieval and two NIAH variants) and reports that DCA significantly improves performance when context far exceeds training length.
- A specific quantitative statement included in the excerpt: for Passkey Retrieval, DCA enables both `Qwen2.5-7B-Instruct` and `Qwen2.5-14B-Instruct` to reach **over 80% accuracy up to 1M tokens** despite being trained only up to **32K** (Section 5.1 discussion of Figure 3).

##### B) Sparse attention acceleration: MInference + chunked prefill + accuracy refinements (Section 5.2)

**Motivation (Section 5.2):**
- Full attention is quadratic in context length, and at **1,000,000 tokens** attention can exceed **90%** of total forward time (Section 5.2).
- The goal is to accelerate the **prefill** stage (processing the prompt/context) without large accuracy loss.

**Core sparse attention method: MInference (Section 5.2, Figure 4):**
- MInference exploits observed attention sparsity patterns for long contexts: ‚ÄúVertical-Slash‚Äù patterns (vertical and diagonal lines) in attention maps (Section 5.2, Figure 4(a)).
- Workflow described (Section 5.2):
  1. Perform an **offline search** to choose a per-head sparsity configuration (how many vertical/diagonal lines).
  2. During inference, compute attention between the **last query tokens (‚Äúlast q‚Äù)** and all keys.
  3. Use that partial attention to select **critical tokens** following the ‚ÄúVertical-Slash‚Äù pattern.
  4. Compute attention only over selected critical tokens.
- The excerpt claims ~**10√ó** reduction in compute/memory access costs with minimal accuracy loss in the base MInference idea (Section 5.2).

**Chunked prefill integration to reduce VRAM (Section 5.2, Figure 4(b)):**
- Problem: encoding an entire 1M-token sequence at once makes activation memory huge.
  - Example given: in a single MLP layer of Qwen2.5-7B at 1M tokens, activation VRAM can reach **71 GB** (Section 5.2).
- Chunked prefill processes the input in chunks sequentially; with **chunk length 32,768**, activation VRAM is reduced by **96.7%** (Section 5.2).
- Integration detail: when selecting critical tokens per chunk, the method uses the **last 64 tokens within each chunk** (because the ‚Äúlast tokens of the entire input‚Äù are not yet available when processing early chunks) (Section 5.2).

**Compatibility issue with DCA and the fix (Section 5.2, Figure 5):**
- Observed failure mode: combining MInference with DCA can degrade performance.
- Hypothesis: DCA‚Äôs **non-continuous relative positions** disrupt the diagonal ‚Äúslash‚Äù pattern used to select critical tokens (Section 5.2).
- Fix: use **continuous relative positions** *only during the critical-token selection phase* for successive- and inter-chunk attentions, while still using DCA‚Äôs non-continuous positions for the final attention computation (Section 5.2, Figure 5).

**Sparsity refinement for true 1M behavior (Section 5.2, Algorithm 1, Eqs. (2)‚Äì(4)):**
- Problem: the offline sparsity configuration search is typically run on sequences ‚â§32k due to quadratic full-attention memory, which can be suboptimal at 1M tokens (Section 5.2).
- The refinement calibrates sparsity configs on **1M-token sequences** using an ‚Äúattention recall‚Äù metric derived from FlashAttention‚Äôs `softmax lse`:
  - Full: `softmax lse_full = log sum_{0<=j<=i} exp(q^T k_j / sqrt(D))` (Eq. (2)).
  - Sparse: `softmax lse_sparse = log sum_{j in Critical} exp(q^T k_j / sqrt(D))` (Eq. (3)).
  - `Attention Recall = exp(softmax lse_sparse ‚àí softmax lse_full)` in `[0,1]` (Eq. (4)).
- Algorithm 1 loops over layers and heads; if attention recall falls below a `Threshold`, it increases the vertical/slash budgets in the sparsity config (Algorithm 1).

**Evidence that refinements recover accuracy (Section 5.2, Figure 6):**
- Needle-in-a-Haystack evaluation on `Qwen2.5-7B-Instruct-1M` up to 1M tokens:
  - Full attention retrieves ‚Äúthe majority of needles‚Äù even at 1M (Figure 6(a) description).
  - Original MInference drops significantly; beyond **400k** tokens retrieval can fall to **60% or lower** (Section 5.2 discussion of Figure 6).
  - With continuous-position token selection + sparsity refinement, performance ‚Äúrecovers most‚Äù while keeping about **4√ó** prefill speedup (Section 5.2 discussion of Figure 6(c)).

##### C) Inference engine optimizations: kernels, parallelism, scheduling (Section 5.3)

The API deployment uses `BladeLLM`, and parts are open-sourced / integrated into `vLLM` (Section 5.3).

**Kernel optimization (Section 5.3.1, Figures 7‚Äì8):**
- Sparse attention kernel:
  - BladeLLM uses multi-stage pipeline parallelism and instruction-level optimization for loading sparse KV pairs (Section 5.3.1).
  - Claims up to **90%** peak FLOPs utilization (Section 5.3.1).
  - Figure 7 reports on A100 at 1M context:
    - `MInference` achieves **13.7√ó** speedup vs FlashAttention.
    - `BladeLLM` achieves **27.8√ó** speedup under the same sparsity configuration (Section 5.3.1, Figure 7 discussion).
- MoE kernel (for `Qwen2.5-Turbo`):
  - Decoding becomes memory-bandwidth bound at batch size ‚â•32 due to large parameter access (Section 5.3.1).
  - On H20 GPU, BladeLLM reports **3.4 TB/s** peak memory access efficiency, **55%** better than vLLM‚Äôs FusedMoE kernels (Section 5.3.1, Figure 8 discussion).

**Dynamic Chunked Pipeline Parallelism (Section 5.3.2, Figure 9):**
- Issue: with chunked prefilling, different chunks have different ‚Äúhistory length‚Äù (KV cache length), causing unequal attention compute times and pipeline bubbles (Section 5.3.2, Figure 9(a)).
- Solution: dynamically adjust chunk sizes based on attention compute complexity so chunk execution times are equalized, reducing bubbles (Section 5.3.2, Figure 9(b)).

**Scheduling: Totally Asynchronous Generator (TAG) (Section 5.3.3, Figure 10):**
- Traditional inference engines run scheduler/model runner/decoder serially, so non-GPU stages can reduce GPU utilization (Section 5.3.3).
- TAG splits scheduler, model runner, decoder into separate processes with no synchronization requirement, using shared memory to reduce IPC overhead (Section 5.3.3, Figure 10).
  - Scheduler allocates KV cache for the next step without waiting for model runner output (Section 5.3.3).
  - Model runner pushes sampled token IDs directly to decoder‚Äôs queue and continues (Section 5.3.3).
  - Decoder converts IDs to text and sends to API server asynchronously (Section 5.3.3).

## 4. Key Insights and Innovations

- **(1) Cost-aware long-context training via progressive context expansion + synthetic long-dependency tasks**
  - What‚Äôs different: instead of training at 1M length directly, the approach climbs from 4k ‚Üí 32k ‚Üí 65k ‚Üí 131k ‚Üí 262k while using synthetic tasks that explicitly require long-range dependency use (Section 3).
  - Why significant: Table 2 shows large gains at 128K evaluation (e.g., **37.6 ‚Üí 87.6**) as training length increases, suggesting the staged approach effectively buys long-context capability without committing full training to the maximum length from the start (Table 2).

- **(2) Training-free length extrapolation to 4√ó (or more) beyond trained length using DCA + YaRN**
  - What‚Äôs different: DCA changes how relative positions are represented at inference by chunking/remapping, explicitly avoiding unseen large relative distances in RoPE attention (Section 5.1, Figure 2).
  - Why significant: it enables 1M-context inference even when training length is far smaller; the excerpt highlights >80% Passkey Retrieval accuracy at 1M for 32K-trained instruction models with DCA (Section 5.1 discussion of Figure 3).

- **(3) Sparse attention deployment recipe that remains accurate at 1M by addressing two concrete failure sources**
  - What‚Äôs different:
    - Adds chunked prefill to make sparse prefill feasible in VRAM (Section 5.2).
    - Fixes DCA-induced non-continuous position issues for critical token selection (Figure 5).
    - Introduces a 1M-length calibration/refinement loop using attention recall from `softmax lse` (Eqs. (2)‚Äì(4), Algorithm 1).
  - Why significant: Figure 6 shows that naive sparse attention can crash retrieval accuracy at >400k tokens, while the refined approach recovers most performance and keeps ~4√ó prefill speedup (Section 5.2, Figure 6).

- **(4) End-to-end systems optimization (BladeLLM) targeted at long-prefill bottlenecks**
  - What‚Äôs different: combines algorithmic sparsity with kernel-level improvements (Figure 7), parallelism that adapts chunk sizes to avoid bubbles (Figure 9), and a fully asynchronous serving pipeline (Figure 10).
  - Why significant: these are the pieces that translate a research idea (sparse attention) into a practical 1M-token TTFT reduction (Section 6.3, Figure 11).

## 5. Experimental Analysis

### Evaluation methodology (what is measured and compared)

- **Long-context retrieval / reasoning-style benchmarks** (Section 6.1):
  - `Passkey Retrieval` up to **1,000,000 tokens** (Figure 1).
  - `RULER` up to **128K tokens** (Section 6.1, Table 4).
  - `LV-Eval` up to **256K tokens** (Section 6.1, Table 5).
  - `Longbench-Chat` up to **100K tokens** (Section 6.1, Table 5; RL effect in Table 3).
- **Baselines** used in long-context comparisons include: `GLM4-9B-Chat-1M`, `Llama-3-8B-Instruct-Gradient-1048k`, `Llama-3.1-70B-Instruct`, `GPT-4o-mini`, `GPT-4o`, `GPT-4` (Section 6.1; Table 4‚Äì5 list several explicitly).
- **Short-context benchmarks** (Section 6.2, Table 6):
  - General: `MMLU-Pro`, `MMLU-redux`, `LiveBench 0831`.
  - Math/science: `GPQA`, `GSM8K`, `MATH`.
  - Coding: `HumanEval`, `MBPP`, `MultiPL-E`, `LiveCodeBench 2305-2409`.
  - Alignment/following: `IFEval` (strict prompt-level accuracy), `MT-Bench`, `Arena-Hard`.
- **Speed / TTFT** measured as time-to-first-token vs context length on **NVIDIA H20 and A100**, batch size 1, with tensor parallelism configurations:
  - 14B and Turbo: **8-way TP**
  - 7B: **4-way TP** due to GQA constraints (Section 6.3).

### Main quantitative results (key numbers)

**A) Progressive pre-training improves long-length performance (Table 2):**
- `Qwen2.5-14B-1M` on RULER at 128K:
  - **37.6** (32,768 training length)
  - **56.0** (65,536)
  - **83.8** (131,072)
  - **87.6** (262,144) (Table 2)

**B) Long-context benchmark performance (Tables 4‚Äì5):**
- On `RULER Avg.`:
  - `Qwen2.5-14B-Instruct-1M`: **95.7** (Table 4)
  - `GPT-4`: **91.6** (Table 4)
  - `GPT-4o-mini`: **87.3** (Table 4)
- On `RULER 128K` specifically:
  - `Qwen2.5-14B-Instruct-1M`: **92.2** (Table 4)
  - `GPT-4`: **81.2** (Table 4)
  - `GPT-4o-mini`: **65.8** (Table 4)
- On `LV-Eval` at longer lengths (Table 5):
  - `Qwen2.5-14B-Instruct-1M`: **50.1 (64K)**, **47.6 (128K)**, **43.3 (256K)** (Table 5)
  - `GPT-4o-mini`: **46.0 (64K)**, **40.7 (128K)**, `N/A (256K)` (Table 5)
- `LongBench-Chat`:
  - `Qwen2.5-14B-Instruct-1M`: **8.76** (Table 5)
  - `GPT-4o-mini`: **8.48** (Table 5)
  - `Qwen2.5-Turbo`: **8.34** (Table 5)
  - (And RL deltas are shown separately in Table 3.)

**C) Short-context performance largely maintained (Table 6):**
- `Qwen2.5-14B` vs `Qwen2.5-14B-1M` examples:
  - `MMLU-Pro`: **63.7 ‚Üí 63.3** (Table 6)
  - `GSM8K`: **94.8 ‚Üí 94.8** (Table 6)
  - `HumanEval`: **83.5 ‚Üí 88.4** (Table 6)
- Some metrics decline for 7B when moving to 1M (e.g., `MMLU-Pro` **56.3 ‚Üí 54.3**, `MBPP` **79.2 ‚Üí 75.9**) while others improve (e.g., `GPQA` **36.4 ‚Üí 41.4**, `HumanEval` **84.8 ‚Üí 86.0**) (Table 6). This indicates ‚Äúno compromise‚Äù is not uniformly monotonic across all tasks, especially at smaller scale.

**D) Speed / TTFT improvements at 1M context (Section 6.3, Figure 11):**
- Reported overall speedups: **3.2√ó to 6.7√ó** at 1M tokens across model sizes/devices (Section 6.3, Figure 11).
- Concrete examples on H20 (Section 6.3):
  - `Qwen2.5-14B-Instruct-1M`: **12.2 minutes ‚Üí 109 seconds**.
  - `Qwen2.5-Turbo`: **4.9 minutes ‚Üí 68 seconds**.

### Do the experiments support the claims?

- **Supported well:**
  - The combination of training + inference methods improves long-context benchmarks, with clear numeric gains over baselines on RULER and competitive numbers on LV-Eval / LongBench-Chat (Tables 4‚Äì5).
  - The need for refinement when mixing sparse attention and length extrapolation is concretely demonstrated (failure of naive MInference at >400k; recovery with refinements) (Figure 6 + Section 5.2 narrative).
  - TTFT reductions are quantified with before/after comparisons at 1M context (Section 6.3, Figure 11).

- **Less fully supported / unclear from the provided excerpt:**
  - Exact training cost reductions are claimed qualitatively (‚Äúreducing training costs‚Äù), but the excerpt does not provide compute budgets, token counts, wall-clock times, or GPU-hours for training (Abstract; Section 3 lacks explicit cost numbers).
  - Some ‚Äúnearly identical to full attention‚Äù claims for MInference are qualitative here; the excerpt provides a Needle-in-a-Haystack case study, but not broad quantitative parity tables across many tasks (Section 5.2, Figure 6).

### Ablations / robustness / failure cases

- **Ablation-like evidence exists** in multiple places:
  - Stage-by-stage pre-training length effect (Table 2).
  - Before/after RL effect (Table 3).
  - Full attention vs original MInference vs refined MInference (Figure 6).
  - Kernel speed comparisons (Figure 7) and MoE kernel bandwidth (Figure 8).
- **Failure case documented**: original MInference loses retrieval accuracy badly at long lengths (>400k tokens) (Section 5.2, Figure 6(b)).

## 6. Limitations and Trade-offs

- **Missing reproducibility-critical training details**
  - The excerpt does not specify optimizer, learning rate schedule, batch size, tokenizer, total training tokens, compute budget, or training hardware; this limits the ability to exactly replicate the training pipeline from the report content provided.

- **Training length vs claimed inference length**
  - Pre-training is described up to **262,144 tokens**, while the models ‚Äúsupport‚Äù **1,000,000 tokens** via inference-time extrapolation (Section 3; Table 1; Section 5.1). This means 1M performance depends materially on DCA+YaRN behavior and may vary by task type beyond retrieval-oriented tests.

- **Sparse attention introduces an accuracy‚Äìspeed trade-off**
  - The report shows that naive sparsification can significantly hurt retrieval accuracy at very long lengths (Figure 6(b)), and additional complexity (continuous position selection + 1M calibration) is required to recover performance (Section 5.2).
  - The refinement process depends on a **threshold** and calibration on 1M-token sequences (Algorithm 1), but the excerpt does not specify the threshold value or calibration set construction details.

- **Operational complexity**
  - Deploying the full stack (DCA+YaRN + MInference + chunked prefill + refined sparsity + engine optimizations like TAG/DCPP) increases engineering complexity compared to standard full-attention serving (Sections 5.1‚Äì5.3).

- **Evaluation coverage limits**
  - Long-context benchmarks emphasize retrieval and multi-evidence understanding (Passkey, RULER, LV-Eval). Less is shown (in the provided excerpt) about long-context generation quality, long-horizon planning, or adversarial robustness at 1M tokens.

## 7. Implications and Future Directions

- **How this changes the field / practice**
  - The report demonstrates a practical recipe where **training to ~256K** plus **training-free extrapolation** can yield usable **1M-token** behavior, shifting the focus from ‚Äútrain at 1M‚Äù to ‚Äútrain enough + extrapolate safely‚Äù (Sections 3 and 5.1).
  - It also argues that 1M-context LLMs are not just a modeling problem: kernel efficiency, memory management, and scheduling architecture materially affect feasibility (Sections 5.2‚Äì5.3; Figures 7‚Äì11).

- **Follow-up research suggested by the presented gaps**
  - Better theoretical/empirical understanding of when DCA‚Äôs remapped relative positions preserve reasoning vs when they distort long-range relations (Section 5.1 + Figure 2 motivation).
  - More principled sparse attention calibration objectives beyond attention-recall thresholds, and clearer generalization guarantees across task types (Section 5.2, Algorithm 1).
  - Training strategies that close the remaining gap between ‚Äútrained length‚Äù and ‚Äúclaimed inference length,‚Äù potentially reducing reliance on extrapolation for the hardest tasks (Sections 3 and 5.1).

- **Practical applications enabled**
  - Repository-scale code assistance (read many files at once), multi-document research assistants, and long-term context chat agents‚Äîany setting where the input context can reach hundreds of thousands to a million tokens (Introduction; Section 6.1‚Äôs 1M Passkey test demonstrates this scale).

- **Repro/Integration Guidance (when to use what, based on this report)**
  - If you need **‚â§256K** context and want maximum faithfulness to training distribution, prefer operating within the trained regime (pre-training described up to 262,144 tokens in Section 3).
  - If you need **>trained length up to ~1M**, the report‚Äôs recommended inference stack is **`DCA + YaRN`** for length extrapolation (Section 5.1, Eq. (1)), and **sparse attention with refinements** for cost reduction (Section 5.2, Algorithm 1).
  - If deploying at very long contexts, the report suggests that *engine-level choices* (chunked prefill, kernel implementations, pipeline parallelism, and asynchronous scheduling) can be the difference between minutes and seconds of TTFT at 1M tokens (Section 6.3, Figure 11; Sections 5.3.2‚Äì5.3.3).



# Scalable Diffusion Models with Transformers

**ArXiv:** [2212.09748](https://arxiv.org/abs/2212.09748)

## ğŸ¯ Pitch

This paper introduces Diffusion Transformers (DiT), replacing the conventional U-Net backbone in image diffusion models with a pure Vision Transformer architecture. The authors demonstrate that DiT scales smoothly with compute and achieves state-of-the-art image generation quality, establishing a strong empirical link between model FLOPs and sample fidelity. This innovation not only challenges longstanding assumptions about convolutional inductive biases but also paves the way for unified, scalable architectures across vision and language, unlocking easier cross-domain research and more efficient training practices.

---

## 1. Executive Summary
This paper replaces the longâ€‘standing Uâ€‘Net backbone used in diffusion image generators with a pure Vision Transformer and shows that this â€œDiffusion Transformerâ€ (`DiT`) scales smoothly with compute and achieves stateâ€‘ofâ€‘theâ€‘art results on classâ€‘conditional ImageNet. A central finding is a strong, quantitative link between forwardâ€‘pass compute (measured in `Gflops`) and sample quality (measured by FID), plus a transformerâ€‘specific conditioning/initialization trick (`adaLNâ€‘Zero`) that makes training stable and efficient.

## 2. Context and Motivation
- Problem/gap:
  - Diffusion models dominate highâ€‘quality image generation but almost universally rely on the convolutional Uâ€‘Net architecture introduced in early DDPM work. The field lacked a careful study of whether the Uâ€‘Net inductive bias is necessary and how a transformer backbone would scale for diffusion (Section 1).
  - Model comparison often uses parameter counts, which ignore factors like input resolution and sequence length. This can be misleading for generative architectures whose perâ€‘token compute dominates runtime (Architecture complexity discussion, p.3).

- Why it matters:
  - If a transformer backbone works as well or better, diffusion models can inherit the transformer ecosystemâ€™s favorable scaling, training practices, and crossâ€‘domain unification, simplifying research and deployment across vision and language (Section 1).

- Prior approaches and shortcomings:
  - Uâ€‘Net based DDPMs and variants (e.g., ADM, LDM) achieve strong results but are tied to convolutional hierarchies and architectural conventions tuned over years (Related Work, p.2â€“3).
  - Prior transformer uses in diffusion largely targeted nonâ€‘spatial latents or autoregressive modeling, not a full diffusion backbone for images (Related Work, p.2).

- Positioning:
  - The paper introduces `Diffusion Transformers (DiT)`, a ViTâ€‘style backbone that operates on latent patches rather than pixels to keep compute manageable, and studies scaling using `Gflops` (Sections 3 and 4). It then compares DiT to Uâ€‘Nets on ImageNet and analyzes scaling behavior across 12 model/patch configurations (Figures 2, 6, 7, 8).

## 3. Technical Approach
This section explains the full pipeline from data to generated image, and the core design choices.

- Data space: latent diffusion rather than pixels
  - Images are encoded by a preâ€‘trained variational autoencoder (`VAE`) from Stable Diffusion (Diffusion, Section 4, p.6). At 256Ã—256 pixels the latent `z` has shape 32Ã—32Ã—4; at 512Ã—512 it is 64Ã—64Ã—4. Diffusion is trained and sampled in this latent space (LDM, Section 3.1).

- Diffusion training objective (DDPM refresher; Section 3.1):
  - Forward noising: real image `x0` is gradually perturbed to `xt` by adding Gaussian noise: q(xt|x0) = Normal(âˆšÎ±Ì„t x0, (1âˆ’Î±Ì„t) I).
  - Reverse denoising: train a network to predict the noise `ÎµÎ¸(xt, c)` and (diagonal) covariance `Î£Î¸(xt, c)` so that sampling `xtâˆ’1 âˆ¼ pÎ¸(xtâˆ’1|xt, c)` inverts the process.
  - Loss: main term is meanâ€‘squared error between predicted and true noise (Equation â€œLsimpleâ€, p.3). The covariance is trained with the full KL term (Nichol & Dhariwal trick, p.3).

- Conditioning mechanism: classifierâ€‘free guidance (CFG; Section 3.1)
  - During training, class labels `c` are randomly dropped and replaced by a learned â€œnullâ€ embedding. At sampling, combine unconditional and conditional predictions:
    ÎµÌ‚Î¸(xt, c) = ÎµÎ¸(xt, âˆ…) + s Â· (ÎµÎ¸(xt, c) âˆ’ ÎµÎ¸(xt, âˆ…)) with guidance scale `s > 1` (p.3â€“4).
  - This generally improves fidelity at the cost of diversity (used in final benchmarks; Tables 2â€“3).

- From latent grid to transformer tokens (â€œpatchifyâ€; Section 3.2; Figure 4):
  - The 2D latent `z` is split into nonâ€‘overlapping pÃ—p patches; each patch is linearly embedded to a token. Sequence length is T = (I/p)^2 where `I` is the latent spatial size (e.g., I=32 for 256Ã—256 images). Smaller `p` â†’ more tokens â†’ higher `Gflops`, with negligible change in parameters (p.4).
  - Standard sineâ€‘cosine positional embeddings are added (p.4).

- Transformer backbone and conditioning (Figure 3; p.4â€“5):
  - A stack of `N` DiT blocks operates on tokens. Several conditioning strategies are compared:
    1) Inâ€‘context conditioning: append two tokens for time `t` and class `c` to the sequence; no architectural changes.
    2) Crossâ€‘attention: add a multiâ€‘head crossâ€‘attention layer that queries the 2â€‘token condition sequence; â‰ˆ15% `Gflops` overhead (p.5).
    3) Adaptive LayerNorm (`adaLN`): replace standard LayerNorm with a version whose scale/shift `(Î³, Î²)` are learned from the combined `t+c` embedding; minimal compute overhead (p.5).
    4) `adaLNâ€‘Zero` (new): like `adaLN` but also predict perâ€‘residual scaling gates `Î±` applied right before residual connections, and initialize those gates to zero so each transformer block starts as the identity function (p.5). This mirrors zeroâ€‘init tricks known to stabilize deep ResNets.

- Output head (Transformer decoder; p.5):
  - Apply a final LayerNorm and a linear layer to each token to produce pÃ—pÃ—2C values, where `C` is latent channels (4 here). The 2C channels are split into predicted noise and predicted diagonal covariance, then reshaped back to the latent grid.

- Model scale (Table 1; p.5) and design grid:
  - Four ViTâ€‘style sizes: `DiTâ€‘S`, `B`, `L`, `XL` with increasing depth/width. Combined with three patch sizes `p âˆˆ {8,4,2}` â†’ 12 models total (e.g., `DiTâ€‘XL/2` is XL with p=2).
  - Gflops span roughly 0.36 to 118.6 at 256Ã—256; the 512Ã—512 `XL/2` uses 524.6 Gflops (Table 4).

- Training setup (Section 4; p.6):
  - Common hyperparameters across all models: AdamW, constant LR 1eâˆ’4, batch 256, only horizontal flips, no LR warmup or regularization, EMA decay 0.9999. Quote:
    > â€œWe use a constant learning rate of 1Ã—10âˆ’4, no weight decay and a batch size of 256â€¦ training was highly stable across all model configsâ€ (p.6).
  - Diffusion schedule: 1000 linear betas from 1eâˆ’4 to 2eâˆ’2, embeddings for time/labels, 250 sampling steps for FID evaluation (p.6). Implemented in JAX on TPU v3 pods (Compute, p.6).

- Compute modeling:
  - Architectural complexity tracked primarily via forward `Gflops` (p.3). For training compute, they estimate:
    training compute â‰ˆ (model `Gflops`) Ã— (batch size) Ã— (steps) Ã— 3
    where â€œ3â€ accounts for forward + backward passes (Figure 9 caption).

## 4. Key Insights and Innovations
1) A pure ViT backbone can replace the Uâ€‘Net in DDPMs without lossâ€”and with gains.
   - The `DiT` family consistently improves as either transformer size increases or patch size decreases (Figures 2 left and 6). Visual samples show clearly higher fidelity as `Gflops` rise (Figure 7).

2) `adaLNâ€‘Zero`: transformerâ€‘native conditioning that initializes each block as identity.
   - Mechanism: predict LayerNorm `(Î³, Î²)` and residual gates `Î±` from the combined time+label embedding; initialize `Î±=0` so residual paths are off at the start (Figure 3, p.5).
   - Evidence: On the largest setup (`XL/2`) this conditioning beats crossâ€‘attention and inâ€‘context conditioning at every point in training (Figure 5). Quote:
     > â€œAt 400K training iterations, the FID achieved with the adaLNâ€‘Zero model is nearly half that of the inâ€‘context modelâ€ (p.6).

3) Computeâ€“quality scaling law using `Gflops`.
   - Across 12 models, FID strongly correlates with forward `Gflops` regardless of whether compute comes from depth/width or token count; correlation âˆ’0.93 (Figure 8). Holding parameters roughly fixed and increasing tokens still improves FID (Figure 6 bottom), showing that computeâ€”not just parameter countâ€”drives quality here.

4) Model compute cannot be replaced by more sampling steps.
   - Increasing the number of diffusion sampling steps (testâ€‘time compute) helps but does not close the gap to larger backbones. Example (Figure 10):
     > `DiTâ€‘L/2` with 1000 steps uses ~80.7 Tflops per sample and still has worse FIDâ€‘10K (25.9) than `DiTâ€‘XL/2` with 128 steps (15.2 Tflops, FIDâ€‘10K 23.7).

5) Stateâ€‘ofâ€‘theâ€‘art ImageNet results with competitive compute.
   - On ImageNet 256Ã—256, `DiTâ€‘XL/2` with CFG=1.5 achieves FIDâ€‘50K 2.27, improving over prior diffusion and even over the best GAN baseline (StyleGANâ€‘XL, FID 2.30) (Table 2). At 512Ã—512, `DiTâ€‘XL/2` achieves FID 3.04, best among diffusion baselines and using far fewer `Gflops` than pixelâ€‘space ADM variants (Table 3).

Together, these are more than incremental tweaks: they establish a transformerâ€‘first design space for diffusion, a conditioning scheme that makes it work well, and a computeâ€‘centric lens that predicts performance.

## 5. Experimental Analysis
- Evaluation setup
  - Dataset: ImageNet, classâ€‘conditional at 256Ã—256 and 512Ã—512 (Section 4).
  - Metrics: FIDâ€‘50K with 250 DDPM steps as the main metric, plus Inception Score, sFID, and Precision/Recall (p.6). They use ADMâ€™s TensorFlow evaluation suite to reduce implementation variance in FID (p.6). Note: FID measures distance between the feature distributions of generated vs real images under the Inception network; lower is better.
  - Baselines: ADM, ADMâ€‘U/ADMâ€‘G (pixelâ€‘space Uâ€‘Net DDPM variants), LDMâ€‘4/LDMâ€‘8 (latent Uâ€‘Net diffusion), GANs (BigGANâ€‘deep, StyleGANâ€‘XL) (Tables 2â€“3).
  - Design grid and training regime:
    - 12 DiT models spanning four sizes (S/B/L/XL) and three patch sizes (p=8/4/2), trained for up to 400K steps for scaling analyses (Figure 6; Table 4). Final `XL/2` models are trained much longer: 7M steps for 256Ã—256 and 3M for 512Ã—512 (Table 4).
    - Identical optimization hyperparameters across all models (Section 4).

- Main quantitative results
  - Scaling behavior:
    - Increasing transformer size at fixed patch size reduces FID across training (Figure 6 top).
    - Decreasing patch size at fixed model size (i.e., more tokens) also reduces FID (Figure 6 bottom).
    - Strong global correlation between `Gflops` and FID at 400K steps (âˆ’0.93; Figure 8). Inception Score and Precision also rise with `Gflops` (Figure 12).
    - Larger models are more computeâ€‘efficient: for the same total training compute, bigger DiTs achieve lower FID (Figure 9).
    - Visual confirmation: same noise seed and class produce higherâ€‘quality images as either transformer size or token count grows (Figure 7).

  - Conditioning ablation:
    - `adaLNâ€‘Zero` dominates crossâ€‘attention and inâ€‘context across training (Figure 5), with less `Gflops` than crossâ€‘attention (â‰ˆ15% overhead for crossâ€‘attention, p.5).

  - Stateâ€‘ofâ€‘theâ€‘art benchmarks:
    - ImageNet 256Ã—256 (Table 2):
      > `DiTâ€‘XL/2â€‘G (cfg=1.50)` FIDâ€‘50K = 2.27, Inception Score = 278.24, Precision = 0.83, Recall = 0.57.
      - This improves over LDMâ€‘4â€‘G at cfg=1.50 (FID 3.60) and over StyleGANâ€‘XL (FID 2.30). It also shows higher recall than LDM variants at the same guidance scales (p.9).
      - Without guidance, the 7Mâ€‘step model achieves FIDâ€‘50K 9.62 (Table 2; Table 4), showing CFGâ€™s importance for fidelity.
    - ImageNet 512Ã—512 (Table 3):
      > `DiTâ€‘XL/2â€‘G (cfg=1.50)` FIDâ€‘50K = 3.04, beating prior diffusion models (ADMâ€‘G,U best = 3.85) with far fewer `Gflops` (524.6 vs 1983â€“2813 for ADM variants; Table 3).
      - StyleGANâ€‘XL still has a lower FID (2.41) at this resolution; the contribution here is SOTA among diffusion models plus compute efficiency.

  - Samplingâ€‘compute vs modelâ€‘compute (Figure 10):
    - Even extreme increases in sampling steps for a smaller model cannot match the FID of a larger model at moderate steps; e.g., `L/2` (1000 steps) vs `XL/2` (128 steps) example quoted above.

  - Additional ablations and checks:
    - VAE decoder choice: swapping between original LDM decoder and two fineâ€‘tuned Stable Diffusion decoders yields similar scores (Table 5); the best (ftâ€‘EMA) gives the headline results.
    - Classifierâ€‘free guidance applied to only the first 3 of 4 latent channels performs comparably to guiding all channels after rescaling the guidance factor (Appendix A: â€œClassifierâ€‘free guidance on a subset of channels,â€ p.12).
    - Training loss curves consistently improve with scale (Figure 13).

- Do the experiments support the claims?
  - The scaling law is supported by a broad sweep (12 architectures Ã— multiple patch sizes), consistent trends across training, and correlation analyses (Figures 6, 8, 9, 12).
  - Conditioning conclusions are backed by direct ablations holding everything else fixed (Figure 5).
  - Benchmark claims rely on standardized FID evaluation and widely used baselines (Tables 2â€“3). The 256Ã—256 result is stateâ€‘ofâ€‘theâ€‘art across generative models; the 512Ã—512 result is SOTA among diffusion methods but not against the best GAN.

## 6. Limitations and Trade-offs
- Reliance on a preâ€‘trained VAE:
  - All training and sampling occur in latent space, not pixels (Section 3.1; Diffusion). This assumes the VAE preserves information needed for highâ€‘fidelity synthesis. While decoder ablations look favorable (Table 5), performance could still be bounded by the autoencoderâ€™s capacity or bias.

- Classâ€‘conditional only:
  - Experiments are on classâ€‘conditional ImageNet. Extension to textâ€‘toâ€‘image or unconditioned settings is discussed as future work (Conclusion, p.9), but not demonstrated here.

- Compute demands:
  - Although computeâ€‘efficient relative to pixelâ€‘space Uâ€‘Nets, the best results come from very large models trained for millions of steps (e.g., `XL/2` at 7M steps). Training required TPU v3â€‘256 pods with ~5.7 it/s for the largest model (Compute, p.6), so practical training remains expensive.

- Evaluation dependence:
  - FID is known to be sensitive to implementation details (p.6). The paper mitigates this by using ADMâ€™s evaluation code but still relies primarily on FID and Inceptionâ€‘based metrics; no human evaluation or downstream tasks are reported.

- Transformer memory/latency:
  - Decreasing patch size increases sequence length quadratically (Figure 4), which can stress memory and slow training/sampling notwithstanding favorable `Gflops`.

- Scope of architectural exploration:
  - The study focuses on ViTâ€‘like blocks and four conditioning variants. Other transformer designs (e.g., local attention, sparse attention, hybrid convâ€‘transformers) are not explored.

## 7. Implications and Future Directions
- Field impact:
  - Demonstrates that diffusion models need not be tied to Uâ€‘Nets; a ViT backbone works, scales predictably with compute, and can surpass prior stateâ€‘ofâ€‘theâ€‘art. This unifies generative modeling architecture with modern vision/language stacks and opens the door to shared scaling laws and training recipes (Section 1; Figure 2).

- Research enabled or suggested:
  - Scaling: Figures 8â€“9 and 12 indicate â€œmore tokens + bigger transformerâ€ should continue to pay off. Expect further gains from longer training, larger models, or smaller patches (more tokens).
  - Conditioning: `adaLNâ€‘Zero` is simple and effective; it may generalize to text condition, multiâ€‘modal conditioning, or other iterative refinement models.
  - Efficiency: Explore attention variants (local/sparse), patch routing, or distillation to reduce the tokenâ€‘length bottleneck introduced by small `p`.
  - Beyond class labels: Integrate DiT as a dropâ€‘in backbone for textâ€‘toâ€‘image systems (e.g., Stable Diffusion) as proposed in the conclusion (p.9).

- Practical applications:
  - Highâ€‘fidelity classâ€‘conditional generation for data augmentation, simulation, and content creation with better compute/quality tradeâ€‘offs than pixelâ€‘space diffusion.
  - As a modular backbone, DiT may simplify integrating vision generation with transformerâ€‘based perception or language models in multiâ€‘modal systems.

Overall, this work replaces a widely assumed architectural choice (Uâ€‘Net) with a transformer design that is simple, computeâ€‘predictable, and highâ€‘performing. The detailed scaling study (Figures 6â€“10, 12) and the strong ImageNet results (Tables 2â€“3) make a convincing case that transformers are a firstâ€‘class backbone for diffusion image generation.

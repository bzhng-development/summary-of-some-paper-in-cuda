# Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing (SAPO)

**ArXiv:** [2509.08721](https://arxiv.org/abs/2509.08721)
**Authors:** Jeffrey Amico, Gabriel Passamani Andrade, John Donaghy, Ben Fielding, Tristin Forbus, Harry Grieve, Semih Kara, Jari Kolehmainen, Yihua Lou, Christopher Nies, Edward Phillip Flores NuÃ±o, Diogo Ortega, Shikhar Rastogi, Austin Virts, Matthew J. Wright
**Institutions:** 

## ğŸ¯ Pitch

SAPO introduces a decentralized, asynchronous RL method that significantly enhances language model training by enabling nodes to share and learn from decoded experiences without central synchronization. This innovation democratizes training across heterogeneous hardware, boosting reasoning capabilities and collective intelligence while reducing traditional infrastructure costs and bottlenecks, marking a pivotal step in scalable and effective multi-agent reinforcement learning.

---

## 1. Executive Summary
This paper introduces SAPO (Swarm sAmpling Policy Optimization), a decentralized, asynchronous reinforcement-learning (RL) postâ€‘training method where many independent nodes train their own language models but â€œshareâ€ experience by exchanging decoded rollouts. In controlled studies on verifiable reasoning tasks, balanced experience sharing (4 local / 4 external rollouts per update) nearly doubles cumulative reward versus standard singleâ€‘agent RL, and a large open demo shows swarm training overtakes isolated training for midâ€‘sized models after sufficient rounds.

## 2. Context and Motivation
- The problem: RL postâ€‘training can substantially improve reasoning without extra labeled data, but scaling RL for language models typically requires centralized clusters that keep a single policy synchronized across many GPUs. This brings latency, memory and communication bottlenecks, fragility, and high cost (Â§1â€“Â§2; references to distributed RL systems in Â§2).
- Why it matters:
  - Practical: Many potential contributors only have heterogeneous, edgeâ€‘class hardware (e.g., laptops). A way to get RL benefits without centralized orchestration would democratize postâ€‘training.
  - Scientific: Multiâ€‘agent diversity can drive exploration and â€œAha momentsâ€ that single policies might miss; enabling those effects during training could raise sample efficiency (Â§1â€“Â§2).
- Prior approaches and gaps:
  - Centralized distributed RL for LMs (e.g., weightâ€‘synchronized PPO/GRPO in large clusters) is effective but costly and communicationâ€‘heavy (Â§2).
  - Multiâ€‘agent methods often require designed roles or orchestration (debate, verifier/generator roles, etc.; Â§2), adding engineering complexity.
- Positioning:
  - SAPO is a bridge between singleâ€‘agent RL fineâ€‘tuning and multiâ€‘agent systems: it keeps each node independent yet lets nodes learn from othersâ€™ experience by sampling shared rollouts. It assumes no synchronized weights, no model homogeneity, and no timing guarantees (Â§1, Â§3).

Key terms (paperâ€‘specific or uncommon):
- `swarm`: a decentralized network of N nodes, each training its own policy and sharing experience (Â§3.1).
- `rollout`: a set of completions/answers that a policy generates for one question/task (Â§3.1).
- `verifiable task`: a task whose answer can be checked programmatically by a `verifier`, giving a deterministic reward signal (used here via ReasoningGYM; Â§3.1, Â§4.1).
- `advantage`: in policy-gradient RL, how much better a sampled action performed than a baseline for the same context. SAPO uses it to filter out uninformative samples (zero-advantage rollouts) before training (Â§3.2, Â§5).
- `GRPO`: Group Relative Policy Optimization, a PPOâ€‘style objective that uses a group baseline across multiple responses to the same prompt. Used here for policy updates (Â§4.2).

## 3. Technical Approach
At a high level, each node trains locally but augments its training data with other nodesâ€™ experiences. Crucially, rollouts are shared as decoded text so any node can reâ€‘encode them to compute tokenâ€‘level likelihoods under its own policyâ€”enabling policy-gradient updates without sharing weights.

Stepâ€‘byâ€‘step (see Â§3.1â€“Â§3.2 and Algorithm 1):
1. Swarm setting and data
   - Each node n has:
     - A local set of verifiable tasks `D_n = {(q, y_q)}` with metadata `M_n` describing how to verify answers (Â§3.1).
     - Its own policy `Ï€_n` (the LM) and a local reward function `Ï_n` (a verifier; Â§3.2).
   - Compatibility: rollouts must have compatible modalities (e.g., textâ€‘only across nodes). Nodes can ignore incompatible items (Â§3.1).

2. Local rollout generation
   - Per training round, node n samples a batch of tasks `B_n` from its local tasks `Q_n` (Â§3.2).
   - For each question `q âˆˆ B_n`, it generates `L_n` completions `R_n(q) = {a^n_1(q), â€¦, a^n_{L_n}(q)}`â€”this is the local rollout (Â§3.1â€“Â§3.2).

3. Sharing decoded experience
   - The node broadcasts a subset `S_n âŠ† B_n` along with tuples
     - `C_n(q) := (q, y_q, R_n(q), M_n)` (Â§3.2),
     - i.e., the question, groundâ€‘truth, the decoded completions, and the metadata for verification.
   - Why decoded text matters: any other node can reâ€‘encode these completions with its own tokenizer/model, compute logâ€‘probabilities, and apply its policy-gradient algorithm â€œas ifâ€ it had generated those tokensâ€”even if they were unlikely under its current policy (Â§3.2).

4. Constructing the perâ€‘node training set
   - Each node n chooses how many items to use from its own rollouts (`I_n`) and how many to sample from the swarm (`J_n`) to form a training set `T_n` (Â§3.2):
     - `T_n = SampleSelf({C_n(q) | q âˆˆ B_n}, I_n) âˆª SampleExternal(â‹ƒ_{mâ‰ n}{C_m(q) | q âˆˆ S_m}, J_n)`.
   - Sampling is fully local and customizable. In the controlled experiments (Â§5), nodes:
     - Drop rollouts with zero advantage,
     - Then uniformly sample `J_n` items from the remaining shared pool (Â§5).

5. Rewarding and updating
   - Node n computes rewards for items in `T_n` using its local verifier `Ï_n` (Â§3.2).
   - It updates its policy with a policyâ€‘gradient method (here, GRPO; Â§4.2). Setting `J_n = 0` recovers standard singleâ€‘agent RL (Â§3.2).

Design choices and why:
- Share decoded rollouts instead of weights or gradients:
  - Avoids synchronization and network bottlenecks; works across heterogeneous models and hardware (Â§1â€“Â§3).
- Require verifiable tasks:
  - Enables decentralized, ruleâ€‘based reward computation without sharing a reward model or humanâ€‘inâ€‘theâ€‘loop labels (Â§3.1, Â§4.1).
- Local control over sampling/filtering:
  - Lets nodes tailor how much they rely on others, mitigate lowâ€‘quality or offâ€‘distribution samples, and adapt to network conditions (Â§3.2, Â§5).

Concrete example (mirroring Â§4â€“Â§5):
- In one round, a node draws 8 ReasoningGYM tasks, generates 8 answers per task, shares those, then forms `T_n` by keeping 4 of its own tasks and 4 tasks sampled from the swarm (dropping any with zero advantage). It computes rewards with the corresponding verifiers and applies a GRPO update.

Training specifics used in the controlled experiments (Â§4):
- Models: eight `Qwen2.5-0.5B` SLMs, each in its own Docker container; multiâ€‘GPU orchestration with PyTorch/NCCL, 1 GPU per agent (Â§4).
- Tasks: nine specialties from ReasoningGYM (e.g., base conversion, binary matrix reasoning; Â§4.1).
- Policy updates: GRPO with no KL penalty (following observations in DAPO), asymmetric clipping thresholds `Îµ_low = 0.2`, `Îµ_high = 0.28`, Adam lr=0.001, 2000 rounds (Â§4.2).
- Rewards: 1 if the verifier parses a correct answer, else 0, with rare partial credit exceptions (Â§4.3). A separate â€œformatting rewardâ€ was tested then removed because correct formatting propagated through sharing (Â§4.3).
- Platform: GenRL, the backend for Gensynâ€™s RLSwarm, which integrates ReasoningGYM and supports peerâ€‘toâ€‘peer coordination (Â§4.4).

## 4. Key Insights and Innovations
1. Decentralized, asynchronous RL via decoded experience sharing
   - Novelty: Nodes never synchronize weights or share gradients; they only exchange decoded rollouts plus minimal metadata (Â§3.2).
   - Why it matters: Removes central bottlenecks and cost, tolerates heterogeneity (different models/hardware), and functions under arbitrary latency (Â§1â€“Â§3).

2. Reâ€‘encoding foreign rollouts for onâ€‘policy style updates
   - Mechanism: A node reâ€‘encodes othersâ€™ completions to compute its own tokenâ€‘level logâ€‘probs and advantages, then applies GRPO/PPO locally (Â§3.2).
   - Significance: Enables learning from â€œunlikelyâ€ but successful trajectories discovered by other policiesâ€”so â€œAha momentsâ€ can propagate (Â§1, Â§5).

3. Balanced sampling of local vs. external experience is critical
   - Empirical finding: Too much reliance on external data increases oscillations and forgetting; a 50/50 split (4 local / 4 external) maximizes cumulative reward (Â§5; Figures 1â€“2).
   - Importance: Offers a principled knob for stabilityâ€‘performance tradeâ€‘offs in collective RL.

4. Verifierâ€‘driven rewards make collective RL practical at the edge
   - Using programmatic verifiers (ReasoningGYM) allows each node to score both local and external rollouts without shared reward models or humans (Â§4.1, Â§4.3).
   - Observation: Formatting conventions spread through the swarm without an explicit formatting reward, reducing rewardâ€‘engineering burden (Â§4.3).

Incremental vs. fundamental:
- Fundamental: The â€œshare decoded rollouts + reâ€‘encode for policy gradientsâ€ paradigm and the fully decentralized swarm structure.
- Incremental: Specific GRPO settings (no KL, clipping thresholds) and the particular ReasoningGYM task mix.

## 5. Experimental Analysis
Evaluation design
- Controlled study (Â§4â€“Â§5):
  - Setup: 8 agents (Qwen2.5â€‘0.5B), each round draws 8 tasks (from 9 ReasoningGYM specialties), generates 8 completions per task (Â§4.1â€“Â§4.2).
  - Sharing configurations (I local / J external with I + J = 8): baseline 8/0; and 6/2, 4/4, 2/6 (Â§5).
  - Sampling rule: discard zeroâ€‘advantage shared rollouts; uniformly sample the rest (Â§5).
  - Metric: perâ€‘round rewards and cumulative rewards across agents; also movingâ€‘average â€œexpected rewardâ€ with window=100 (Â§5, Figure 2).
- Large open demo (Â§6):
  - Thousands of heterogeneous nodes; a â€œjudgeâ€ issues random ReasoningGYM questions after each round and scores pass@1 via verifiers (Â§6).
  - Metric: cumulative and perâ€‘round performance vs. an isolated baseline, aligned by normalized rounds to handle intermittent participation (Â§6, Figure 3).

Main quantitative results
- Controlled experiments (Figure 1â€“2; Â§5):
  - Cumulative reward across agents and rounds:
    - 4 local / 4 external: 1093.31
    - 2 local / 6 external: 945.87
    - 6 local / 2 external: 854.43
    - Baseline 8 / 0: 561.79
  - The 4/4 configuration â€œyields a 94% improvement over the baselineâ€ (Figure 1 caption; Â§5).
  - Expected average reward over training (moving average, Figure 2): 4/4 consistently beats baseline and usually beats 6/2; it also surpasses 2/6 for most rounds, though by a smaller margin (Â§5).
  - Dynamics: Heavier external reliance (2/6) shows stronger oscillations, interpreted as (i) highâ€‘performers polluted by weaker agents and (ii) overall pool quality dropping when many sample but few contribute (Â§5).
- Large demo (Figure 3; Â§6):
  - For `Qwen2.5-0.5B`, swarm training significantly outperforms isolated training after about 175 normalized rounds:
    > â€œAfter a certain number of rounds, in this case approximately 175, the performance per round of the models in the swarm significantly exceeds that of the model trained in isolation.â€ (Figure 3 caption; Â§6)
  - For stronger `Qwen3-0.6B`, performance with and without the swarm is similar (Â§6).
  - Note: In this demo, nodes sampled shared rollouts uniformly without filtering, likely overâ€‘representing lowâ€‘value samples (Â§6).

Do the experiments support the claims?
- The controlled study convincingly shows that sharing improves both peak and cumulative reward, with a clear optimum around balanced sharing (Figures 1â€“2). The movingâ€‘average analysis in Figure 2 supports that these gains are not mere noise.
- The large demo indicates the effect persists at scale and in heterogeneous conditions for midâ€‘sized models, though benefits appear modelâ€‘capacity dependent and sensitive to sampling strategy (Â§6).

Ablations, failure modes, robustness
- Explicit ablations:
  - Sharing ratio ablation (8/0, 6/2, 4/4, 2/6) and the zeroâ€‘advantage filtering rule (Â§5).
- Observed failure/instability:
  - Heavy external reliance causes oscillations and forgetting (Â§5, Figure 2).
- Not present:
  - No systematic ablation of alternative sampling/weighting strategies for shared rollouts, or comparisons of RL algorithms beyond GRPO (noted in Â§4.2 and Â§7 as future work).
  - No communicationâ€‘overhead measurements, though the method communicates text rather than weights (Â§2 acknowledges overhead but argues fewer rounds offset it).

## 6. Limitations and Trade-offs
Assumptions
- Verifiable tasks: SAPOâ€™s experiments require tasks with programmatic verifiers to compute rewards locally (Â§3.1, Â§4.1). Pure preferenceâ€‘based or hardâ€‘toâ€‘verify objectives would need different mechanisms (e.g., reward models or generative verifiers).
- Compatible modalities: Nodes must share rollouts in modalities other nodes can process; otherwise items are ignored (Â§3.1).

Scenarios not fully addressed
- Humanâ€‘preference alignment (RLHF) and openâ€‘ended tasks without deterministic checks are not evaluated; integration is suggested as future work (Â§7).
- Security/trust in large swarms: The method assumes nodes can filter lowâ€‘quality samples, but robustness to malicious or systematically biased contributions is not studied (Â§7 hints at trust-aware sampling).

Computational and system tradeâ€‘offs
- Communication and reâ€‘encoding overhead:
  - The system sends decoded rollouts and metadata; each node reâ€‘encodes them locally (Â§2). While lighter than weight synchronization, it still adds overhead that grows with swarm size.
- Stability vs. sharing ratio:
  - More external data improves exploration but can destabilize learning (oscillations/forgetting) if it overwhelms local experience (Figures 1â€“2; Â§5).
- Modelâ€‘capacity dependence:
  - In the open demo, a stronger SLM (`Qwen3-0.6B`) did not benefit over isolation under naive uniform sampling (Â§6), suggesting benefits may depend on model capacity and sampling quality.

Open questions
- How to optimally balance local vs. external data online?
- How to design robust, trustâ€‘aware sampling/weighting in open swarms?
- How does SAPO perform on larger LMs, other domains (e.g., code), or nonâ€‘verifiable objectives?

## 7. Implications and Future Directions
How this changes the landscape
- Makes RL postâ€‘training accessible without centralized infrastructure: sharing decoded experience rather than weights lets heterogeneous, edge devices collaborate (Â§1â€“Â§3).
- Provides a practical path to capture multiâ€‘agent exploration benefits without orchestrating roles or synchrony. Balanced sharing emerges as a simple, effective recipe (Figures 1â€“2).

What it enables next
- Smarter sampling from the swarm:
  - Rewardâ€‘guided filters, trust scores, perâ€‘peer reliability, or curriculumâ€‘style selection to avoid the oscillations seen with heavy external reliance (Â§5). The demoâ€™s uniform sampling likely underestimates SAPOâ€™s potential (Â§6).
- Hybrid training:
  - Combine SAPO with RLHF or generative verifiers to cover nonâ€‘verifiable objectives and richer reward signals (Â§7; refs. to generative verifiers).
- Adaptive controllers:
  - Metaâ€‘policies that tune `I_n` vs. `J_n` per node based on performance, variance, or peer quality (Â§7).
- Heterogeneous/multiâ€‘modal swarms:
  - Specialize nodes by domain or modality (text, code, images) and study crossâ€‘modal influence; the paper notes intriguing effects when â€œtasteâ€ or aesthetics become rewards (Â§7).
- Humanâ€‘inâ€‘theâ€‘loop participation:
  - Since any â€œpolicyâ€ can contribute rollouts, humans or other nonâ€‘LM policies could seed highâ€‘value examples, provided incentives and verification exist (Â§3.1 note; Â§7).

Practical applications
- Reasoningâ€‘centric SLMs on consumer hardware (e.g., educational tools, local assistants) that improve collaboratively.
- Federatedâ€‘style enterprise training where data cannot be centralized but verifiable tasks exist.
- Open research platforms (like Gensynâ€™s RLSwarm/GenRL) to study collective learning dynamics at scale (Â§4.4, Â§6).

Quoted highlights grounded in the paper:
- â€œBalanced experience sharing (4 local / 4 external) â€¦ yields a 94% improvement over the baseline.â€ (Figure 1 and Â§5)
- â€œThe 4 local / 4 external configuration consistently achieves higher expected average reward than the baselineâ€ (Figure 2; Â§5).
- â€œAfter approximately 175 normalized rounds, the performance per round of the models in the swarm significantly exceeds that of the model trained in isolation.â€ (Figure 3; Â§6)

In sum, SAPOâ€™s core mechanismâ€”learning from othersâ€™ decoded experience by reâ€‘encoding and applying local policy gradientsâ€”is both simple and powerful. The controlled and atâ€‘scale results show clear gains when sharing is balanced and filtered, and the framework opens a wide space for principled sampling, robustness, and hybrid reward research.

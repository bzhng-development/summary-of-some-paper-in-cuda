# PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model

**ArXiv:** [2510.14528](https://arxiv.org/abs/2510.14528)

## ğŸ¯ Pitch

PaddleOCR-VL presents a cutting-edge, resource-efficient document parsing framework that decouples layout analysis from content recognition, combining a lightweight detector (PP-DocLayoutV2) with the ultra-compact PaddleOCR-VL-0.9B vision-language model. This innovation achieves state-of-the-art accuracy on multilingual, structurally complex documentsâ€”including expert-level handling of text, tables, formulas, and chartsâ€”while delivering unprecedented speed and efficiency over both expert pipelines and heavyweight VLMs. Its robust, language-agnostic, and scalable design empowers real-world applications like search, analytics, and RAG, unlocking high-fidelity structured data extraction for over 100 languages on practical hardware.

---

## 1. Executive Summary (2â€“3 sentences)
PaddleOCRâ€‘VL introduces a twoâ€‘stage, resourceâ€‘efficient system for parsing complex, multilingual documents: a lightweight layout analyzer (`PP-DocLayoutV2`) followed by a compact visionâ€‘language model (`PaddleOCRâ€‘VLâ€‘0.9B`) specialized for element recognition (text, tables, formulas, charts). Across public and inâ€‘house benchmarks, it achieves stateâ€‘ofâ€‘theâ€‘art accuracy while running faster and with lower memory than competing systems; e.g., it tops OmniDocBench v1.5 with an overall score of 92.56 and is 15.8% faster in endâ€‘toâ€‘end throughput than the strongest baseline on A100 GPUs (Table 2 and Table 13).

## 2. Context and Motivation
- Problem addressed
  - Parsing realâ€‘world documents into structured, machineâ€‘usable formats requires: detecting layout regions (paragraphs, tables, formulas, charts), ordering them for correct reading flow, and recognizing content precisely in 100+ languages. Documents often have multiâ€‘column layouts, mixed text/graphics, handwriting, and dense small text (Section 1).
- Why it matters
  - Reliable parsing is the substrate for search, analytics, and Retrievalâ€‘Augmented Generation (RAG). Errors in structure (e.g., wrong reading order) or content (e.g., misread formulas) degrade downstream LLM applications (Section 1).
- Prior approaches and gaps
  - Pipeline systems with â€œexpertâ€ modules (e.g., `PPâ€‘StructureV3`, Marker, OpenParse, MinerU pipeline) can be accurate but suffer from integration complexity and error propagation, and they struggle on very complex layouts (Section 1; Table 2 and Table 3 show lower scores on several metrics).
  - Endâ€‘toâ€‘end VLMs (e.g., `GOT`, `olmOCR`, `MonkeyOCR`, `dots.ocr`, general VLMs like `Qwen2.5â€‘VL`, `GPTâ€‘4o`) simplify pipelines but tend to hallucinate, misorder long texts, and are expensive for long sequences (Section 1).
- Positioning of this work
  - The paper deliberately decouples page layout from content recognition: a small, stable detector predicts regions and reading order; a compact VLMâ€”optimized for dynamic highâ€‘resolution visionâ€”recognizes element content. This hybrid design targets both stability and efficiency (Section 2; Figure 2).

## 3. Technical Approach
The system has two stages (Figure 2): (1) layout analysis with reading order, and (2) elementâ€‘level recognition with a compact VLM. A lightweight postâ€‘processor merges both into Markdown/JSON outputs (Section 2.1).

1) Stage 1: `PPâ€‘DocLayoutV2` (Layout analysis + reading order)
- Detection backbone: `RTâ€‘DETR` (a realâ€‘time variant of DETR for object detection). It localizes and classifies elements (text blocks, tables, formulas, charts) (Section 2.1.1; Figure 3).
- Reading order with a `pointer network`:
  - A pointer network is a sequence model that outputs an ordering (a permutation) over input items; here, the â€œitemsâ€ are the detected layout boxes.
  - Inputs to the ordering network:
    - Absolute 2D positional encodings of each box and class label embeddings.
    - A geometric bias in attention inspired by `Relationâ€‘DETR` to model pairwise spatial relations (Section 2.1.1).
  - Pairwise relation head:
    - Projects element features into queries/keys and computes bilinear similarities, yielding an `NÃ—N` matrix of â€œwhich comes before whichâ€ pairwise logits (Section 2.1.1).
  - Decoding:
    - A deterministic `winâ€‘accumulation` algorithm converts the pairwise matrix into a globally consistent reading order (Section 2.1.1).
- Training (Section 2.2.1):
  - Twoâ€‘stage: train detection/classification first (initialized from `PPâ€‘DocLayout_Plusâ€‘L`), then freeze it and train the pointer network for ordering.
  - Loss for ordering: `Generalized Cross Entropy (GCE)` to be robust to noisy labels; optimizer `AdamW`, constant LR 2eâ€‘4 for 200 epochs.
  - Detection is trained 100 epochs following RTâ€‘DETR practice on >20k curated pages.

2) Stage 2: `PaddleOCRâ€‘VLâ€‘0.9B` (Elementâ€‘level recognition)
- Goal: Given each cropped element from Stage 1, output its content in a structured format (text, `OTSL` tokens for tables, LaTeX for formulas, Markdown tables for charts).
- Key components (Figure 4; Section 2.1.2):
  - Dynamic highâ€‘resolution vision encoder (`NaViT`â€‘style): NaViT (â€œPatchâ€‘nâ€‘Packâ€) accepts native image resolutions by packing flexibleâ€‘sized patches instead of forcibly resizing/tiling; this reduces distortions and hallucinations on textâ€‘dense images.
  - Vision weights initialized from `Keyeâ€‘VL` (Section 2.1.2).
  - A 2â€‘layer MLP projector (with `GELU`) maps vision features to the language embedding space; a â€œmerge size of 2â€ reduces visual token count to cut decoding cost (Section 2.1.2).
  - Language backbone: `ERNIEâ€‘4.5â€‘0.3B` (a 0.3Bâ€‘parameter multilingual LLM) for fast autoregressive decoding, enhanced with `3Dâ€‘RoPE` positional encoding to better represent multiâ€‘axis positions in multimodal sequences (Section 2.1.2).
- Training recipe (Section 2.2.2; Table 1):
  - Stage 1 (visionâ€“language alignment): 29M imageâ€“text pairs, batch 128, seq length 16384, max image resolution up to â€œ1280Ã—28Ã—28â€ (paperâ€™s NaViT notation), LR max/min 5eâ€‘5/5eâ€‘6 for 1 epoch. All modules trainable.
  - Stage 2 (instruction fineâ€‘tuning): 2.7M curated samples, batch 128, seq length 16384, higher max resolution â€œ2048Ã—28Ã—28â€, LR max/min 5eâ€‘6/5eâ€‘7 for 2 epochs. Trains four instruction families:
    - OCR text: lines, blocks, pages.
    - Table recognition: outputs `OTSL` (Optimized Table Tokenization) sequences for structure + content (Section 2.2.2; [28]).
    - Formula recognition: outputs LaTeX; distinguishes inline `\(...\)` vs display `\[...\]`.
    - Chart to table: converts charts to normalized Markdown tables.
- Data engine (Section 3; Appendix A):
  - Automatic annotation: use `PPâ€‘StructureV3` to produce pseudoâ€‘labels; refine with powerful multimodal LLMs (`ERNIEâ€‘4.5â€‘VL`, `Qwen2.5â€‘VL`) through prompt engineering; filter hallucinations and invalid outputs (Section 3.2).
  - Hardâ€‘case mining: build fineâ€‘grained eval suites (23 text types, 20 table types, 4 formula types, 11 chart types). Identify weak spots by metric (EditDist/TEDS/RMSâ€‘F1/BLEU), then synthesize similar hard cases using fonts, CSS, corpora, and renderers (XeLaTeX, browsers) (Section 3.3).
  - Scale and coverage:
    - Text: 20M imageâ€“text pairs, 109 languages, multiâ€‘style (printed, handwritten, vertical) (Appendix A.1).
    - Table: >5.5M pairs from automatic labeling, arXiv HTML mining, and highâ€‘speed synthesis with structural/style controls; uses `OTSL` as target format (Appendix A.2).
    - Formula: 34,816 manually curated eval samples; training gathers LaTeX from arXiv sources, public sets (UniMERâ€‘1M, MathWriting), filtered by renderability and image similarity; targeted synthesis for longâ€‘tail patterns (Appendix A.3).
    - Chart: ~0.8M bilingual pairs via cleaned public data, twoâ€‘stage LLM annotation of axes/data, personaâ€‘based style diversification, and longâ€‘tail augmentation (Appendix A.4).
- Inference system (Section 4.3)
  - Asynchronous threeâ€‘thread pipeline: PDF rendering â†’ layout model â†’ VLM, with queues between stages. Crossâ€‘page batching for VLM triggers either by batch size or timeout to maintain GPU utilization.
  - Serves on highâ€‘throughput backends (`vLLM`, `SGLang`), tuning `max-num-batched-tokens` and `gpu-memory-utilization`.

Terminology that may be unfamiliar (defined above when first used):
- `NaViT`: a vision transformer that packs variableâ€‘sized image patches so models can ingest native resolutions without fixed resizing/tiling artifacts.
- `RTâ€‘DETR`: a DETRâ€‘style detector optimized for realâ€‘time object detection.
- `Pointer network`: a neural module that outputs an order (permutation) over input items by â€œpointingâ€ to them sequentially.
- `Relationâ€‘DETR` geometric bias: an attention bias that explicitly encodes pairwise geometric relations among boxes.
- `3Dâ€‘RoPE`: a threeâ€‘dimensional rotary positional embedding extending standard RoPE to better encode multiple positional axes in multimodal sequences.
- `OTSL`: a compact tokenization of table structure that simplifies learning table layout compared to raw HTML.
- `TEDS`: Treeâ€‘Edit Distance Similarity; higher is better; it compares predicted vs groundâ€‘truth table trees.
- `CDM`: Character Detection Matching; evaluates formula recognition by matching rendered characters and their positions.
- `RMSâ€‘F1`: Rootâ€‘meanâ€‘square F1 score across multiple column/row aggregations of chartâ€‘toâ€‘table predictions.

## 4. Key Insights and Innovations
1) Decoupled, orderâ€‘aware layout analysis for stability and speed
- Whatâ€™s new: Instead of relying on a single VLM to â€œthinkâ€ about layout and content jointly, the paper introduces a small, specialized layout model with a pointer network that predicts a globally consistent reading order from pairwise relations (Section 2.1.1; Figure 3).
- Why it matters: It avoids longâ€‘sequence decoding for layout, reduces hallucinations on complex multiâ€‘column pages, and makes adding new layout categories easier (Section 2.1; Figure 2). The readingâ€‘order metric is best on OmniDocBench v1.5 (Table 2).

2) Dynamicâ€‘resolution vision + compact language model for element recognition
- Whatâ€™s new: A `NaViT`â€‘style encoder handles nativeâ€‘resolution crops with fewer distortions, coupled with a lightweight `ERNIEâ€‘4.5â€‘0.3B` decoder augmented by `3Dâ€‘RoPE` (Section 2.1.2).
- Why it matters: Better text fidelity on tiny/dense regions while keeping decoding fast. It delivers stateâ€‘ofâ€‘theâ€‘art element recognition at ~0.9B parameters, showing a strong accuracy/efficiency Pareto point (Tables 2, 8, 10, 11, 12; Section 4.3).

3) Largeâ€‘scale, qualityâ€‘controlled data engine with hardâ€‘case mining
- Whatâ€™s new: A pipeline that (a) seeds pseudoâ€‘labels from specialist models, (b) upgrades them using strong VLMs via carefully designed prompts, (c) filters hallucinations, and (d) continually mines/synthesizes targeted hard cases guided by an internal eval engine (Sections 3.1â€“3.3; Appendix A).
- Why it matters: Supports 109 languages and improves robustness on longâ€‘tail patterns (handwriting, vertical scripts, messy scans), which is reflected in multilingual and handwriting benchmarks (Table 6; Table 7).

4) Practical endâ€‘toâ€‘end throughput gains
- Whatâ€™s new: A multithreaded, crossâ€‘page batching inference design integrated with `vLLM`/`SGLang` (Section 4.3).
- Why it matters: Real deployments care about pages/s, tokens/s, and VRAM. The system attains the best measured throughput while using less GPU memory than some larger baselines (Table 13).

Overall, (2) is an architectural innovation; (1) and (4) are systemâ€‘level innovations; (3) is a data/engineering advance that materially impacts robustness.

## 5. Experimental Analysis
- Evaluation methodology
  - Pageâ€‘level document parsing: OmniDocBench v1.5 and v1.0 (Section 4.1; Tables 2, 3) and `olmOCRâ€‘Bench` unit tests (Table 4). Metrics include normalized `Textâ€‘Edit` (lower is better), `Formulaâ€‘CDM` (higher), `Tableâ€‘TEDS`/`TEDSâ€‘S` (higher), and `Reading Order Edit` (lower).
  - Elementâ€‘level recognition:
    - OCR text: `OmniDocBenchâ€‘OCRâ€‘block` (17,148 cropped text blocks; Table 5), large inâ€‘house multilingual/typology sets (Table 6), and handwriting `Oceanâ€‘OCRâ€‘Bench` (Table 7).
    - Tables: `OmniDocBenchâ€‘Tableâ€‘block` (512 tables; Table 8) and inâ€‘house tables (Table 9).
    - Formulas: `OmniDocBenchâ€‘Formulaâ€‘block` (1,050 formulas; Table 10) and inâ€‘house (34,816; Table 11).
    - Charts: inâ€‘house (1,801) scored by `RMSâ€‘F1` (Table 12).
  - Inference throughput: endâ€‘toâ€‘end from PDF path to Markdown on OmniDocBench v1.0 with 512â€‘PDF batches, single A100, comparing `vLLM` baselines (Table 13).
- Main quantitative results (all numbers from the paper)
  - OmniDocBench v1.5 (Table 2):
    > Overall score 92.56, beating the next best 90.67 (`MinerU2.5â€‘1.2B`). Subâ€‘metrics: `Textâ€‘Edit` 0.035 (lower is better), `Formulaâ€‘CDM` 91.43, `Tableâ€‘TEDS` 89.76, `Tableâ€‘TEDSâ€‘S` 93.52, `Readingâ€‘Orderâ€‘Edit` 0.043.
    - This outperforms general VLMs (e.g., `Qwen2.5â€‘VLâ€‘72B` overall 87.02) and specialized VLMs (`MonkeyOCRâ€‘proâ€‘3B` 88.85; `dots.ocr` 88.41).
  - OmniDocBench v1.0 (Table 3):
    > Average Overall Edit Distance 0.115 (lower is better). Strong on Chinese/English text edit distance (0.062 ZH; 0.041 EN) and formulas (0.241 EN, 0.316 ZH). Chinese `Tableâ€‘TEDS` 92.14 is best; English `Tableâ€‘TEDS` 88.0 is competitive but not the top.
  - olmOCRâ€‘Bench (Table 4):
    > Overall unitâ€‘test pass rate 80.0 Â± 1.0, highest among compared methods; category highs include ArXiv 85.7 and Headers/Footers 97.0, and secondâ€‘best on Multiâ€‘column (79.9) and Long Tiny Text (85.7).
  - Text recognition (Tables 5â€“7):
    - `OmniDocBenchâ€‘OCRâ€‘block` (Table 5): lowest edit distance across 9 document genres (e.g., Academic Literature 0.021; Newspaper 0.034; Note 0.081).
    - Inâ€‘house multilingual (Table 6a): best on all shown scriptsâ€”Arabic 0.122, Korean 0.052, Tamil 0.043, Greek 0.135, Thai 0.081, Telugu 0.011, Devanagari 0.097, Cyrillic 0.109, Latin 0.013, Japanese 0.086.
    - Handwriting (Oceanâ€‘OCRâ€‘Bench, Table 7): best edit distance `EN 0.118`, `ZH 0.034`, with top F1/Precision/Recall/BLEU/METEOR.
  - Table recognition (Tables 8â€“9):
    - `OmniDocBenchâ€‘Tableâ€‘block` (Table 8): `TEDS` 0.9195, `Structural TEDS` 0.9543, `Overall Edit Dist` 0.0561â€”best among all listed.
    - Inâ€‘house Tables (Table 9): `TEDS` 0.8699 and `Structural TEDS` 0.9066â€”best.
  - Formula recognition (Tables 10â€“11):
    - `OmniDocBenchâ€‘Formulaâ€‘block` (Table 10): `CDM` 0.9453 overall (EN 0.9677; ZH 0.9228)â€”best.
    - Inâ€‘house (Table 11): `CDM` 0.9882 overall (EN 0.9914; ZH 0.9849)â€”best.
  - Chart recognition (Table 12):
    > `RMSâ€‘F1` 0.844 overall (EN 0.822; ZH 0.855), surpassing `Qwen2.5â€‘VLâ€‘72B` (0.730) and the specialist `PPâ€‘StructureV3` (0.806).
  - Inference performance (Table 13):
    > With `vLLM`, total time 800.9s (pages/s 1.2241; tokens/s 1881.2) vs `MinerU2.5` 927.3s (pages/s 1.0574; tokens/s 1647.9). Uses 43.7 GB VRAM (less than `dots.ocr` at 78.5 GB).
- Do experiments support the claims?
  - Yes on public benchmarks: The model leads on OmniDocBench v1.5 overall and on most subâ€‘metrics (Table 2), performs strongly on v1.0 (Table 3), and tops olmOCRâ€‘Bench unit tests (Table 4). Elementâ€‘level SOTAs on table/formula crops (Tables 8, 10) indicate that decoupling layout from recognition did not hamper element accuracy.
  - Breadth: The paper includes multilingual and handwriting evaluations (Table 6, Table 7) and charts (Table 12), matching the stated â€œ109 languages and multiple element types.â€
- Notable omissions/considerations
  - Ablation studies are limited: there is no quantitative isolation of `NaViT` vs fixedâ€‘resolution encoders, nor of the pointer network vs alternative ordering methods. Robustness to severe detection errors is not systematically studied.
  - Public vs inâ€‘house: Many strongest numbers for language coverage, tables, charts, and formulas come from inâ€‘house benchmarks; while informative, generalization beyond those distributions should be validated further.

## 6. Limitations and Tradeâ€‘offs
- Dependence on Stageâ€‘1 layout quality
  - Although decoupling improves stability, misdetections or misâ€‘classified regions (e.g., formulas detected as images) will propagate to Stage 2. The paper does not quantify sensitivity to detection thresholds or propose recovery strategies.
- Reading order assumptions
  - The pointer network assumes that pairwise order relations are consistent and recoverable by `winâ€‘accumulation`. Highly irregular layouts (e.g., complex sidebars, marginalia, bidirectional scripts with nested figures) may break these assumptions; no specific evaluation on such edge cases is shown beyond benchmark distributions.
- Data and evaluation coverage
  - Support for 109 languages is reported (Appendix B), but public multilingual evaluations cover a subset. Some very lowâ€‘resource scripts get limited thirdâ€‘party benchmarking; performance may vary outside the inâ€‘house sets.
- Charts and formulas
  - Chart evaluation is only inâ€‘house (Section 4.2.4; Table 12), and thus comparability to established chart QA/dataset standards is indirect (though many public sources were used in training; Appendix A.4).
- Efficiency vs memory
  - Despite being compact for a VLM, endâ€‘toâ€‘end processing with dynamic high resolution still needs substantial GPU memory at scale (e.g., ~44 GB average with `vLLM` on A100 in Table 13). Deployments on edge devices may need further distillation or quantization.
- Transparency/ablations
  - The method integrates several design choices (NaViT, 3Dâ€‘RoPE, token merge size 2, ERNIEâ€‘0.3B). Without ablations, it is hard to attribute gains to specific components or to tune tradeâ€‘offs (speed vs accuracy) for new settings.

## 7. Implications and Future Directions
- How this changes the landscape
  - Demonstrates that a small, wellâ€‘engineered VLMâ€”paired with a dedicated layout modelâ€”can beat much larger general VLMs and specialized document models on both accuracy and throughput. For industry, this validates decoupled, efficiencyâ€‘focused architectures for largeâ€‘scale document conversion.
- Followâ€‘up research enabled/suggested
  - System ablations: quantify the contribution of `NaViT`, `3Dâ€‘RoPE`, token merging, and the pointer network; explore alternative graphâ€‘based ordering or learnable decoding from pairwise relations.
  - Errorâ€‘tolerant interfaces: design feedback loops where Stage 2 can flag likely misâ€‘detections to reâ€‘query Stage 1 (closing the loop on error propagation).
  - Crossâ€‘page and multiâ€‘document context: extend reading order and coreference across pages/sections (e.g., linking table references, figure captions, and bibliographic citations).
  - Open multilingual benchmarks: establish broader public evaluations for underâ€‘represented scripts (beyond Latin/CJK), charts (with standardized structure extraction metrics), and formula varieties (print vs handwritten, bilingual LaTeX).
  - Edge deployment: further compress/quantize `PaddleOCRâ€‘VLâ€‘0.9B` and study CPUâ€‘only performance; investigate knowledge distillation into even smaller decoders.
- Practical applications
  - Enterprise document ingestion (reports, invoices, forms), digital archiving of historical texts (vertical/ancient scripts; Figure A22), academic/scientific literature conversion (with formulas and tables), and highâ€‘quality RAG pipelines that require faithful reading order and structure (Section 1; Figures A5â€“A8).

Overall, the workâ€™s main contribution is an engineeringâ€‘sound, accuracyâ€‘andâ€‘efficiencyâ€‘balanced architecture that proves a compact VLM plus a dedicated layout analyzer is a winning recipe for realâ€‘world, multilingual document parsing. The extensive results on public benchmarks (Tables 2â€“4, 8, 10) substantiate this claim, while the inâ€‘house analyses and data engine point to a robust path for continued improvement.

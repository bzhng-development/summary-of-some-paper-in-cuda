# Qwen3â€‘Omni Technical Report

**ArXiv:** [2509.17765](https://arxiv.org/abs/2509.17765)
**Authors:** Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, BaosongÂ Yang, BinÂ Zhang, ZiyangÂ Ma, XipinÂ Wei, ShuaiÂ Bai, KeqinÂ Chen, XuejingÂ Liu, PengÂ Wang, MingkunÂ Yang, DayihengÂ Liu, XingzhangÂ Ren, BoÂ Zheng, RuiÂ Men, FanÂ Zhou, BowenÂ Yu, JianxinÂ Yang, LeÂ Yu, JingrenÂ Zhou, JunyangÂ Lin
**Institutions:** Alibabaâ€™s Qwen Team (Alibaba Cloud / Alibaba AI Research)

## ğŸ¯ Pitch

Qwen3-Omni revolutionizes multimodal AI by achieving state-of-the-art performance across text, vision, and audio without compromising any modalities, thanks to its innovative Thinkerâ€“Talker Mixture-of-Experts architecture. This advancement enables real-time, seamless interaction for applications like multilingual voice assistants and video analysis, setting a new standard for integrated AI systems that excel in complex, cross-modal tasks.

---

## 1. Executive Summary
Qwen3â€‘Omni is a single endâ€‘toâ€‘end multimodal model that deliberately avoids the usual â€œmodality tradeâ€‘offâ€: it matches sameâ€‘size textâ€‘only and visionâ€‘only Qwen models while achieving stateâ€‘ofâ€‘theâ€‘art results in audio and audioâ€‘visual tasks. It does so with a Thinkerâ€“Talker Mixtureâ€‘ofâ€‘Experts (MoE) architecture, a new audio encoder (`AuT`), and a streaming speech stack (multiâ€‘codebook + ConvNet vocoder) that yields a theoretical firstâ€‘packet latency of 234 ms for speech (Table 1, Table 2; Fig. 2).

## 2. Context and Motivation
- The gap addressed
  - Many LLMâ€‘centric multimodal systems improve one modality (e.g., vision) at the expense of others (e.g., text), creating â€œmodality tradeâ€‘offs.â€ The paper targets an integrated training recipe and architecture that avoids degrading any modality while enabling new crossâ€‘modal abilities (Introduction; Fig. 1).
- Why it matters
  - Realâ€‘world agents must hear, see, reason, and speak in real time, often over long contexts (meetings, videos). Qwen3â€‘Omni: 
    - Handles ASR and spokenâ€‘language understanding up to 40 minutes per instance (Abstract, Sec. 2.5).
    - Supports 119 written languages, 19 spoken for understanding, 10 for speech generation (Table 3).
    - Streams natural speech with subâ€‘second perceived start time (234 ms theoretical firstâ€‘packet latency; Table 1â€“2).
- Prior approaches and shortcomings
  - Cascaded pipelines (ASR â†’ LLM â†’ TTS) add latency and lose crossâ€‘modal cues; diffusion or blockâ€‘wise vocoders delay first audio packet; Whisperâ€‘based audio encoders limit generality and streaming prefill (Sec. 2.1â€“2.2; Sec. 2.5).
  - Previous Qwen2.5â€‘Omni used Thinkerâ€“Talker but still relied on Whisper and blockâ€‘wise vocoding, and split audioâ€‘visual inputs into fixed chunks (Sec. 2.1).
- Positioning
  - Qwen3â€‘Omni builds on Thinkerâ€“Talker but:
    - Upgrades both modules to MoE for higher throughput (Sec. 2.1, Table 1).
    - Introduces `AuT` (trained from scratch on 20M hours of supervised audio) for generalâ€‘purpose audio representations with streamingâ€‘friendly attention (Sec. 2.2, Fig. 3).
    - Replaces diffusion/block vocoders with a lightweight causal ConvNet (`Code2Wav`) plus multiâ€‘codebook streaming prediction for immediate synthesis (Sec. 2.4â€“2.5; Fig. 2).
    - Uses timeâ€‘aligned multimodal position embeddings (`TMâ€‘RoPE`) for precise audioâ€‘video alignment and arbitraryâ€‘length streaming (Sec. 2.3).

## 3. Technical Approach
Stepâ€‘byâ€‘step architecture and training pipeline.

- System overview (Fig. 2; Sec. 2.1)
  - `Thinker` (text generator): an MoE Transformer that performs multimodal understanding and text generation.
  - `Talker` (speech generator): an MoE Transformer that consumes multimodal features and generates discrete speech codec frames in a streaming manner.
  - Decoupling choice: Talker no longer consumes Thinkerâ€™s textual embeddings; instead it conditions directly on audio/visual features and the conversation history (Sec. 2.1). Rationale:
    - Text tokens vs. embeddings are informationâ€‘equivalent for content.
    - For tasks like speech translation or voiceâ€‘over, conditioning on audio/visual features helps preserve prosody/timbre and sync with video.
    - Enables separate system prompts for text style vs. audio style and permits external modules (RAG, safety) to intervene on text before speech (Sec. 2.1).
- Perception modules (Sec. 2.2â€“2.3; Fig. 3)
  - `AuT` audio encoder
    - What it is: an attention encoderâ€“decoder trained on 20M hours of supervised audio (ASR + audio understanding). Input: 16 kHz waveform â†’ 128â€‘channel mel spectrogram; Conv2D downsampling Ã—8 to a token rate of 12.5 Hz (â‰ˆ1 token per 80 ms). Uses flash attention with dynamic windows covering 1â€“8 s to support realâ€‘time prefill (Sec. 2.2; Fig. 3). ~0.6B parameters.
    - Why it matters: stronger, generalâ€‘purpose audio features than Whisper; streamingâ€‘oriented prefill via blockâ€‘wise windows (Sec. 2.2).
  - Vision encoder
    - SigLIP2â€‘So400m (~543M params), trained on mixed image+video data; provides image/video features (Sec. 2.3).
  - Timeâ€‘aligned Multimodal RoPE (`TMâ€‘RoPE`)
    - What it is: a positional encoding scheme that splits rotary angles into temporal, height, width components and aligns audio/video by absolute time IDs every 80 ms (Sec. 2.3).
    - Why it matters: improves longâ€‘range temporal modeling vs. earlier Mâ€‘RoPE allocations and supports arbitraryâ€‘length streaming without fixed 2â€‘s chunking (contrast with Qwen2.5â€‘Omni; Sec. 2.3).
- Speech generation stack (Sec. 2.4â€“2.5; Fig. 2)
  - Discrete speech representation: residual vectorâ€‘quantized (`RVQ`) â€œcodebooks.â€ Thinker provides highâ€‘level context; Talker autoregressively predicts one codec frame per step.
  - `MTP` module (multiâ€‘token prediction): a tiny dense Transformer that, for each frame, predicts the residual codebooks beyond the first â€œzerothâ€ codebook predicted by Talker (Sec. 2.4).
    - Purpose: captures fine acoustic details (prosody, timbre) without delaying synthesis.
  - `Code2Wav`: a lightweight causal ConvNet that reconstructs waveform from the multiâ€‘codebook tokens incrementally, with leftâ€‘context only attention (Sec. 2.4â€“2.5). 
    - Key design tradeâ€‘off: replace computeâ€‘intensive diffusion/DiT vocoders with a small ConvNet to reduce latency and increase throughput (Sec. 2.4â€“2.5).
  - Streaming path
    - As soon as Talker emits the first token for a frame, MTP fills residual tokens; `Code2Wav` renders a short waveform segment immediately. With 12.5 Hz codec rate, a single token produces 80 ms of audio, enabling realâ€‘time streaming (Sec. 2.5).
- Concurrency and latency engineering (Sec. 2.5; Table 1â€“2)
  - Chunked prefilling: audio/vision encoders output temporal chunks; Thinker and Talker prefill asynchronously so each can begin decoding earlier (Sec. 2.5).
  - MoE benefits: lower KVâ€‘cache I/O, higher tokens/s under long sequences and concurrency (Sec. 2.5; Table 2).
  - Measured on vLLM with CUDA Graph and torch.compile: theoretical firstâ€‘packet latency is 234 ms for audio, 547 ms for video (Table 1â€“2). Generation RTF stays <1 across concurrency levels (Table 2).
- Training pipeline
  - Pretraining in three stages (Sec. 3)
    1) `S1` Encoder alignment: initialize Thinker from Qwen3 and bring audio/vision encoders (AuT, SigLIP2) into alignment on a frozen LLM via adapters, then encoders, avoiding the pitfall where encoders compensate for a frozen LLM (Sec. 3).
    2) `S2` General: ~2 trillion tokens spanning text (0.57T), audio (0.77T), image (0.82T), video (0.05T), and videoâ€‘audio (0.05T) with mixed prompts from early training to prevent modality siloing (Sec. 3).
    3) `S3` Long context: extend max length from 8,192 to 32,768 tokens; increase long audio/video proportion (Sec. 3).
  - Postâ€‘training Thinker (Sec. 4.1)
    - Supervised fineâ€‘tuning (SFT) with ChatML dialogues spanning text/vision/audio.
    - Strongâ€‘toâ€‘weak distillation: offâ€‘policy (teacher responses) then onâ€‘policy (student aligns logits to teacher via KL; teachers include Qwen3â€‘32B / Qwen3â€‘235Bâ€‘A22B).
    - RL with GSPO: ruleâ€‘based rewards for verifiable tasks; modelâ€‘based LLMâ€‘asâ€‘aâ€‘judge for subjective multimodal tasks (Sec. 4.1).
  - Postâ€‘training Talker (Sec. 4.2)
    - Stage 1: largeâ€‘scale multimodalâ€‘context speech mapping.
    - Stage 2: continual pretraining (CPT) on highâ€‘quality data + longâ€‘context training to reduce hallucinations and improve stability.
    - Stage 3: multilingual DPO for generalization and stability.
    - Stage 4: speaker fineâ€‘tuning for voice cloning and controllability.
  - Audio captioner (Sec. 4.3; Appx. 9.2)
    - Fineâ€‘tune `Qwen3â€‘Omniâ€‘30Bâ€‘A3B` into `â€¦â€‘Captioner` to fill a gap in generalâ€‘purpose audio captioning.

Definitions of less common terms used above:
- `MoE` (Mixtureâ€‘ofâ€‘Experts): a model where a router activates only a small subset of expert feedâ€‘forward networks per token, improving throughput at similar quality.
- `RVQ` (Residual Vector Quantization): a way to represent audio as a stack of discrete codebooks; each codebook encodes the residual left by the previous one.
- `MTP` (Multiâ€‘Token Prediction): predicts multiple discrete tokens for a frame in one shot, reducing steps.
- `TMâ€‘RoPE`: a rotary position embedding that separates temporal, height, and width angles and assigns absolute time IDs at 80 ms resolution for audio/video.

## 4. Key Insights and Innovations
- Integrated training without modality degradation (fundamental)
  - Evidence: A controlled 30Bâ€‘scale study trains a textâ€‘only baseline, a visionâ€‘only baseline, and `Omni` on identical text/vision corpora and matched FLOPs; `Omni` additionally sees audio/audioâ€‘visual data. `Omni` matches or exceeds unimodal baselines in text and vision and improves some vision/OCR tasks (Table 16). Example: `MMMUval` (collegeâ€‘level problems) improves from 57.22 (visionâ€‘only) to 59.33 (Omni). Text benchmarks remain on par (e.g., MMLU 81.69 vs 81.24; Table 16).
- Streaming lowâ€‘latency speech with multiâ€‘codebook + ConvNet vocoder (fundamental)
  - Immediate perâ€‘frame synthesis: Talker emits first token, MTP predicts residuals, `Code2Wav` streams waveform with leftâ€‘context attention only (Fig. 2; Sec. 2.4â€“2.5).
  - Result: 234 ms theoretical firstâ€‘packet audio latency in cold start; RTF < 1 under load (Table 1â€“2).
  - Significance: removes block dependence and diffusion overhead; enables natural realâ€‘time agents.
- `AuT` audio encoder trained from scratch at scale (novel subsystem)
  - 20M hours supervised audio with dynamic attention windows; 12.5 Hz token rate; blockâ€‘wise prefill caching (Sec. 2.2). 
  - Impact: strong general audio performance and longâ€‘form streaming ability; underpins SOTA ASR/S2TT and audio reasoning (Table 6â€“8).
- Timeâ€‘aligned positional encoding for multimodal streams (incremental but impactful)
  - `TMâ€‘RoPE` allocates more temporal angles (24) and ties absolute 80 ms IDs to audio/video frames; eliminates fixed 2â€‘s chunking used before, enabling arbitraryâ€‘length streaming and better longâ€‘range temporal extrapolation (Sec. 2.3).
- Thinking variant for crossâ€‘modal reasoning + audio captioner (new capability)
  - `â€¦â€‘Thinking` explicitly reasons over inputs from any modality; excels on audioâ€‘visual reasoning tasks (Table 12) but is not optimal for pure perception benchmarks (Appx. 9.1).
  - `â€¦â€‘Captioner` supplies detailed lowâ€‘hallucination audio descriptions for the community (Sec. 4.3; Appx. 9.2).

## 5. Experimental Analysis
- Evaluation methodology and setup
  - Modalities and tasks (Sec. 5)
    - Textâ†’Text: general knowledge (MMLUâ€‘Redux, GPQA), reasoning (AIME25, ZebraLogic), coding (MultiPLâ€‘E), alignment/creative writing (IFEval, Creative Writing v3, WritingBench), agents (BFCLâ€‘v3), multilingual (MultiIF, PolyMath).
    - Audioâ†’Text: ASR and S2TT (LibriSpeech, WenetSpeech, FLEURS, CommonVoice), voice chat (VoiceBench), audio reasoning (MMAU, MMSU), and music understanding (RULâ€‘MuchoMusic, GTZAN, MTGâ€‘Jamendo, MagnaTagATune).
    - Visionâ†’Text: general VQA (MMStar, HallusionBench, MMâ€‘MTâ€‘Bench), math/STEM (MMMU, MMMUâ€‘Pro, MathVista, MATHâ€‘Vision), documents/OCR (AI2D, ChartQA), counting (CountBench), and video understanding (Videoâ€‘MME, LVBench, MLVU).
    - Audioâ€‘Visualâ†’Text: WorldSense (integration), DailyOmni and VideoHolmes (reasoning).
    - Xâ†’Speech: zeroâ€‘shot TTS on SEED, multilingual TTS on MiniMax set, crossâ€‘lingual cloning on CosyVoice3 suite (Sec. 5.2).
  - Metrics
    - ASR/S2TT uses WER/BLEU; VoiceBench reports multiple subâ€‘scores and overall; music uses accuracy or microâ€‘F1 for multiâ€‘label tagging; text/vision benchmarks use established metrics per dataset (Tables 4â€“15).
- Main quantitative results (selected highlights; all tables in Sec. 5)
  - Audio and audioâ€‘visual leadership
    - ASR & S2TT (Table 6; `â€¦â€‘Instruct`):
      - LibriSpeech WER: 1.22% (clean), 2.48% (other), surpassing GPTâ€‘4oâ€‘Transcribe (1.39/3.75) and Voxtralâ€‘Small (1.56/3.30).
      - WenetSpeech (net|meeting) WER: 4.69|5.89 vs Seedâ€‘ASR 4.66|5.69 and far below GPTâ€‘4oâ€‘Transcribe 15.30|32.27.
      - FLEURSâ€‘avg (19 langs) WER: 5.33 vs Voxtralâ€‘Small 8.09 and Geminiâ€‘2.5â€‘Pro 5.55.
      - S2TT BLEU (FLEURS enâ†’xx/xxâ†’en/zhâ†’xx/xxâ†’zh): 37.50/31.08/25.17/33.13, broadly competitive with Voxtralâ€‘Small and Geminiâ€‘2.5â€‘Pro (Table 6).
    - VoiceBench overall (Table 7):
      - `â€¦â€‘Thinking`: overall 88.8, second only to Geminiâ€‘2.5â€‘Pro (89.6) and ahead of GPTâ€‘4oâ€‘Audio (86.8) and Qwen2.5â€‘Omni (73.6).
    - Audio reasoning (Table 7):
      - MMAU v05.15.25: `â€¦â€‘Instruct` 77.5 and `â€¦â€‘Flashâ€‘Instruct` 77.6, higher than Geminiâ€‘2.5â€‘Pro (77.4) and far above GPTâ€‘4oâ€‘Audio (62.5).
      - MMSU: `â€¦â€‘Flashâ€‘Thinking` 71.3, better than GPTâ€‘4oâ€‘Audio (56.4) and Geminiâ€‘2.5â€‘Flash (70.2).
    - Music understanding (Table 8):
      - RULâ€‘MuchoMusic: 52.0â€“52.1 vs Geminiâ€‘2.5â€‘Pro 49.4 and best specialist 47.6.
      - GTZAN accuracy: 93.0â€“93.1 (vs GPTâ€‘4oâ€‘Audio 76.5).
      - Multiâ€‘label MTG and MagnaTagATune: Qwen3â€‘Omni achieves the best microâ€‘F1 across genre, mood/theme, instrument, topâ€‘50 tags, and MagnaTagATune.
    - Audioâ€‘Visualâ†’Text (integration and reasoning):
      - WorldSense: 54.0â€“54.1, beating Geminiâ€‘2.5â€‘Flash (50.9) and Qwen2.5â€‘Omni (45.4) (Table 11).
      - DailyOmni: `â€¦â€‘Thinking` 75.8â€“76.2, exceeding Geminiâ€‘2.5â€‘Flashâ€‘Thinking 72.7 (Table 12).
      - VideoHolmes: `â€¦â€‘Thinking` 57.3 vs previous openâ€‘source SOTA 55.6 and Geminiâ€‘2.5â€‘Flashâ€‘Thinking 49.5 (Table 12).
  - Text and vision parity with sameâ€‘size unimodal models
    - Textâ†’Text (Table 4â€“5):
      - `â€¦â€‘Instruct` outperforms much larger Qwen3â€‘235Bâ€‘A22B Nonâ€‘Thinking on several benchmarks: AIME25 65.0 vs 24.7; ZebraLogic 76.0 vs 37.7; PolyMath 37.9 vs 27.0.
      - `â€¦â€‘Thinking` is close to Geminiâ€‘2.5â€‘Flashâ€‘Thinking: GPQA 73.1 vs 82.8; WritingBench 85.5 vs 83.9; AIME25 73.7 vs 72.0 (Table 5).
      - Crucially, vs textâ€‘only Qwen3â€‘30B models, Qwen3â€‘Omni matches or is comparable (Table 4â€“5; Sec. 5.1.1).
    - Visionâ†’Text (Table 9â€“10):
      - `â€¦â€‘Instruct` competitive with Qwen2.5â€‘VLâ€‘72B; especially strong in math/STEM: MATHâ€‘Visionfull 56.3 vs GPT4â€‘o 30.4 and Geminiâ€‘2.0â€‘Flash 48.6 (Table 9).
      - `â€¦â€‘Thinking` gains further on math+reasoning (e.g., MathVistaâ€‘mini 80.0; CountBench 88.6â€“92.5; Table 10). Long video understanding lags Geminiâ€‘2.5â€‘Flashâ€‘Thinking (Table 10; discussed as a limitation).
  - Xâ†’Speech generation (Sec. 5.2)
    - Zeroâ€‘shot TTS on SEED (WER; lower is better): `â€¦â€‘30Bâ€‘A3B` achieves 1.07 (zh) and 1.39 (en). It is close to the best zh (CosyVoice3 0.71) and best en (CosyVoice3 1.45), and improves over prior Qwen2.5â€‘Omniâ€‘7B (1.42/2.33) (Table 13). With RL optimization, English stability/consistency improves further (Table 13 note).
    - Multilingual TTS on MiniMax (Table 14): lower contentâ€‘consistency numbers are better (WERâ€‘like); higher SIM is closer voice match. Qwen3â€‘Omni shows strong WER in Chinese (0.716) and English (1.069) and high SIM (0.77 range), outperforming or matching MiniMax/ElevenLabs on several languages.
    - Crossâ€‘lingual cloning (Table 15): Qwen3â€‘Omni achieves lower errors in many anyâ†’en/ko directions (e.g., koâ†’en 3.34 vs CosyVoice3 4.19; enâ†’ko 4.96 vs 5.87) and competitive anyâ†’ja even without phonetic normalization.
  - Latency and throughput (Table 1â€“2)
    - Firstâ€‘packet audio latency 234 ms at 1Ã— concurrency; Talker token rate 140 tok/s, Thinker 75 tok/s; RTF 0.47. Even at 6Ã— concurrency, RTF stays 0.66 (Table 2).
- Robustness, ablations, and nuanced findings
  - Controlled nonâ€‘degradation experiment (Table 16) isolates multimodality effects at matched compute. Finding: early multimodal mixing does not harm language and can help vision/OCR; adding audio improves `MMMU` and OCR tasks while language gains from adding vision/audio are not observed (Sec. 6 discussion).
  - Thinking vs Instruct for perception: The `â€¦â€‘Thinking` variant underperforms `â€¦â€‘Instruct` on ASR and music tagging (Appx. 9.1; Tables 17â€“18). Insight: explicit chainâ€‘ofâ€‘thought is unnecessary or even harmful for perceptionâ€‘heavy tasks due to hallucination risk.
  - Failure modes: Long video benchmarks show lag vs top closed models (Table 10). The paper attributes this to limited positional extrapolation and context length (Sec. 5.1.3 narrative).
- Do the experiments support the claims?
  - The breadth of standardized benchmarks (text, vision, audio, audioâ€‘visual) with competitive closedâ€‘source baselines (GPTâ€‘4o, Geminiâ€‘2.5) and specialists (Seedâ€‘ASR, Voxtral, music specialists) supports the SOTA claims in audio/audioâ€‘visual and parity in text/vision.
  - The matchedâ€‘compute study (Table 16) is particularly convincing for the â€œno degradationâ€ claim at 30B scale, though scalability to other sizes remains to be fully swept (Sec. 6).

## 6. Limitations and Tradeâ€‘offs
- Long video reasoning lags and scaling constraints
  - Performance on Videoâ€‘MME/LVBench/MLVU is below Geminiâ€‘2.5â€‘Flashâ€‘Thinking (Table 10). The paper attributes this to position extrapolation capacity and context window limitations (Sec. 5.1.3), implying architectural or training adjustments are needed.
- Heavy data and compute demands
  - Pretraining includes ~2T multimodal tokens and 20M hours of supervised audio for `AuT` (Sec. 2.2; Sec. 3). Replication requires significant resources; public checkpoints are 30Bâ€‘scale, but broader size sweeps are not reported due to cost (Sec. 6).
- â€œTheoreticalâ€ latency numbers
  - Firstâ€‘packet latencies are derived under specific serverâ€‘side settings (vLLM, CUDA Graph, torch.compile) and reported as â€œtheoreticalâ€ in coldâ€‘start (Table 2). Realâ€‘world network delays, device heterogeneity, and warmth of caches may alter observed latencies.
- Coverage of languages and domains
  - Speech understanding/generation is limited to 19/10 languages (Table 3). Lowâ€‘resource or codeâ€‘switching scenarios beyond this set are not deeply evaluated.
- Reward modeling and evaluation bias
  - RL uses modelâ€‘based judges (Qwen3, Qwen2.5â€‘VL) for nonâ€‘verifiable tasks (Sec. 4.1), which can introduce bias or reward hacking if not carefully controlled, despite safeguards like referenceâ€‘aware prompting.
- Perception vs reasoning tradeâ€‘off
  - The `Thinking` variant shows weaker ASR/music results (Appx. 9.1), suggesting that adding explicit reasoning pathways may increase hallucination or distract from lowâ€‘level perception unless gated appropriately.
- Audio codec rate choice
  - The 12.5 Hz (80 ms) codec rate is latencyâ€‘friendly but may coarsen ultraâ€‘fine prosodic control in edge cases like rapid phonetic transitions or singing ornaments; although the multiâ€‘codebook setup adds capacity, such tradeâ€‘offs arenâ€™t ablated in detail.

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that fully integrated, endâ€‘toâ€‘end multimodal training can avoid modality degradation while enabling strong crossâ€‘modal reasoning (Sec. 7; Table 16). This challenges the assumption that bestâ€‘inâ€‘class assistants must be cascades or ensembles.
  - Establishes a practical blueprint for realâ€‘time, speechâ€‘centric agents: multiâ€‘codebook streaming, MTP, and ConvNet vocoding for low latency (Fig. 2; Table 2).
- Followâ€‘up research enabled
  - Position/extrapolation and longâ€‘video context: augment TMâ€‘RoPE or combine with learned time embeddings; extend context windows; evaluate memoryâ€‘augmented decoding for hourâ€‘long videos (Sec. 5.1.3 limitation).
  - Adaptive routing between `Instruct` and `Thinking` modes for perception vs reasoning to curb hallucinations on ASR/music (Appx. 9.1).
  - Broaden language coverage for speech, codeâ€‘switching ASR/S2TT, and dialectal robustness (Table 3 hints at dialect work in â€œFlashâ€ variants; Sec. 5).
  - Audio captioning as a foundation: leverage `â€¦â€‘Captioner` to bootstrap audioâ€‘grounded supervision for multimodal agents (Sec. 4.3; Appx. 9.2).
  - Transparent latency reporting across hardware tiers (edge, mobile) and integration with function calling, RAG, and safety interposers enabled by Thinkerâ€“Talker decoupling (Sec. 2.1).
- Practical applications
  - Realâ€‘time multilingual voice assistants and meeting copilots (40â€‘minute ASR; Abstract; Fig. 1).
  - Audioâ€‘visual reasoning for video understanding, surveillance/event analysis, and media QA (Tables 11â€“12).
  - Lowâ€‘latency speech translation or dubbing that preserves speaker traits and syncs with video via multimodal conditioning (Sec. 2.1, 2.4).
  - Music and sound understanding for media indexing, recommendation, and content moderation (Table 8).

> Bottom line: By carefully coâ€‘training modalities from the start, aligning them in time (`TMâ€‘RoPE`), and engineering an endâ€‘toâ€‘end streaming speech path (multiâ€‘codebook + `Code2Wav`), Qwen3â€‘Omni shows that a single model can be competitive in text and vision while achieving stateâ€‘ofâ€‘theâ€‘art audio performance with practical, lowâ€‘latency speech interaction (Fig. 2; Tables 1â€“2, 6â€“8, 11â€“12, 16).

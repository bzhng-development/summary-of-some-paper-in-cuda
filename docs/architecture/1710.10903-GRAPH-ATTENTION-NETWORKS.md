# GRAPH ATTENTION NETWORKS

**ArXiv:** [1710.10903](https://arxiv.org/abs/1710.10903)

## üéØ Pitch

This paper introduces Graph Attention Networks (GATs), a novel graph neural architecture that uses masked self-attention to learn adaptive, per-neighbor weights during message passing. By avoiding costly spectral operations and enabling parallel, inductive-ready aggregation over variable-sized neighborhoods, GATs improve expressivity and transferability for node classification tasks and achieve state-of-the-art results on both transductive and inductive benchmarks.

---

## 1. Executive Summary (2-3 sentences)

Graph Attention Networks (GATs) introduce an attention-based message-passing layer for graphs that lets each node learn how much to ‚Äúlisten‚Äù to each of its neighbors when computing its next-layer representation. This matters because many earlier graph neural methods either require graph-structure-dependent spectral operations or treat all neighbors uniformly, while GAT provides a simple, parallelizable mechanism that supports both transductive node classification (same graph at train/test) and inductive generalization to unseen graphs.

## 2. Context and Motivation

- **What specific problem or gap is addressed?**
  - Graph-structured data (citations, proteins, social networks) is irregular: nodes have variable numbers of neighbors and there is no fixed grid structure like in images.
  - Prior graph ‚Äúconvolution‚Äù methods often have one or more of these drawbacks:
    - **Spectral methods** define convolutions via the graph Laplacian‚Äôs eigenbasis, which:
      - Can be computationally heavy (eigendecomposition or approximations).
      - Produces filters tied to a specific graph structure, making direct transfer to different graphs difficult.
    - **Spatial/non-spectral methods** aggregate over neighborhoods but must handle variable neighborhood sizes while keeping weight sharing; some approaches require degree-specific parameters, fixed-size neighborhood extraction, or neighbor sampling.
  - A key representational limitation highlighted is that some popular operators (e.g., GCN-style) effectively **weight neighbors uniformly** (up to normalization), rather than learning which neighbors are more informative for a given node.

- **Why is this problem important?**
  - Real graph tasks frequently require:
    - Handling **variable degrees** (some nodes have many neighbors, some few).
    - **Inductive learning**: deploying a trained model on graphs not seen during training (explicitly evaluated in the paper on PPI).
    - Efficient computation without expensive global matrix operations.

- **What prior approaches existed, and where do they fall short (as framed here)?**
  - **Spectral CNNs on graphs** (Bruna et al.; Henaff et al.; Defferrard et al.; Kipf & Welling) progressively reduce cost and enforce localization, but remain tied to the Laplacian eigenbasis / graph structure, limiting transfer across graphs.
  - **Spatial methods** (Duvenaud et al.; Atwood & Towsley; Niepert et al.; Monti et al.; GraphSAGE) avoid spectral dependence but may:
    - Need degree-dependent weights, fixed-size neighborhood construction, or neighbor sampling (GraphSAGE) that does not necessarily see the full neighborhood at inference.
    - Use aggregators like LSTMs that can implicitly assume an ordering of neighbors (even if randomized).
  - **Attention mechanisms** in sequence modeling are known to handle variable-sized inputs by learning relevance weights, motivating a graph analogue.

- **How does the paper position itself relative to existing work?**
  - It proposes a **masked self-attention** layer on graphs:
    - ‚ÄúMasked‚Äù means attention is computed only over graph neighbors (typically 1-hop, including the node itself).
    - It is designed to be **efficient and parallelizable** over edges and nodes.
    - It is described as not requiring costly operations like matrix inversion or eigendecomposition, and as not requiring upfront access to the full graph structure in the way many spectral methods do.
  - It also notes a conceptual connection: GAT can be reformulated as a particular instance of `MoNet` under specific choices of pseudo-coordinates and weights, but differs by using node features (rather than structural properties) to compute similarities.

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **graph neural network layer** that updates each node‚Äôs feature vector by taking a **learned, weighted combination of its neighbors‚Äô features**.
- It solves node representation learning / node classification on graphs by replacing fixed neighbor averaging with a **learned attention weighting** over each node‚Äôs (1-hop) neighborhood.

### 3.2 Big-picture architecture (diagram in words)

- **Input**: a graph with node features `h = {h_1, ..., h_N}` and edges defining neighborhoods `N_i`.
- **Graph Attentional Layer (repeated/stacked)**:
  1. Apply a shared linear transform `W` to node features.
  2. For each edge `(i, j)` with `j ‚àà N_i`, compute an **attention score** `e_{ij}` from transformed features.
  3. Normalize scores across `j ‚àà N_i` with a softmax to get weights `Œ±_{ij}`.
  4. Aggregate neighbors as a weighted sum to produce the next features `h'_i`.
  5. Optionally use **multi-head attention**: repeat steps (1‚Äì4) in parallel `K` times and concatenate/average heads.
- **Output layer**: uses attention to produce class logits (softmax for single-label; sigmoid for multi-label).

### 3.3 Roadmap for the deep dive

- Explain the **single-head attentional layer** (inputs/outputs, equations).
- Show how **masking** injects graph structure (attention only over neighbors).
- Explain **multi-head attention** and why concatenation vs averaging is used.
- Describe the **full model instantiations** used in experiments (transductive vs inductive).
- Cover **computational complexity and implementation constraints** discussed in the paper.
- Provide a **worked micro-example** of how one node‚Äôs attention weights and aggregation are computed.

### 3.4 Detailed, sentence-based technical breakdown

This is an **algorithmic architectural contribution** paper with empirical evaluation, and the core idea is to perform graph message passing using a **shared attention mechanism over edges** so that each node can assign different importances to different neighbors.

#### Core layer: inputs, outputs, and the ‚Äúwhat happens first/second/third‚Äù flow

- **Inputs and outputs.**
  - Each node `i` starts with an input feature vector $ \mathbf{h}_i \in \mathbb{R}^{F} $, and the layer outputs $ \mathbf{h}'_i \in \mathbb{R}^{F'} $.
  - The graph has `N` nodes and a neighborhood set $N_i$ for each node (in experiments, 1-hop neighbors **including the node itself**).

- **Step 1 (shared linear projection).**
  - The layer first applies the same learned linear map to every node:
    - A weight matrix $ \mathbf{W} \in \mathbb{R}^{F' \times F} $ transforms $ \mathbf{h}_i \mapsto \mathbf{W}\mathbf{h}_i $.

- **Step 2 (edge-wise attention scoring with masking).**
  - For each node `i`, the layer computes an unnormalized importance score for each neighbor $j \in N_i$:
    - $$ e_{ij} = a(\mathbf{W}\mathbf{h}_i, \mathbf{W}\mathbf{h}_j). \tag{1} $$
  - ‚ÄúMasked attention‚Äù means the model **does not** compute $e_{ij}$ for non-neighbors; the attention graph is restricted to the adjacency-defined neighborhood.

- **Step 3 (softmax normalization over the neighborhood).**
  - Scores are normalized across all neighbors of `i` so they are comparable and sum to 1:
    - $$ \alpha_{ij} = \operatorname{softmax}_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k\in N_i}\exp(e_{ik})}. \tag{2} $$

- **Step 4 (neighbor aggregation to form new features).**
  - The output features for node `i` are a nonlinearity applied to a weighted sum of transformed neighbor features:
    - $$ \mathbf{h}'_i = \sigma\!\left(\sum_{j\in N_i}\alpha_{ij}\mathbf{W}\mathbf{h}_j\right). \tag{4} $$
  - Here, $\sigma$ is a chosen activation function (the experiments use `ELU` in hidden layers; output layers use `softmax` for single-label and logistic sigmoid for multi-label).

#### Specific attention function used in experiments

- The attention mechanism $a(\cdot,\cdot)$ is instantiated as a **single-layer feedforward network** applied to the concatenation of transformed node features, followed by `LeakyReLU`.
- Concretely, with a learned vector $ \mathbf{a} \in \mathbb{R}^{2F'} $, negative slope $0.2$, and concatenation denoted by $\Vert$:
  - $$ \alpha_{ij} = \frac{\exp\Big(\operatorname{LeakyReLU}\big(\mathbf{a}^{\top}[\mathbf{W}\mathbf{h}_i \Vert \mathbf{W}\mathbf{h}_j]\big)\Big)}{\sum_{k\in N_i}\exp\Big(\operatorname{LeakyReLU}\big(\mathbf{a}^{\top}[\mathbf{W}\mathbf{h}_i \Vert \mathbf{W}\mathbf{h}_k]\big)\Big)}. \tag{3} $$

- Intuitively (in plain language before notation): the layer learns a scoring rule that looks at ‚Äú(current node features, neighbor features)‚Äù and outputs a scalar; softmax turns these scalars into weights that decide how much each neighbor contributes.

#### Multi-head attention: why and how

- The layer is extended to **multi-head attention** to stabilize learning (analogous to multi-head self-attention in sequence models).
- With `K` heads, each head `k` has its own parameters $(\mathbf{W}^k, \mathbf{a}^k)$ and attention coefficients $\alpha^k_{ij}$, and each computes its own aggregated features.
- For intermediate layers, the heads‚Äô outputs are **concatenated**:
  - $$ \mathbf{h}'_i = \mathbin\Vert_{k=1}^{K}\ \sigma\!\left(\sum_{j\in N_i}\alpha^k_{ij}\mathbf{W}^k\mathbf{h}_j\right). \tag{5} $$
  - This makes the effective output dimension $K F'$ per node.

- For the final prediction layer, concatenation is replaced by **averaging** (since the goal is typically to produce class logits of a fixed dimension):
  - $$ \mathbf{h}'_i = \sigma\!\left(\frac{1}{K}\sum_{k=1}^{K}\sum_{j\in N_i}\alpha^k_{ij}\mathbf{W}^k\mathbf{h}_j\right). \tag{6} $$

#### Worked micro-example (single head, one node update)

Consider a node `i` with neighborhood $N_i = \{i, j, k\}$ (itself plus two neighbors).

1. Compute transformed features:
   - $ \mathbf{z}_i = \mathbf{W}\mathbf{h}_i$, $ \mathbf{z}_j = \mathbf{W}\mathbf{h}_j$, $ \mathbf{z}_k = \mathbf{W}\mathbf{h}_k$.
2. Compute unnormalized attention scores:
   - $ e_{ii} = a(\mathbf{z}_i, \mathbf{z}_i)$, $ e_{ij} = a(\mathbf{z}_i, \mathbf{z}_j)$, $ e_{ik} = a(\mathbf{z}_i, \mathbf{z}_k)$.
3. Softmax over the three scores:
   - $$ \alpha_{ii} = \frac{e^{e_{ii}}}{e^{e_{ii}}+e^{e_{ij}}+e^{e_{ik}}},\quad
      \alpha_{ij} = \frac{e^{e_{ij}}}{e^{e_{ii}}+e^{e_{ij}}+e^{e_{ik}}},\quad
      \alpha_{ik} = \frac{e^{e_{ik}}}{e^{e_{ii}}+e^{e_{ij}}+e^{e_{ik}}}. $$
4. Aggregate:
   - $$ \mathbf{h}'_i = \sigma(\alpha_{ii}\mathbf{z}_i + \alpha_{ij}\mathbf{z}_j + \alpha_{ik}\mathbf{z}_k). $$
This shows exactly where ‚Äúdifferent neighbor importances‚Äù enter: the weights $\alpha_{ij}$ are learned functions of node features, not fixed constants.

#### Full model configurations used in the experiments (hyperparameters included where provided)

**Transductive citation benchmarks (Cora, Citeseer, Pubmed; Section 3.3 and Table 1):**
- Model depth: **2-layer GAT**.
- Hidden layer:
  - Number of heads: $K = 8$.
  - Features per head: $F' = 8$.
  - Total hidden features after concatenation: $8 \times 8 = 64$.
  - Nonlinearity: `ELU`.
- Output layer:
  - For Cora/Citeseer: **1 attention head** producing $C$ outputs (number of classes), followed by `softmax`.
  - For Pubmed: **$K = 8$ output heads** (instead of 1), and then averaged (per Eq. (6)) before the final nonlinearity.
- Regularization (explicitly emphasized for small training sets):
  - L2 regularization:
    - $\lambda = 0.0005$ for Cora/Citeseer.
    - $\lambda = 0.001$ for Pubmed.
  - Dropout:
    - Dropout probability $p = 0.6$ applied to **both layers‚Äô inputs**.
    - Dropout also applied to the **normalized attention coefficients** $\alpha_{ij}$, which the paper highlights as exposing each node to a stochastically sampled neighborhood during training.

**Inductive PPI benchmark (Section 3.3 and Table 1):**
- Model depth: **3-layer GAT**.
- First two layers:
  - $K = 4$ heads.
  - $F' = 256$ features per head.
  - Total per-layer output features after concatenation: $4 \times 256 = 1024$.
  - Nonlinearity: `ELU`.
- Final layer (multi-label classification with 121 labels):
  - $K = 6$ heads computing 121 features each.
  - Heads are averaged, followed by logistic sigmoid activation.
- Regularization choices:
  - No dropout and no L2 regularization are used for PPI (the paper states the training set is sufficiently large).
  - Skip connections are used across the intermediate attentional layer.
- Batching:
  - Batch size is **2 graphs** during training.
- A key controlled comparison:
  - `Const-GAT` uses the **same architecture** but sets attention to a constant function $a(x,y)=1$, yielding uniform neighbor weighting (described as ‚ÄúGCN-like inductive operator‚Äù).

**Optimization (Section 3.3):**
- Initialization: Glorot initialization.
- Optimizer: Adam.
- Learning rates:
  - Initial learning rate `0.01` for Pubmed.
  - Initial learning rate `0.005` for all other datasets.
- Early stopping:
  - Monitors validation **loss and accuracy** (transductive) or **micro-F1** (inductive).
  - Patience: **100 epochs**.

#### Computational complexity and practical constraints (Section 2.2)

- For a single attention head producing $F'$ features, the paper gives time complexity:
  - $$ O(|V|\,F\,F' + |E|\,F'). $$
  - Interpretation:
    - The $|V|\,F\,F'$ term corresponds to applying the linear transformation to all nodes.
    - The $|E|\,F'$ term corresponds to computing attention and aggregating along edges.
- Multi-head attention multiplies storage and parameter counts by **a factor of $K$**, while allowing parallel execution because heads are independent.
- Implementation note:
  - The paper reports a sparse-matrix-ops version with storage linear in $|V|$ and $|E|$, but highlights a limitation: the used framework supports sparse matrix multiplication only for rank-2 tensors, which restricts batching (especially for multi-graph datasets like PPI).
- Expressivity vs depth:
  - The receptive field is bounded by network depth (like GCN): with 2 layers, information propagates up to 2 hops.

## 4. Key Insights and Innovations

- **Feature-dependent, masked self-attention on graphs (core novelty).**
  - Difference from many graph convolution baselines: instead of using a fixed normalization that treats neighbors similarly, GAT computes learned weights $\alpha_{ij}$ based on node features (Eq. (3)).
  - Significance: enables the model to prioritize informative neighbors and downweight irrelevant ones within the same neighborhood, increasing capacity without introducing global spectral machinery.

- **Multi-head attention adapted to graph neighborhoods.**
  - The paper operationalizes multi-head attention for graphs with a clear concatenation/averaging scheme (Eq. (5) vs Eq. (6), Figure 1).
  - Significance: improves training stability and expressivity, and provides a structured way to scale representational power (e.g., 8 heads √ó 8 features/head = 64-dimensional hidden representation in transductive tasks).

- **Inductive applicability via edge-local computation.**
  - The attention mechanism is shared across edges and is described as not requiring upfront access to the global graph structure or all nodes‚Äô features, supporting inductive settings where test graphs are unseen.
  - Significance: motivates evaluation on PPI, where test graphs are completely held out during training (Table 1), and where strong gains are shown (Table 3).

- **Regularization via dropout on attention coefficients.**
  - Applying dropout to $\alpha_{ij}$ is an explicit design choice for small-label regimes; it effectively performs stochastic neighbor exposure during training.
  - Significance: this is a graph-specific analogue of dropping connections, potentially improving generalization when only 20 labels/class are available (Cora/Citeseer setup).

## 5. Experimental Analysis

### Evaluation methodology (datasets, tasks, splits, metrics)

- **Datasets (Section 3.1, Table 1):**
  - Transductive node classification (single graph; train uses all node features):
    - `Cora`: 2708 nodes, 5429 edges, 1433 features/node, 7 classes.
    - `Citeseer`: 3327 nodes, 4732 edges, 3703 features/node, 6 classes.
    - `Pubmed`: 19717 nodes, 44338 edges, 500 features/node, 3 classes.
    - Training labels: 20 nodes per class (140/120/60 training nodes for Cora/Citeseer/Pubmed), with 500 validation nodes and 1000 test nodes.
  - Inductive multi-label node classification (multiple graphs; test graphs unseen):
    - `PPI`: 56944 nodes across 24 graphs, 818716 edges, 50 features/node, 121 labels.
    - 20 training graphs (44906 training nodes), 2 validation graphs (6514 nodes), 2 test graphs (5524 nodes).

- **Metrics (Section 3.4):**
  - Transductive: mean classification **accuracy** on test nodes (reported over 100 runs for GAT; baseline numbers reused from prior works as cited).
  - Inductive PPI: **micro-averaged F1** on nodes of the two unseen test graphs, averaged over 10 runs.

- **Baselines (Section 3.2):**
  - Transductive: LP, SemiEmb, ManiReg, DeepWalk, ICA, Planetoid, Chebyshev filters, GCN, MoNet, plus an MLP ignoring graph structure.
  - Inductive: supervised GraphSAGE variants (GCN/mean/LSTM/pool), plus MLP and Random.
  - Additional controlled baselines introduced by the paper:
    - `GCN-64*`: a GCN configured to compute 64 hidden features (to match GAT‚Äôs hidden width), choosing the better of ReLU/ELU after 100 runs.
    - `Const-GAT`: same architecture as GAT but with constant attention $a(x,y)=1$.

### Main quantitative results (with numbers and comparisons)

**Transductive results (Table 2):**
- `Cora`:
  - `GAT`: **83.0 ¬± 0.7%**
  - `GCN`: 81.5%
  - `GCN-64*`: 81.4 ¬± 0.5%
  - Best listed baseline among those near the top: MoNet 81.7 ¬± 0.5%, GCN 81.5%.
  - Improvement emphasized by the paper is consistent with comparing to `GCN-64*`: 83.0 ‚àí 81.4 = **+1.6%**.

- `Citeseer`:
  - `GAT`: **72.5 ¬± 0.7%**
  - `GCN`: 70.3%
  - `GCN-64*`: 70.9 ¬± 0.5%
  - Improvement vs `GCN-64*`: 72.5 ‚àí 70.9 = **+1.6%**.

- `Pubmed`:
  - `GAT`: **79.0 ¬± 0.3%**
  - `GCN`: 79.0%
  - `GCN-64*`: 79.0 ¬± 0.3%
  - Here GAT matches the listed GCN/GCN-64* result (no gain reported in Table 2).

**Inductive PPI results (Table 3):**
- `GAT`: **0.973 ¬± 0.002** micro-F1.
- `Const-GAT`: **0.934 ¬± 0.006** micro-F1.
  - Difference: 0.973 ‚àí 0.934 = **+0.039** micro-F1, used as evidence that learned attention (non-uniform neighbor weighting) matters beyond architecture alone.
- `GraphSAGE` baselines (as listed):
  - Best standard GraphSAGE variant shown: GraphSAGE-LSTM 0.612.
  - `GraphSAGE*` (paper-tuned architecture): **0.768**.
  - Difference vs `GraphSAGE*`: 0.973 ‚àí 0.768 = **+0.205** micro-F1, highlighted in the paper as a large gain in this inductive setting.

### Do the experiments support the claims?

- The experiments directly test two central claims:
  1. **Learned neighbor weighting improves performance**: supported by GAT beating `Const-GAT` on PPI (Table 3) and beating `GCN-64*` on Cora/Citeseer (Table 2).
  2. **Inductive applicability to unseen graphs**: supported by the PPI setup where test graphs are unobserved during training (Table 1) and by the strong reported micro-F1 (Table 3).
- The paper also provides a qualitative visualization:
  - A `t-SNE` plot of first-layer features on Cora (Figure 2) showing clustering by class labels, and an illustration of attention coefficient strengths (aggregated across heads).

### Ablations / controlled comparisons / robustness checks

- Explicit controlled comparisons included:
  - `GCN-64*` to roughly match hidden dimensionality with the transductive GAT setup (Table 2).
  - `Const-GAT` to isolate the effect of attention from the effect of architecture/width/depth (Table 3).
- Repeated runs and variability:
  - Transductive: 100 runs for reporting mean ¬± std for GAT (and GCN-64*).
  - Inductive: 10 runs for mean ¬± std for GAT and Const-GAT on PPI.

## 6. Limitations and Trade-offs

- **Depth-limited receptive field (Section 2.2).**
  - Like GCN-style models, the information a node can use is bounded by the number of layers (e.g., 2 layers ‚Üí up to 2-hop information), which may limit performance on tasks needing long-range dependencies unless deeper architectures (and potentially skip connections) are used.

- **Sparse implementation and batching constraints (Section 2.2).**
  - Although sparse operations reduce storage to linear in $|V|$ and $|E|$, the described framework limitation (sparse multiplication only for rank-2 tensors) restricts batching, especially for datasets with multiple graphs.

- **Hardware efficiency may vary with graph sparsity/regularity (Section 2.2).**
  - The paper notes GPUs may not provide major performance benefits over CPUs in sparse scenarios depending on the regularity of the graph structure.

- **Potential redundant computation in edge-parallel processing (Section 2.2).**
  - Parallelization across all edges can involve redundancy because neighborhoods overlap heavily, which may matter in distributed settings.

- **Variable computational footprint vs neighbor sampling (Section 2.2).**
  - GAT uses the ‚Äúentire neighborhood‚Äù (in the stated setup), which can increase compute for high-degree nodes, whereas sampling-based methods keep fixed footprint but may lose information.

- **Interpretability is suggestive but not fully developed (Section 3.4, Figure 2).**
  - The paper notes attention weights could aid interpretability, but a thorough domain-grounded interpretation is left as future work.

## 7. Implications and Future Directions

- **How this changes the landscape (within the scope of the paper).**
  - The paper demonstrates that replacing fixed neighbor aggregation with feature-dependent attention can yield consistent gains on standard citation benchmarks (Cora/Citeseer) and large gains on an inductive multi-graph benchmark (PPI), suggesting attention is a strong general-purpose primitive for graph representation learning.

- **Follow-up research directions suggested explicitly (Section 4 and Section 2.2).**
  - Improve sparse/batched implementations to handle larger batch sizes and multiple graphs more naturally.
  - Explore attention coefficients for **interpretability** more systematically.
  - Extend from node classification to **graph classification**.
  - Incorporate **edge features** (relationship attributes) into the attention mechanism to handle richer relational data.

- **Practical applications / downstream use cases (as implied by evaluated tasks).**
  - Node classification on citation networks (document topic labeling) and multi-label functional prediction on protein interaction graphs are direct applications demonstrated by the benchmarks used.

- **Repro/Integration Guidance (based on the provided setup).**
  - When to prefer GAT over uniform-aggregation graph models in this paper‚Äôs framing:
    - Prefer GAT when neighbor relevance is likely heterogeneous (some neighbors are more informative than others), since the attention mechanism explicitly models this (Eq. (3)‚Äì(4)) and shows measurable gains over `Const-GAT` on PPI (Table 3).
  - Minimal recipe consistent with the paper:
    - Use a 2-layer, 8-head hidden GAT with dropout on inputs and attention coefficients for small-label transductive citation setups (Section 3.3).
    - Use a deeper (3-layer) wider multi-head GAT without dropout/L2 for larger supervised inductive datasets like PPI (Section 3.3), and include a constant-attention control if you want to quantify the value of learned attention in your setting.

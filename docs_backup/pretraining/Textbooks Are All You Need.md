# Textbooks Are All You Need

**ArXiv:** [2306.11644](https://arxiv.org/abs/2306.11644)
**Authors:** Arka Sadhu, Mohammadamin Barekatain, Xinglong Chen, Arnaud Cojean, Alessandro Sordoni, Alessandro Lazaric, Yann Dauphin, Eldad Haber, Yao Fu, Nicolas Usunier, Yuhuai Wu, Laurent Sifre, Jeffrey Wang, Xia Song
**Institutions:** Microsoft Research

## üéØ Pitch

Textbooks Are All You Need introduces phi-1, a 1.3B-parameter code LLM that achieves 50.6% pass@1 on HumanEval and 55.5% on MBPP using roughly 7B tokens of carefully curated "textbook-quality" data plus <200M tokens of exercise-style finetuning. The core insight is that aggressively improving data quality and task alignment can bend typical scaling laws, enabling small models trained with modest compute (‚âà4 days on 8√óA100) to match or exceed much larger models trained on orders-of-magnitude more data.

---

## üìñ Full Analysis

## 1. Executive Summary
- The paper tackles code generation with a small model by rethinking the data, not the architecture. It trains `phi-1` (1.3B parameters) primarily on carefully filtered web data and synthetic, textbook-like content, then lightly finetunes on synthetic exercise-style prompts.
- Despite the modest scale, `phi-1` reaches strong results ‚Äî pass@1 of 50.6% on HumanEval and 55.5% on MBPP (Table 1) ‚Äî showing that higher data quality and better task alignment can substitute for sheer volume and model size.

## 2. Context and Motivation
- Problem/gap: Standard code datasets (e.g., The Stack, StackOverflow) are noisy and often not pedagogically structured. Many samples are incomplete, boilerplate-heavy, or unbalanced across topics (Section 2; bullet list on p. 3). This weak signal hampers learning of algorithmic reasoning and language-to-code mapping.
- Importance: Improving data quality could reshape scaling behavior ‚Äî making strong performance achievable with smaller models and far less compute. This reduces cost and environmental impact and opens access to high-quality code LLMs.
- Prior approaches and shortcomings:
  - Large models with huge datasets (e.g., 15‚Äì540B parameters trained on 100s of billions to trillions of tokens) dominate performance on HumanEval/MBPP (Table 1). But they rely on massive, noisy crawls and expensive compute.
  - Some works explore scaling laws or synthetic data generation, but few show state-of-the-art code results with tiny models and carefully curated, small datasets.
- Positioning: Building on Eldan & Li‚Äôs TinyStories idea that high-quality synthetic data can bend scaling rules, this work applies a ‚Äútextbook-first‚Äù recipe to code. The architecture is conventional; the novelty lies in dataset curation and a targeted finetuning regimen.

## 3. Technical Approach
The pipeline has three pillars: data curation, a standard decoder-only Transformer, and a two-stage training regimen.

1) Data curation focusing on ‚Äútextbook quality‚Äù
- Sources and composition (Section 2):
  - Filtered web code-language data: subset of The Stack (Python) + StackOverflow. About 6B tokens after filtering.
  - Synthetic textbooks: <1B tokens generated by GPT-3.5, emphasizing natural language exposition interleaved with code and covering reasoning/algorithmic skills (Section 2.2; sample given).
  - Synthetic exercises (`CodeExercises`): ~180M tokens of short Python function docstrings plus solutions designed to mirror HumanEval-style prompts but created with constraints to promote diversity (e.g., constrained function names). This is used only for finetuning.
- Why this design: The goal is a clear, self-contained, instructive, balanced dataset ‚Äî like a well-written textbook ‚Äî rather than raw code dumps that are hard to learn from.

How the filtering works (Section 2.1)
- Label a 100k-sample subset from the raw pool using GPT-4 for ‚Äúeducational value for learning basic coding concepts.‚Äù
- Extract features: use a pretrained CodeGen model to compute embeddings of snippets.
- Train a random forest classifier on embeddings to predict ‚Äúeducational value,‚Äù then filter the full corpus accordingly.
- Effect: Even without synthetic data, training on filtered data improves a 350M model‚Äôs HumanEval pass@1 from 12.19% (on unfiltered data after long training) to 17.68% (36k steps). Adding synthetic textbooks lifts it to 20.12% (Section 2.1). This shows the filter isolates more instructive code samples.

Diversifying synthetic data (Section 2.2)
- To avoid repetitiveness in LLM-generated content, prompts inject structured randomness:
  - Textbooks: vary topics and target audience constraints to induce coverage and diversity.
  - Exercises: restrict function names and other prompt constraints to elicit varied tasks and solutions.

2) Model architecture (Section 2.3)
- Decoder-only Transformer with parallel MHA/MLP blocks (as in CodeGen/PaLM/NeoX), FlashAttention for efficient attention, and rotary position embeddings.
- `phi-1` (1.3B): 24 layers, hidden size 2048, MLP inner 8192, 32 heads, RoPE dim 32; tokenizer from codegen-350M-mono.
- `phi-1-small` (350M): 20 layers, hidden 1024, inner 4096, 16 heads.

3) Training regimen (Section 2.3)
- Pretraining:
  - Dataset: `CodeTextbook` = filtered web data + synthetic textbooks (~7B tokens). Sequence length 2048; next-token prediction; AdamW; warmup+decay; dropout 0.1; fp16; batch 1024.
  - Steps: 36k total; `phi-1-base` checkpoint at 24k steps ‚âà 8 epochs (a bit over 50B tokens seen). Runtime: under 4 days on 8√óA100 (770 GPU hours noted for the 1.3B runs in Fig. 2.1 caption).
- Finetuning:
  - Dataset: `CodeExercises` (~180M tokens).
  - Batch 256; max LR 1e-4; 50 step warmup; 6k steps; select best checkpoint. About 7 hours on the same hardware.
- Why two stages: Pretraining teaches general code/language patterns from curated ‚Äútextbook‚Äù sources; finetuning aligns the model to the target task format (completing small Python functions from docstrings), which unlocks out-of-distribution capabilities as well.

Analogy: Think of pretraining as giving a student a good textbook and exercises across topics, then finetuning as a targeted problem set that mirrors the test‚Äôs format. The second stage helps the student recall and organize prior knowledge efficiently.

## 4. Key Insights and Innovations
1) Quality-over-quantity data curation bends scaling behavior
- Novelty: Instead of scaling model/data size, the work scales ‚Äúdata quality‚Äù using a filter trained on GPT-4-annotated ‚Äúeducational value‚Äù plus diverse synthetic textbooks (Sections 2.1‚Äì2.2).
- Significance: With only ~7B pretraining tokens and <200M finetuning tokens, `phi-1` surpasses or matches much larger models trained on 100√ó more tokens (Table 1).

2) Small, targeted finetuning triggers ‚Äúcapability spikes‚Äù beyond the finetuned tasks
- Observation: After finetuning on basic Python exercises, `phi-1` improves at tasks not present in finetuning data, such as using external libraries (Pygame, Tkinter) and better instruction following (Section 3; qualitative examples and Fig. 3.1 import distribution).
- Interpretation: Finetuning reorganizes latent knowledge acquired during pretraining, making it easier to activate even for tasks outside the finetuning distribution.

3) Strong decontamination via pruning by semantic and syntactic similarity
- Mechanism: Identify near-duplicates between `CodeExercises` and HumanEval using both embedding L2 distance (on CodeGen-350M embeddings) and AST-edit-distance ‚Äúmatch rate‚Äù thresholds (Section 5.2). Retrain after aggressively removing up to >40% of `CodeExercises`.
- Result: Even with heavy pruning (œÑ as low as 0.8), retrained `phi-1` still beats StarCoder-Prompted on HumanEval (Table 3), suggesting gains are not from leakage.

4) Efficient, conventional architecture and training
- Distinctive aspect: Gains come without exotic tricks like Fill-in-the-Middle or Multi-Query Attention (Section 2.3). This isolates the effect of data quality and finetuning alignment.

## 5. Experimental Analysis
Evaluation setup
- Benchmarks and metrics:
  - HumanEval: generate Python functions from docstrings; pass@1 measures whether the first generated solution passes unit tests.
  - MBPP: ‚ÄúMostly Basic Python Problems‚Äù; similar pass@1.
- Baselines: A wide range from Codex and CodeGen to StarCoder, PaLM-Coder, WizardCoder, GPT-3.5, and GPT-4 (Table 1).
- Additional evaluation: 50 new, unconventional problems created by a separate team and graded by GPT-4 for ‚ÄúUnderstanding‚Äù (0‚Äì10 scale aggregated) to avoid contamination and capture partial-credit quality (Section 4; Table 2).
- Decontamination/robustness:
  - N-gram overlap up to 13-grams shows only false positives (Section 5.1).
  - Embedding + AST similarity pruning, followed by retraining, to stress-test leakage hypotheses (Section 5.2; Table 3).

Main quantitative results
- Headline numbers (Abstract; Table 1):
  > HumanEval pass@1: 50.6% (`phi-1`), MBPP pass@1: 55.5%.
  - `phi-1-base` (before exercise finetuning) reaches 29% HumanEval.
  - `phi-1-small` (350M) reaches 45% HumanEval with the same pipeline (Fig. 2.1 label and Table 2).
- Relative comparisons (Table 1):
  - `phi-1` outperforms StarCoder (33.6% HE; 52.7% MBPP) and StarCoder-Prompted (40.8% HE) despite being ~12√ó smaller and trained on a tiny fraction of tokens.
  - It trails GPT-4 (67% HE) and WizardCoder (57.3% HE; but lower MBPP at 51.8%).

Ablations on training dimensions (Figure 2.1)
- Structure of Fig. 2.1: compares (A) standard unfiltered Stack code vs (B) `CodeTextbook` vs (C) `CodeTextbook ‚Üí CodeExercises` finetuning across model sizes and tokens seen.
- Key takeaways:
  - For 350M models, unfiltered training saturates ‚âà12%; filtering alone raises to ‚âà17.7%; adding textbooks ‚âà20.1%; adding finetuning yields a large jump (up to 45%).
  - For 1.3B models, `phi-1-base` (after ~51B seen tokens on `CodeTextbook`) hits 29%; finetuning to `phi-1` reaches 51%. A ‚ÄúThe Stack+‚Äù 1.3B model trained longer (76B tokens seen; 1090 GPU hours) still lags far behind without the curated/synthetic data.

Unconventional problems with LLM grading (Table 2)
- On 50 novel problems outside training distribution, scores align with HumanEval ranking:
  > `phi-1`: 52% (LLM-graded Understanding), HumanEval 51% shown in the table‚Äôs column; `phi-1-base`: 37% / 29%; StarCoder: 51% / 34%.
- This supports that improvements generalize beyond potential contamination and that finetuning improves reasoning quality, not just test familiarity.

Decontamination stress test (Table 3)
- Split HumanEval into problems with/without close matches in `CodeExercises` under AST match rate œÑ.
- After pruning and retraining:
  - Total accuracy remains 45‚Äì46% depending on œÑ (vs. 50.6% original), and still exceeds StarCoder-Prompted‚Äôs 41.5% across all œÑ.
  - Accuracy is higher on ‚Äúsimilar‚Äù subsets for all models, indicating those are easier problems, not necessarily contaminated.

Qualitative evidence of ‚Äúcapability spikes‚Äù (Section 3)
- Instruction-following and reasoning: Better handling of intricate logical constraints after finetuning (Sec. 3.1 examples).
- External libraries: Despite exercises rarely importing Pygame/Tkinter (Fig. 3.1), `phi-1` composes runnable event loops and GUI callbacks more reliably than `phi-1-base` or `phi-1-small` (Sec. 3.2 snippets).
- Chat-like responses: `phi-1` engages in short Q&A style completions even though chat data was not emphasized in pretraining but present during finetuning alignment (Sec. 3 end).

Do the experiments support the claims?
- Yes, across multiple angles: direct benchmark wins vs. strong open-source baselines (Table 1), alignment with an independent ‚ÄúUnderstanding‚Äù evaluation (Table 2), ablations isolating data composition and finetuning effects (Figure 2.1), and rigorous decontamination checks (Section 5; Table 3).
- Caveat: The GPT-4 grader introduces a dependency on another LLM; however, results match traditional unit-test rankings and the grader is used primarily for the new evaluation, not to replace HumanEval/MBPP.

## 6. Limitations and Trade-offs
- Scope and specialization:
  - Focused on Python; multi-language coding and specialized APIs remain weaker (Conclusion; Appendix B).
- Prompt robustness:
  - Sensitive to stylistic and grammatical variations; long or ambiguous prompts degrade performance (Appendix B: examples show failures on small wording changes, counting/spatial layout in GUIs, and ambiguous terms like ‚Äúunchanged‚Äù).
- Counting and spatial reasoning:
  - Struggles with precise layout/counting tasks (Appendix B GUI example). These are common pain points for smaller models.
- Synthetic data quality:
  - GPT-3.5 textbooks/exercises contain errors (Conclusion notes high error rate). The model is surprisingly resilient, but better teachers (e.g., GPT-4) could further help.
- Evaluation nuances:
  - LLM grading is informative but subjective; while results align with unit-test benchmarks, full reproducibility depends on grader prompts and versions.
- Compute vs data reuse:
  - Pretraining sees ~50B tokens through multiple epochs; while the unique token count is small (~7B), repeated exposure is necessary ‚Äî a trade-off between diversity and repeated reinforcement.

## 7. Implications and Future Directions
- Landscape shift:
  - Demonstrates that ‚Äúdata quality + alignment‚Äù can be a first-class scaling axis. High-caliber small models can rival much larger ones on focused tasks, lowering entry barriers and environmental costs.
- Practical applications:
  - Education and coding assistance where small, efficient models are preferable: on-device or resource-constrained environments, enterprise settings requiring tight control and retrainability.
- Follow-up research enabled:
  - Better ‚Äútextbook constructors‚Äù: use stronger teachers (GPT-4), programmatic diversity controls, and automatic error correction for synthetic data.
  - Systematic measures of data pedagogy: quantifying clarity, self-contained-ness, and conceptual coverage; developing diversity/novelty metrics beyond n-grams.
  - Alignment methods that provoke capability transfer: finetuning curricula that elicit library/API competence without direct exposure; exploring chain-of-thought or tool-augmented exercises.
  - Broader domains: replicate the textbook+exercise recipe for other programming languages, math reasoning, or domain-specific coding (e.g., data science notebooks, embedded systems).
- Open questions:
  - How far can quality substitute for quantity before hitting a ceiling? What is the optimal mix between curated real code and synthetic pedagogy? How to avoid ‚Äúmodel collapse‚Äù when training on model-generated data at scale?

Overall, the work provides a clear, reproducible playbook: filter for educational value, synthesize pedagogically diverse textbooks, and align with small, task-shaped exercises. With this, a conventional 1.3B Transformer can reach competitive code-generation capability with a fraction of the data and compute (Table 1; Figure 2.1).

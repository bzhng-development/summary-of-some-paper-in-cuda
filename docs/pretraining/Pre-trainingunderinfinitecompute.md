# Pre-training under infinite compute

**ArXiv:** [2509.14786](https://arxiv.org/abs/2509.14786)

## ðŸŽ¯ Pitch

This paper tackles the regime where training compute is plentiful but high-quality pre-training data is limited, a setting increasingly relevant as model compute scales outpace web data growth. The authors show that properly tuned, much stronger regularizationâ€”especially weight decayâ€”enables predictable gains from larger models, and that scaling ensembles of independently trained models yields even greater data efficiency than scaling a single model. By composing these strategies and using distillation, they achieve dramatic, robust improvements in data efficiency that transfer to real downstream tasks and continued pre-trainingâ€”offering new recipes for getting much more out of limited data in a compute-rich future.

---

## 1. Executive Summary
This paper studies how to pre-train language models when training compute is effectively unlimited but highâ€‘quality training data is fixed and scarce. It shows that (a) tuning strong regularization (especially much larger weight decay) yields predictable, monotonic scaling with model size, and (b) scaling ensembles of independently trained models beats scaling a single model in this dataâ€‘constrained regime. Composing both and then distilling recovers most of the gains at small inference cost, delivering large and robust dataâ€‘efficiency improvements that also transfer to downstream tasks and continued preâ€‘training.

## 2. Context and Motivation
- Problem and gap
  - Compute devoted to preâ€‘training grows 4Ã— per year while available web text grows ~1.03Ã— per year (Introduction; citing [Sevilla & RoldÃ¡n 2024; Villalobos et al. 2024]). The field is entering a regime where compute vastly exceeds data.
  - Standard scaling recipes assume fresh data can grow with compute. Under fixed data, common practices are to repeat (epoch) the dataset and to increase model size. Both eventually overfitâ€”validation loss risesâ€”even if one spends more compute (Section 2.1; Figure 2).
  - Open question: What training algorithms give the best possible performance when data is fixed and compute is unconstrained?

- Why it matters
  - Practical: Labs and developers increasingly face deâ€‘duplicated or domainâ€‘limited corpora. Getting more from the same tokens lowers costs, unlocks domain models, and reduces dataâ€‘collection risks.
  - Scientific: Understanding generalization under severe overâ€‘parameterization and repeated exposure to the same data clarifies how scaling laws behave once â€œmore dataâ€ is no longer the cure.

- Prior approaches and their limits
  - Chinchillaâ€‘style computeâ€‘optimal scaling couples data and parameters; it does not apply when data cannot grow (Section 2).
  - Data repetition/epoching shows monotone improvements in some reports but in practice often overfits for language modeling at scale (Section 2.1; Figure 2 left; Appendix D.1 discusses tuning).
  - Simply making models larger at fixed data also overfits (Section 2.1; Figure 2 right; echoes Kaplan et al. Figure 9 for singleâ€‘pass fixedâ€‘data behavior).

- Position relative to existing work
  - The paper reframes evaluation: with data fixed and compute unconstrained, compare training recipes by the asymptote of their scaling laws (the predicted loss as model size or ensemble size tends to infinity), not by performance at a fixed compute budget (Section 3, â€œasymptote EDâ€ in the fit LÌ‚D,N = AD/N^Î±D + ED).
  - It combines classic techniquesâ€”regularization, ensembling, distillationâ€”in a computeâ€‘rich/dataâ€‘limited setting and quantifies them via scalingâ€‘law asymptotes and crossâ€‘dataâ€‘scale fits (Sections 3â€“6).

## 3. Technical Approach
This section reconstructs the paperâ€™s methods step by step.

- Formalization of the objective (Section 2)
  - Define a training routine `A(D, H)` that takes a token budget `D` and hyperparameters `H` (including model size `N`, epochs `E`, LR, weight decay, etc.) and outputs a model `M`.
  - Quality is measured by validation loss `L(M)` on a heldâ€‘out i.i.d. split from the same corpus.
  - With data fixed at `D`, the goal is to minimize `L(A(D, H))`, unconstrained by compute (Section 2; â€œProblem settingâ€ in Appendix A).
  - Evaluation reframing: since compute is unconstrained, the relevant comparison for monotone recipes is the asymptote (limit loss as we scale the knobâ€”parameters or ensemble membersâ€”to infinity), not a point on a computeâ€“performance curve (Section 3).

- Experimental environment (Sections 2, A)
  - Data: DCLM web corpus; default focus on D = 200M tokens, plus 400M/800M/1.6B to study data scaling (Section 2; Figure 7).
  - Architecture: Llamaâ€‘style decoderâ€‘only models with context length 4096, bf16 compute, AdamW, cosine LR schedule with 1% warmup, gradient norm clipping 1.0 (Appendix A).
  - Model sizes: ~150M/300M/600M/1.4B parameters (Appendix A Table 2); note the 1.4B preset is relatively wide/few layersâ€”a nonstandard scaling acknowledged later as a caveat (Appendix B.5).
  - Validation: fixed set of 1024 sequences (~4M tokens) across all experiments (Appendix A).
  - Metrics: validation loss (perâ€‘token negative logâ€‘likelihood) and downstream accuracy on ARCâ€‘Easy, PIQA, SciQ to check correlation (Section 7; Figure 10; Table 5).

- Baseline: â€œstandard recipeâ€ under fixed data (Section 2.1; Figure 2)
  - Two knobs: increase epoch count `E` (more passes over the same tokens) and increase model size `N`.
  - Tuning: LR and `E` tuned per `N` (Appendix B.1).
  - Observation: Increasing `E` initially lowers loss, but too many epochs overfit (loss rises, Figure 2 left). Increasing `N` past a point also increases validation loss (Figure 2 right), even after tuning LR and `E`. Training loss keeps dropping in both casesâ€”clear overfitting (Appendix B.5; Figure 15).

- Regularized parameter scaling (Section 3)
  - Key idea: Stronger regularizationâ€”especially much larger weight decay than the de facto 0.1â€”can prevent overfitting and restore monotone scaling with `N`.
  - Hyperparameter search: Coordinateâ€‘descentâ€‘style search for â€œlocally optimalâ€ `H` per (`D`, `N`) over a discretized grid of LR, `E`, and weight decay; a point is â€œlocally optimalâ€ if none of its 1â€‘step neighbors improves validation loss (Appendix B.1). Ablations show this joint tuning is necessary; naive transfer of best `E` or decay across `N` breaks monotonicity (Appendix B.2; Figure 11).
  - Result: Optimal weight decay grows with `N` and can be ~30Ã— higher than standard practice (e.g., up to 3.2 vs 0.1; Figure 3 table; Appendix B.3 Figure 12). With this tuning, loss decreases monotonically with `N` and follows a power law:
    - LÌ‚D,N = AD / N^Î±D + ED, fit over four `N` values (Section 3).
    - For D = 200M, the fit is LÌ‚200M,N = 0.05 / N^1.02 + 3.43 (Figure 3). The exponent ~1 is steep compared to Chinchillaâ€™s parameter exponent ~0.34.

- Ensembling: scaling number of independently trained members `K` (Section 4)
  - Mechanism: Train `K` independent copies of the same architecture and hyperparameters (different random seeds affect data order and initialization; Appendix C.1), and average their preâ€‘softmax outputs (â€œlogitsâ€) at inference. This â€œlogit averagingâ€ produces the ensemble prediction (Section 4.1).
    - Define EA(D, N, K, H) = LogitAvg({A(D, N, Zi, H)}iâˆˆ[K]).
  - Compute model: Inference cost and total parameters scale linearly with `K`; total â€œparameters usedâ€ is `N*K` (Section 4.1).
  - Observation: For fixed `N`, increasing `K` reduces loss roughly as 1/K and, crucially, approaches a lower asymptote than scaling `N` alone (Figure 4).
  - Hyperparameters for the `K â†’ âˆž` limit differ from singleâ€‘model optima: more epochs and less weight decay per member give a better ensemble asymptote (Section 4.2; Figure 5).
    - Heuristic that works broadly: â€œdouble the epochs, halve the weight decayâ€ vs the singleâ€‘model optimum, with the same LR (Appendix C.2; Figure 17).

- Joint scaling: compose parameter scaling and ensemble scaling (Section 4.3)
  - Goal: Estimate the best possible loss when `K â†’ âˆž` and `N â†’ âˆž`. They take the limits in the order limNâ†’âˆž limKâ†’âˆž minH L(EA(D, N, K, H)) and argue the value is orderâ€‘independent under monotonicity (Appendix C.4).
  - Procedure (Figure 6):
    1) For each `N` at fixed `D`, fit a power law over `K` and extract its `K â†’ âˆž` asymptote (left panel).
    2) Fit a second power law over those asymptotes as `N` increases and read off the `N â†’ âˆž` asymptote (right panel).
  - Hyperparameters for ensembles use the above â€œdoubleâ€‘epoch/halfâ€‘decayâ€ heuristic, validated across scales with one exception in the most overâ€‘parameterized corner (Appendix C.2).

- Dataâ€‘scaling analysis (Section 5)
  - For each recipeâ€”standard, regularized single model, and the joint N+K ensembleâ€”estimate the best achievable loss at D âˆˆ {200M, 400M, 800M, 1.6B}.
    - Standard recipe: grid search over `N`, `E`, LR (weight decay fixed 0.1) and take the best model per `D` (Section 5.1; Appendix D.1; Figure 7 right, red points).
    - Regularized single model: for each `D`, fit the `N` power law and take its asymptote `ED` (Figure 7 left; purple points migrate to right panel).
    - Joint scaling: for each `D`, take `K â†’ âˆž` asymptotes per `N`, then `N â†’ âˆž` asymptotes to get one value per `D` (Figure 8 leftâ†’middleâ†’right).
  - Then fit dataâ€‘scaling laws of the form LÌ‚D = A / D^Î± + E for each recipe (Figures 7 right, 8 right).

- Distillation to reduce inference compute (Section 6; Figure 9; Appendix E)
  - Sequenceâ€‘level knowledge distillation: generate a large pool of synthetic tokens from a stronger â€œteacherâ€ (here, ensembles of 300M models) and train a â€œstudentâ€ (same 300M architecture) on a mixture of real tokens and synthetic tokens (Appendix E.1â€“E.3).
  - Key knobs: mixing ratio (batches of real : synthetic), epochs, LR, weight decay. Optimal distillation uses much smaller weight decay (0.1) than regularized preâ€‘training (Appendix E.2 Table 3). Teacher data is generated unconditionally at temperature 1 with a highâ€‘throughput engine (Appendix E.1).
  - Selfâ€‘distillation: teacher and student have the same size; mixing real data with synthetic avoids model collapse and yields a student that surpasses the teacher (Figure 9 and Appendix E.3 Table 4).

- Downstream evaluation and continued preâ€‘training (Section 7)
  - Downstream benchmarks: ARCâ€‘Easy, PIQA, SciQ using lmâ€‘evaluationâ€‘harness, mostly 200Mâ€‘token models/ensembles (Section 7.1; Figure 10 right; Table 5).
  - Continued preâ€‘training (CPT) case study: Llama 3.2 3B base on MegaMathâ€‘Webâ€‘Pro; restrict to 4B seed tokens (out of 73B) and apply small batch size + epoching + ensembling (Section 7.2; Table 1). Also compare ensembling vs weightâ€‘averaged â€œmodel soupsâ€ here (Appendix G.2; Table 7).

## 4. Key Insights and Innovations
- Asymptoteâ€‘based evaluation for dataâ€‘constrained, computeâ€‘unconstrained preâ€‘training
  - Innovation: Evaluate training â€œrecipesâ€ by the asymptote of their scaling law as the scalable knob (parameters or ensemble size) tends to infinity at fixed data (Section 3). This directly targets â€œbest achievable with infinite compute,â€ not â€œcomputeâ€‘optimal today.â€
  - Why it matters: It ranks algorithms by their ceiling performance in the regime the field is heading into.

- Heavy regularization unlocks monotone parameter scaling far beyond Chinchilla ratios
  - Finding: With locally optimal tuning, weight decay needs to be ~30Ã— larger than common defaults to prevent overfitting when tokens are scarce and models are very large (Figure 3 table; Appendix B.3).
  - Significance: Loss decreases smoothly with `N` following LÌ‚200M,N = 0.05/N^1.02 + 3.43 up to parameterâ€‘toâ€‘token ratios ~140Ã— Chinchilla (Figure 3). This is both a practical recipe and a conceptual bridge to theory showing optimal regularization mitigates double descent.

- Ensembles beat singleâ€‘model scaling under fixed dataâ€”and need different member hyperparameters
  - Finding: For `N = 300M`, scaling `K` gives an asymptote â‰ˆ3.34 vs 3.43 for `N â†’ âˆž` singleâ€‘model scaling (Figure 4). After reâ€‘tuning member hyperparameters for the `K â†’ âˆž` regime (more epochs, less weight decay), the asymptote improves further to â‰ˆ3.27 (Figure 5).
  - Significance: Under fixed data, allocating compute to train multiple smaller models that specialize (and then average) outperforms training one huge model. This contrasts with some classic scaling intuitions and is central for a computeâ€‘rich future.

- Joint scaling (parameters + ensemble members) achieves the best ceiling
  - Result: The doubleâ€‘limit asymptote at 200M tokens is â‰ˆ3.17 (Figure 6 right), improving upon the regularized singleâ€‘model asymptote 3.43 and the unregularized recipe â‰ˆ3.75.

- Distillation compresses most of the ensembling gain into a single small model
  - Result: Distilling an 8â€‘member 300M ensemble into a single 300M student attains loss 3.36â€”retaining 83% of the ensemble improvement and even beating the regularized singleâ€‘model asymptote (Figure 9; Appendix E.2).
  - Selfâ€‘distillation also works if synthetic data is mixed with real: at matched tokens, selfâ€‘distill (1:1 mixing) reaches 3.437 vs teacher 3.710; omitting real data collapses to 4.069 (Appendix E.3 Table 4). This links distillation to an â€œimplicit ensemblingâ€ view and gives a computeâ€‘heavy path to better small models without training larger models first.

## 5. Experimental Analysis
- Evaluation methodology
  - Datasets and splits
    - Preâ€‘training: DCLM web data; main seed token counts 200M, plus 400M/800M/1.6B; fixed 4Mâ€‘token validation set (Section 2; Appendix A).
    - Downstream: ARCâ€‘Easy, PIQA, SciQ, using standard harness (Section 7.1; Table 5).
    - Continued preâ€‘training: MegaMathâ€‘Webâ€‘Pro; base model Llama 3.2 3B; restrict to 4B tokens to simulate data scarcity (Section 7.2; Table 1).
  - Metrics
    - Validation loss for scalingâ€‘law analysis; accuracy for downstream and math benchmarks (Sections 5, 7).
  - Baselines/recipes compared
    - Standard (epoching + model scaling, decay=0.1).
    - Regularized model scaling (joint LR/epoch/decay tuning).
    - Ensemble scaling (logit averaging) and joint scaling.
    - Distillation (ensembleâ†’student; selfâ€‘distill).
    - For CPT: default hyperparameters vs small batch size + epoching + ensembling (Section 7.2; Table 1).

- Main quantitative results
  - Overfitting of standard recipe under fixed data
    - â€œToo many epochsâ€ increases validation loss (Figure 2 left), even though train loss declines (Appendix B.5 Figure 15 left).
    - Larger single models at fixed data start to hurt at 1.4B vs 600M in tuned runs (Figure 2 right); train loss keeps falling (Appendix B.5 Figure 15 right), confirming overfitting.
  - Regularized singleâ€‘model scaling
    - With tuned large weight decay per `N`, loss falls monotonically with `N` and fits LÌ‚200M,N = 0.05 / N^1.02 + 3.43 (Figure 3).
    - Hyperparameters evolve systematically with scale: as `N` grows, optimal LR and epochs decrease, weight decay increases (Appendix B.3 Figure 12).
  - Ensemble scaling
    - For fixed `N = 300M` with singleâ€‘modelâ€‘optimal hyperparameters, fit suggests asymptote â‰ˆ3.34 (Figure 4).
    - Tuning member hyperparameters for the `K â†’ âˆž` regime improves the asymptote to â‰ˆ3.27 and flips the hyperparameter ranking relative to singleâ€‘model optima (Figure 5). â€œDouble epochs, half weight decayâ€ is the robust rule (Appendix C.2; Figure 17).
    - Either data order or initialization randomness alone suffices to harvest most ensemble gains (Appendix C.1; Figure 16).
  - Joint scaling
    - Doubleâ€‘limit at 200M tokens: â‰ˆ3.17 (Figure 6 right), best among all.
  - Dataâ€‘efficiency comparisons across D
    - Dataâ€‘scaling fits (Figures 7 right and 8 right) have similar exponents across recipes (~0.23â€“0.24) and similar infiniteâ€‘data asymptotes (~1.89â€“1.96), implying a roughly constant dataâ€‘efficiency multiplier at all scales (Section 5.4).
    - At D = 200M:
      > â€œRegularized recipe is 2.29Ã— more dataâ€‘efficient than the standard recipe; joint scaling is 5.17Ã—â€ (Sections 5.2â€“5.3; Figures 7â€“8).
      - Without extrapolating asymptotes, a 5Ã—1.4B ensemble already delivers 3.75Ã— data efficiency over the standard recipe at 200M (Section 5.3).
  - Distillation
    - Ensembleâ†’student (300M): teacher 8â€‘ensemble â‰ˆ3.32; student 3.36; best regularized single 300M â‰ˆ3.57; student surpasses singleâ€‘model asymptote (Figure 9; Appendix E).
    - Selfâ€‘distill ablation:
      > â€œTeacher 3.7103; selfâ€‘distill with 1:1 real:synthetic 3.4373; selfâ€‘distill without real data 4.0693â€ (Appendix E.3 Table 4).
  - Downstream benchmarks
    - Validation loss improvements translate to accuracy gains (Figure 10: left vs right). Example summary (Table 5, averages):
      > Best unregularized 300M: 58.47 avg; best regularized 1.4B: 60.73; 300M K=5 ensemble: 63.00; 1.4B K=5 ensemble: 64.39. Distilled 300M (ensemble teacher): 62.19; selfâ€‘distilled 300M: 60.54.
    - Model soups (weight averaging) perform poorly in preâ€‘training context (Table 5, ~35% avg), supporting the view that independent preâ€‘training runs land in different basins (Appendix C.3.2).
  - Continued preâ€‘training (math)
    - With only 4B tokens:
      > Default CPT: 30.59 avg; small batch only: 34.48; add epoching (K=1): 35.82; K=8 ensemble: 40.58 (Table 1).
      - This exceeds a baseline that used all 73B tokens (39.23), i.e., a 17.5Ã— dataâ€‘efficiency gain.
    - In CPT, weightâ€‘averaged soups slightly outperform ensembles as K grows (Appendix G.2; Table 7).

- Ablations, robustness, and diagnostics
  - Batch size: Smaller is better for fixed tokens (e.g., batch 64 best; Figure 13 left), aligning with generalization literature.
  - Weight decay dynamics: High decay slows early improvement but wins decisively by end of training (Figure 14).
  - Sensitivity of powerâ€‘law fits: Reâ€‘fitting across seeds for model scaling and subâ€‘sampling Kâ€‘points for ensembles shows small asymptote variance (Appendix H.1; Figure 20), though the authors caution these are rough estimates.

- Do the experiments support the claims?
  - The paper triangulates its core claims with multiple, complementary analyses: overfitting diagnostics (train vs val), controlled hyperparameter searches, clear monotone scaling fits with small residuals, crossâ€‘dataâ€‘scale checks, downstream transfer, and a second, independent setting (CPT) where the techniques again help. The asymptote estimation remains extrapolative, but sensitivity checks and many direct points (e.g., K=3 already beating singleâ€‘model asymptote; Figure 4) make the qualitative conclusions convincing.

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Fixedâ€‘data, computeâ€‘unconstrained framing: valuable for the emerging regime but not a replacement for computeâ€‘optimal tradeâ€‘off studies. Asymptote estimates depend on functional forms and fit ranges (Appendix H).
  - Validation data is i.i.d. from the same distribution; real deployments may care about domain shift or alignment objectives not probed here (Section 7.1 focuses on three standard smallâ€‘model benchmarks).
- Algorithmic choices not fully explored
  - Regularization space is vast. The study varies weight decay, epochs, LR, batch size, but not, e.g., dropout, data augmentation beyond distillation, or alternative objectives (Appendix B.4 notes dropout wasnâ€™t tuned; Section 8 suggests many avenues).
  - Alternatives to ensembles:
    - Mixtureâ€‘ofâ€‘Experts is discussed conceptually and in small internal tests but not deeply benchmarked; intuition is that MoEâ€™s shared learning trajectory reduces the â€œmultiâ€‘viewâ€ benefit (Appendix C.3.1).
    - Model soups underperform for preâ€‘training but worked well for CPT (Appendix C.3.2; G.2), indicating contextâ€‘specific behavior that deserves deeper study.
- Architectural caveat
  - The 1.4B configuration is nonâ€‘standard (wider, fewer layers; Appendix A Table 2), which might affect relative scaling. The authors argue heavy decay compensates (Appendix B.5), but strict comparability to canonical depthâ€‘scaled models is a caveat.
- Compute and inference costs
  - Ensembles linearly increase inference cost with `K`. Distillation recovers most gains at fixed inference cost, but requires additional training compute and a generation pipeline for synthetic data (Appendix E.1 notes 10B synthetic tokens generated; some teacher data was epoched due to limits).
- Statistical uncertainty
  - Asymptote extrapolations are based on four `N` values (and five `K` values per `N`), and the fits are sensitive in principle to run noise; sensitivity checks are promising but limited (Appendix H.1).

## 7. Implications and Future Directions
- How this changes the landscape
  - It reframes â€œscalingâ€ for the dataâ€‘scarce era: rather than chasing computeâ€‘optimal single models, use regularization to unlock stable scaling, invest compute into ensembles to reach a lower loss ceiling, then distill to deployable size. This pipeline is explicitly designed for the trajectory where compute outpaces data.
  - The asymptoteâ€‘centric view gives a principled way to compare recipes when compute is not the bottleneck and encourages designing methods that improve the limiting performance, not just the pointâ€‘wise tradeâ€‘off.

- Followâ€‘up research enabled or suggested
  - Regularization: systematic studies of dropout, data augmentation, noise injection, and alternative optimizers under this fixedâ€‘data regime; characterize how the optimal decay scales with parameterâ€‘toâ€‘token ratio (Appendix B.3 suggests ~0.8 when that ratio is fixed).
  - Structured ensembles: beyond independent seedsâ€”diversityâ€‘encouraging training (e.g., negative correlation learning), coâ€‘training, or subâ€‘architectures to improve â€œmultiâ€‘viewâ€ coverage while easing inference via smarter distillation.
  - Asymptote estimation: more rigorous uncertainty quantification for multiâ€‘tier scaling fits (N, K, D), potentially Bayesian fits; evaluating whether equal infiniteâ€‘data asymptotes across recipes (Section 5.4) empirically converge to the entropy bound.
  - Alternatives to ensembling: revisit MoE with diversityâ€‘oriented training or dropout; study why CPT soups outperform ensembles and whether that can transfer back to preâ€‘training.
  - Selfâ€‘generated data curricula: The selfâ€‘distillation result (Figure 9; Appendix E.3) suggests broader syntheticâ€‘data training that carefully mixes real data to avoid collapse; explore taskâ€‘aware sampling and temperature schedules.

- Practical applications
  - Training small and midâ€‘sized foundation models in dataâ€‘poor domains (healthcare, legal, enterprise internal text) by combining strong regularization, ensembles during training time, and distillation for deployment.
  - Continued preâ€‘training on midâ€‘training domains (e.g., math, code, safety) with scarce curated tokens: the CPT case shows 17.5Ã— dataâ€‘efficiency gains over a 73Bâ€‘token baseline using only 4B tokens (Table 1).
  - Budget planning: when compute is cheap but data acquisition is costly or capped, these recipes offer a computeâ€‘centric roadmap to higher quality without new data.

> Representative headline results to carry forward:
> - Regularized singleâ€‘model asymptote at 200M tokens: 3.43 (Figure 3).
> - Ensemble member reâ€‘tuning reduces the `K â†’ âˆž` asymptote to 3.27 (Figure 5).
> - Joint scaling `N, K â†’ âˆž` at 200M tokens: 3.17 (Figure 6 right).
> - Dataâ€‘efficiency at 200M: 2.29Ã— (regularized) and 5.17Ã— (joint scaling) over the standard recipe; similar multipliers across higher `D` (Figures 7â€“8; Section 5.4).
> - Distill 8Ã—300M ensemble â†’ 300M student: loss 3.36, retaining 83% of the ensembling gain (Figure 9; Appendix E).
> - CPT on math with 4B tokens and ensembles beats a 73Bâ€‘token baseline (Table 1), a 17.5Ã— data efficiency improvement.

Overall, the work provides a concrete, wellâ€‘tested blueprint for the computeâ€‘rich, dataâ€‘poor future: regularize hard, ensemble broadly, measure by asymptotes, and distill to deploy.

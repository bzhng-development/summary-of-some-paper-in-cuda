# PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model

**ArXiv:** [2510.14528](https://arxiv.org/abs/2510.14528)

## ðŸŽ¯ Pitch

PaddleOCR-VL introduces a state-of-the-art, ultra-compact document parsing framework that couples a lightweight, dedicated layout analyzer with a high-efficiency 0.9B vision-language model to achieve exceptional accuracy in multilingual, page-level, and element-level recognitionâ€”including text, tables, formulas, and chartsâ€”across 109 languages. By decoupling layout detection from element recognition and leveraging an innovative NaViT-style visual encoder, PaddleOCR-VL surpasses prior solutions in both speed and accuracy while drastically reducing resource requirements, making robust document understanding possible on practical, resource-constrained deployments and enabling new capabilities for information extraction and retrieval in global, real-world scenarios.

---

## 1. Executive Summary
PaddleOCR-VL introduces a twoâ€‘stage, resourceâ€‘efficient system for multilingual document parsing that combines a lightweight layout analyzer with a compact 0.9B visionâ€‘language model (`PaddleOCRâ€‘VLâ€‘0.9B`). It delivers stateâ€‘ofâ€‘theâ€‘art accuracy on pageâ€‘level and elementâ€‘level tasks (text, tables, formulas, charts) across 109 languages while running faster and with less memory than competing systems (see Table 2, Table 4, Table 13).

## 2. Context and Motivation
- Problem addressed
  - Parsing real documents involves identifying layout regions (text blocks, tables, formulas, charts, images), ordering them for correct reading, and converting each element into structured content (e.g., Markdown, JSON) with high fidelity. Section 1 stresses that modern documents mix dense text, complex tables/charts, mathematical expressions, multiple languages, and handwritingâ€”making accurate, scalable parsing difficult.
- Why it matters
  - Reliable parsing is a foundation for information retrieval and RAG systems and is critical for digitizing reports, contracts, academic papers, forms, and historical or multilingual materials (Section 1).
- Prior approaches and their limitations
  - Pipeline systems of â€œexpertâ€ models achieve strong accuracy but suffer from integration complexity and error compounding across stages (Section 1; e.g., PPâ€‘StructureV3, MinerUâ€‘pipeline; Table 2 and Table 3 include these baselines).
  - Endâ€‘toâ€‘end visionâ€‘language models (VLMs) simplify workflows but incur high latency/memory due to long autoregressive outputs and can hallucinate layout/reading order, especially on multiâ€‘column pages (Section 1, Section 2.1.1).
- Positioning of this work
  - The work decouples layout analysis from recognition:
    - Stage 1: a dedicated, small layout model (`PPâ€‘DocLayoutV2`) detects elements and predicts reading order (Figure 3).
    - Stage 2: a compact VLM (`PaddleOCRâ€‘VLâ€‘0.9B`) recognizes each cropped element with a dynamicâ€‘resolution NaViT vision encoder and a small, efficient language model (ERNIEâ€‘4.5â€‘0.3B) (Figure 4).
  - This design aims to keep layout stable and fast while achieving high recognition accuracy with minimal compute (Section 2.1).

## 3. Technical Approach
The system is a twoâ€‘stage pipeline (Figure 2), followed by light postâ€‘processing to produce Markdown/JSON.

1) Stage 1: Layout analysis with `PPâ€‘DocLayoutV2` (Section 2.1.1; Figure 3)
- Tasks: detect element boxes and classes; infer reading order.
- Detection and classification: extends `RTâ€‘DETR` (a realâ€‘time transformer detector) to localize and classify text blocks, tables, formulas, charts (Section 2.1.1).
- Reading order with a `pointer network` (six transformer layers):
  - A pointer network is a sequence model that outputs an ordering over a set of inputs by â€œpointingâ€ to items in the input. Here, it orders detected layout regions.
  - Preâ€‘selection: apply perâ€‘class thresholds to keep â€œforegroundâ€ boxes to be ordered.
  - Embeddings: each proposal is embedded with absolute 2D positional encodings and classâ€‘label embeddings.
  - Relation modeling: the encoder attention adds a geometric bias from `Relationâ€‘DETR` to capture pairwise spatial relations (near/far, left/right, above/below).
  - Pairwise relation head: linearly projects region features to queries/keys, computes bilinear similarities, and produces an NÃ—N matrix of pairwise â€œwho comes before whomâ€ logits.
  - Decoding: a deterministic â€œwinâ€‘accumulationâ€ algorithm turns pairwise preferences into a topologically consistent reading order (Section 2.1.1).
- Training (Section 2.2.1):
  - Two stages: (i) train `RTâ€‘DETR` on 20k+ curated pages for 100 epochs (initialized from `PPâ€‘DocLayout_Plusâ€‘L`), then freeze; (ii) train the pointer network for 200 epochs with `Generalized Cross Entropy` loss (robust to noisy labels), constant LR 2eâ€‘4, `AdamW`.

2) Stage 2: Elementâ€‘level recognition with `PaddleOCRâ€‘VLâ€‘0.9B` (Section 2.1.2; Figure 4)
- Inputs: crops of each detected element in reading order.
- Vision encoder: a `NaViT`â€‘style encoder initialized from Keyeâ€‘VL, which ingests images at their native resolution by â€œpatchâ€‘andâ€‘pack,â€ avoiding forced resizing/tiling. This reduces distortions and hallucinations on dense text (Section 2.1.2).
- Projector: a 2â€‘layer MLP with GELU activation and merge size 2 bridges visual features to the language embedding space (Section 2.1.2).
- Language model: `ERNIEâ€‘4.5â€‘0.3B`â€”a small LLM chosen to speed up autoregressive decoding; enriched with `3Dâ€‘RoPE` positional encoding. 3Dâ€‘RoPE extends rotary positional embeddings to capture multiple axes (e.g., sequence and modality), improving alignment of visual and textual token positions (Section 2.1.2).
- Outputs: for each element type, the model emits structured text:
  - OCR: characters/words/lines/blocks; simple page structure hints.
  - Tables: an `OTSL` representation. OTSL (Optimized Table Tokenization) is an efficient, tokenâ€‘friendly table serialization format (Section 2.2.2, Table 1; [28]).
  - Formulas: LaTeX with distinctions between inline `\(...\)` and display `\[...\]`.
  - Charts: normalized Markdown tables (Section 2.2.2).

3) Training the VLM (Section 2.2.2; Table 1)
- Stage 1 (alignment pretraining): 29M imageâ€‘text pairs; 1 epoch; batch 128; sequence length 16384; max resolution 1280Ã—28Ã—28; data augmentation on; LR decays from 5eâ€‘5 to 5eâ€‘6. Goal: align vision features with language space across diverse content.
- Stage 2 (instruction fineâ€‘tuning): 2.7M curated samples; 2 epochs; same batch and context length; higher max resolution (2048Ã—28Ã—28); smaller LR (5eâ€‘6 â†’ 5eâ€‘7). Tasks explicitly cover OCR, tables (OTSL), formulas (LaTeX), charts (Markdown table).

4) Data construction (Section 3; Figure 5; Appendix A)
- Sources (Section 3.1):
  - Open datasets (e.g., CASIAâ€‘HWDB for handwriting [29]; UniMERâ€‘1M & MathWriting for formulas [30, 31]; wide chart corpora including ChartQA/PlotQA/DVQA/Unichart/etc. [32â€“40]).
  - Synthesized data to fix longâ€‘tail gaps and balance distributions.
  - â€œNetwork accessibleâ€ web data (papers, newspapers, scans, slides, exams) to diversify style/quality.
  - Inâ€‘house datasets from prior OCR research.
- Automatic annotation (Section 3.2):
  - Use `PPâ€‘StructureV3` expert models to produce pseudo labels, then craft prompts for strong VLMs (`ERNIEâ€‘4.5â€‘VL`, `Qwen2.5â€‘VL`) to refine them; apply hallucination filtering and rule checks.
- Hard case mining (Section 3.3):
  - Build an evaluation engine with fineâ€‘grained categories across text, tables, formulas, charts and measure with taskâ€‘specific metrics: Edit Distance (text), `TEDS` (tables), `CDM` (formulas), `RMSâ€‘F1` (charts).
    - `TEDS` (Tree Edit Distanceâ€‘based Similarity) compares predicted vs. groundâ€‘truth table trees.
    - `CDM` (Character Detection Matching) matches rendered character positions for formulasâ€”robust to LaTeX surface variations (Section 4.2.3; [64]).
    - `RMSâ€‘F1` summarizes table reconstruction from charts (Section 4.2.4; [42]).
  - Identify weaknesses and synthesize targeted â€œhardâ€ cases using font/CSS libraries, XeLaTeX, and browsers.

5) Inference system (Section 4.3)
- Multiâ€‘threaded asynchronous pipeline with three threads: data loading (PDF â†’ images), layout model, VLM; queues connect stages to overlap computation. VLM batches are formed either by size or waitâ€‘time, allowing crossâ€‘page batching for higher GPU utilization.
- Deployed on highâ€‘throughput backends (`vLLM`, `SGLang`), tuning batch tokens and memory utilization (Section 4.3; Table 13; Table A2).

## 4. Key Insights and Innovations
- Decoupled, geometryâ€‘aware reading order (fundamental)
  - Instead of relying on a VLM to â€œwrite outâ€ layout sequences, `PPâ€‘DocLayoutV2` first detects regions and then orders them via a pointer network with geometric bias (Section 2.1.1; Figure 3).
  - Significance: removes longâ€‘sequence layout generation, improving stability and speed on complex multiâ€‘column pages and graphicsâ€‘text mixtures (Section 2.1.1). Table 2 shows top readingâ€‘order accuracy (Edit 0.043 on OmniDocBench v1.5).
- Native dynamicâ€‘resolution vision encoder in a compact VLM (fundamental)
  - A `NaViT`â€‘style encoder processes arbitraryâ€‘resolution inputs without tiling; coupled with a small `ERNIEâ€‘4.5â€‘0.3B` decoder for fast autoregressive generation (Section 2.1.2; Figure 4).
  - Significance: fewer hallucinations, better dense text performance, strong multilingual coverage (109 languages; Appendix B), with lower compute (Tables 2, 3, 12; and Table 13 for speed).
- LLMâ€‘assisted, qualityâ€‘controlled data pipeline (incremental but impactful)
  - Combining expert pseudo labels, strong VLM refiners, ruleâ€‘based validation, and targeted hardâ€‘case synthesis yields 30M+ highâ€‘quality training pairs across elements (Section 3; Appendix A).
  - Significance: scales highâ€‘fidelity labels while mitigating hallucinations; drives SOTA elementâ€‘level accuracy (Tables 5, 8, 10, 12).
- Highâ€‘throughput, crossâ€‘page batching inference (incremental)
  - Asynchronous, multiâ€‘threaded queues and microâ€‘batching across documents on `vLLM/SGLang` deliver best pages/s and tokens/s among compared systems on A100 (Table 13).

## 5. Experimental Analysis
Evaluation setup spans pageâ€‘level and elementâ€‘level tasks with public and inâ€‘house sets (Section 4).

- Datasets and metrics
  - Pageâ€‘level: OmniDocBench v1.5 and v1.0 (weighted combinations of text Edit Distance, formula CDM, table TEDS; includes readingâ€‘order Edit Distance), and olmOCRâ€‘Bench unit tests (pass rates) (Sections 4.1; Tables 2â€“4).
  - Elementâ€‘level: text (OmniDocBenchâ€‘OCRâ€‘block, Inâ€‘houseâ€‘OCR, Oceanâ€‘OCRâ€‘Bench), tables (OmniDocBenchâ€‘Tableâ€‘block, Inâ€‘houseâ€‘Table), formulas (OmniDocBenchâ€‘Formulaâ€‘block, Inâ€‘houseâ€‘Formula), charts (Inâ€‘houseâ€‘Chart). Metrics: Edit Distance, `TEDS`, `CDM`, `RMSâ€‘F1` (Section 4.2; Tables 5â€“12).

- Main quantitative results
  - Pageâ€‘level
    - OmniDocBench v1.5 (Table 2): 
      > Overall score `92.56` (best), Textâ€‘Edit `0.035` (lower is better), Formulaâ€‘CDM `91.43`, Tableâ€‘TEDS `89.76`, Tableâ€‘TEDSâ€‘S `93.52`, Readingâ€‘order Edit `0.043`.  
      Next best overall is MinerU2.5 at `90.67`.
    - OmniDocBench v1.0 (Table 3):
      > Avg overall edit `0.115` (lower is better). Text Edit: `0.041` EN, `0.062` ZH. Reading order: `0.045` EN (nearâ€‘SOTA), `0.063` ZH (best).  
      Table TEDS: `88.0` EN (slightly below SOTA; the paper attributes this to annotation typos in v1.0), `92.14` ZH (strong).
    - olmOCRâ€‘Bench (Table 4):
      > Overall pass rate `80.0 Â± 1.0` (best). Category highlights: ArXiv `85.7` (best), Headers&Footers `97.0` (best), Multiâ€‘column text `79.9` (2nd), Long Tiny Text `85.7` (2nd).  
      Stronger than dots.ocr (`79.1`), MinerU2.5 (`77.5`), and MonkeyOCRâ€‘proâ€‘3B (`75.8`).
  - Elementâ€‘level
    - Text
      - OmniDocBenchâ€‘OCRâ€‘block (Table 5): best or tiedâ€‘best Edit Distance in all nine document types; e.g., PPT2PDF `0.049`, Academic literature `0.021`, Newspaper `0.034`.
      - Inâ€‘houseâ€‘OCR (Table 6): multilingual Edit Distance best in all reported scripts, e.g., Arabic `0.122` vs Qwen2.5â€‘VLâ€‘72B `0.405`; Japanese `0.086`; Latin `0.013`. Across text types: Handwritten CN `0.089`, Printed EN `0.016`, Vertical text `0.005`, Rare characters `0.001`.
      - Oceanâ€‘OCRâ€‘Bench (Table 7): 
        > EN Edit `0.118` (best); ZH Edit `0.034` (best) with highest F1/Precision/Recall/BLEU/METEOR in both EN and ZH.
    - Tables
      - OmniDocBenchâ€‘Tableâ€‘block (Table 8): 
        > Overall `TEDS 0.9195` (best), Structural `TEDS 0.9543` (best), Overall Edit Dist `0.0561` (best).
      - Inâ€‘houseâ€‘Table (Table 9): 
        > Overall `TEDS 0.8699` and Structural `0.9066` (both best).
    - Formulas
      - OmniDocBenchâ€‘Formulaâ€‘block (Table 10): 
        > Overall `CDM 0.9453` (best), with EN `0.9677`, ZH `0.9228`.  
        Note: dots.ocr scores are low because cropped formulas are often treated as images (table note).
      - Inâ€‘houseâ€‘Formula (Table 11): 
        > Overall `CDM 0.9882` (best).
    - Charts
      - Inâ€‘houseâ€‘Chart (Table 12): 
        > Overall `RMSâ€‘F1 0.844` (best), surpassing `PPâ€‘StructureV3` (`0.806`) and Qwen2.5â€‘VLâ€‘72B (`0.730`).
  - Inference efficiency (Table 13; Section 4.3)
    - On OmniDocBench v1.0 endâ€‘toâ€‘end, with vLLM backend:
      > `1.2241 pages/s` and `1881.2 tokens/s` (best), using `43.7 GB` VRAM.  
      MinerU2.5: `1.0574 pages/s`, `1647.9 tokens/s`, `41.9 GB`; dots.ocr: `0.3522 pages/s`, `78.5 GB`.
    - Crossâ€‘hardware stability (Table A2) on A100, A10, RTX 3060/4090D shows consistent speedâ€‘memory tradeâ€‘offs.

- Do the experiments support the claims?
  - Yes, across three independent pageâ€‘level benchmarks (Tables 2â€“4) and multiple elementâ€‘level tasks (Tables 5â€“12), the system consistently reaches SOTA or nearâ€‘SOTA with concrete gains. The speed study (Table 13) substantiates efficiency claims. The only caveat is that several elementâ€‘level benchmarks are inâ€‘house (tables, formulas, charts), so external reproducibility depends on future releases.

- Ablations and robustness
  - The paper details training procedures and data construction but does not present ablations isolating the impact of NaViT vs. fixedâ€‘resolution encoders, or pointerâ€‘network design choices (no ablation tables reported). Robustness is indirectly evidenced by categoryâ€‘wise results (e.g., handwriting, vertical text, multilingual; Tables 5â€“7).

## 6. Limitations and Trade-offs
- Dependence on accurate detection
  - The twoâ€‘stage pipeline hinges on Stageâ€‘1 detection quality; missed or misâ€‘classified boxes propagate to recognition. Perâ€‘class thresholding for proposal selection (Section 2.1.1) can trade recall for precision.
- Pairwise ordering complexity
  - The pointer network computes an NÃ—N pairwise matrix; while page N is moderate, this is O(NÂ²) and might stress very dense pages (Section 2.1.1).
- Specialized scope
  - The VLM is tailored for document parsing. It is not evaluated on general multimodal reasoning or openâ€‘ended VQA; capabilities beyond text/tables/formulas/charts are out of scope (Sections 2.2.2, 4.2).
- Data and benchmarking constraints
  - Heavy reliance on automatic labels and synthesis (Section 3; Appendix A) can introduce biases from the teacher models and rendering engines. Several strong results are on inâ€‘house datasets (Tables 9, 11, 12), limiting external verification until those sets or equivalents are public.
- Compute requirements, while efficient, are nonâ€‘trivial
  - Best throughput is reported on A100 with ~44 GB average VRAM (Table 13). Consumer GPUs (e.g., RTX 3060) work (Table A2) but at significantly lower throughput (~0.35 pages/s).
- Chart benchmarking
  - The chart evaluation is only inâ€‘house due to issues with public test quality and imbalance (Section 4.2.4), so crossâ€‘paper comparability is limited.

## 7. Implications and Future Directions
- Field impact
  - This work demonstrates that a compact, taskâ€‘specialized VLM with native dynamicâ€‘resolution vision and a dedicated layout frontâ€‘end can outperform much larger general VLMs on document parsing while being faster (Tables 2â€“4, 13). It challenges the assumption that bigger endâ€‘toâ€‘end models are required for highâ€‘fidelity document conversion.
- Practical applications
  - Highâ€‘throughput enterprise ingestion of PDFs into structured Markdown/JSON; multilingual digitization of archives and newspapers; ingestion of scientific literature and financial reports; robust OCR for complex layouts and handwriting; improved RAG pipelines thanks to accurate reading order and structural parsing (Section 1; Demos in Appendix D).
- Research directions
  - Public release of the inâ€‘house datasets or creation of standardized chart/table/formula testbeds with reliable annotations to improve comparability (Section 4.2.4).
  - Ablations on NaViT vs. alternatives, projector designs, and pointer network components (geometric bias, decoding strategies) to quantify each contribution.
  - Joint training of layout and recognition with shared features while preserving the current pipelineâ€™s stabilityâ€”e.g., using the VLM to refine detection/ordering proposals from `PPâ€‘DocLayoutV2`.
  - Further compression and distillation for edge deployment; mixedâ€‘precision and sparse attention to reduce A100â€‘class dependence (Table A2 suggests feasibility).
  - Extending chart understanding from table reconstruction to codeâ€‘level generation (e.g., matplotlib/seaborn programs) and semantic captioning, leveraging the data synthesis pipeline (Appendix A.4).
  - Documentâ€‘level reasoning across pages (crossâ€‘page reading order, footnoteâ€“reference linking, figureâ€“caption alignment) by augmenting the pointer network and VLM context.

Overall, PaddleOCRâ€‘VL offers a clear, wellâ€‘engineered path to accurate and efficient document parsing at scale: decouple layout from recognition, use a nativeâ€‘resolution vision backbone, keep the decoder small for speed, and invest in strong data construction and inference engineering. The reported results and speed (Tables 2â€“4, 13) indicate that this approach currently sets the standard for practical, multilingual document conversion.

# Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving

**ArXiv:** [2407.00079](https://arxiv.org/abs/2407.00079)

## ğŸ¯ Pitch

Mooncake introduces a pioneering disaggregated architecture for large language model (LLM) serving, centered on distributed KVCache management and a global scheduler that intelligently coordinates prefill and decoding phases. By maximizing KVCache reuse, implementing chunked pipeline prefill, and leveraging prediction-based early rejection policies, Mooncake significantly boosts throughputâ€”up to 525% in simulations and 75% more real-world requestsâ€”while reliably meeting strict latency SLOs in highly overloaded conditions. This approach addresses the critical challenge of scalable, efficient LLM serving under resource constraints, making advanced models more accessible and responsive for real-world applications.

---

## 1. Executive Summary (2-3 sentences)
Mooncake introduces a KVCacheâ€‘centric, disaggregated serving architecture for large language models (LLMs) that separates the â€œprefillâ€ phase (processing the input prompt) from the â€œdecodingâ€ phase (autoregressive generation), and treats cache scheduling as the core optimization problem. By combining a distributed KVCache, a global scheduler (â€œConductorâ€), chunked pipeline prefill, and a predictionâ€‘based earlyâ€‘rejection policy, Mooncake improves throughput under strict latency SLOs and overloaded conditions, reporting up to 525% higher throughput in longâ€‘context simulations and 75% more served requests on real workloads while meeting SLOs (Abstract; Â§8.1.2; Â§8.1.3).

## 2. Context and Motivation
- The specific gap addressed
  - LLM serving has two very different phases: a computationâ€‘heavy prefill stage and a memoryâ€‘bound decoding stage. Service providers must maximize goodput (throughput that meets SLOs) while ensuring two latency SLOs: time to first token (`TTFT`) and time between tokens (`TBT`) (Â§2).
  - Most prior work assumes sufficient capacity and focuses on utilization; in reality, providers often run in overloaded conditions where GPUs are scarce. In overload, naÃ¯vely accepting all requests wastes resources (e.g., when a request completes prefill but is later rejected for decoding) (Â§1.1; Â§7.1â€“Â§7.2).
  - KVCache reuse (reusing attention keys/values computed for a shared prefix) can save compute but makes scheduling harder: remote cache reuse raises `TTFT`, larger decoding batches lower `TBT` performance (Â§1.1, Fig. 2).

- Why this matters
  - Revenue and user experience hinge on serving more requests within SLOs. Longâ€‘context workloads (e.g., 16kâ€“128k token prompts) make prefill extremely expensive while decoding is memoryâ€‘bounded, so poorly coordinated scheduling can severely violate either `TTFT` or `TBT` (Â§2; Fig. 2).

- Prior approaches and shortcomings
  - Coupled serving (e.g., vLLM) shares resources between prefill and decoding, so long prefill jobs can disrupt decoding latency (Â§8 Baseline; Â§8.1.2).
  - Disaggregation (e.g., Splitwise, DistServe, TetriInfer) recognizes prefill/decoding differences, but:
    - They do not center the global scheduler on KVCache placement/migration; cache hotspots and remote fetch congestion are underexplored (Â§6.2).
    - Overloadâ€‘specific policies (predictive early rejection to avoid wasted prefill) and the resulting loadâ€‘fluctuation problem are not addressed (Â§7).
    - Longâ€‘context acceleration via sequence parallelism (SP) demands frequent crossâ€‘node communication and complex elastic resizing; it competes with KVCache transfers (Â§5.1).

- Positioning
  - Mooncake disaggregates prefill and decoding, builds a distributed KVCache using underutilized CPU/DRAM/SSD with RDMA transfer (Fig. 1, Fig. 3), and makes KVCache placement the firstâ€‘class scheduling target (Algorithm 1; Â§6). It also introduces chunked pipeline prefill for long contexts (Â§5.1) and predictionâ€‘based early rejection that stabilizes load in overload (Â§7.4; Fig. 10b).

Definitions used once then assumed:
- `KVCache`: the perâ€‘layer attention keys/values saved during prefill and extended during decoding; reusing it avoids recomputing shared prefixes.
- `Prefill`: oneâ€‘shot parallel processing of the entire input prompt to produce the first token and populate KVCache.
- `Decoding`: autoregressive generation, one token at a time per sequence.
- `TTFT`: latency from request arrival to first token.
- `TBT`: latency between consecutive tokens for a request.
- `Disaggregated architecture`: separating prefill and decoding into distinct node pools.
- `MFU` (Model FLOPs Utilization): how much of theoretical compute is realized.

## 3. Technical Approach
Mooncakeâ€™s core is a KVCacheâ€‘centric, disaggregated system with a global scheduler (â€œConductorâ€) that jointly chooses where to prefill, where to decode, and where to place/migrate cache (Fig. 1; Â§3; Algorithm 1).

Stepâ€‘byâ€‘step architecture and workflow (Fig. 4; Â§3):
1) KVCache reuse on prefill
   - Requests are tokenized and split into fixedâ€‘size blocks (512 tokens used in the trace; Â§4.1).
   - A prefixâ€‘hash is computed per block that chains all prior blocksâ€™ hashes, enabling deduplication of any prefix (Fig. 3; â€œA=Hash(a) â€¦ E=Hash(D+e)â€).
   - Conductor selects a prefill node considering:
     - Prefix match length available locally vs. remotely (more reuse = less compute),
     - Current queueing load (shorter wait = lower `TTFT`),
     - DRAM availability (cache residency constraint; Fig. 1; Â§6.1).
   - If beneficial and under a threshold, it preâ€‘migrates hot blocks to reduce future remote fetches (Â§6.2).

2) Incremental prefill (possibly chunked)
   - If uncached prompt length exceeds `prefill_chunk` (typically >1k tokens), the prefill splits the prompt into chunks that run in a pipeline across multiple nodes (Â§5.1, â€œchunked pipeline parallelism/CPPâ€). This reduces `TTFT` for long contexts by parallelizing different prompt chunks across nodes, with minimal perâ€‘layer communication (only at pipeline boundaries).

3) Layerâ€‘wise KVCache transfer
   - A dedicated RDMA service (â€œMessengerâ€) asynchronously streams each layerâ€™s KVCache to the chosen decoding nodeâ€™s CPU DRAM as soon as itâ€™s produced (Â§3; Fig. 4 â€œLayerâ€‘wise Load and Storeâ€; Â§5.2). This overlap hides transfer latency and reduces GPU VRAM residency during prefill (Fig. 7).

4) Decoding with continuous batching
   - After the full KVCache lands in the decoding nodeâ€™s CPU DRAM, it is loaded to GPU memory and the request joins the next decoding iteration (Â§3; Â§2). A local scheduler doubleâ€‘checks `TBT` SLO given the most recent load; if violated, it rejects late, wasting any prefill workâ€”this motivates early rejection (Â§3 step 4; Â§7).

Key design choices and why:
- Disaggregate prefill vs. decoding (Fig. 1)
  - Different objectives and constraints: maximize cache reuse and meet `TTFT` in prefill (DRAM bound), maximize throughput and meet `TBT` in decoding (VRAM bound). Coupling them forces tradeâ€‘offs and interference (Fig. 2; Â§2).
- KVCacheâ€‘centric scheduling (Algorithm 1; Â§6.1â€“Â§6.2)
  - Scheduling minimizes `TTFT` considering prefix hit length and queue time, and triggers cache replication/hotspot migration to avoid remote fetch congestion and to balance cache locality across prefill nodes (Â§6.2, Fig. 8).
- Chunked pipeline prefill (CPP) instead of sequence parallelism (SP)
  - SP reduces compute per node but still requires perâ€‘layer crossâ€‘node communication (ring/striped attention), degrades MFU, and needs complex elastic resizing (Â§5.1). CPP pipelines chunks across nodes with only stageâ€‘boundary transfers, overlapping comm/compute, and naturally fits both short and long prompts without dynamic reconfiguration (Â§5.1).
- Layerâ€‘wise prefill with async load/store
  - Overlaps perâ€‘layer KVCache writes/reads with compute so prefill latency is close to the max of compute or KV transfer time; reduces VRAM residency during prefill and lets prefill scheduling ignore VRAM as long as a single request fits (Fig. 4; Fig. 7; Â§5.2).
- Predictionâ€‘based early rejection
  - Rejects requests at admission if future decoding load will breach `TBT`, preventing wasted prefill and stabilizing disaggregated load (Fig. 9, Fig. 10b; Â§7.2â€“Â§7.4).

Algorithmic details (Algorithm 1; Â§6.1):
- For each request, compute `block_keys = PrefixHash(tokens, B)` and find the best prefix match across prefill nodes.
- Estimate `TTFT = Ttransfer + Tqueue + Tprefill`, where:
  - `Ttransfer` depends on remote block length and instantaneous network congestion (Â§6.1),
  - `Tqueue` is sum of queued prefill times on the instance,
  - `Tprefill` is predicted from offline profiling by request length and matched prefix (Â§6.1).
- Choose the prefill node with minimal predicted `TTFT` that meets the `TTFT` SLO.
- Independently select a decoding node with a predicted `TBT` that meets the `TBT` SLO.
- If either SLO would be violated, reject early; otherwise, proceed and optionally replicate hotspot cache blocks from the bestâ€‘match holder if the â€œbalancing thresholdâ€ indicates consolidation is needed (Â§6.2, footnote on the manually tuned threshold).

Overload scheduling and the fluctuation fix (Â§7):
- Problem: Early rejection based on current decoding load causes antiâ€‘phase oscillations between prefill and decoding loads due to prefillâ†’decode lag (Fig. 9; Fig. 10a).
- Fix: Predict decoding load at the time the request would arrive for decoding, using a systemâ€‘level model that assumes a uniform decoding time `t_d` per request and updates the predicted batch/TBT status accordingly. Admit only if predicted `TBT` meets SLO (Fig. 10b; Â§7.4).

KVCache system and APIs (Â§3â€“Â§4):
- KVCache is paged and stored in CPU DRAM/SSD pools with eviction (LRU/LFU/LengthAware; Table 1).
- Prefixâ€‘hash chaining enables deduplication and safe sharing of any prefix across sessions (Fig. 3).
- Messenger uses GPUDirect RDMA to move cache blocks between nodes asynchronously (Fig. 4).
- The system exposes a prefix cache API to external users for higher reuse (Â§3).

## 4. Key Insights and Innovations
- KVCacheâ€‘centric global scheduling as the firstâ€‘class objective
  - Whatâ€™s new: Request placement and cache placement are jointly optimized, including onâ€‘theâ€‘fly cache replication/migration to reduce remote fetches and network hotspots (Â§6.2).
  - Why it matters: Directly targets the core compute savings lever (prefix reuse) while respecting `TTFT` and `TBT` SLOs; reduces network congestion and rebalances cache locality (Fig. 8).

- Disaggregated prefill/decoding with a distributed KVCache
  - Whatâ€™s new: Treats CPU/DRAM/SSD attached to GPU nodes as an RDMAâ€‘connected, nearâ€‘GPU cache pool; streams KVCache layerâ€‘wise to decoders (Â§3; Fig. 1, Fig. 4).
  - Why it matters: Enables large, lowâ€‘cost cache capacity and highâ€‘bandwidth transfers without extra hardware; supports nearâ€‘GPU prefix caching while freeing VRAM pressure (Â§3; Â§5.2).

- Chunked pipeline prefill (CPP) for long contexts in inference
  - Whatâ€™s new: Applies pipeline parallelism to the prefill phase of inference (distinct from training), splitting long prompts across nodes with low comms overhead (Â§5.1).
  - Why it matters: Cuts `TTFT` for very long prompts while avoiding SPâ€™s frequent perâ€‘layer communication and complex elasticity; reduces network contention with KV transfers (Â§5.1).

- Predictionâ€‘based early rejection to stabilize disaggregated overload
  - Whatâ€™s new: An admission policy that predicts future decoding load (systemâ€‘level) at the time a prefilling request would arrive to decode; it avoids antiâ€‘phase load oscillations inherent to disaggregated early rejection (Â§7.3â€“Â§7.4; Fig. 10b).
  - Why it matters: Saves wasted prefill compute when decoding will be overloaded, and increases effective capacity under overload (Table 3).

- Layerâ€‘wise KV load/store overlap during prefill
  - Whatâ€™s new: Preload the next layerâ€™s KV and asynchronously store the current layerâ€™s KV, overlapping with compute (Â§5.2).
  - Why it matters: Reduces endâ€‘toâ€‘end prefill latency overhead from KV I/O and allows prefill scheduling to largely ignore VRAM constraints for singleâ€‘request capacity (Fig. 7).

## 5. Experimental Analysis
- Evaluation methodology
  - Testbed: Multiâ€‘node cluster, each node with 8Ã— NVIDIA A800 80GB (NVLINK), 800 Gbps RDMA; nodes run either prefill or decoding (Â§8 Testbed).
  - Datasets and workloads (Table 2; Â§8.1):
    - Public: ArXiv Summarization (avg input 8,088; output 229; ~0% cache reuse), Lâ€‘Eval (avg input 19,019; output 72; >80% cache reuse).
    - Simulated: prompts of 16k/32k/64k/128k with 512â€‘token outputs, ~50% cache reuse.
    - Real trace: 23k requests sampled from production with timestamps, input/output lengths, and prefixâ€‘hash IDs (Â§4; Fig. 5 shows length distributions).
  - Metrics and SLOs: P90 `TTFT` and `TBT` normalized to SLO limits. In endâ€‘toâ€‘end tests, `TTFT_P90 = 10Ã—` and `TBT_P90 = 5Ã—` of the singleâ€‘request baseline; realâ€‘trace replay uses absolute caps of 30 s TTFT and 0.1 s/token TBT (Â§2; Â§8 Metric).
  - Baseline: vLLM with continuous batching and PagedAttention; coupled prefill/decoding (Â§8 Baseline).

- Main results
  - Public datasets (Fig. 11):
    - With 4 nodes total, Mooncake [3 prefill + 1 decode] outperforms vLLM [4 monolithic] by ~20% (ArXiv) and ~40% (Lâ€‘Eval) in achievable request rate while meeting both SLOs; [2P+2D] yields better `TBT` but worse `TTFT` due to prefill/decoding imbalance.
  - Longâ€‘context simulated data (Fig. 12):
    - Mooncake sustains batching and SLOs while vLLM must fall back to singleâ€‘request processing to protect `TBT`. Reported throughput gains range from 50% up to 525% as prompt length grows to 128k, reflecting the value of disaggregation and cache reuse.
  - Real production trace (Fig. 13):
    - With Mooncake [10P+10D] vs vLLM [20 mixed], both show nearly 100% SLO satisfaction for `TTFT`. For `TBT`, Mooncake satisfies ~100% of requests while vLLM satisfies ~57%. Under these SLOs, Mooncake processes ~75% more requests.
  - Overload policies (Table 3; Â§8.2):
    - Replaying the real trace at 2Ã— speed on an 8P+8D cluster, the number of rejected requests drops from 4,183 (baseline late rejection) to 3,771 (early rejection) and further to 3,589 with predictionâ€‘based early rejectionâ€”evidence that predictive admission increases effective capacity while stabilizing load (Â§7.4, Fig. 10b).
  - Cache analysis (Table 1; Fig. 6; Â§4.2):
    - In the sample trace, moving from 1k to 50k cache blocks raises hit ratio from ~30% to ~50% (LRU best). Popularity is highly skewed: >50% of blocks never hit while some hit tens of thousands of times (Fig. 6), justifying hotspot replication (Â§6.2).

- Support for claims
  - The results directly align with the design goals:
    - Disaggregation plus KVCacheâ€‘centric scheduling protects `TBT` under long contexts (Fig. 12) while maintaining `TTFT` via cacheâ€‘aware prefill placement (Fig. 11, Fig. 8).
    - Predictionâ€‘based early rejection reduces wasted prefill and stabilizes disaggregated load (Table 3; Fig. 10).
  - Caveats:
    - The endâ€‘toâ€‘end experiments use a â€œdummy modelâ€ with LLaMA2â€‘70Bâ€‘like architecture and replayed traces, not proprietary models or content (Abstract; Â§1.2; Â§8). This aids reproducibility but may limit generality across architectures and workloads.

- Ablations and robustness
  - Scheduling ablation (Fig. 8): Compared random selection, basic loadâ€‘balancing, cacheâ€‘aware, and full KVCacheâ€‘centric strategies on 8P+8D and 23k real requests; the latter markedly lowers average `TTFT` and improves SLO attainment.
  - Cache policy ablation (Table 1): Compares LRU, LFU, and LengthAware; LRU performs best on this trace.
  - Load fluctuation analysis (Fig. 9, Fig. 10): Demonstrates the oscillation induced by naÃ¯ve early rejection and how prediction mitigates it.

## 6. Limitations and Trade-offs
- Assumptions and model choices
  - Predictionâ€‘based early rejection currently uses a systemâ€‘level model assuming uniform decoding time `t_d` per request (Â§7.4). This is coarse; requestâ€‘level output length prediction is left as future work.
  - The KVCache hotspot balancing uses a manually tuned threshold (`kvcache_balancing_threshold`) to decide between transfer vs. recompute; it is not yet fully adaptive (Â§6.2, footnote).
- Scenarios not fully addressed
  - Workloads with extremely low prefix reuse benefit less from KVCacheâ€‘centric scheduling; the design still helps via disaggregation but cache replication gains diminish (Table 1 â€œ~0% cache ratioâ€ for ArXiv).
  - Multiâ€‘tenant priority policies and mixed SLO classes are future work (Â§10).
- System complexity and resource demands
  - Requires RDMA networking and tight orchestration across prefill/decoding pools plus a distributed cache; network contention between SPâ€‘like schemes and cache transfers motivated CPP, but largeâ€‘scale deployments still need careful bandwidth management (Â§5.1; Â§6.1).
  - Layerâ€‘wise overlap reduces but does not eliminate KV I/O costs; misâ€‘sized chunks or skewed workloads can still expose transfer bottlenecks (Fig. 7; Â§5.2).
- External validity
  - Results are on a highâ€‘end cluster (A800s, 800 Gbps) and a dummy model; performance portability to other hardware stacks (e.g., PCIeâ€‘only, lower bandwidth) and nonâ€‘LLaMA architectures may vary (Â§8 Testbed).
- Operational tradeâ€‘offs
  - Disaggregation mandates capacity planning for prefill vs decoding pools; misallocation can hurt `TTFT` or `TBT` (Fig. 11 shows [2P+2D] vs [3P+1D] tradeâ€‘off).
  - Aggressive early rejection improves goodput but can reduce perceived availability; admission control needs productâ€‘level tuning.

## 7. Implications and Future Directions
- How this changes the landscape
  - Treating KVCache as the scheduling center of gravityâ€”and building the serving system around cache placement, migration, and reuseâ€”shifts LLMâ€‘serving design from monolithic utilization tuning to dataâ€‘centric, disaggregated orchestration. This is especially impactful for longâ€‘context workloads where prefill dominates compute and decoding is memoryâ€‘bound (Fig. 2; Â§2).
- Followâ€‘up research enabled
  - Smarter prediction: requestâ€‘level outputâ€‘length predictors (learned or retrievalâ€‘assisted) to refine admission decisions (Â§7.4).
  - Adaptive cache governance: online learning to tune replication thresholds, eviction policies for partial hits vs expiration, and congestionâ€‘aware placement (Â§6.2; Â§10).
  - Heterogeneous accelerators and operator disaggregation: offload attention (memoryâ€‘bound) to bandwidthâ€‘optimized devices while keeping MLPs on computeâ€‘optimized GPUs; early simulation suggests promising throughput gains (Â§10).
  - KVCache reduction methods: compression/quantization, salientâ€‘token selection, layer sharing, or hybrid architectures (surveyed in Â§10) compound with Mooncake by increasing batch sizes and cache hit rates.
- Practical applications
  - Production LLM APIs with strict latency SLOs and bursty, longâ€‘context traffic (chat with large documents, code assistants).
  - Costâ€‘effective batch APIs: Mooncakeâ€™s layerâ€‘wise overlap and freed VRAM on prefill nodes can absorb lowâ€‘priority batch decoding where `TBT` is relaxed (Â§5.2).
  - Contextâ€‘caching as a product: the exposed prefix cache API allows enterprise workflows to seed reusable context across sessions (Â§3).

> Headline results to remember:
> - â€œUp to a 525% increase in throughput in simulated longâ€‘context scenarios while meeting SLOsâ€ (Fig. 12; Â§8.1.2).
> - â€œ~75% more requests processed under real traces with `TTBT` SLO met for nearly 100% of requests vs ~57% on vLLMâ€ (Fig. 13; Â§8.1.3).
> - â€œPredictive early rejection reduces overload rejections from 4,183 to 3,589 in 2Ã— trace replayâ€ (Table 3; Â§8.2).

Overall, Mooncake demonstrates that a KVCacheâ€‘first, disaggregated designâ€”augmented with pipeline prefill, cacheâ€‘aware scheduling, and predictive admissionâ€”can deliver substantial, measurable gains for longâ€‘context LLM serving under realistic SLOs and overloaded conditions.

# A Comprehensive Survey on Long Context Language Modeling

**ArXiv:** [2503.17407](https://arxiv.org/abs/2503.17407)
**Authors:** Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, Zhaoxiang Zhang
**Institutions:** 

## ğŸ¯ Pitch

This paper offers a comprehensive survey of Long-Context Language Models (LCLMs), addressing the challenge of processing extremely long sequences beyond the typical limits of modern LLMs. By systematizing the entire fieldâ€”from data strategies to infrastructure, evaluation, and practical applicationsâ€”it empowers researchers and practitioners to design, evaluate, and deploy efficient LCLMs, crucial for advancing AI's capabilities in real-world tasks involving lengthy documents and complex multi-step reasoning.

---

## 1. Executive Summary (2â€“3 sentences)
This paper is a comprehensive survey of Longâ€‘Context Language Models (`LCLMs`)â€”models designed to read, remember, reason over, and generate very long sequences (from 128K to multiâ€‘million tokens). It systematizes the field endâ€‘toâ€‘end: data pipelines, architectures, workflowâ€‘level methods (e.g., memory, RAG, agents), training/inference infrastructure, evaluation paradigms for both longâ€‘context comprehension and longâ€‘form generation, analyses of what actually works (and why), applications, and future directions (Figures 2, 6; Sections 2â€“9).

## 2. Context and Motivation
- Problem/gap addressed
  - Modern LLMs historically handled short contexts (e.g., 4K tokens); many real tasks (long documents, large code repos, long conversations, long videos) exceed these limits. Even when LCLMs advertise very long â€œsupportedâ€ windows, the â€œeffectiveâ€ window (where they truly use information) is much shorter (Section 7.1.1; Table 9).
  - The field lacks unified guidance that spans: how to build LCLMs (data + architectures), how to train/serve them efficiently (infrastructure), how to evaluate them credibly (benchmarks + metrics), and where/why they succeed or fail (analysis). Table 1 shows prior surveys typically covered only a subset (e.g., architectures or evaluation), whereas this work covers all six pillars.

- Why it matters
  - Realâ€‘world impact: Long context unlocks â€œtestâ€‘time scalingâ€ (o1â€‘like long reasoning), multiâ€‘document RAG with minimal retrieval steps, repositoryâ€‘level coding assistants, longâ€‘term chat memory, and long video understanding (Introduction; Figure 1; Section 9.1).
  - Theoretical significance: It clarifies how positional encodings extrapolate, how attention sparsity and recurrence change scaling, and how perplexity relates (or not) to longâ€‘context performance (Sections 3.1â€“3.2, 7.1.2).

- Prior approaches and shortcomings
  - Architectures: Many propose longer context via position encodings (e.g., `RoPE` tricks like `PI`, `NTK`, `YaRN`), sparse attention (Longformer), recurrent memory (Transformerâ€‘XL), or linearâ€‘time models (Mamba, RetNet). But results are scattered and their tradeâ€‘offs unclear (Sections 3.1â€“3.2).
  - Workflows: RAG, memory modules, and prompt compression help, yet there is no unifying view of when to choose which (Section 4).
  - Infrastructure: Training/inference at 100Kâ€“1M tokens is I/Oâ€‘ and memoryâ€‘bound; optimizations (quantization, FlashAttention, disaggregated serving) are hard to navigate (Section 5).
  - Evaluation: Synthetic benchmarks abound (needleâ€‘inâ€‘aâ€‘haystack and variants), but they measure only parts of the problem; longâ€‘form generation lacks reliable automatic metrics (Section 6).

- Positioning
  - The paper provides a single taxonomy and set of â€œrecipesâ€ that span the whole pipelineâ€”data (Sec. 2), architecture (Sec. 3; Figure 4), workflows (Sec. 4; Figure 6), infrastructure (Sec. 5), evaluation (Sec. 6; Figures 7â€“8; Tables 6â€“7), and analyses with concrete evidence such as the gap between supported vs. effective context (Sec. 7; Table 9).

## 3. Technical Approach
This is a survey; its â€œapproachâ€ is a carefully structured framework with stepâ€‘byâ€‘step descriptions of mechanisms you can apply. Below is the scaffold, aligned with Figures 2, 4, 5, 6.

### 3.1 Data strategies (Section 2; Figure 3; Table 2)
Goal: construct longâ€‘context pretraining and postâ€‘training data that actually contain longâ€‘range dependencies and tasks that force models to use them.

- Preâ€‘training
  - Data filtering (Sec. 2.1.1): score long texts for coherence, cohesion, complexity (`LongWanjuan`, Sec. 2.1.1) and for â€œlongâ€‘range dependencyâ€ via attention patterns (`LongAttn`, Sec. 2.1.1). Intuition: models trained on data that truly require crossâ€‘document linking learn to use long windows.
  - Data mixture (Sec. 2.1.2): oversample long items while maintaining domain diversity (`ProLong`) and progressively grow sequence length during training (`GrowLength`). This avoids overfitting to short contexts and improves retrieval over long inputs.
  - Data synthesis (Sec. 2.1.3): construct long examples by semantically clustering texts into one window, packing via structured strategies (`SPLICE`), or queryâ€‘centric aggregation (`Quest`).

- Postâ€‘training
  - Instruction/data filtering (Sec. 2.2.1): score longâ€‘context SFT samples with â€œcontextual awarenessâ€ and homologous modelsâ€™ agreement (`GATEAU`).
  - Instruction/data synthesis (Sec. 2.2.2): create tasks that force looking across farâ€‘apart segments (e.g., long multiâ€‘doc QA; â€œlostâ€‘inâ€‘theâ€‘middleâ€ targeting); multiâ€‘agent pipelines generate multiâ€‘hop questions (`MIMG`).
  - Preference optimization for long context: extend DPOâ€‘style alignment to long inputs/outputs (`LongReward`, `LOGO`, `LongDPO`).

Why these choices: Filtering and synthesis explicitly inject longâ€‘range dependencies and positionâ€‘agnostic cues, fixing the common failure mode where models ignore middle segments (Sec. 7.1.1).

### 3.2 Architecture (Section 3; Figures 4â€“5)
A map of mechanisms that make long contexts feasible.

- Position embeddings (Sec. 3.1)
  - Types (Sec. 3.1.1; Table 3):
    - Absolute (e.g., sinusoidal), Relative (e.g., `ALiBi`, `RoPE`), and Contentâ€‘Aware (`CoPE`, `DAPE`).
    - `RoPE` (rotary embeddings) rotates queries/keys by position; the score depends only on relative distance (Eq. (2)â€“(3) on p. 13). This is prevalent in LCLMs (LLaMA, Qwen).
  - Length extrapolation (Sec. 3.1.2; Table 4; Figure 10):
    - `Position reorganization`: reuse inâ€‘range positions by grouping or dilating indices (`SelfExtend`, `ReRoPE`).
    - `Position interpolation`: compress indexes to an inâ€‘range scale (`PI`: scale `nâ†’n/Î±`; `NTK`/`YaRN`: frequencyâ€‘aware scaling that preserves highâ€‘frequency components; Figure 10 shows waveâ€‘length differences).
    - `Hierarchical`: multiâ€‘level encodings for withinâ€‘segment and crossâ€‘segment distances (`BiPE`, `HiRoPE`).
    - `Position simulation`: train with â€œskippedâ€ or randomized positions so shortâ€‘window training simulates long ranges (`PoSE`, `CREAM`, `SkipAlign`).

- Attention/backbones (Sec. 3.2; Figure 5)
  - Transformerâ€‘based variants (Sec. 3.2.1)
    - Sparse attention: limit each tokenâ€™s receptive field (e.g., slidingâ€‘window in Longformer), or keep only â€œheavy hittersâ€ in cache (H2O, SnapKV). Headâ€‘level split between â€œretrievalâ€ and â€œnonâ€‘retrievalâ€ heads reduces memory (RazorAttention, DuoAttention).
    - Hierarchical attention: wordâ†’sentenceâ†’document aggregation (HAN; Hiâ€‘Transformer).
    - Recurrent transformers: carry compressed memory across segments (Transformerâ€‘XL; RMT; Infinite Attention).
    - KVâ€‘cache reductions: `MQA`/`GQA` share K/V across query heads; `MLA` compresses K/V into a latent space.
  - Linearâ€‘complexity models (Sec. 3.2.2)
    - `SSM/Mamba`: selective stateâ€‘space models update state per token in O(n); parameters become inputâ€‘dependent (Eq. (9) on p. 21).
    - Linear attention: kernelize attention to compute in O(n) (Linear Transformer, Performer) or chunkâ€‘wise recurrent paradigms (RetNet; Lightning Attentionâ€‘2).
    - `RWKV`: timeâ€‘mixing and channelâ€‘mixing layers that behave like parallelizable RNNs with linear decoding cost.
  - Hybrid architectures (Sec. 3.2.3)
    - Layerâ€‘wise hybrids: interleave linear and full attention layers (Jamba, Samba, RecurrentGemma, Minimaxâ€‘01). Empirically sweet spots around ~6â€“7 linear for 1 full layer are reported (Sec. 3.2.3).
    - Prefillâ€“decoding hybrid: use cheap linear attention to â€œprefillâ€ a global cache once, reuse it across layers that do full attention at decode (`YOCO`), or extremely compressed caches (`GoldFinch`).
    - Headâ€‘wise hybrids: split heads between fullâ€‘attention and SSM heads in the same layer (`Hymba`).

Why these choices: They directly target quadratic attention cost and KVâ€‘cache explosion while retaining recall ability across long ranges.

### 3.3 Workflowâ€‘level designs (Section 4; Figure 6)
Augment an (unchanged) LLM with systems that reduce or structure the long input.

- Prompt compression (Sec. 4.1)
  - Hard (tokenâ€‘level) compression: delete lowâ€‘information tokens using smaller LMsâ€™ perplexity or sentence encoders (LLMLingua, LongLLMLingua, AdaComp, CPC); or rewrite prompts (Nanoâ€‘Capsulator; CompAct).
  - Soft (embeddingâ€‘level) compression: learn compressed vectors fed directly as â€œvirtual tokensâ€ (ICAE; `xRAG`; UniICL; `Gist`/Activation Beacon). Some methods keep the LLM frozen; others finetune to produce and consume gists.

- Memoryâ€‘based methods (Sec. 4.2)
  - Language memory: store textual â€œmemoriesâ€ and retrieve them by recency/importance (Generative Agents; MemoryBank).
  - Continuous memory: store/retrieve intermediate KV pairs or learned memory tokens (LongMem; MemoryLLM).
  - Parametric memory: memorize document IDs or adapters inside the model (`DSI/DSI++`; Generative Adapter).

- RAGâ€‘based methods (Sec. 4.3)
  - Chunking: smarter splits (late chunking with longâ€‘context embedders; sliding windows; contextual augmentation).
  - Retrieval: dense, sparse, or hybrid retrievers; multiâ€‘step reasoning retrievers (REAPER).
  - Generation: concatenate passages; crossâ€‘attention fusion (FiD); decodeâ€‘time blending (kNNâ€‘LM); retrievalâ€‘aware decoders (Retro).

- Agentâ€‘based methods (Sec. 4.4)
  - Singleâ€‘agent readers/planners (ReadAgent, GraphReader, RecurrentGPT, PEARL).
  - Multiâ€‘agent systems that divide a long document and coordinate answers (CoA, LongAgent).

### 3.4 Infrastructure for training and inference (Section 5; Table 5)
Concrete systems techniques to make LCLMs practical.

- Training (Secs. 5.1.1â€“5.1.3)
  - I/O: data packing, dynamic windowing, prefetching/caching, and distributed file systems (3FS) reduce input bottlenecks.
  - GPU memory/compute: mixed and low precision (BF16, FP8, INT8), activationâ€‘outlier suppression (SmoothQuant, FPTQ), IOâ€‘aware attention kernels (FlashAttention v1â†’v3), blockâ€‘sparse attention (NSA, MoBA), and parallelism strategies (Sequence/Context/Ulysses).
  - Communication overlap: gradient accumulation tuned for ZeRO stage; multiâ€‘stream CUDA overlap; pipeline bubble reduction (DualPipe).

- Inference (Sec. 5.2)
  - Quantization: model weights and KVâ€‘cache (KVQuant, KIVI, WKVQuant); mixedâ€‘precision kernels for speed.
  - Memory management: virtualized, paged KV (PagedAttention), defragmentationâ€‘free virtual tensors (vTensor), radix sharing for common prefixes (SGLang).
  - Prefillâ€“decode disaggregation: separate server pools and KV shipping (DistServe, Splitwise, Mooncake, CacheGen).
  - GPUâ€“CPU parallelism: overlap PCIe transfer and compute; CPUâ€‘side attention to shrink GPU workload (FlexGen, PipeSwitch, FastDecode).
  - Speculative decoding: draft multiple tokens and verify in batch; can be selfâ€‘speculative (LayerSkip) or extraâ€‘head based (Medusa, Eagle).

### 3.5 Evaluation frameworks (Section 6; Figures 7â€“8; Tables 6â€“7)
A unified view covering both â€œlongâ€‘input comprehensionâ€ and â€œlongâ€‘output generation.â€

- Longâ€‘context comprehension (Sec. 6.1)
  - Capability scaffold (Figure 7): language modeling â†’ retrieval (explicit/semantic) â†’ aggregation (statistical/semantic) â†’ reasoning (parallel/iterative) â†’ realâ€‘world tasks (QA, summarization, retrieval/reranking, RAG, manyâ€‘shot ICL, code).
  - Benchmarks:
    - Synthetic (Table 6): many NIAH variants; semantic retrieval; SQL/DB reasoning; longâ€‘math (MathHay); code retrieval (RepoQA).
    - Realâ€‘world (Table 7): multiâ€‘domain long QA/summarization (LongBench, HELMET), extremeâ€‘length tasks (LOFT 1M; Lâ€‘Eval 200K), citationâ€‘aware QA (Lâ€‘CiteEval; LongBenchâ€‘Cite), code (LongCodeArena), finance/medicine (DocFinQA; MedOdyssey).
- Longâ€‘form generation (Sec. 6.2; Figure 8; Table 8)
  - Task types: QA, summarization, instruction following (incl. structured and creative writing), and mixed suites (HelloBench).
  - Metrics: ROUGE/BLEU/BERTScore (semantics), Distinct/Repetition/PPL (fluency), taskâ€‘specific (factuality with FActScore; retrieval nDCG; KPR). LLMâ€‘asâ€‘aâ€‘Judge is widely adopted when references are insufficient.

### 3.6 Analyses you can act on (Section 7; Figure 9; Table 9)
- Effective vs. supported context (Sec. 7.1.1): often <50% of the claim (Table 9).
- Perplexity and performance (Sec. 7.1.2): vanilla longâ€‘PPL correlates weakly; `LongPPL` and controlled setups recover correlation.
- RAG vs. longâ€‘reader LCLMs (Sec. 7.1.3): LCLMs can beat RAG when compute is sufficient, but hybrids (Selfâ€‘Route; LongRAG) are best in practice.
- Mechanismâ€‘level insights (Sec. 7.2): retrieval heads matter; alternating attention types and mixing `NoPE` with `RoPE` can help extrapolation.

## 4. Key Insights and Innovations
- A unifying endâ€‘toâ€‘end taxonomy (Figure 2; Sections 2â€“8)
  - Whatâ€™s new: one place that connects data engineering, architectures, workflow methods, infrastructure, evaluation, and applications. Prior surveys (Table 1) rarely covered infrastructure + evaluation + mechanisms together.
  - Why it matters: Practitioners can trace concrete choices from data curation to serving architecture without getting lost in siloed literatures.

- A clear, mechanismâ€‘first map of positionâ€‘length extrapolation (Sec. 3.1.2; Figure 10; Table 4)
  - Whatâ€™s new: separates reorganization vs. interpolation vs. hierarchical vs. simulation families, and explains their behavior (e.g., `NTK/YaRN` preserve high frequencies).
  - Impact: demystifies how to push beyond training length with minimal finetuning and what each method preserves or distorts.

- Supported vs. effective context length as an evidenceâ€‘backed caution (Sec. 7.1.1; Table 9)
  - Novelty: a collated, multiâ€‘model snapshot revealing large gaps (often â‰¤50%) between advertised windows and usable range.
  - Significance: shifts focus from â€œhow big is your window?â€ to â€œhow much of it works?â€ and motivates better training and evaluation.

- A practical, capabilityâ€‘oriented evaluation scaffold (Figure 7; Tables 6â€“7; Section 6)
  - Novelty: aligns synthetic tasks (retrieval/aggregation/reasoning) with realistic tasks (QA/RAG/ICL/code), plus a parallel treatment of longâ€‘form generation (Figure 8; Table 8).
  - Significance: helps avoid overâ€‘fitting to needle tasks; promotes tests that reflect real workflows and output quality.

- Bridging engineering with modeling (Section 5; Table 5)
  - Novelty: a sideâ€‘byâ€‘side map of compute, I/O, HBM memory, and communication bottlenecks, and which optimization addresses which.
  - Significance: makes it feasible to deploy 100Kâ€“1Mâ€‘token workflows with known tradeâ€‘offs (e.g., KV quantization vs. recall accuracy).

## 5. Experimental Analysis
This survey aggregates results, and it also performs integrative analyses. Key evidence and how to read it:

- Evaluation methodology (Section 6)
  - Comprehension: The scaffold (Figure 7) ensures tests are not just retrieval but include aggregation and reasoning. Synthetic benchmarks in Table 6 map cleanly to these subâ€‘skills (e.g., `RULER` for retrieval/aggregation; `BABILong` and NeedleBench for multiâ€‘needle reasoning).
  - Realâ€‘world tasks (Table 7): cover QA, summarization, document reranking, RAG, manyâ€‘shot ICL, code, and domain suites (finance, medical). Samples reach up to 1M tokens (LOFT), ensuring tests match modern windows.

- Main quantitative evidence included in the survey
  - Effective context length (Table 9, Section 7.1.1). Selected entries:
    > GPTâ€‘4: â€œ128K claimed; 64K effective (50%).â€  
    > LLaMAâ€‘3.1â€‘70B: â€œ128K claimed; 64K effective (50%).â€  
    > Qwen2â€‘72B: â€œ128K claimed; 32K effective (25%).â€  
    > LWMâ€‘7B: â€œ1M claimed; <4K effective (<4%).â€
    This demonstrates the widespread â€œlostâ€‘inâ€‘theâ€‘middleâ€/utilization gap and supports the recommendation to evaluate effective usage, not only declared limits (also see Section 7.1.1 and [317]).

  - Perplexity vs. longâ€‘context performance (Section 7.1.2):  
    > Vanilla longâ€‘document perplexity correlates poorly with downstream longâ€‘context tasks, but `LongPPL`â€”which computes perplexity only on contextâ€‘sensitive tokensâ€”restores strong correlation (Sec. 7.1.2; [121]).  
    The survey further notes that controlled studies (same base model, different lengthâ€‘extension methods) show correlation is recoverable (Sec. 7.1.2; [338]).

- Robustness checks and failure modes
  - Lostâ€‘inâ€‘theâ€‘middle (Section 7.1.1): performance is Uâ€‘shaped by positionâ€”good at beginning and end, poor in the middleâ€”even at lengths far below the declared maximum (evidence across multiple models in RULER and [317]; Table 9).
  - RAG vs. LCLM (Section 7.1.3): LCLM â€œlong readersâ€ can beat RAG when compute is plentiful, but hybrids (Selfâ€‘Route, LongRAG) are more reliable across resource settings (Sec. 7.1.3).

- Mechanismâ€‘level observations that are empirically grounded
  - Position encodings (Figure 10): `PI` scales positions linearly; `NTK/YaRN` preserve highâ€‘frequency â€œshortâ€‘rangeâ€ signals longer, improving extrapolation stability.
  - Attention heads: â€œretrieval headsâ€ and â€œretrievalâ€‘reasoning headsâ€ matter; allocating KV budgets headâ€‘wise or identifying retrieval heads enables aggressive KV compression while keeping accuracy (Section 3.2.1 â€œHeadâ€‘Level Optimizationâ€; Section 7.2.2; [489, 565, 129]).

- Assessment of support
  - The compiled results convincingly justify the paperâ€™s warnings (effective vs. supported lengths), its evaluation scaffold (need beyond needle tasks), and its engineering guidance (why KV cache is the dominant decode bottleneck; Section 5.2, Table 5). Where results are mixed (e.g., pure longâ€‘reader vs. RAG), conditions and hybrids are clearly discussed (Section 7.1.3).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - The survey focuses primarily on text LCLMs, with a later dedicated section for multimodal long context (Section 8.6). Some fastâ€‘moving subareas (e.g., new o1â€‘like RL training recipes) are discussed at a conceptual level (Section 9.1) but not experimentally compared.
  - Reported effectiveâ€‘length comparisons (Table 9) aggregate external evaluations; scoring details can vary by benchmark and prompt format.

- Scenarios not deeply addressed
  - Endâ€‘toâ€‘end costâ€“quality tradeâ€‘off curves for production systems under real SLA constraints (throughput, latency, cost) are discussed qualitatively (Table 5; Sections 5.2.3â€“5.2.5) but not benchmarked across vendors.
  - Robustness to adversarial long inputs (e.g., poisoning or promptâ€‘injection at distant positions) is outside scope.

- Computational constraints
  - Long contexts force I/Oâ€‘ and HBMâ€‘bound regimes. Even with quantization and paged KV, millionâ€‘token prompts remain expensive; decoding remains bandwidthâ€‘bound (Section 5.2). Many techniques trade compute for fewer memory transfers (e.g., speculative decoding), which can affect determinism or acceptance rates.

- Open questions
  - How to design reward models that evaluate very long reasoning chains or longâ€‘form outputs reliably (Section 9.2, 9.4).
  - How to close the gap between supported and effective window without drastic compute (Section 7.1.1; Section 9.2).
  - Standardized, leakageâ€‘resistant longâ€‘form generation metrics that correlate with human judgment (Section 6.2.3â€“6.2.4).

## 7. Implications and Future Directions
- Landscape impact
  - Moves the field from â€œjust make the window longerâ€ to â€œdesign for effective use,â€ backed by evaluation scaffolds and mechanismâ€‘level recipes (Figures 7â€“10; Table 9).
  - Encourages hybridization at every layer: data (short+long), architecture (linear+full, headâ€‘wise or layerâ€‘wise), workflow (RAG+long reader), and infrastructure (prefillâ€‘decode disaggregation, CPU+GPU pipelines).

- Followâ€‘up research
  - Longâ€‘reasoning with long contexts (Section 9.1): Build process reward models that can score multiâ€‘thousandâ€‘token chains reliably; compress and structure CoT (e.g., with memory, prompt compression) so models can â€œthink longerâ€ without prohibitive cost.
  - Data recipes (Section 9.2): Systematic longâ€‘dependency filtering; synthesize positionâ€‘agnostic, integrationâ€‘heavy tasks; identify optimal short/long/domain mixtures for a given budget.
  - Lengthâ€‘generalization theory and practice (Sections 3.1.2, 7.2.1): Better frequencyâ€‘preserving encodings; principled alternation of `NoPE`/`RoPE` and of attention types across layers; contentâ€‘aware positions (`CoPE`, `DAPE`).
  - Training/inference frameworks and hardware (Section 9.3): FP8â€‘first training beyond matrixâ€‘mults; wider use of activation quantization; decodeâ€‘optimized accelerators with larger HBM and higher bandwidth; tighter integration of paged KV and schedulers.
  - Evaluation (Section 9.4): Scenarioâ€‘specific long comprehension suites (legal, medical, finance) with efficient humanâ€‘inâ€‘theâ€‘loop protocols; coarseâ€‘toâ€‘fine LLMâ€‘asâ€‘aâ€‘Judge pipelines for longâ€‘form outputs.
  - Mechanistic interpretability (Section 9.5): Identify modules that cause â€œlostâ€‘inâ€‘theâ€‘middleâ€ and length failures; use insights to design microâ€‘interventions (e.g., head gating, positionâ€‘aware MLPs).

- Practical applications (Section 8; Figure 11)
  - Agentic systems with durable memory, deep browsing, and longâ€‘horizon plans; enterprise RAG that ingests whole corpora at once; chatbot personalization with longâ€‘term memory; repoâ€‘level coding copilots; longâ€‘document translation/summarization; longâ€‘video QA.

---

Definitions of selected nonâ€‘standard terms used above
- `LCLM`: Longâ€‘Context Language Model; an LLM trained/tuned to process very long inputs (â‰¥128K tokens) or produce long outputs.
- `KV cache`: The stored key/value tensors for past tokens during autoregressive decoding; dominates GPU memory as sequence grows.
- `SWA`: Slidingâ€‘Window Attention; each token attends only to a fixedâ€‘size local window of past tokens.
- `RoPE`, `PI`, `NTK`, `YaRN`: Families of positional encoding and length extrapolation techniques (Sec. 3.1.2).
- `SSM/Mamba`: State Space Models with selective, inputâ€‘dependent parameters that update a latent state per token in linear time (Sec. 3.2.2).
- `Prefill` vs. `Decoding`: Prefill computes the KV cache for the entire prompt (computeâ€‘bound); decoding generates tokens one by one using the cache (bandwidthâ€‘bound).
- `Speculative decoding`: Draft multiple next tokens with a cheap model (or with skipped layers) and verify them in batch with the full model to reduce memory traffic (Sec. 5.2.5).
- `RAG`: Retrievalâ€‘Augmented Generation; retrieve external knowledge before generation.
- `ICL`: Inâ€‘Context Learning; provide task examples within the prompt at inference time.
- `LLMâ€‘asâ€‘aâ€‘Judge`: Use a strong LLM with grading prompts/rubrics to evaluate another modelâ€™s long output when references are inadequate.
- `NoPE`: â€œNo Position Embeddingâ€â€”a design where positional information is learned implicitly; alternating NoPE with RoPE can help extrapolation (Sec. 7.2.3).
- `MQA/GQA/MLA`: Multiâ€‘Query/Groupedâ€‘Query/Multiâ€‘head Latent Attentionâ€”ways to reduce KV heads or compress K/V for memory savings (Sec. 3.2.1).

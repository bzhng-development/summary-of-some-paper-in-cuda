# Preâ€‘training Distillation for Large Language Models: A Design Space Exploration

**ArXiv:** [2410.16215](https://arxiv.org/abs/2410.16215)
**Authors:** Hao Peng, Xin Lv, Yushi Bai, Zijun Yao, Jiajie Zhang, Leiâ€¯Hou, Juanziâ€¯Li
**Institutions:** 

## ðŸŽ¯ Pitch

This paper introduces Pre-Training Distillation (PD), a method for transferring knowledge from larger to smaller language models during pre-training, which dramatically reduces storage needs and improves model efficiency. By systematically optimizing the distillation process, this approach enables smaller models to achieve higher performance with fewer resources, significantly impacting the practical deployment of language models by making high-quality training accessible and cost-effective even at large scales.

---

## 1. Executive Summary
This paper proposes pre-training distillation (PD): transferring knowledge from a larger â€œteacherâ€ language model to a smaller â€œstudentâ€ during the pre-training stage, not just during post-training. It systematically explores a design space across four dimensionsâ€”logits processing, loss selection, scaling law, and offline vs. online teacher logitsâ€”and identifies practical configurations that reliably improve student models while dramatically reducing storage overhead (e.g., 4,000Ã— via truncated teacher logits) (Â§Â§3.1â€“3.5, Fig. 1, Tables 1, 5, 8â€“13).

## 2. Context and Motivation
- Problem addressed
  - Knowledge distillation (KD) for large language models (LLMs) is usually applied after pre-training (post-training), where students learn from instructionâ€“response pairs generated by stronger teachers. This paper asks whether KD can be beneficial during the pre-training stage itself and how best to do it at LLM scale (Â§1).
- Why it matters
  - Pre-training billions of tokens is the most expensive phase in building LLMs; if a student can absorb â€œsofter,â€ richer supervision from a strong teacher during pre-training, it may train faster and reach higher ceilings. This impacts practical deployment (smaller models with better quality) and efficiency (fewer tokens or steps to achieve target quality) (Â§1, Â§3.4).
- Prior approaches and gaps
  - Post-training KD is now standard for alignment and instruction-following (e.g., Alpaca, Vicuna; Â§1, Â§4) but does not change the base modelâ€™s pre-trained knowledge.
  - Earlier KD on small models (e.g., BERT-scale) explored logit-based and intermediate-feature distillation, but training dynamics can differ significantly at billion-parameter scale (Â§4).
  - Several recent LLM efforts mention pre-training distillation (e.g., Gemma 2, AFM, LokiLM) but provide limited methodological detail or systematic analysis (Â§4).
- Positioning
  - This work offers the first thorough empirical design-space study of PD for LLMs, identifying key choices that matter (and those that do not) and quantifying trade-offs across storage, performance, and model sizes (Â§Â§3.2â€“3.5).

## 3. Technical Approach
The paper frames PD as training a student on a mixture of conventional language-modeling loss and a distillation loss computed from teacher probabilities (Â§2).

- Objective formulation (plain-language first, then notation)
  - At each token position, the student is trained to both:
    1) predict the next token from the ground-truth corpus (standard language modeling), and
    2) match the teacherâ€™s probability distribution over tokens (distillation).
  - The total loss is a weighted sum: `L = (1 âˆ’ Î±) * L_lm + Î± * L_kd`, where `Î±` is the fraction of distillation loss (Â§2, Eq. 1).
  - `L_lm` is standard negative log-likelihood on true next tokens (Â§2, Eq. 2).
  - `L_kd` compares the studentâ€™s predictive distribution to the teacherâ€™s processed distribution for each token (Â§2, Eq. 3). The teacherâ€™s raw logits are first truncated and normalized: `F(z) = softmax(Truncate(z)/Ï„)`, where `Ï„` (â€œtemperatureâ€) controls how sharp or flat the distribution is (Â§2, Eq. 4).

- What are â€œlogitsâ€ and why process them?
  - A `logit` is the unnormalized score a model assigns to each vocabulary token. Softmax converts logits to probabilities. Storing teacher logits for every token and every position is prohibitively large (e.g., ~58.6 petabytes for 100B tokens with a ~150k vocabulary at float32; Â§3.1). The paper truncates logits to only the most probable tokens and renormalizes probabilities to make storage manageable (Â§3.1).
  - Truncation schemes
    - `top-p` (nucleus) truncation: keep the smallest set of tokens whose cumulative probability mass â‰¥ `p`.
    - `top-k` truncation: keep the `k` highest-probability tokens.
    - The paper composes them as `top-p-k`: apply `top-p` first, then cap with `top-k` to cover cases with long tails (Â§3.2). This cut storage by ~4,000Ã— in the preliminary setup (to ~15 TB for 100B tokens; Â§3.1).

- Training pipeline (preliminary configuration; Â§3.1)
  1) Sample 100B pre-training tokens from a mixed Englishâ€“Chinese corpus. Fix chunk length to 4096 tokens (the studentâ€™s context length).
  2) Run the teacher (`GLM-4-9B`) over these tokens to generate logits per position; apply `top-p=0.95` then `top-k=100`; renormalize with `Ï„=1.0`. Store these logits offline on disk.
  3) Train the student (1.9B parameters) from scratch using KD with negative log-likelihood (treating teacher probabilities as soft targets). In this preliminary run, `Î±=1` (only KD loss).
  4) Because small models score near-chance on some benchmarks immediately after pre-training, perform supervised fine-tuning (SFT) on 10B instruction-tuning tokens plus 10B additional pre-training text. During SFT, use only LM loss (`Î±=0`) so comparisons isolate pre-training effects (Â§3.1, A.1).
  5) Evaluate across English and Chinese tasks with fixed decoding temperature 0 and specified shot settings (A.1).

- Design dimensions explored (Â§2; experiments in Â§Â§3.2â€“3.5)
  - Logits processing: truncation (`p`, `k`) and normalization temperature `Ï„`.
  - Loss selection: which KD loss (`NLL`, `KLD`, `MSE`) and how to combine with LM loss (static or scheduled `Î±`).
  - Scaling law: sizes of teacher and student, and total pre-training tokens.
  - Logits generation: offline (pre-trained teacher) vs online (collect during teacherâ€™s own pre-training).

- Advanced scheduling (why and how)
  - The paper investigates coupling the KD weight schedule with the learning rate schedule via a â€œWarmupâ€“Stableâ€“Decayâ€ (`WSD`) scheduler (Â§3.3; Table 5). Intuition: when the learning rate is at or near its maximum, emphasizing KD can accelerate learning from rich teacher signals; when decaying, gradually re-increase reliance on LM loss.

- Online PD (what it is and why itâ€™s tricky)
  - â€œOnlineâ€ means storing teacher logits on the fly while the teacher itself is still training (Â§3.5). This removes extra inference cost later, but early teacher checkpoints are noisy and produce worse targets; care is needed to only use later-stage logits or to temper KD strength.

## 4. Key Insights and Innovations
- A practical recipe for pre-training distillation at LLM scale
  - Novelty: Thorough empirical exploration of PD design choices for billion-parameter LLMsâ€”what truncation to use, how to schedule distillation loss, what teacherâ€“student size ratios work, and whether to use online or offline logits (Â§Â§3.2â€“3.5).
  - Significance: Identifies robust, resource-aware configurations that consistently improve students versus LM-only training, with especially strong gains when combining KD weighting with `WSD` learning-rate scheduling (Table 5).

- Storage-efficient teacher signal with minimal performance loss
  - Insight: `top-p-k` truncation preserves the â€œmassâ€ of teacher probabilities that the student needs while cutting storage by ~4,000Ã— in the 100B-token setup (Â§3.1).
  - Evidence: Across a range of `p` and `k`, performance is similar; smaller `p`/`k` can be chosen to save disk space with little difference (Figs. 2â€“3; Table 8). Best observed around `top-0.95-50` in some settings (Table 8).

- Loss and schedule synergy matters more than loss type alone
  - Insight: `KLD` and `NLL` as KD losses both help, but the combination strategy dominates. Scheduling `Î±` with `WSD` and pairing it with a `WSD` learning-rate schedule yields the strongest results (Table 5).
  - Evidence: `WSD-Î± + WSD-LR` achieves the highest average score (+8.0% relative over LM baseline; Table 5), outperforming static `Î±` and linear schedules. `MSE` loss is unreliable at LLM pre-training scale (Table 5).

- Scaling law observations for PD
  - Insight: Larger students benefit more from PD; larger teachers do not always help. A student roughly â‰¥10% of the teacherâ€™s size benefits reliably; widening the capacity gap can hurt transfer (Â§3.4, Fig. 4; Table 11).
  - Interpretation: If the teacher compresses knowledge too effectively (being much larger), the student may not be able to absorb it; using a closer-capacity teacher or growing the student narrows this gap.

- PD remains useful at 500B tokens and throughout training
  - Insight: KD advantages persist and are visible at intermediate checkpoints, suggesting both faster learning and higher end performance (Fig. 5; Table 12).

## 5. Experimental Analysis
- Evaluation methodology
  - Datasets and metrics (Â§3.1; A.1)
    - English: HellaSwag, WinoGrande, PIQA, MMLU.
    - Chinese: KBQA, C3, C-Eval.
    - Math: GSM8K.
    - Evaluation uses zero-shot for some tasks, 5â€“8-shot for others, with decoding temperature 0 (A.1).
  - Baselines
    - `LLM-LM`: pre-train student from scratch with only LM loss, then same SFT.
    - `LLM-KD`: pre-train with PD under various settings, then same SFT.
  - Models and data
    - Teacher: `GLM-4-9B` (and `GLM-4-32B` in scaling experiments).
    - Students: 330M, 670M, 1.9B, 3.8B, 6.8B (Â§3.4; Table 7 for architectures).
    - Tokens: 100B in most studies; 500B in scaling-with-data (Â§3.4).
  - Training details
    - Preliminary: `top-p=0.95`, `top-k=100`, `Ï„=1.0`, `Î±=1` (KD-only) during pre-training; `Î±=0` during SFT (Â§3.1).
    - Optimizer, batch size, LR schedule in Â§3.1; BF16 training (A.1, Table 7).

- Main quantitative results
  - Baseline feasibility of PD (100B tokens; Table 1)
    - Average score improves from 37.7 (`LLM-LM`) to 38.3 (`LLM-KD`)â€”a +1.6% relative gain. Notably:
      > GSM8K: 8.6 â†’ 10.8 (+24.6% relative)  
      > C-Eval: 25.9 â†’ 26.7 (+3.2% relative)
    - Gains are modest overallâ€”indicating PD works but configuration matters (Â§3.1).
  - Logits processing ablations (Â§3.2)
    - Varying `p` in `top-p-100`: similar improvements across p; allows using smaller `p` to save space (Fig. 2; Table 8).
    - Varying `k` in `top-0.95-k`: all `k` help; `k=50` often best (Table 8). Even `k=1` (effectively label replacement) helps slightly, likely due to teacher â€œnoise filtering.â€
    - Temperature `Ï„`: moderate or low `Ï„` (â‰¤2.0) works best; high `Ï„` (â‰¥5.0) harms results (Table 9). Adaptive temperature methods (NormKD, WTTM, AdaKD) do not outperform a good static setting, with `AdaKDH` best among them but not clearly superior to `Ï„=0.5` (Table 3 vs. Table 9).
  - Loss selection and scheduling (Â§3.3; Tables 4â€“5)
    - KD loss type (Î±=1 during pre-training):
      > `KLD` average 38.7 (+2.6% relative vs. LM baseline)  
      > `NLL` average 38.3 (+1.6%)  
      > `MSE` average 34.9 (âˆ’7.6%)
    - Combining LM and KD losses (static Î±)
      - Increasing `Î±` generally helps up to Î±â‰ˆ0.9; best static Î±=0.9 with +3.6% relative gain (Table 4).
    - Dynamic schedules
      - `Linear Dec` (from Î±=1â†’0) beats `Linear Inc` (0â†’1): +4.1% vs +1.1% (Table 5).
      - Periodic scheme offers small gains (+0.9%; Table 5).
      - `WSD-Î± + WSD-LR` is best: average 40.7, +8.0% relative (Table 5). Using `WSD-LR` without `WSD-Î±` helps, but less.
  - Scaling laws (Â§3.4; Fig. 4; Table 11)
    - Larger students gain more from PD. Example with `GLM-4-9B` teacher:
      > 1.9B: 37.7 â†’ 38.3 (+1.6% relative)  
      > 3.8B: 42.0 â†’ 44.9 (+6.9% relative)  
      > 6.8B: 44.9 â†’ 48.0 (+6.9% relative)
    - Bigger teacher (32B) does not guarantee better PD for a given student; capacity gap can reduce transferability (Fig. 4; Table 11).
  - More data (500B tokens; Fig. 5; Table 12)
    - PD consistently outperforms LM-only across checkpoints.
      > 1.9B final averages: 44.2 (LM) vs 45.4 (KD)  
      > 3.8B final averages: 50.2 (LM) vs 53.7 (KD)
    - Gains grow then slightly taper as training progresses (Fig. 5).
  - Online vs. offline logits (Â§3.5; Table 6)
    - Using early-teacher online logits hurts (average 29.8; âˆ’20.9% relative).
    - Using later-teacher online logits reduces harm (36.3; âˆ’3.9%).
    - With a weaker KD weight and `top-0.95-50`, online achieves a slight gain (37.9; +0.5%) but still lags offline KD (Table 6).
  - â€œPD*â€ best configuration summary (A.6; Table 13; Fig. 1)
    - Using `top-0.95-50`, `Ï„=2.0`, `KLD` loss, `WSD-Î±` (max Î±=0.9) + `WSD-LR`, offline logits:
      > 1.9B: 41.2 average (vs baseline 37.7 and vanilla PD 38.3; Fig. 1; Table 13)  
      > 3.8B: 45.7 average  
      > 6.8B: 49.8 average

- Do the experiments support the claims?
  - Yes, under controlled comparisons (same corpus, same SFT, same evaluation), PD improves over LM-only pre-training, and the identified design choices (especially schedules and truncation) produce consistent additional gains. The systematic ablations across four axes and the 500B-token scaling runs strengthen the evidence (Â§Â§3.2â€“3.5; Tables 1, 3â€“6, 8â€“13; Figs. 1â€“5).
  - Caveat: absolute gains at 100B tokens are modest for smaller students unless better scheduling is used; large gains emerge with `WSD-Î± + WSD-LR` and with larger students.

## 6. Limitations and Trade-offs
- Storage and data pipeline complexity
  - Even after 4,000Ã— compression, storing teacher logits for 100B tokens is ~15 TB (Â§3.1); online collection of 400B tokens required ~180 TB (A.5). This adds significant MLOps complexity for most labs.
- Dependence on SFT for evaluation
  - Because small students perform near chance on some benchmarks pre-SFT, the study adds an SFT phase for comparability (Â§3.1). While applied uniformly, it introduces a layer that can interact with the pre-training signal in subtle ways.
- Capacity-gap sensitivity
  - Benefits depend on the teacherâ€“student size ratio; very large teachers may not transfer optimally to small students (Â§3.4, Fig. 4). Results suggest students â‰¥~10% of teacher size are safer.
- Limited exploration of factor interactions
  - The paper isolates variables across four dimensions but does not exhaustively study their combinatorial interactions due to compute cost (Limitations).
- Online PD is fragile
  - Targets from a non-converged teacher are noisy; naive online PD can degrade performance substantially (Â§3.5, Table 6). Online PD needs careful staging and tuning.
- Generalization scope
  - Experiments focus on two teachers (9B, 32B), five student sizes up to 6.8B, and mixed Englishâ€“Chinese corpora. Findings likely generalize but are not guaranteed for, e.g., Mixture-of-Experts teachers, very small students (<330M), or multilingual, domain-specific corpora beyond those tested.

## 7. Implications and Future Directions
- Practical guidance for building better small/medium LLMs
  - For organizations training families of models, PD offers a way to raise the quality of smaller variants with manageable extra cost, especially when using `top-p-k` truncation and `WSD-Î± + WSD-LR`.
- Design rules of thumb
  - Use offline logits when feasible. If doing online PD, store logits from later teacher checkpoints and lower `Î±` early (Â§3.5).
  - Prefer `KLD` or `NLL` for KD loss; avoid `MSE` at LLM pre-training scale (Table 5).
  - Keep `Ï„` moderate (â‰¤2.0); very high temperatures flatten useful signal (Table 9).
  - Consider `top-0.95-50` as a good default; smaller `p`/`k` trade off storage and quality (Table 8).
  - Couple KD emphasis with the learning-rate schedule (`WSD-Î± + WSD-LR`) to maximize gains (Table 5).
- Research enabled by this work
  - Studying teacherâ€“student curriculum strategies: adaptive selection of which teacher checkpoints and which samples to use for KD throughout pre-training (Â§3.5).
  - Bridging large capacity gaps: teacher assistants or multi-stage PD to transfer from very large teachers to small students (Â§3.4; related to teacher-assistant KD literature).
  - Extending to MoE teachers, multimodal pre-training, or multilingual settings; examining different tokenizers or vocabulary mismatches.
  - Joint optimization of truncation, temperature, and Î± scheduling as a single policy (the paper notes not exploring multi-factor interactions; Limitations).
- Downstream applications
  - Faster or more accurate compact models for on-device use, cost-efficient deployment, or domains with capped memory/latency budgets (e.g., edge devices), while retaining more pre-trained knowledge than LM-only training would allow.
  - Data filtering: the observation that `k=1` with teacher labels already helps suggests teacher logits could guide data selection/noise mitigation in large corpora (Â§3.2, Table 8).

Overall, this paper turns â€œpre-training distillationâ€ from an idea into a concrete, evidence-backed recipe. The most impactful takeaway is that the right combination of storage-aware logits processing and synchronized loss/learning-rate scheduling makes PD both feasible and beneficial at billion-parameter scale (Fig. 1; Tables 5, 8â€“13).

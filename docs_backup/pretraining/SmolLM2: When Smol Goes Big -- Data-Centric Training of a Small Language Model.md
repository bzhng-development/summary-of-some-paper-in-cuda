# SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model

**ArXiv:** [2502.02737](https://arxiv.org/abs/2502.02737)
**Authors:** Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel MartÃ­n BlÃ¡zquez, Guilherme Penedo, Lewis Tunstall, AndrÃ©s Marafioti, Hynek KydlÃ­Äek, AgustÃ­n Piqueres LajarÃ­n, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuanâ€‘Son Nguyen, ClÃ©mentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, Thomas Wolf
**Institutions:** 

## ğŸ¯ Pitch

SmolLM2 redefines small-model efficiency by leveraging a novel data-first strategy, combining targeted datasets and a multi-stage, data-rebalancing training schedule. This approach not only makes a 1.7B model rival larger counterparts in reasoning and coding but also democratizes access to powerful language models by reducing compute costs, paving the way for more resource-efficient applications in cost-sensitive environments.

---

## 1. Executive Summary
SmolLM2 is a 1.7B-parameter language model trained with a data-first strategy: it â€œovertrainsâ€ a small model on ~11 trillion tokens using a multiâ€‘stage, continuously rebalanced mixture of carefully filtered web, math, and code data, plus targeted instruction and preference tuning (Sections 3â€“5). The result is a small model that rivals or beats other 1â€“2B models on many knowledge and reasoning tasks while staying competitive on coding and math, achieved largely through three new open datasetsâ€”`FineMath`, `Stackâ€‘Edu`, and `SmolTalk`â€”and a structured training schedule (Figure 2; Tables 4â€“5).

## 2. Context and Motivation
- Problem addressed
  - Small models (â‰¤3B parameters) are attractive for on-device and costâ€‘sensitive deployment but typically lag behind larger models in knowledge, reasoning, math, and coding. The key bottleneck is not only model size, but the quality, composition, and schedule of training data and how it is mixed over very long trainings (Introduction; Section 2).
- Why it matters
  - Lowerâ€‘compute LMs enable broader access (edge devices, privacyâ€‘sensitive domains). Improving them with data/mixing rather than more parameters reduces inference costs and expands their practical utility (Introduction; Section 4).
- Where prior approaches fall short
  - Web corpora aloneâ€”even when filteredâ€”underperform on specialized domains like math and code, and small specialized datasets get drowned in large general corpora if mixed naively from the start (Sections 2â€“3).
  - Running many full training runs to tune mixtures is too expensive for longâ€‘trained small LMs (SmolLM2â€™s pretraining is ~1e23 FLOPs, â‰ˆ$250k GPU; Section 4).
- Positioning
  - SmolLM2 organizes training around â€œdata-centricâ€ interventions:
    - Systematic ablations to choose web, math, and code datasets (Section 3; Table 1; Figure 1; Table 2).
    - New, larger, higher-quality specialized datasets (`FineMath`, `Stackâ€‘Edu`) to fix gaps in public data (Sections 3.3â€“3.4).
    - A multiâ€‘stage, manually rebalanced schedule to upsample the right data at the right time, with finalâ€‘stage â€œannealingâ€ on the best math and code data (Section 4; Figure 2).

## 3. Technical Approach
The pipeline has four pillars: dataset ablations and construction, multi-stage pretraining with online rebalancing, longâ€‘context extension, and postâ€‘training.

- Definitions used once where needed
  - `token`: a chunk of text (word or subword) the model reads or predicts.
  - `annealing ablation`: start from a midâ€‘training checkpoint, then linearly decay learning rate to 0 while training on a mixture that includes a candidate dataset; this reveals the datasetâ€™s marginal impact without a full reâ€‘train (Section 3.1).
  - `WSD` (Warmupâ€‘Stableâ€‘Decay): a learningâ€‘rate schedule with warmup, a long flat â€œstableâ€ phase, and a switchable linear decay to zero, so training length is not fixed up front (Appendix A; Figure 3).
  - `MMLU CF` vs `MMLU MCF`: cloze formulation computes answer likelihood; multipleâ€‘choice formulation requires explicit A/B/C/D output (Section 4.3; Figure 6).
  - `RoPE`: rotary positional embeddings; SmolLM2 raises RoPE base to 130k for long context (Section 4.6).
  - `DPO`: Direct Preference Optimization for preference learning without explicit reward models (Section 5.3).

A) Empirical dataset ablations and new datasets (Section 3)
- English web (Section 3.2; Table 1)
  - Tested `FineWebâ€‘Edu` (educationâ€‘filtered web) vs `DCLM` (DataCompâ€‘LM; web filtered by explain-likeâ€‘Iâ€™mâ€‘five style).
  - 350Bâ€‘token ablations (same architecture and hyperparameters; Section 3.1) show complementary strengths:
    - > â€œFineWebâ€‘Edu achieves higher scores on MMLU, ARC, and OpenBookQA, while DCLM performs better on HellaSwag and CommonsenseQA.â€ (Table 1)
  - Choice: mix 60% `FineWebâ€‘Edu` + 40% `DCLM` for early stages (Table 1; Section 4.2), later tilt more toward DCLM to improve MMLU MCF (Section 4.3).

- Math (Sections 3.3.1â€“3.3.2; Figure 1)
  - Public datasets (`OpenWebMath`, `InfiMMâ€‘WebMath`) are too small or skewed toward advanced papers with little stepâ€‘byâ€‘step reasoning (Section 3.3.1; Figure 5).
  - Built `FineMath`:
    - Start from 6.5T tokens reâ€‘extracted from 7.1B pages in mathâ€‘rich domains; preserve LaTeX; heavy dedup/language filtering (Section 3.3.2).
    - Two classifier passes using Llamaâ€‘3.1â€‘70Bâ€‘Instruct â€œsilverâ€ labels: first to find math domains, second to target mid/highâ€‘school level, stepâ€‘byâ€‘step reasoning (Appendix C.2â€“C.3).
    - Variants: `FineMath4+` (10B tokens, scores 4â€“5) and `FineMath3+` (34B tokens, scores 3â€“5), with 13â€‘gram decontamination against GSM8K, MATH, MMLU (Section 3.3.2).
    - Result in ablations: 
      > â€œFineMath4+ achieves a 2x improvement on GSM8K and a 6x improvement on MATH compared to InfiMMâ€‘WebMath.â€ (Figure 1 summary)

- Code (Section 3.4; Table 2)
  - Baselines (`StarCoder2Data`, `Stack v2`) are large but contain a lot of nonâ€‘pedagogical code.
  - Built `Stackâ€‘Edu`: filter `StarCoder2Data` for educational quality using perâ€‘language classifiers (trained on 500k syntheticâ€‘labeled samples per language; F1 > 0.7 for most; Appendix D.1). Keep 15 most common languages; ~125B tokens after filtering (Appendix D.2).
  - Annealing ablations find threshold 3 works best for most languages; MultiPLâ€‘E improves across languages:
    > Python: 20.7 â†’ 25.6; C++: 16.7 â†’ 24.8; JavaScript: 18.2 â†’ 22.4; Java: 17.6 â†’ 22.7 (Table 2)

B) Multiâ€‘stage pretraining with online rebalancing (Section 4; Figure 2)
- Model and setup (Appendix A)
  - `SmolLM2â€‘1.7B`: 24 layers, d_model 2048, 32 heads, SwiGLU, RoPE, 2M tokens/batch; AdamW; tokenizer with 49,152 vocab (Appendix A). Trained on 256 H100s using `nanotron` (Section 4.1).
  - Learning rate: WSD with 2k warmup steps, peak 5eâ€‘4, then final 10% linear decay (Figure 3).

- Stage 1 (0â€“6T tokens; Section 4.2)
  - Mixture: ~90% English web (60/40 FineWebâ€‘Edu/DCLM), 10% `StarCoderData`. No math yet due to small math corpora.
  - Observation: good knowledge/reasoning; weak code and math (Table 3; Stage 1 row).

- Stage 2 (6â€“8T; Section 4.3)
  - Mixture: 75% web (still 60/40), 20% code (upsampled), 5% `OWM` math.
  - Observation: code improves; math barely moves; MMLU MCF rises above random for a small model (Figure 6).

- Stage 3 (8â€“10T; Section 4.4)
  - Mixture changes:
    - Web: flip to 40/60 `FineWebâ€‘Edu`/`DCLM` (helps MMLU MCF at this point).
    - Code: switch to `Stackâ€‘Edu` (+ Jupyter notebooks).
    - Math: add textâ€‘only `InfiMMâ€‘WebMath` alongside `OWM` to ~10% math.
  - Observation: general improvement; a transient â€œloss spikeâ€ occurs (cause unclear) but most metrics recover by stage end (Section 4.4).

- Stage 4 (10â€“11T; final decay; Section 4.5)
  - Linear LR decay to zero; â€œannealâ€ on the best specialized data:
    - Math: `FineMath4+` + `InfiWebMathâ€‘3+` dominate math portion; tiny `OWM` (0.08%) and `AugGSM8K` (0.02%) for coverage.
    - Code: `Stackâ€‘Edu` at 24%, broader language coverage.
    - Web: 58% (DCLMâ€‘heavy) + 4% `Cosmopedia v2` (synthetic textbooks/stories).
  - Observation: largest gains in math and code show up here (Table 3 and Table 8).

C) Longâ€‘context extension (Section 4.6)
- Raise context from 2k â†’ 8k tokens by taking a late stageâ€‘4 checkpoint, setting RoPE base to 130k, and training on a mixture where 40% are long documents (â‰¥8k) from DCLM, FineWebâ€‘Edu, and Dolmaâ€‘Books, with 60% following the stageâ€‘4 mix.

D) Postâ€‘training (Section 5)
- Supervised fineâ€‘tuning (SFT) on `SmolTalk` (Section 5.1â€“5.2; Table 9)
  - Motivation: offâ€‘theâ€‘shelf instruction datasets underperform for this base model; build a tailored mix of 1.1M pairs.
  - `MagPieâ€‘Ultra` (431k): threeâ€‘turn, systemâ€‘prompted conversations generated by Llamaâ€‘3.1â€‘405B and filtered for quality/safety (Section 5.1.1).
  - Taskâ€‘specific sets: `Smolâ€‘Constraint` (36k instructionâ€‘following with constraints), `Smolâ€‘Summarization` (101k), `Smolâ€‘Rewrite` (56k) (Section 5.1.2).
  - Math SFT: combine `NuminaMathâ€‘CoT` and `MetaMathQA` (Section 5.1.3).
  - Plus code (Selfâ€‘OSSâ€‘StarCoder2â€‘Instruct), systemâ€‘prompt and functionâ€‘calling data, and small longâ€‘context SFT (LongAlign) (Section 5.1.4).
  - Train SFT for 2 epochs, 8k context, LR 3eâ€‘4 (Section 5.2).

- Preference learning with `DPO` (Section 5.3)
  - Use `UltraFeedback` as the most effective feedback pool in experiments; 2 epochs, LR 1eâ€‘6, beta 0.5, 1k context during DPO (Section 5.3).

## 4. Key Insights and Innovations
- Multiâ€‘stage, performanceâ€‘driven data rebalancing for small models (Section 4; Figure 2)
  - Innovation: Rather than fix a single mixture, SmolLM2 monitors capabilities during training and â€œintervenesâ€ in later stages (especially the decay stage) by upsampling the most effective specialized data (e.g., `FineMath4+`, `Stackâ€‘Edu`).
  - Significance: It delivers large lateâ€‘stage gains in math/code while preserving general knowledge, without multiple endâ€‘toâ€‘end restarts (Table 3; Table 8).

- `FineMath`: targeted, highâ€‘quality math corpus emphasizing stepâ€‘byâ€‘step reasoning at appropriate difficulty (Section 3.3.2; Figure 1)
  - Whatâ€™s different: domainâ€‘level mining from billions of URLs, twoâ€‘stage classifier prompts explicitly aiming at middle/highâ€‘school reasoning, heavy dedup and decontamination.
  - Why it matters: Ablations show much stronger math learning than prior open math corpora:
    > â€œFineMath4+ â€¦ 2x improvement on GSM8K and 6x on MATH vs InfiMMâ€‘WebMath.â€ (Figure 1 summary)

- `Stackâ€‘Edu`: educationâ€‘filtered code pretraining (Section 3.4; Table 2)
  - Whatâ€™s different: perâ€‘language classifiers trained on synthetic labels rate pedagogical quality, not just license/format.
  - Why it matters: Large, consistent MultiPLâ€‘E gains across languages at manageable size, fitting smallâ€‘model capacity.

- `SmolTalk`: instruction dataset engineered for small models (Section 5.1; Table 9; Table 10)
  - Whatâ€™s different: a balanced, qualityâ€‘filtered conversational core (`MagPieâ€‘Ultra`) plus targeted components (constraints, summarization, rewriting, math, code, function calling, longâ€‘context).
  - Why it matters: boosts instructionâ€‘following and reasoning after SFT and DPO (Table 5 and Appendix F, Table 10).

- A practical annealingâ€‘ablation protocol (Section 3.1)
  - Whatâ€™s different: evaluate candidate datasets by resuming from a midâ€‘training checkpoint and decaying LR on a short burst including that dataset.
  - Why it matters: Enables evidenceâ€‘based mixture decisions under tight compute budgets.

## 5. Experimental Analysis
Evaluation design
- Pretraining ablations: identical 1.7B config runs (350B tokens for web; Section 3.1), and annealing ablations for math (60B tokens) and code (200B tokens) starting from a 3Tâ€‘token checkpoint (Section 3.1).
- Stageâ€‘byâ€‘stage tracking: category averages and perâ€‘benchmark metrics after each pretraining stage (Table 3; Table 8).
- Final model comparisons: zeroâ€‘shot or fewâ€‘shot against `Llama3.2â€‘1B` and `Qwen2.5â€‘1.5B` for base and instruct models (Tables 4â€“5).
- Longâ€‘context: Needleâ€‘inâ€‘aâ€‘Haystack (NIAH) and HELMET at 8k context (Appendix G; Figure 7; Table 11).

Main quantitative results
- Web mix choice (Table 1)
  > â€œFineWebâ€‘Edu â€¦ better on MMLU, ARC, OpenBookQA; DCLM â€¦ better on HellaSwag and CommonsenseQA. The 60/40 mix balances both.â€  
  This guided early mixing; later stages tilt to DCLM (Section 4.3).

- Math ablations (Figure 1; Section 3.3.2)
  > FineMath subsets â€œconsistently outperform OWM and InfiMMâ€‘WebMath on GSM8K, MATH, and MMLUâ€‘STEM.â€  
  Notably, `Infiâ€‘WebMath4+` plateaus after ~10 epochs due to repetition, whereas `FineMath4+` keeps improving (Figure 1), justifying reserving `FineMath4+` for final annealing (Stage 4).

- Code ablations (Table 2)
  > MultiPLâ€‘E improves for major languages after `Stackâ€‘Edu` filtering (e.g., C++ 16.7 â†’ 24.8).  
  This underpins the Stageâ€‘3 switch to `Stackâ€‘Edu`.

- Stage progression (Table 3; Table 8; Figure 6)
  - Category averages (Table 3):
    > Knowledge/Reasoning: 55.50 â†’ 60.24; Math: 3.21 â†’ 22.07; Code: 8.87 â†’ 23.21; Generative: 31.54 â†’ 36.12 (Stage 1 â†’ 4).  
  - Perâ€‘benchmark (Table 8):
    > MMLU MCF: 29.62 â†’ 48.87; GSM8K: 4.32 â†’ 32.60; MATH: 2.10 â†’ 11.54; HumanEval: 10.97 â†’ 22.60.  
  - Training dynamics:
    > â€œWe observed above-random (>25%) MMLU accuracy with MCF after 6T tokensâ€ (Figure 6), unusual for such small models, and a transient loss spike in Stage 3 that mostly recovers (Section 4.4).

- Base model comparison (Table 4)
  - Strengths vs contemporaries (1â€“2B):
    > HellaSwag: 68.7 (SmolLM2) vs 61.2 (Llama3.2â€‘1B) and 66.4 (Qwen2.5â€‘1.5B); ARC: 60.5 vs 49.2 and 58.5; CommonsenseQA: 43.6 vs 41.2 and 34.1.  
  - Heldâ€‘out generalization:
    > MMLUâ€‘Pro: 19.4 vs 11.7 (Llama3.2â€‘1B) and 13.7 (Qwen2.5â€‘1.5B); TriviaQA: 36.7 vs 28.1 and 20.9.  
  - Math/coding remain competitive but not best:
    > GSM8K: 31.1 (SmolLM2) vs 61.7 (Qwen2.5â€‘1.5B); MATH: 11.6 vs 34.3; HumanEval: 22.6 vs 37.2 (Qwen2.5â€‘1.5B).  
    This reflects the strong specialized training Qwen uses; SmolLM2 narrows the gap with `FineMath` and `Stackâ€‘Edu` but does not surpass Qwen2.5 on these two domains.

- Instruct model comparison (Table 5)
  - Instruction following:
    > IFEval avg: 56.7 (SmolLM2â€‘Instruct) vs 53.5 (Llama3.2â€‘1Bâ€‘Instruct) and 47.4 (Qwen2.5â€‘1.5Bâ€‘Instruct).  
  - Reasoning and math:
    > GSM8K: 48.8 vs 37.4 (Llama3.2â€‘1Bâ€‘Instruct) and 63.3 (Qwen2.5â€‘1.5Bâ€‘Instruct); MATH: 21.0 vs 19.5 and 19.6.  
  - Coding:
    > HumanEval: 28.1 vs 33.5 (Llama3.2â€‘1Bâ€‘Instruct) and 30.5 (Qwen2.5â€‘1.5Bâ€‘Instruct).  
  - Takeaway: strong instructionâ€‘following and solid math for its size; coding remains behind the best small instruct models.

- Longâ€‘context (Appendix G)
  - Needleâ€‘inâ€‘aâ€‘Haystack: consistent retrieval across depths up to 8k (Figure 7 shows nearâ€‘perfect detectionâ€”green throughout).
  - HELMET (Table 11):
    > SmolLM2 leads on LongQA (33.00 vs 21.99 and 26.23), competitive on RAG (47.17 vs 42.13 and 47.54), but behind on ICL (23.20 vs 51.20 and 52.00).  
    So longâ€‘document QA is strong; fewâ€‘shot inâ€‘context learning lags.

Do the experiments support the claims?
- Yes, for the central thesis that careful data design and stageâ€‘wise rebalancing can make a small model broadly competitive:
  - The staged schedule yields stepâ€‘wise capability improvements (Table 3 and Table 8).
  - New datasets demonstrably outperform prior open alternatives in targeted ablations (Figure 1; Table 2).
  - Final base/instruct comparisons show clear strengths on knowledge/reasoning and instruction following (Tables 4â€“5).
- Mixed results:
  - Math and coding are improved but still trail Qwen2.5 on some benchmarks; ICL under HELMET is notably weaker (Table 11).

## 6. Limitations and Trade-offs
- Heavy reliance on synthetic labeling and filtering
  - Many classifiers are trained on labels from large proprietary/open models (e.g., Llamaâ€‘3.1â€‘70Bâ€‘Instruct; Section 3.3.2; Appendix Câ€“D). This may encode their biases and stylistic preferences into the datasets.
- Manual, â€œonlineâ€ mixture tuning
  - Rebalancing is humanâ€‘inâ€‘theâ€‘loop and guided by observed metrics (Section 4). While effective, it can be subjective and less reproducible than a fully automated policy.
- Compute and data scale
  - The model is â€œovertrainedâ€ on 11T tokens (Sections 4, 4.7), deviating from Chinchillaâ€‘style computeâ€‘optimality; this is expensive even for small models (~$250k compute). Others may find it hard to replicate.
- Instability episode
  - A loss spike occurs in Stage 3 that does not have a clear cause even after rewind/skip attempts (Section 4.4), indicating some brittleness to mixture changes.
- Domain/language scope
  - The focus is primarily English text, code, and math; multilingual coverage is not a goal in this release (Sections 3â€“5).
- Evaluation coverage
  - Zeroâ€‘/fewâ€‘shot leaderboards are informative but not exhaustive; realâ€‘world application robustness, safety beyond Llamaâ€‘Guard filtering, and longâ€‘tail reasoning behaviors are not deeply audited here.
- ICL performance
  - HELMET shows weaker inâ€‘context learning (ICL 23.2) compared to peers (Table 11), which may matter for promptâ€‘only adaptation use cases.

## 7. Implications and Future Directions
- Field impact
  - SmolLM2 demonstrates that small models can extract significant capability from data quality, curation, and schedule aloneâ€”without architectural novelty or parameter growth. The public release of `FineMath`, `Stackâ€‘Edu`, and `SmolTalk` gives the community highâ€‘leverage building blocks (Sections 3 and 5).
- What this enables next
  - Automated mixture controllers: replace manual rebalancing with bandit/RL controllers trained to optimize heldâ€‘out metrics during very long runs.
  - Data freshness and continual learning: extend the stageâ€‘wise framework to streaming or periodic updates where new specialized data is annealed in without catastrophic forgetting.
  - Multilingual extensions: apply the same dataâ€‘centric recipe to other languages and crossâ€‘lingual transfer.
  - Better math/coding specialization at small scale: explore curriculum schedules that interleave dataset difficulty (e.g., dynamic `FineMath4+` â†’ `3+` cycles) and toolâ€‘use integration for code.
  - ICL improvements: targeted pretraining on ICLâ€‘friendly corpora or synthetic ICL traces to close the HELMET ICL gap.
- Practical applications
  - Onâ€‘device assistants with strong instruction adherence (IFEval 56.7; Table 5).
  - Education and tutoringâ€”`FineMath` and `Stackâ€‘Edu` biases toward stepâ€‘byâ€‘step reasoning and pedagogical code are well aligned with tutoring use cases.
  - Lightweight coding helpers and math solvers in constrained environments, where slightly lower peak accuracy is acceptable for latency/privacy gains.
  - Retrievalâ€‘augmented systemsâ€”strong LongQA/RAG scores (Table 11) suggest good performance when paired with external knowledge.

Overall, SmolLM2 shifts the conversation from â€œsmall models are too limitedâ€ to â€œsmall models benefit disproportionately from the right data and schedule.â€ The resultsâ€”and the released datasetsâ€”should accelerate research on dataâ€‘centric training for efficient, capable LMs.

# ON-POLICY DISTILLATION OF LANGUAGE MODELS: LEARNING FROM SELF-GENERATED MISTAKES

**ArXiv:** [2306.13649](https://arxiv.org/abs/2306.13649)

## ğŸ¯ Pitch

This paper introduces Generalized Knowledge Distillation (GKD), a novel approach for distilling large language models by training student models directly on their own generated outputs, using teacher feedback to correct mistakes in real time. By addressing the critical trainâ€“inference distribution mismatch that limits prior distillation methods, GKD achieves substantial improvements across summarization, translation, and reasoning tasks, and can be seamlessly integrated with reinforcement learning fine-tuningâ€”making it a transformative solution for crafting efficient and high-quality language models ready for real-world deployment.

---

## 1. Executive Summary
This paper introduces Generalized Knowledge Distillation (`GKD`), a simple but powerful way to distill large language models into smaller ones by training on the student modelâ€™s own generated outputs (onâ€‘policy) while using the teacherâ€™s tokenâ€‘level probabilities as feedback. By addressing the core trainâ€“inference mismatch that plagues prior distillation methods, `GKD` delivers large, consistent gains across summarization, translation, and reasoning, and it integrates cleanly with reinforcement-learning (RL) fine-tuning.

## 2. Context and Motivation
- The specific gap:
  - Distilling auto-regressive language models typically uses a fixed set of sequences: either groundâ€‘truth outputs (â€œsupervised KDâ€) or sequences produced by the teacher (â€œsequenceâ€‘level KDâ€). During inference, however, the student conditions on its own previous tokens, not those from ground truth or the teacher. This causes a trainâ€“test distribution mismatch (also known as exposure bias in sequence generation) that cascades errors across tokens. Section 3 explains this mismatch and links it to imitation learning; Figures 1â€“3 empirically show the limits of existing KD methods.
- Importance:
  - Compressing large models cuts inference cost and memory footprint without sacrificing quality. Addressing the trainâ€“test mismatch is key to making distilled models reliable for real deployments (e.g., summarization, translation, reasoning).
- Prior approaches and shortcomings:
  - Supervised KD (Eq. 3) uses teacher tokenâ€‘level probabilities on a fixed dataset and ignores the studentâ€™s own trajectories.
  - Sequence-level KD (Kim & Rush, 2016) trains on teacher-generated sequences but still uses a fixed set of outputs.
  - Mixed approaches like ImitKD and fâ€‘distill partly use student data but do not fully commit to onâ€‘policy training nor explore alternative divergences beyond the standard forward KL. See comparisons in Figures 2, 6, 9, and A.15.
- Positioning:
  - `GKD` reframes distillation for auto-regressive models as onâ€‘policy imitation learning with an interactive expert (teacher). It unifies supervised and onâ€‘policy distillation in one objective, allows different divergences (forward KL, reverse KL, generalized Jensenâ€‘Shannon), and plugs directly into RL fineâ€‘tuning (Section 3.2; Eq. 5).

## 3. Technical Approach
At a high level: train the student on the sequences it actually generates, and use the teacher to provide tokenâ€‘level guidance on those exact sequences.

- Preliminaries (Section 2)
  - Autoâ€‘regressive generation: for input `x` and token sequence `y = (y1, â€¦, yL)`, the model predicts nextâ€‘token distributions `p(Â· | y<n, x)`.
  - Teacher and student: `p_T` and `p_S`, with student parameters `Î¸`. The tokenâ€‘wise discrepancy between teacher and student for a sequence `y` is averaged over positions (Eq. 2).
  - Divergences:
    - `forward KL`: `DKL(p_T || p_S)` encourages covering the teacherâ€™s full support (modeâ€‘covering).
    - `reverse KL`: `DKL(p_S || p_T)` is modeâ€‘seeking (focuses on teacherâ€™s highâ€‘mass tokens).
    - `JSD(Î²)` (Eq. 1): a bounded divergence that smoothly interpolates between forward (Î²â†’0) and reverse KL (Î²â†’1). This flexibility matters when the student cannot match the teacherâ€™s full distribution.

- Baseline objectives (Section 3)
  - Supervised fineâ€‘tuning: maximum likelihood on ground truth.
  - Supervised KD (Eq. 3): minimize `DKL(p_T || p_S)` at each token on a fixed dataset of (x, y).
  - Sequenceâ€‘level KD: maximize student likelihood of teacherâ€‘generated sequences (a form of supervised FT on the teacherâ€™s outputs).

- Onâ€‘policy distillation (Section 3.1; Eq. 4)
  - Key idea: generate outputs from the student itself and then match the teacherâ€™s tokenâ€‘level probabilities on those exact (potentially erroneous) trajectories.
  - Objective:
    - `LOD(Î¸) = E_x E_{y ~ p_S(Â·|x)} [ D(p_T || p_S)(y | x) ]` (Eq. 4),
    - where gradients do not backpropagate through the student sampling process, keeping training stable and efficient (no REINFORCE variance).
  - Why it works: the student receives targeted feedback exactly where it errs (its own partial sequences `y<n`), directly addressing trainâ€“inference mismatch.

- Generalized KD (Section 3.1; Algorithm 1)
  - Mixture of supervised and onâ€‘policy data:
    - `LGKD(Î¸) = (1 âˆ’ Î») E_{(x,y)âˆ¼(X,Y)} [ D(p_T || p_S)(y|x) ] + Î» E_x E_{y ~ p_S(Â·|x)} [ D(p_T || p_S)(y|x) ]`
    - `Î» âˆˆ [0, 1]` controls the student data fraction. `Î»=0` recovers supervised KD; `Î»=1` is purely onâ€‘policy.
  - Algorithm 1 in practice:
    1) Sample a minibatch of inputs.
    2) With probability `Î»`, generate outputs `y` from the student; otherwise, use outputs from the fixed dataset (ground truth or teacherâ€‘generated).
    3) For every token position, compute the chosen divergence `D` between teacher and student distributions.
    4) Take a gradient step on `Î¸` with respect to this loss.
  - Choice of `D`:
    - Forward KL encourages coverage but can waste capacity on lowâ€‘probability teacher tokens (potentially increasing hallucination).
    - Reverse KL and highâ€‘Î² JSD are more modeâ€‘seeking, improving quality at the cost of diversity.
    - The paper shows the best `D` is taskâ€‘ and samplingâ€‘temperatureâ€‘dependent (Figures 4, 6, 7, 10).

- RL fineâ€‘tuning + onâ€‘policy GKD (Section 3.2; Eq. 5)
  - When optimizing a sequenceâ€‘level reward `r(y)`, combine policy optimization with GKD regularization:
    - `E_x[(1âˆ’Î±) E_{y ~ p_S}[r(y)] âˆ’ Î± E_{y ~ p_S} D(p_T || p_S)(y|x)]` (Eq. 5).
  - `Î±` trades off reward maximization and distillation strength.
  - Practical note: if you already run RLHF/RLAIF, you can add GKD with reverse KL or highâ€‘Î² JSD with minimal changes (Remark in Section 3.2).

- Implementation essentials
  - Teacher: `T5â€‘XL` (~3B params) fineâ€‘tuned per task; Students: `T5â€‘Small/Base/Large` (77M/250M/800M) or `FLANâ€‘T5` variants for instruction tuning and reasoning (Section 4; Appendix A.2).
  - Training lengths: typically 40K steps (XSum, GSM8K), 100K (WMT), 50K (FLAN instruction tuning), with Adafactor optimizer; details in Tables A.1â€“A.4.
  - Compute overhead of onâ€‘policy sampling (Appendix A.2): roughly 1.8Ã—â€“2.2Ã— over using a fixed dataset, but serving-time benefits usually dominate total cost.

## 4. Key Insights and Innovations
- Onâ€‘policy distillation for autoâ€‘regressive LMs
  - Novelty: train the student on its own trajectories and ask the teacher for tokenâ€‘wise guidance at the exact states it will visit at inference (Eq. 4; Algorithm 1). This directly tackles exposure biasâ€”a core failure mode of prior distillation.
  - Significance: consistently better quality across tasks and model sizes (Figures 1â€“3, 6â€“9, 10).

- Generalized objective that unifies KD variants
  - Novelty: a single formulation (mixture over data sources `Î»` + choice of divergence `D`) that subsumes supervised KD, SeqKD, and prior â€œmixedâ€ methods (Section 3.1).
  - Significance: flexibility to adapt to capacity limitations (choose `D`) and data availability (choose `Î»`).

- Divergence choice as a taskâ€‘dependent knob
  - Novelty: systematic comparison of forward KL, reverse KL, and `JSD(Î²)` (Eq. 1) under onâ€‘policy training.
  - Significance: clear qualityâ€“diversity tradeâ€‘offs (Figure 4); for some tasks and sampling temperatures, modeâ€‘seeking divergences (reverse KL/JSD with high Î²) yield better quality; for others or with greedy decoding, the choice matters less (Figures A.12â€“A.13).

- Seamless integration with RL fineâ€‘tuning
  - Novelty: a simple additive objective (Eq. 5) that improves factuality while preserving or enhancing task performance (Figure 5).
  - Significance: helps mitigate the â€œalignment taxâ€ by letting RL pursue a reward (e.g., entailment) while GKD transfers broad teacher competence.

## 5. Experimental Analysis
- Evaluation setup (Section 4)
  - Datasets and metrics:
    - XSum summarization: ROUGEâ€‘2 (Figure 1; details in A.3).
    - WMT14 enâ†’de translation: BLEU with beam search (Figure 1; A.5).
    - GSM8K arithmetic reasoning: exactâ€‘match accuracy with fewâ€‘shot chainâ€‘ofâ€‘thought (CoT) and calculator (Figures 1, 7â€“9; A.4).
    - Taskâ€‘agnostic instruction tuning (FLAN2021): evaluate on heldâ€‘out MMLU (57 tasks) and BBH (23 tasks) with fewâ€‘shot accuracy (Figure 10; A.6).
  - Baselines:
    - Supervised FT, Supervised KD (Eq. 3), SeqKD, ImitKD, fâ€‘distill; all start from the same supervised FT student checkpoint.

- Main results (Figures 1, 2, 6, 7, 9, 10; text in Section 4)
  - Overall gains:
    > Figure 1 and Section 1 report that averaged over student sizes, onâ€‘policy GKD achieves relative improvements of about 2.1Ã— (XSum), 1.7Ã— (WMT), and 1.9Ã— (GSM8K) over the improvements obtained by baseline KD approaches.
  - Summarization (XSum):
    - Onâ€‘policy `GKD(JSD(0.9))` outperforms Supervised KD, SeqKD, ImitKD, and fâ€‘distill under both greedy and temperature sampling (Figure 2).
    - Data efficiency: with only 5% of training inputs and no groundâ€‘truth summaries, onâ€‘policy GKD surpasses Supervised KD and ImitKD trained on the full dataset (Figure 3).
    - Divergence vs diversity: at higher sampling temperatures, modeâ€‘seeking divergences (reverse KL/JSD(0.9)) improve ROUGEâ€‘2 but reduce diversity (Selfâ€‘BLEU rises); differences shrink under greedy decoding (Figure 4; A.12â€“A.13).
    - RL + GKD for factual consistency: using entailment reward and varying `Î±`, the method traces a Pareto frontierâ€”higher `Î±` increases ROUGEâ€‘2 but reduces entailment gains; the combined method achieves higher ROUGEâ€‘2 than RLAIFâ€‘style regularization while being more factually consistent than the teacher (Figure 5).
  - Machine translation (WMT14 enâ†’de):
    - Teacher (T5â€‘XL) BLEU â‰ˆ 28 with temperature 1.0; T5â€‘Small starts at 25.58, T5â€‘Base at 26.98 (Section 4.2).
    - Onâ€‘policy `GKD(JSD(0.1))` consistently beats supervised and mixed variants (Figure 6). Gains shrink with larger students, but onâ€‘policy remains best. Figure A.15 shows larger improvements over ImitKD and fâ€‘distill (53% and 162% higher BLEU improvement on average).
  - Arithmetic reasoning (GSM8K):
    - Setup: 4â€‘shot CoT prompting; teacher FLANâ€‘T5â€‘XL reaches 27.9% accuracy with greedy decoding (Section 4.3).
    - Results: onâ€‘policy GKD outperforms Supervised KD, SeqKD, ImitKD, and fâ€‘distill across all student sizes (Figure 9). Forward KL and reverse KL both work well; using only studentâ€‘generated CoTs beats mixing with fixed CoT datasets (Figure 7; A.14).
    - Onâ€‘policy fraction matters: accuracy improves monotonically once the studentâ€‘data fraction `Î»` exceeds â‰ˆ25% (Figure 8).
  - Taskâ€‘agnostic instruction tuning (FLAN2021 â†’ evaluate on MMLU, BBH):
    - Onâ€‘policy GKD with reverse KL yields the strongest improvements: 
      > Figure 10 shows absolute gains of roughly +2% on MMLU and +1% on BBH over the already instructionâ€‘tuned student baseline (teacher: 52.4% MMLU, 41% BBH; student: 35.6% MMLU, 31.25% BBH).
    - Reverse KL likely helps the student focus on the core behavior specified by instructions (Section 4.4).
  - Selfâ€‘distillation (Appendix A.1):
    - Even when teacher and student have the same architecture (FLANâ€‘T5â€‘Large on GSM8K), onâ€‘policy GKD improves over the teacher; onâ€‘policy variants outperform Supervised KD (Figure A.11).

- Do the experiments support the claims?
  - Yes: Across four task families with diverse metrics, `GKD` is consistently better than standard KD baselines and prior mixed methods. Extensive ablations show the benefit of onâ€‘policy data (`Î»>0`) and clarify when different divergences help (Figures 2â€“4, 6â€“8, A.12â€“A.15). The RL integration shows a clear qualityâ€“factuality tradeâ€‘off controlled by `Î±` (Figure 5).

- Notable ablations, robustness, and caveats:
  - Divergence vs sampling temperature: qualityâ€“diversity tradeâ€‘offs (Figure 4).
  - Student data fraction `Î»`: onâ€‘policy or mixed generally beats purely supervised; performance improves with higher `Î»`, especially beyond 25% (Figures 6â€“8).
  - Compute overhead: onâ€‘policy sampling is â‰ˆ1.8â€“2.2Ã— costlier than using a fixed dataset (Appendix A.2), though inference/serving dominates total costs in practice.

## 6. Limitations and Trade-offs
- Assumptions and prerequisites:
  - The student should be reasonably capable before distillation (typically after supervised FT), so that its onâ€‘policy outputs are meaningful for teacher feedback (Remark in Section 3.1).
  - Access to teacher logits (tokenâ€‘level probabilities) is required for every token of every training sequence; this may be unavailable for proprietary teachers.

- Sensitivity and task dependence:
  - The best divergence (`forward KL`, `reverse KL`, `JSD(Î²)`) depends on task and decoding temperature (Figures 4, 6, 7, 10). This introduces an extra hyperparameter to tune.
  - Modeâ€‘seeking divergences can reduce output diversity (Figure 4), which may be undesirable in creative generation settings.

- Computational considerations:
  - Onâ€‘policy data collection increases training cost (1.8Ã—â€“2.2Ã—; Appendix A.2). The approach trades training efficiency for improved inference reliability and quality.

- Evaluation scope:
  - Summarization quality and factuality rely on automatic metrics (ROUGEâ€‘2; entailment scores from a T5â€‘XXL NLI model in Figure 5). Human evaluations are not reported.
  - RL experiments use textual entailment as a proxy reward; optimizing this may not capture all aspects of factuality or faithfulness.

- Open questions:
  - No formal guidance on how to schedule `Î»` over training or how to adaptively choose the divergence per task/instance.
  - Theoretical understanding of why particular divergences win under certain temperatures or capacities remains empirical.

## 7. Implications and Future Directions
- How this changes the landscape:
  - `GKD` reframes distillation for autoâ€‘regressive LMs as onâ€‘policy imitation, closing the gap between training and inference. This is a conceptual and practical shift that unifies old KD variants and aligns distillation practices with how the student is actually used.
  - The clean integration with RLHF/RLAIF suggests a path to reduce the â€œalignment taxâ€ by combining reward optimization with distillation-based capability preservation (Section 3.2; Figure 5).

- Practical applications:
  - Deploy smaller, cheaper models for summarization, translation, and reasoning with quality closer to larger teachers.
  - Improve instructionâ€‘tuned models in a taskâ€‘agnostic way (Figure 10), benefiting generalâ€‘purpose assistants.
  - Enhance speculative decoding by aligning draft and target models via GKD (related applications noted in Section 5).

- Followâ€‘up research:
  - Adaptive selection/scheduling of `Î»` and divergence `D` during training based on online signals (e.g., disagreement or uncertainty).
  - Sequenceâ€‘level divergences and credit assignment that remain stable without backpropagating through sampling.
  - Extending onâ€‘policy GKD to other autoâ€‘regressive domains (audio, video, textâ€‘toâ€‘image) as suggested in the conclusion.
  - Humanâ€‘inâ€‘theâ€‘loop evaluation of quality/diversity/factuality tradeâ€‘offs and exploration of safety/robustness impacts.

Overall, `GKD` offers a simple training loop (Algorithm 1) that squarely addresses a longâ€‘standing issueâ€”exposure bias in distillationâ€”while providing practical levers (`Î»`, divergence choice, RL integration) to adapt to capacity constraints and task goals.

# AGENTIC REINFORCED POLICY OPTIMIZATION

**ArXiv:** [2507.19849](https://arxiv.org/abs/2507.19849)

## ğŸ¯ Pitch

Agentic Reinforced Policy Optimization (ARPO) introduces a novel RL algorithm that dynamically branches rollouts at moments of high uncertainty immediately following tool calls, allowing large language model agents to better explore and internalize step-level tool-use behaviors. By adaptively targeting these critical decision points and attributing advantage credit with fine granularity, ARPO achieves state-of-the-art performance across 13 multi-turn reasoning benchmarksâ€”while using only half the tool-call budget of prior methods. This establishes a new paradigm for efficient, scalable alignment of LLM-based agents with real-world, dynamic, tool-rich environments.

---

## 1. Executive Summary
Agentic Reinforced Policy Optimization (ARPO) is a reinforcement-learning method for training multiâ€‘turn, toolâ€‘using languageâ€‘model agents. It adapts the rollout process to branch precisely at highâ€‘uncertainty moments after tool calls and updates the policy so that shared versus branched tokens receive appropriate advantage credit. Across 13 benchmarks in math, knowledge reasoning, and deep search, ARPO outperforms trajectoryâ€‘level RL baselines while using roughly half the number of tool calls during training (Figure 1 right; Figure 7).

## 2. Context and Motivation
- Problem addressed
  - Modern reinforcement learning with verifiable rewards (RLVR) has unlocked strong singleâ€‘turn reasoning in LLMs. However, real tasks often require multiâ€‘turn interactions with external tools (search engines, browsers, code execution), where the agent must decide when and how to call tools over long horizons (Â§1â€“Â§2.1).
  - Existing â€œtrajectoryâ€‘levelâ€ RL methods (e.g., GRPO, DAPO, REINFORCE++; Â§1, Â§2.1, Â§4.2) sample complete trajectories and compare whole outputs with final rewards. They underâ€‘explore fineâ€‘grained, stepâ€‘level behaviors inside toolâ€‘use loops, especially right after tool feedback arrives.

- Why it matters
  - In multiâ€‘turn settings, tools provide frequent, informative feedback. Effective agents must exploit these signals at the right steps rather than only optimizing endâ€‘toâ€‘end sequences. This has practical importance for search, research assistance, and codeâ€‘assisted reasoning (Â§1, Â§2.3).

- Observed gap
  - A pilot analysis shows tokenâ€‘level predictive uncertainty (â€œtoken entropyâ€) spikes immediately after each tool call (Figure 1 left; Figure 2; Â§2.2). This indicates the model is most unsure right when external information is injectedâ€”precisely when exploration is most valuable. Trajectoryâ€‘level RL largely ignores this.

- Positioning
  - ARPO introduces an agentâ€‘specific RL algorithm that (a) detects highâ€‘entropy postâ€‘tool steps and branches sampling at those points, and (b) attributes advantages differently to shared versus branched tokens so the policy internalizes which stepâ€‘level behaviors were beneficial (Â§3).

## 3. Technical Approach
ARPO consists of two core components plus a theory justification (Figures 3â€“4; Â§3):

1) Entropyâ€‘based Adaptive Rollout (Â§3.1)
- Key idea in plain terms: Start several â€œglobalâ€ samples for a question. Each time the agent receives tool output, briefly generate a few tokens to measure uncertainty (â€œtoken entropyâ€). If uncertainty increased relative to the start, selectively branch more rollouts from that step to explore alternatives right where the model is unsure.
- Definitions
  - `token entropy` H_t measures uncertainty of the token distribution at step t:
    - H_t = âˆ’âˆ‘_j p_t,j log p_t,j, where p_t = Softmax(z_t / Ï„) (Eq. 3). Higher H_t means the modelâ€™s nextâ€‘token distribution is more spread out (more uncertain).
- Stepâ€‘byâ€‘step mechanics (Algorithm 1; Figure 4a)
  - Rollout initialization: With a rollout budget `M`, perform `N` global trajectory samples for a question q, and record the initial entropy of the first `k` tokens to form H_initial (Â§3.1 step 1).
  - Entropy variation monitoring: After each tool call at step t, append the toolâ€™s response to the context and generate `k` tokens to compute a stepwise entropy vector H_t. Compute normalized change Î”H_t = Normalize(H_t âˆ’ H_initial) (Eq. 4; Â§3.1 step 2).
  - Adaptive â€œbeamingâ€ (branching): Compute a branching probability
    - P_t = Î± + Î²Â·Î”H_t. If P_t > Ï„, create Z branched partial rollouts from that node; otherwise continue the current trajectory (Eq. 5; Â§3.1 step 3; Figure 4a).
  - Termination and budget control: Stop branching when the partialâ€‘sampling budget (Mâˆ’N) is consumed or when answers end. If branching used less than the budget, top up with extra global samples so total samples equal M (Â§3.1 step 4).
- Why this design
  - Entropy spikes immediately after tool outputs (Figure 1 left; Figure 2), indicating rich but uncertain decision points. Branching there increases behavioral diversity exactly where it matters.
  - Complexity: With global expansion size and tokens per trajectory n, the partial rollout reduces perâ€‘rollout complexity from O(n^2) (plain trajectory beaming) to between O(n log n) and O(n^2) (Â§3.1, footnote 2).

2) Advantage Attribution Estimation (Â§3.2)
- Intuition: When samples share a prefix and then branch, shared tokens should receive a shared â€œadvantageâ€ signal, and branched tokens should receive distinct signals based on their own outcomes (Figure 4b).
- Two realizations
  - Hard setting: For d trajectories that share a prefix, assign each branch i a normalized advantage Ã‚_i,t from its reward R_i; the shared prefix tokens get the average advantage over the d branches Ã‚_shared = (1/d)âˆ‘_i Ã‚_i,t (Â§3.2).
  - Soft setting (default): Use GRPO (Group Relative Policy Optimization; Eq. 6) with importance ratios r_i,t(Î¸) = Ï€_Î¸(y_i,t | x, y_i,<t) / Ï€_ref(y_i,t | x, y_i,<t). If two trajectories share a prefix at token t, their r_i,t are equal; thus shared tokens effectively receive the same credit, while postâ€‘branch tokens get different weights (Eq. 7; Appendix D.1 formalizes the equivalence).
- Empirical choice: The soft setting yields higher and more stable rewards during training (Figure 5), so ARPO defaults to it.

3) Hierarchical Reward and Implementation (Â§3.2, Eq. 8; Â§C)
- Rewards combine correctness, output format, and a small bonus r_M=0.1 for using multiple tools (both `<search>` and `<python>`) when the answer is correct and formatted (Eq. 8).
- Training excludes tool outputs themselves from loss to avoid bias (Â§C.2). Tools include: search engine, browser agent, and code interpreter (Â§2.3; Â§C.3).
- Evaluation answers are extracted from \box{} markers (Â§4.4).

4) Theoretical foundation (Â§3.3; Appendix D.2)
- Generalized Policy Gradient (GPG) Theorem (Eq. 9): Treat any contiguous output segment as a â€œmacro actionâ€ (e.g., from one tool call to the next). The policy gradient can be written as a sum over macro steps T:
  - âˆ‡_Î¸ J(Î¸) = E_Ï„ [ âˆ‘_T âˆ‡_Î¸ log Ï€_Î¸(MA_T | MS_T) Â· A_T(Ï„) ]
- This justifies optimizing with partial rollouts (macro segments) rather than only tokenâ€‘level actions, aligning with ARPOâ€™s branchâ€‘atâ€‘step design (Â§3.3; Appendix D.2).

Analogy
- Think of solving a puzzle while occasionally asking an expert. Right after the expert answers, uncertainty jumps: you may go in several directions. ARPO spends extra â€œtriesâ€ at just those moments, then learns which shared reasoning led to good branches and which branch choices paid off.

## 4. Key Insights and Innovations
- Entropyâ€‘triggered, stepâ€‘level exploration (fundamental)
  - Novelty: Uses tokenâ€‘entropy spikes after tool calls as a branching trigger (Figures 1â€“2; Â§3.1). Prior RL methods sample full trajectories uniformly or with generic beaming, missing these highâ€‘value moments.
  - Significance: Focuses exploration exactly where external information creates the biggest uncertainty, improving sample efficiency and behavior diversity.

- Advantage attribution that respects shared vs. branched tokens (principled, practical)
  - Novelty: Either explicit (hard) or implicit (soft GRPO) credit assignment so shared prefixes share credit while branches receive distinct credit (Eq. 6â€“7; Figure 4b; Appendix D.1).
  - Significance: Helps the model internalize which stepâ€‘level toolâ€‘use decisions improved outcomes, stabilizing training (Figure 5).

- Macroâ€‘action policy gradient for Transformer agents (theoretical)
  - Novelty: A generalized policy gradient (Eq. 9) that legitimizes partial rollouts over macro segments (Appendix D.2, Eq. 20â€“21).
  - Significance: Provides a formal foundation for stepâ€‘aware agent training instead of only trajectory/tokenâ€‘level updates.

- Toolâ€‘callâ€“efficient training (practical impact)
  - Observation: ARPO reaches higher accuracy with roughly half as many tool calls as GRPO during training (Figure 7), reducing cost for webâ€‘tool agents.

## 5. Experimental Analysis
- Evaluation setup (Â§4)
  - Tasks (13 datasets; Â§4.1)
    - Mathematical: AIME24, AIME25, MATH500, MATH, GSM8K.
    - Knowledgeâ€‘intensive QA: WebWalker, HotpotQA, 2WikiMultihopQA, Musique, Bamboogle.
    - Deep Search: GAIA, WebWalkerQA, Humanityâ€™s Last Exam (HLE), xbenchâ€‘DeepSearch.
  - Models and baselines (Â§4.2)
    - Direct reasoning: Qwen2.5, Llama3.1, Qwen3, plus large closed/open models (e.g., DeepSeekâ€‘R1â€‘671B, GPTâ€‘4o) for reference in Table 2.
    - Trajectoryâ€‘level RL: GRPO, DAPO, REINFORCE++.
    - Search agents (for Deep Search): Vanilla RAG, Searchâ€‘o1, WebThinker, ReAct.
  - Metrics (Â§4.4)
    - F1 for four Wikipedia QA tasks; for others, pass@1 with temperature 0.6 and topâ€‘p 0.95; LLMâ€‘asâ€‘Judge uses Qwen2.5â€‘72B-instruct. Answers are parsed from \box{}.
  - Training protocol (Â§4.3; Â§C)
    - Coldâ€‘start SFT on 54k Toolâ€‘Star data + 0.8k STILL for math; RL with 10k samples for deep reasoning and only 1k mixed hard search samples for deep search. Tooling via Bing search and a sandboxed Python interpreter; browser agent is used for deep search.
    - ARPO rollout hyperparameters include entropy weight, base probability Î±, threshold Ï„; examples: total rollout size M=16, initial N=8 (Â§C.2).

- Main quantitative results
  - Mathematical + Knowledgeâ€‘intensive (Table 1)
    - Qwen2.5â€‘3Bâ€‘Instruct: ARPO avg 52.8 vs GRPO 50.4, DAPO 50.6, REINFORCE++ 49.7.
    - Llama3.1â€‘8Bâ€‘Instruct: ARPO 55.3 vs GRPO 51.1 and DAPO 50.4.
    - Qwen2.5â€‘7Bâ€‘Instruct: ARPO 58.3 vs GRPO 56.5, DAPO 54.8, REINFORCE++ 54.9.
    - Prompting baseline (TIR) often underperforms or only slightly helps; e.g., on Llama3.1â€‘8B avg 36.3 vs direct 28.8, but far behind ARPO 55.3.
    - Quote:
      > Table 1 shows ARPO as the top method on all three backbonesâ€™ averages, with consistent gains across 10 datasets.
  - Deep Search (Table 2; Figure 6)
    - Qwen3â€‘8B: ARPO GAIA avg 38.8 vs GRPO 32.0; WebWalkerQA avg 30.5 vs 29.0; HLE avg 8.8 vs 7.8; xbench 25.0 vs 20.0.
    - Qwen3â€‘14B: ARPO GAIA avg 43.7 vs GRPO 36.9; WebWalkerQA 36.0 vs 30.0; HLE 10.0 vs 8.6; xbench 32.0 vs 27.0.
    - Against nonâ€‘RL search agents, ARPO is generally stronger; e.g., on Qwen3â€‘14B, ARPOâ€™s GAIA 43.7 beats WebThinker 33.0 and Searchâ€‘o1 30.1.
    - Reference models struggle on HLE (e.g., DeepSeekâ€‘R1â€‘671B 8.6; GPTâ€‘4o 2.6), while small ARPO models reach 10.0 (14B) and 8.8 (8B).
    - Scaling in sampling (Figure 6): ARPO improves Pass@3 and Pass@5. Notably:
      > Qwen3â€‘14B+ARPO achieves GAIA Pass@5 of 61.2%, HLE 24.0%, and xbenchâ€‘DR 59%.
  - Training toolâ€‘call efficiency (Figure 7)
    - Quote:
      > ARPO attains higher accuracy than GRPO while using about half the number of tool calls on Qwen2.5â€‘7B during RL training.
  - Browser ablation (Table 3)
    - No browser (snippets only) performs worst (e.g., Qwen3â€‘14B avg 24.8). A stronger browser agent (QWQâ€‘32B) improves averages further (up to 39.4 on 14B).
  - Hyperparameter scaling (Figure 8)
    - Best entropy weight around 0.4; increasing initial sampling size N up to 8 (with M=16) helps but N=16 hurts (it removes partial sampling); larger M improves performance.

- Do the experiments support the claims?
  - Yes, across backbones and domains ARPO beats trajectoryâ€‘level RL baselines, especially on deep search where stepâ€‘level tool use is critical (Table 2). The efficiency claim is substantiated by Figure 7. The entropyâ€‘driven exploration hypothesis is supported empirically by entropy spikes after tool calls (Figure 1 left; Figure 2) and by improved Pass@K diversity (Figure 6).

- Notable design checks
  - Robustness to tool quality: Browser ablations reveal performance correlates with browser strength (Table 3).
  - Reward shaping: A small multiâ€‘tool bonus (Eq. 8) nudges but does not dominate outcomes.
  - Choice of advantage estimator: Soft > Hard (Figure 5).

## 6. Limitations and Trade-offs
- Dependence on entropy signals
  - Assumes tokenâ€‘entropy increases are reliable markers of valuable exploration moments (Â§2.2). If tool outputs are noisy or misleading, entropy may spike for unhelpful reasons.
  - Requires tuning of Î±, Î², Ï„, Z, k, M, N (Figure 8). Wrong settings (e.g., allâ€‘global N=M) cancel ARPOâ€™s benefits.

- External tool availability and quality
  - Performance, especially in deep search, depends on search/browsing coverage and a capable browser agent (Table 3). Limited APIs or retrieval quality can bottleneck gains.

- Computational considerations
  - Although branching is targeted, partial rollouts still add decoding and entropy computation overhead. Complexity is between O(n log n) and O(n^2) per rollout (Â§3.1), and latency may grow with many highâ€‘entropy steps.

- Reward and judging assumptions
  - Some benchmarks rely on LLMâ€‘asâ€‘Judge (Qwen2.5â€‘72B) for accuracy (Â§4.4). This introduces potential evaluation bias and variance.
  - The multiâ€‘tool bonus (Eq. 8) may favor using both tools, which could be suboptimal in domains where one tool suffices.

- Scope
  - The work optimizes ruleâ€‘based RL with GRPOâ€‘style losses; it does not explore valueâ€‘based or modelâ€‘based RL variants for tool agents.
  - Focuses on textual tools (search, browser, Python). Extensions to richer environments (APIs with stateful effects, robotics) are not studied.

## 7. Implications and Future Directions
- Field impact
  - Shifts agent training from â€œwholeâ€‘trajectoryâ€ thinking to â€œstepâ€‘awareâ€ exploration guided by uncertainty. This reframes RL for LLM agents as a sequence of macro decisions aligned to toolâ€‘feedback points (Eq. 9), a useful perspective for many toolâ€‘rich tasks.

- Practical applications
  - Costâ€‘effective training of research assistants, web agents, and codeâ€‘augmented solvers: fewer tool calls during RL (Figure 7) reduce monetary/API costs while improving outcomes (Tables 1â€“2).

- Research opportunities
  - Adaptive thresholds and learned branching policies: replace handâ€‘tuned Î±, Î², Ï„ with learned controllers using entropy and other uncertainty proxies.
  - Broader uncertainty signals: combine token entropy with verifier disagreement, calibration, or retrieval confidence (beyond Eq. 3).
  - Valueâ€‘based or hierarchical RL: couple ARPOâ€™s partial rollouts with learned value estimates for macro actions; integrate planning over toolâ€‘use graphs.
  - Multiâ€‘agent/tool scheduling: extend advantage attribution to coordinated decisions across multiple tools or agents, possibly with credit assignment across longer horizons.
  - Robust evaluation: complement LLMâ€‘asâ€‘Judge with programmatic verifiers and human audits; study domain shift (languages, domains, lowâ€‘resource web).

> Bottom line: ARPO demonstrates that targeting exploration to postâ€‘tool highâ€‘uncertainty steps and crediting shared vs. branched tokens differently yields better, cheaper training for toolâ€‘using LLM agents, with both empirical wins (Tables 1â€“2, Figures 6â€“7) and a principled macroâ€‘action gradient foundation (Eq. 9; Appendix D.2).

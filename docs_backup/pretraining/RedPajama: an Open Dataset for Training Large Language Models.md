# RedPajama: an Open Dataset for Training Large Language Models

**ArXiv:** [2411.12372](https://arxiv.org/abs/2411.12372)
**Authors:** Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher RÃ©, Irina Rish, Ce Zhang
**Institutions:** Together Computer (and likely affiliated institutions of authors, including Stanford, UC Berkeley, etc.)

## ðŸŽ¯ Pitch

RedPajama revolutionizes open large language model training by providing two extensive pretraining datasets: RedPajama-V1, a detailed replication of LLaMA-1's data blend, and RedPajama-V2, a vast, multilingual â€˜web-onlyâ€™ corpus with fine-grained quality annotations. This transparency in data sources not only tackles the traditional scarcity and opacity in LLM training data but also empowers researchers to refine and standardize open model development, fostering reproducibility and innovation across the AI community.

---

## 1. Executive Summary
RedPajama introduces two large, fully open pretraining datasets for large language models (LLMs): `RedPajamaâ€‘V1`, a bestâ€‘effort reproduction of the original LLaMAâ€‘1 training mix, and `RedPajamaâ€‘V2`, a massive â€œwebâ€‘only, raw+signalsâ€ corpus spanning five languages with documentâ€‘level quality annotations. This work matters because it tackles three blocking issues for open LLMsâ€”lack of transparency about training data, limited access to sufficiently large corpora, and scarce reusable artifacts for data curationâ€”by releasing >100 trillion tokens plus detailed filtering signals, running ablations to show how to turn raw web into highâ€‘quality data, and training open models to validate the recipes.

## 2. Context and Motivation
- Gap addressed
  - Most stateâ€‘ofâ€‘theâ€‘art LLMs either do not disclose their pretraining data or do so at a very high level, which makes it difficult to reproduce results, study dataâ€‘quality tradeâ€‘offs, or build open models at scale. Section 1 identifies three data bottlenecks: transparency in curation, access to large, highâ€‘quality data, and availability of reusable curation artifacts.
- Importance
  - Realâ€‘world impact: Training data decisively shapes model behavior, safety, and capability; openness enables reproducibility and community iteration. Section 1 points to widespread model adoption that relies on unknown data mixtures.
  - Scientific significance: Without shared corpora and curation recipes, the field cannot systematically study how data composition and filtering affect downstream performance.
- Prior approaches and shortfalls
  - Curated composite corpora (e.g., The Pile) and cleaned webâ€‘only corpora (e.g., C4, RefinedWeb, FineWeb) advanced open training but typically ship either a prefiltered dataset or limited quality metadata. Table 1 compares open datasets along transparency, versatility, and scale; few provide both raw web at massive scale and the perâ€‘document quality signals needed to try many filtering strategies.
- Positioning
  - RedPajama provides:
    - `V1`: a transparent replication of the LLaMAâ€‘1 recipe (Table 2 lists the seven slices and 1.2T tokens), plus trained `RedPajamaâ€‘INCITE` models used to sanityâ€‘check the replication (Section 3).
    - `V2`: a gigantic Common Crawlâ€“based corpus with minimal preprocessing but rich quality signals (46 measures per document) so users can construct many highâ€‘quality subsets as â€œviewsâ€ of the raw web (Sections 4.1â€“4.2; Table 3).

## 3. Technical Approach
This work has two complementary data products and a validation track.

- RedPajamaâ€‘V1: open reproduction of the LLaMAâ€‘1 data mix (Section 3)
  - Whatâ€™s inside (Table 2; total ~1.2T tokens):
    - `CommonCrawl` (processed with `CCNet`: a pipeline that deduplicates within snapshots and assigns a â€œhead/middle/tailâ€ quality bucket using a Wikipediaâ€‘trained nâ€‘gram language model)
    - `C4` (a cleaned Common Crawl subset)
    - `GitHub` (Apache/BSD/MIT licensed code; filtered by file heuristics listed in Appendix C.1)
    - `Books` (PGâ€‘19; Books3 initially included then removed for copyright)
    - `ArXiv` (LaTeX source; cleaned to remove preambles, comments, bibliography)
    - `Wikipedia`
    - `StackExchange` (28 largest sites; HTML stripped; answers sorted by score)
  - Recreating LLaMAâ€‘1 involved filling underspecified steps (Table 10 summarizes â€œuncertaintiesâ€ and decisions):
    - Selected five English Common Crawl snapshots (2019â€“2023) and trained a `fastText` Wikipediaâ€‘references classifier to filter (Section 3.1).
    - Applied languageâ€‘specific preprocessing consistent with cited sources (e.g., arXiv LaTeX cleaning following [29]).
  - Validation by training models (`RedPajamaâ€‘INCITE`, Section 3.2):
    - Trained 3B and 7B parameter decoderâ€‘only models on the Summit supercomputer (Section 3.2.1).
    - Summit specifics constrained training: V100 GPUs donâ€™t support `bf16`, so they used `fp16` with loss scaling; reduced learning rates; 12â€‘stage pipeline parallelism for 7B, 6â€‘stage for 3B, and 2â€‘way tensor parallel for both; 512 nodes (7B) and 256 nodes (3B); 4M token global batch (Section 3.2.1).
    - Tokens seen: 800B (3B) and ~1.0T (7B) (Section 3.2.1).

- RedPajamaâ€‘V2: raw web text + perâ€‘document quality signals (Section 4)
  - Data acquisition and minimal processing (Section 4.1.1):
    - Includes text extracted from all 84 Common Crawl â€œWETâ€ snapshots between 2014 and April 2023; passed through `CCNet` to produce >100B documents.
    - Languages: English, German, French, Spanish, Italian.
    - Unlike common practice, retains `head`, `middle`, and `tail` perplexity buckets to preserve breadth.
  - Quality signals: how they work and what they measure (Section 4.1.2; Appendix D)
    - Natural language heuristics (Table 12): e.g., fraction of allâ€‘caps words, fraction of lines ending with ellipsis, stopword ratios, unigram entropyâ€”aimed at catching boilerplate or nonâ€‘linguistic text.
    - Repetitiveness (Table 14): fractions of characters in duplicated nâ€‘grams (5â€“10) and in the most frequent nâ€‘grams (2â€“4)â€”repeatâ€‘heavy pages often correlate with low informativeness.
    - Contentâ€‘based flags (Table 15): counts of blocklisted words (`LDNOOBW`) and domain blacklist categories (`UT1`) to help exclude NSFW or spammy sites.
    - ML heuristics (Table 13): 
      - `fastText` classifiers that score similarity to highâ€‘quality domains (Wikipedia pages, Wikipedia references, OpenWebText, books).
      - `DSIR` importance weights (Data Selection via Importance Resampling): logâ€‘likelihood ratios between bigram models of the target vs. source distributionsâ€”higher means more â€œtargetâ€‘like.â€
    - Deduplication signals:
      - Exact duplicates via `Bloom` filter (1% error rate), tracked by IDs; dedup proceeds snapshotâ€‘byâ€‘snapshot from newest to oldest (Section 4.1.2; footnote 6).
      - `MinHash` signatures for fuzzy deduplication at multiple Jaccard similarities (Appendix B.1.2 â€œMinhashesâ€).
    - Storage format binds signals to text spans: each signal is stored as triplets `[start, end, score]` pointing into the original text so both lineâ€‘level and documentâ€‘level features coexist (Appendix B.1.2 â€œQuality Signals Structureâ€).
  - Scale and composition (Section 4.2; Table 3):
    - 113.3B documents, 123.7T tokens estimated with the Mistral BPE tokenizer.
    - `head+middle` buckets: 32.8B docs, 50.7T tokens; after dedup: 20.8B docs, 30.4T tokens.
    - Typical length: tail ~850 tokens vs. head/middle ~1,500 tokens.
    - Perâ€‘language counts detailed in Table 3.
  - â€œVersatility by designâ€: The dataset is intentionally not prefiltered; instead, it provides the metadata needed to instantiate many different highâ€‘quality â€œviewsâ€ (Section 4 and Appendix B).

- Experimental ablations: how to turn raw V2 into good training sets (Section 4.3)
  - Models and training setup (Section 4.3.1):
    - Decoderâ€‘only LLaMAâ€‘2â€“style models at 468M and 1.6B parameters, sequence length 2048; 24 layers, 16 attention heads, MLP ratio 4.0. Hidden size 1024 (468M) and 2048 (1.6B).
    - Tokens: 100B (468M) and 350B (1.6B). AdamW; peak LR 5eâ€‘3 (468M) and 5eâ€‘4 (1.6B); cosine decay; 1% warmup.
    - Trained with OLMo framework using FSDP on H100s (Section 4.3.1 â€œHardware and Training Stackâ€).
  - Filtering recipes evaluated (Section 4.3.2; Tables 5â€“6):
    - â€œC4 rules,â€ â€œGopher rulesâ€ (a widely used set of web filtering heuristics), exact/fuzzy deduplication, ML heuristics (`fastText` vs `DSIR`), custom rules mixing Wikipedia perplexity and classifiers.
    - Two data scopes: a single 2023â€‘14 snapshot and a set of nine snapshots (2021â€‘49 to 2023â€‘14).

## 4. Key Insights and Innovations
- Open release of raw web + rich quality signals at unprecedented scale
  - Whatâ€™s new: `RedPajamaâ€‘V2` does not just ship a prefiltered corpus; it ships 46 perâ€‘document signals and dedup artifacts so practitioners can derive many datasets (Section 4.1.2; Appendix D.2.1). This contrasts with datasets like C4 or RefinedWeb that provide cleaned text but limited annotations. Table 1 lists V2 as â€œOpen Access + Open Code + Raw Data + Multilingualâ€ with 270 TB scale.
  - Why it matters: It enables rapid, principled exploration of filtering strategies without reâ€‘crawling or recomputing expensive diagnostics, supporting reproducible data science at web scale.

- Transparent, documented replication of LLaMAâ€‘1 data with explicit uncertainty resolution
  - Whatâ€™s new: Section 3.1 and Table 10 enumerate missing details in the original LLaMA recipe and document concrete choices (snapshots, classifier thresholds, code filtering rules). This level of documentation is rare for highâ€‘profile datasets.
  - Why it matters: It provides a stronger baseline for reproducibility and educates the community about sensitive steps (e.g., classifier training for Wikipedia references).

- Empirical evidence that â€œsignals + minimal processingâ€ can match or approach curated web datasets
  - Whatâ€™s new: With the 468M model, combining fuzzy deduplication and full `Gopher` rules on RPv2 yields competitive aggregate benchmark scoresâ€”second only to RefinedWeb on some aggregations and better rankâ€‘consistency across tasks (Table 5 and Appendix Tables 18â€“20).
  - Why it matters: It supports the premise that large, weakly processed web corpora can be shaped into strong training data using transparent, reusable signals.

- Practical training validation at scale on FP16â€‘only hardware
  - Whatâ€™s new: Section 3.2.1 details how to train multiâ€‘billionâ€‘parameter models on IBM Power9 + V100 with older software stacks, including pipeline/tensor parallel settings and lower LRs for FP16 stability.
  - Why it matters: It demonstrates workable recipes for environments where bf16 is unavailable, helping other teams working on constrained HPC systems.

## 5. Experimental Analysis
- Evaluation methodology (Section 4.3.1; Table 4)
  - Benchmarks: A carefully chosen set with good signal even for small models: ANLI, ARCâ€‘c/e, Winogrande, HellaSwag, LAMBADA, CoQA, MMLU (and subâ€‘domains), OpenBookQA, PIQA, PubMedQA, SciQ, SocialIQA, TruthfulQA.
  - Aggregation: Three metricsâ€”average accuracy, normalized average, and a rankâ€‘based scoreâ€”to avoid scale effects across tasks (Tables 5â€“6).
  - Perplexity probes: Validation perplexity on Paloma and The Pile (Table 5), following Dolmaâ€™s evaluation practice.

- Main quantitative results for RPv2 filtering (468M model; Table 5, with perâ€‘task in Tables 18â€“20)
  - Best overall filters on RPv2:
    - Single snapshot (2023â€‘14): â€œExact + Fuzzy dedup + Gopher (full)â€ achieves:
      > Aggregate BMâ€‘Eval: Avg 37.6, Norm Avg 0.160, Rankâ€‘Score 0.700; Pile ppl 24.9, Paloma ppl 34.5 (Table 5).
    - Nine snapshots: â€œExact + Fuzzy + Gopher (full)â€ achieves:
      > Avg 36.7, Norm Avg 0.149, Rankâ€‘Score 0.556; Pile ppl 43.8, Paloma ppl 63.9 (Table 5).
  - Comparisons against strong web datasets:
    - RefinedWeb: 
      > Avg 37.9 (best), Norm Avg 0.165 (best), Rankâ€‘Score 0.650 (Table 5); but perâ€‘task results show RefinedWeb lags RPv2+Gopher on specific tasks such as HellaSwag, LAMBADA, Winogrande, MMLU, and OpenBookQA (Appendix narrative below Table 5).
    - FineWeb and Dolmaâ€‘v1.7: RPv2 with Gopher generally equals or exceeds their aggregate scores (Table 5).
  - ML heuristics:
    - `fastText` vs. `DSIR` provide similar aggregate gains; neither clearly dominates across the board (Table 5).
  - C4 lineâ€‘level filters:
    - Reduce perplexity but have minor effect on the aggregate benchmark scores (Table 5).

- Results at 1.6B scale (Table 6, Tables 21â€“23)
  - RPv2 (full) with fuzzy dedup + Gopher (natlang only) + `Palmâ€‘Mix` classifier:
    > Aggregate BMâ€‘Eval Avg 47.9, Norm Avg 29.4, Rankâ€‘Score 0.089; Pile ppl 22.2, Paloma ppl 30.7 (Table 6).
  - RefinedWeb remains ahead overall at this scale:
    > Avg 52.0, Norm Avg 34.0, Rankâ€‘Score 0.139; Pile ppl 10.7, Paloma ppl 17.7 (Table 6).
  - Interpretation: With more training tokens (350B) and a larger model, curated datasets like RefinedWeb still lead, but RPv2 can be filtered to approach them on several tasks (Tables 21â€“23).

- Validation of V1 via RedPajamaâ€‘INCITE models (Section 3.2.2; Tables 7â€“9)
  - 3B model (800B tokens):
    > Outperforms GPTâ€‘Neo and Pythiaâ€‘2.8B on HELM by 3â€“5 points and on LM harness subsets by 2â€“7 points (Section 3.2.2). Table 7 shows, for example, LAMBADA 0.654 vs Pythiaâ€‘2.8B 0.647 and HELM avg 0.406 vs 0.377.
  - 7B base model (~1T tokens):
    > Trails LLaMAâ€‘7B by 4.1 points and Falconâ€‘7B by 1.0 point on HELM classic, particularly on â€œlogprobâ€â€‘style tasks; direct generation tasks are comparable (Section 3.2.2; Table 8 and Table 9).
  - Plausible causes: FP16 training constraints and unavoidable dataset recipe mismatches (Section 3.2.2; Table 10).

- Robustness and ablations (Section 4.3.2; Appendix tables)
  - The â€œGopherâ€ family of rules consistently improves results relative to unfiltered or lineâ€‘only recipes (Table 5).
  - Repetitionâ€‘focused Gopher filters help, but â€œnatlangâ€ components contribute even more (Table 5).
  - Using more crawls increases domain coverage but can worsen perplexity on curated validation sets unless combined with strong filtering and deduplication (contrast single vs nine snapshots in Table 5).

- Do the experiments support the claims?
  - The datasetâ€™s central claimâ€”â€œquality signals enable building highâ€‘quality subsets from raw webâ€â€”is supported by consistent gains from principled filters (Gopher, fuzzy dedup) and by competitive performance relative to curated datasets at 468M scale (Table 5).
  - At larger scale (1.6B), curated datasets still lead overall (Table 6), implying that filtering strategies for RPv2 can be further optimized.

## 6. Limitations and Trade-offs
- Assumptions and potential biases
  - ML heuristics rely on `bagâ€‘ofâ€‘words` features (`fastText`, DSIR) that emphasize surface statistics; these can bias selection toward â€œWikipediaâ€‘likeâ€ text and underrepresent other valuable domains (Section 4.1.2 notes known bias risks; cites [15]).
  - `CCNet`â€™s head/middle/tail buckets depend on a Wikipediaâ€‘trained language model; this assumes Wikipediaâ€‘like fluency/structure is the correct proxy for â€œqualityâ€ (Section 3.1 and 4.1.1).
- Scope not covered
  - No full decontamination analysis against evaluation benchmarks; no systematic assessment of personally identifiable information (Conclusion, Section 5).
  - V2 focuses on web text only; domains like code, scientific papers, or books must be sourced separately if needed (Section 4).
- Computational constraints
  - Validation models for V1 were trained on hardware that forced `fp16` and reduced LRs; this likely limited 7B performance (Section 3.2.1â€“3.2.2).
  - Ablations used relatively small models (468M and 1.6B) to explore many filters; results may not extrapolate linearly to 30B+ scales (Section 5).
- Data realities
  - Raw RPv2 includes harmful/offensive content; users must apply filtering signals appropriately (Section 4.1.2; Appendix D.2.1).
  - Deduplication is performed preâ€‘`CCNet` exact (footnote 6) and with `MinHash` for fuzzy matches; different parameterizations yield different tradeâ€‘offs between recall and false positives.

## 7. Implications and Future Directions
- How this changes the landscape
  - Provides a new â€œsubstrateâ€ for open LLM training: rather than prescribing one cleaned corpus, RPv2 makes it easy to iterate on filtering strategies at web scale. Table 1 positions RPv2 as unique on transparency + scale + versatility.
  - Already used to train several open models (Figure 1 highlights OpenELM, OLMo, Snowflake Arctic, and RedPajamaâ€‘INCITE), demonstrating practical value.
- Followâ€‘up research enabled
  - Dataâ€‘centric studies: optimize combinations of signals (natlang + repetition + ML) per task family; learn filters via differentiable or reinforcement learning approaches; compare DSIR vs. learned selectors beyond bagâ€‘ofâ€‘words.
  - Bias and safety audits using the released signals; build fairnessâ€‘preserving filters that retain underrepresented language and domain varieties.
  - Multilingual extension: RPv2 currently covers five languages (Table 3); extending signals (e.g., nonâ€‘English `Palmâ€‘mix` classifiers) could improve nonâ€‘English model performance.
  - Scaling laws for data curation: systematically study how dedup strength, snapshot mix, and quality thresholds interact with model size and training tokens.
- Practical applications
  - Enterprises and researchers can construct taskâ€‘specific corpora quickly (e.g., lowâ€‘toxicity customer support datasets, domainâ€‘specific pretraining for finance or biomedicine) by combining content whitelists with ML/heuristic signals (Appendix D lists all features).
  - Reproducible baselines for open LLM training, including the ability to replicate filtering pipelines and share exact dataset â€œviewsâ€ by referencing the released HTTP manifests (Appendix B).

> Availability and structure: Both datasets are downloadable via Hugging Face and public HTTPS endpoints; files are sharded JSONL with wellâ€‘documented schemas for documents, quality signals, duplicates, and minhashes (Appendix B.1.1â€“B.1.2).

In sum, RedPajama advances openness not just by releasing big data, but by releasing the metadata and ablation evidence needed to transform raw web into strong training sets. The experiments show clear benefits from principled filtering (especially Gopher rules plus fuzzy deduplication), with room to close remaining gaps to carefully curated datasets at larger model scales.

# Voxtral: Openâ€‘Weight Multimodal Audio Chat Models with Whisperâ€‘Based Encoding and Longâ€‘Context Reasoning

**ArXiv:** [](https://arxiv.org/abs/)
**Authors:** 
**Institutions:** 

## ğŸ¯ Pitch

Voxtral presents groundbreaking open-weight multimodal audio chat models, adept at handling up to 40 minutes of audio while mastering transcription, translation, and cross-modal reasoning. By combining Whisper-based encoding, strategic downsampling, and innovative dual-pattern pretraining, Voxtral delivers state-of-the-art speech comprehension, fostering advancements in privacy-preserving and domain-specific audio applications.

---

## 1. Executive Summary
Voxtral introduces two openâ€‘weights multimodal audio chat modelsâ€”`Voxtral Mini` (â‰ˆ4.7B parameters) and `Voxtral Small` (â‰ˆ24.3B)â€”that understand speech and text and answer in text. The models achieve stateâ€‘ofâ€‘theâ€‘art transcription and strong translation and speechâ€‘understanding accuracy while preserving textâ€‘only performance, and they can process up to 40 minutes of audio within a 32Kâ€‘token context (Sections 1â€“2; Table 1; Figure 3â€“6).

## 2. Context and Motivation
- Problem addressed
  - Building a single model that can both transcribe speech accurately and reason over long audio contexts (e.g., answer questions, summarize, translate), with open weights and strong text capabilities.
  - Evaluation gaps: widely used audio benchmarks emphasize transcription/translation; there is less coverage of QA and longâ€‘context reasoning across speech (Section 1, 3.4).

- Why it matters
  - Realâ€‘world use: customer calls, lectures, meetings, podcastsâ€”often long, multilingual, and requiring comprehension rather than just transcription.
  - Practical deployment: open weights under Apache 2.0 enable onâ€‘premise and edge use, privacyâ€‘preserving setups, and domain adaptation (Abstract; Conclusion).

- Prior approaches and limitations
  - ASR models like Whisper excel at transcription but not general instruction following or reasoning.
  - Some multimodal chat models are closed or optimized for short audio and narrow tasks; evaluation suites are not standardized for speech understanding (Section 1, 3.4).
  - Synthetic â€œspeech versionsâ€ of text tasks exist, but coverage and standardization are limited (Section 3.4).

- Positioning
  - Voxtral pairs a Whisperâ€‘based audio encoder with Mistral LLM backbones and introduces a training scheme that balances transcription alignment with crossâ€‘modal reasoning (Sections 2â€“3).
  - It also contributes new evaluation resources: speechâ€‘synthesized versions of GSM8K, TriviaQA, MMLU; and an internal Speech Understanding (SU) benchmark for longâ€‘context QA with an LLM judge (Section 3.4; Appendix A.2â€“A.4).

## 3. Technical Approach
Voxtral is a Transformerâ€‘based system comprising an audio encoder, an audioâ€‘language adapter, and a language decoder (Section 2; Figure 1).

- Audio processing and encoder (Section 2.1)
  - Input waveform â†’ `logâ€‘Mel spectrogram` with 128 Mel bins (a common timeâ€“frequency representation for audio).
  - The encoder is based on `Whisper largeâ€‘v3` and outputs embeddings at 50 Hz.
  - Whisperâ€™s receptive field is 30 seconds. For longer inputs, Voxtral:
    - Computes the spectrogram of the full audio.
    - Splits it into independent 30â€‘second chunks for the encoder.
    - Resets absolute positional encodings for each chunk.
    - Concatenates the resulting embeddings.
  - This is â€œchunkâ€‘wise attentionâ€: the encoder attends within each 30â€‘second window; crossâ€‘chunk reasoning is delegated to the language decoder after embeddings are concatenated.
  - Short audios are padded to the next multiple of 30 seconds. An ablation in Section 5.1 shows removing padding slightly hurts ASR (e.g., +0.5% WER on French in Figure 7), so padding is kept.

- Audioâ€‘language adapter (Section 2.2; 5.2)
  - Purpose: reduce the very long audio sequence length before feeding the decoder.
  - Implementation: an MLP downsampling layer applied to the encoder outputs.
  - Downsampling factor 4Ã— (50 Hz â†’ 12.5 Hz) is selected as the best tradeâ€‘off:
    - Minimal ASR degradation; improved speech understanding on Llama QA vs no downsampling (Figure 8).
    - Enables 40â€‘minute audios to fit within 32K tokens.

- Language decoders (Section 2.3; Table 1)
  - `Voxtral Mini` uses `Ministral 3B` backbone (edgeâ€‘friendly, 3.6B decoder params; total â‰ˆ4.7B).
  - `Voxtral Small` uses `Mistral Small 3.1 (24B)` backbone (22.9B decoder params; total â‰ˆ24.3B).
  - Text embeddings are learned; audio and text tokens are consumed jointly during decoding.

- Threeâ€‘phase training (Section 3)
  1) Pretraining (Section 3.1)
     - Goal: teach the decoder to align audio with text and to continue discourse across modalities.
     - Two data patterns built from segmented audioâ€“transcript pairs `(A_n, T_n)`:
       - `Audioâ€‘toâ€‘text repetition`: input `A_n` and target `T_n` (explicit ASR alignment).
       - `Crossâ€‘modal continuation`: interleave segments as `(A1, T2, A3, T4, â€¦)` so each audio segment is followed by the next text segment `T_{n+1}` (forces modalityâ€‘invariant continuation, like dialog/QA).
     - The model is told which pattern to follow using special tokens `<repeat>` and `<next>`.
     - â€œWarmâ€‘upâ€: initially freeze encoder and decoder; train only the adapterâ€”found beneficial for speech understanding (Section 3.1).
     - A variant trained only with repetition is released as `Voxtral Mini Transcribe` (ASRâ€‘focused).
  2) Supervised finetuning (SFT, Section 3.2)
     - Objective: keep or slightly improve ASR while teaching instruction following and speech understanding.
     - Data creation:
       - `Audio context + text query`: Use long audios (up to ~40 min) with transcripts; prompt a text LLM to generate diverse, speechâ€‘grounded Q/A, including retrieval (â€œneedleâ€‘inâ€‘haystackâ€) and reasoning; also create summarization and translation tasks.
       - `Audioâ€‘only input`: Convert text SFT datasets (incl. function calling) to speech via TTS; to avoid overfitting to TTS, mine genuine questions from longâ€‘form ASR corpora and pair them with LLMâ€‘generated text answers.
       - A special `transcribe mode` token removes the need for a text prompt for pure ASR.
  3) Preference alignment (Section 3.3)
     - Use `Direct Preference Optimization (DPO)`â€”a method that directly optimizes the policy to prefer better responses using pairwise comparisonsâ€”plus an `Online DPO` variant.
     - For each example, sample two candidate responses (temperature 0.5), replace the audio with its transcript, and score with a textâ€‘only reward model. This captures semantics and style even without raw audio and is simpler to deploy at scale.
     - Online DPO improved response quality most (Section 5.4; Table 2).

- Evaluation infrastructure and new benchmarks (Section 3.4; Appendix A.2â€“A.4)
  - `Speechâ€‘synthesized benchmarks`: Turn text tasks (GSM8K, TriviaQA, MMLU) into speech by rewriting nonâ€‘speakable parts (Appendix A.3 prompt) and synthesizing with diverse TTS voices; model outputs remain text, so standard scoring applies.
  - `Speech Understanding (SU) benchmark`: Inâ€‘theâ€‘wild audios up to 19 minutes; LLM judge grades candidate answers using the transcript as context with two metrics:
    - `LLM_JUDGE_SCORE`: 0/1 helpfulness.
    - `GRADE_LLM_JUDGE_SCORE`: 0â€“5 quality. Prompts and scoring rubric provided (Appendix A.4).

Definitions of less common terms used above:
- `Chunkâ€‘wise attention`: restrict selfâ€‘attention to fixed windows (here, 30 s) during encoding; crossâ€‘window integration happens later.
- `WER (Word Error Rate)`: lower is better; measures transcription errors normalized by reference length.
- `BLEU`: a translation quality metric; higher is better.
- `DPO/Online DPO`: preferenceâ€‘based alignment methods that optimize the model to rank better responses higher, with the â€œonlineâ€ variant collecting fresh samples from the current policy during training.

## 4. Key Insights and Innovations
- Balanced dualâ€‘pattern pretraining is crucial (Section 5.3; Figure 9)
  - Novelty: explicitly mixing `audioâ€‘toâ€‘text repetition` (ASR) and `crossâ€‘modal continuation` (reasoning) and signaling them with tokens `<repeat>/<next>`.
  - Evidence:
    - Training only on repetition yields strong ASR but â€œnearly zeroâ€ Llamaâ€‘QA performance.
    - Training only on continuation yields good Llamaâ€‘QA but â‰ˆ60% WER (nonâ€‘functional ASR).
    - A 50/50 mix keeps both strongâ€”this is not a minor tweak; it is the core mechanism that makes a single model good at both transcription and understanding.

- Adapter downsampling sweet spot at 12.5 Hz (Section 5.2; Figure 8)
  - Novelty: an MLP adapter reduces audio token rate by 4Ã— without losing accuracy and even improves QA.
  - Significance: makes 40â€‘min audio feasible in 32K context, and improves understanding (12.5 Hz beats 50 Hz on Llamaâ€‘QA by +1.5% absolute, Figure 8 right) with little ASR degradation.

- Practical chunking strategy for long audio with Whisperâ€™s 30s limit (Section 2.1)
  - Design: reset positional encodings per chunk and concatenate embeddings; functionally equivalent to chunkâ€‘wise attention and efficient for long inputs.
  - Importance: avoids retraining a longâ€‘context encoder and offloads discourse integration to the decoder.

- Preference alignment with transcriptâ€‘only reward improves helpfulness (Section 5.4; Table 2)
  - With `Online DPO`, Voxtral Small raises SU `LLM_JUDGE_SCORE` from 86.61% to 88.31% and `GRADE` from 4.16 to 4.38, though with a small regression on English shortâ€‘form WER.
  - This shows that transcriptâ€‘based rewards suffice to enhance dialog quality even for audio tasks.

- New evaluation resources for speech understanding (Section 3.4; Appendix A.2â€“A.4)
  - Synthesized GSM8K/TriviaQA/MMLU and an SU benchmark with long audios and LLM judging, filling a gap in standardized evaluation of speech comprehension and reasoning.

## 5. Experimental Analysis
- Setup: datasets, metrics, baselines (Sections 3.4â€“4; Appendix A)
  - ASR: English shortâ€‘form (LibriSpeech, GigaSpeech, VoxPopuli, Switchboard, CHiMEâ€‘4, SPGISpeech), English longâ€‘form (Earningsâ€‘21/22, segmented to 10â€‘minute for provider limits), multilingual sets (FLEURS, Common Voice 15.1, MLS); metric = WER (lower is better). Full task breakdown in Table 3 (English) and Tables 4â€“6 (multilingual).
  - Speech translation: FLEURS speech translation, metric = BLEU (higher is better); results in Figure 4 and Table 7.
  - Speech understanding: Llamaâ€‘QA, OpenBookâ€‘QA, plus synthesized MMLU/GSM8K/TriviaQA subsets; internal SU benchmark with LLM judge. Results in Figure 5 and Table 8; SU judge scores in Table 2.
  - Textâ€‘only: five standard text benchmarks (Figure 6; exact tasks not enumerated in the excerpt but compared to Mistral Small 3.1).
  - Baselines: Whisper largeâ€‘v3, ElevenLabs Scribe, GPTâ€‘4o mini (Audio/Transcribe), Gemini 2.5 Flash.

- Headline results
  - ASR: strong to stateâ€‘ofâ€‘theâ€‘art in shortâ€‘form and MCV
    - Figure 3 summary:
      > â€œVoxtral Small outperforms all open and closedâ€‘source models on English Shortâ€‘Form and MCV. Voxtral Mini Transcribe beats GPTâ€‘4o mini Transcribe and Gemini 2.5 Flash in every task.â€
    - Concrete numbers (Table 3, shortâ€‘form examples):
      - LibriSpeech Testâ€‘Clean WER: `Voxtral Small 1.53%` vs `Whisper largeâ€‘v3 1.84%`, `Scribe 1.80%`, `GPTâ€‘4o mini Transcribe 1.92%`, `Gemini 2.5 Flash 2.97%`.
      - LibriSpeech Testâ€‘Other: `3.14%` vs `3.66%` (Whisper), `3.44%` (Scribe), `4.70%` (GPTâ€‘4o mini), `6.15%` (Gemini).
      - SPGISpeech: `1.89%` vs `3.15%` (Whisper), `3.16%` (Scribe), `4.51%` (GPTâ€‘4o mini).
    - Longâ€‘form earnings calls (Table 3, 10â€‘min segments):
      - E21 10m WER: `Voxtral Small 9.55%` vs `Scribe 7.39%`, `Gemini 8.09%`, `Whisper 9.88%`.
      - E22 10m WER: `12.48%` vs `Scribe 9.16%`, `Gemini 10.80%`, `Whisper 13.07%`.
      - Takeaway: shortâ€‘form and MCV are clear wins; longâ€‘form ASR remains competitive but not SOTA.

  - Speech translation (Figure 4; Table 7)
    - Consistent SOTA among tested pairs; e.g., Table 7:
      - `frâ†’en`: `Voxtral Small 54.2 BLEU` vs `GPTâ€‘4o mini Audio 48.2`, `Gemini 42.0`.
      - `enâ†’fr`: `57.3` vs `52.7` (GPTâ€‘4o), `53.9` (Gemini).
      - `deâ†’en`: `56.6` vs `51.8` (GPTâ€‘4o), `39.4` (Gemini).

  - Speech understanding (Figure 5; Table 8)
    - Voxtral Small is competitive with closed models and surpasses GPTâ€‘4o mini Audio on 3/7 tasks (OpenBookâ€‘QA, MMLU*, AU Bench).
      - Table 8 examples:
        - OpenBookâ€‘QA: `Voxtral Small 88.4%` vs `GPTâ€‘4o mini Audio 83.7%` (Gemini 94.7%).
        - MMLU* (speechâ€‘synth): `74.3%` vs `72.6%` (GPTâ€‘4o).
        - AU Bench (internal SU): `86.6%` vs `80.0%` (GPTâ€‘4o), `88.6%` (Gemini).
      - On other synthesized tasks, Voxtral Small trails GPTâ€‘4o mini slightly (e.g., TriviaQA* 79.4% vs 83.7%; GSM8K* 89.7% vs 90.8%).

  - Textâ€‘only performance (Figure 6)
    - > â€œVoxtral Small performs comparably to Mistral Small 3.1, highlighting its strong text capabilities.â€
    - Implication: the audio additions and pretraining do not degrade textâ€‘only skills.

- Alignment gains vs. ASR tradeâ€‘off (Section 5.4; Table 2)
  - `Voxtral Mini`:
    - SU `LLM_JUDGE_SCORE`: 83.47% â†’ 85.59% with Online DPO; Grade: 3.92 â†’ 4.08; essentially no WER change (~6.78â€“6.79).
  - `Voxtral Small`:
    - SU `LLM_JUDGE_SCORE`: 86.61% â†’ 88.31% (Online DPO); Grade: 4.16 â†’ 4.38.
    - English shortâ€‘form WER slightly regresses: 6.31 â†’ 6.50 macro average. Hence the released default is the SFT model; an Onlineâ€‘DPO Small is planned (Section 5.4).

- Ablations and design validations
  - Padding study (Section 5.1; Figure 7): removing 30â€‘second padding hardly affects FLEURS English ASR but degrades French by ~0.5% WER; Llamaâ€‘QA is similar. Padding retained to maximize ASR.
  - Downsampling study (Section 5.2; Figure 8): 12.5 Hz (4Ã— downsampling) offers the best ASRâ€‘understanding tradeâ€‘off; 6.25 Hz harms ASR by >1% WER on FLEURS French.
  - Pretraining pattern ratio (Section 5.3; Figure 9): confirms the necessity of mixing repetition and continuation; either alone fails on the complementary task.

- Do the experiments support the claims?
  - Yes for shortâ€‘form ASR and translation (clear numerical wins), and broadly for speech understanding where results are competitive with carefully chosen baselines and mixed across tasks (Table 8, Figure 5).
  - The ablations make the design choices traceable to measured tradeâ€‘offs, not adâ€‘hoc.

## 6. Limitations and Trade-offs
- Modeling and data assumptions
  - The encoder only sees 30â€‘second chunks; fine crossâ€‘chunk acoustic effects must be integrated downstream by the decoder. This could miss prosodic dependencies spanning longer than 30 seconds (Section 2.1).
  - Short inputs are padded to 30 seconds to preserve ASR quality (Section 5.1), increasing compute/time for very short utterances.
  - Synthetic data is heavily used for SFT: QA pairs, summarization, translations, and TTS prompts (Section 3.2). While mitigated by including genuine human questions, the distribution mismatch with real spontaneous speech may persist.

- Preference alignment signal is transcriptâ€‘only (Section 3.3)
  - The reward model never â€œhearsâ€ the audio; it cannot evaluate audioâ€‘specific qualities (emotion, speaker identity, tone) and relies on ASR transcriptions that may contain errors.

- Evaluation caveats
  - Longâ€‘form ASR results (Earningsâ€‘21/22) are not SOTA; the inputs were segmented to 10 minutes to fit closed providersâ€™ constraints (Appendix A.1), which changes the task slightly.
  - The SU benchmark uses an LLM judge. Although judged multiple times (10), LLMâ€‘asâ€‘judge can encode biases and may prefer certain linguistic styles.

- Computational footprint and latency
  - `Voxtral Small` totals â‰ˆ24.3B parameters (Table 1), plus a 640Mâ€‘parameter encoder; inference on 40â€‘minute audio even with 4Ã— downsampling remains resourceâ€‘intensive.
  - The 32K context still limits maximum audio length; beyond ~40 minutes, inputs must be truncated or summarized.

- Coverage and robustness
  - Some languages remain hard (e.g., Common Voice Arabic WERs are high across models; Table 5). Robustness to heavy accents, background noise, overlapping speech, and codeâ€‘switching is not deeply analyzed.

## 7. Implications and Future Directions
- How this changes the landscape
  - Provides strong, open, endâ€‘toâ€‘end speech understanding models with competitive performance to popular closed systems (Figures 3â€“5), plus a 32K context for long audio. This lowers the barrier to build privacyâ€‘preserving, onâ€‘device, or domainâ€‘specialized audio assistants.

- What it enables next
  - Domain adaptation: fineâ€‘tune Voxtral on callâ€‘center, medical, or legal audio without vendor lockâ€‘in.
  - Research on speech understanding evaluation: the released speechâ€‘synthesised suites and the SU judging framework can become standardized tests for longâ€‘context reasoning over audio (Section 3.4; Appendix A.2â€“A.4).
  - Alignment with audioâ€‘aware rewards: extend Online DPO with reward models that ingest audio features or confidence scores rather than transcripts only.
  - Streaming and lowâ€‘latency variants: replace fixed 30â€‘s chunking with true streaming encoders; explore learned downsampling or adaptive compression instead of fixed 4Ã—.
  - Multimodal function calling: the models natively support function calling with audio (Section 1, â€œPrimary contributionsâ€), suggesting agentic pipelines that trigger tools directly from spoken requests.

- Practical applications
  - Meeting assistants that can ingest a full 40â€‘minute meeting and answer queries or summarize across speakers.
  - Customerâ€‘support analytics: multilingual transcription, translation, and issue extraction with tool calls.
  - Education and accessibility: lecture Q&A, multiâ€‘language translation for hearingâ€‘impaired users.

Blockâ€‘quoted highlights supporting key claims:
- Contributions (Section 1):
  > â€œTwo openâ€‘weights audio models with stateâ€‘ofâ€‘theâ€‘art transcription and multilingual speech understanding for audio durations up to their 32K context window â€¦ Native function calling support with audio â€¦ Evaluation benchmarks that measure speech understanding and reasoning.â€

- Longâ€‘audio handling (Section 2.1):
  > â€œThe audio encoder processes the speech input, attending to 30â€‘second chunks of audio independently â€¦ embeddings computed from each chunk are concatenated â€¦ functionally equivalent to chunkâ€‘wise attention.â€

- Downsampling choice (Section 5.2; Figure 8):
  > â€œ12.5 Hz surpasses the 50 Hz baseline [on Llamaâ€‘QA] â€¦ Based on the tradeâ€‘off â€¦ we select 12.5 Hz as the optimal frameâ€‘rate.â€

- Alignment quality gains (Section 5.4; Table 2):
  > â€œFor both Mini and Small, DPO and Online DPO improve response quality â€¦ the online variant was more effective.â€

In sum, Voxtralâ€™s main technical advance is the integrated recipeâ€”chunked Whisper encoding, a 4Ã— downsampling adapter, balanced dualâ€‘pattern pretraining, and Online DPOâ€”that jointly delivers strong ASR and longâ€‘context speech reasoning in open models, along with useful evaluation artifacts to measure those capabilities.

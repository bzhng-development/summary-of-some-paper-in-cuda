# Qwen3-VL Technical Report

**ArXiv:** [2511.21631](https://arxiv.org/abs/2511.21631)

## üéØ Pitch

Qwen3-VL is a scalable vision‚Äìlanguage model family (dense and MoE variants) that natively ingests interleaved text, images, and video up to 256K tokens by combining interleaved-MRoPE positional encoding, DeepStack multi-layer vision‚Äìlanguage fusion, and text-based timestamping. This architecture and staged pre-/post-training pipeline preserves or improves pure-text capabilities while enabling state-of-the-art long-context multimodal reasoning, video grounding, and agentic GUI/workflow use‚Äîmaking it practical for real-world tasks that require faithful cross-modal understanding over very long documents and hours-long videos.

---

## 1. Executive Summary (2-3 sentences)

Qwen3-VL is a vision‚Äìlanguage model family (dense 2B/4B/8B/32B and MoE 30B-A3B/235B-A22B) designed to simultaneously improve multimodal reasoning (image/multi-image/video), preserve or improve pure-text capability, and scale to *native* ultra-long interleaved contexts up to `256K` tokens (Abstract; Section 1; Section 2). Its core significance is an architecture + training pipeline that targets long-context multimodal grounding (especially video) via `interleaved-MRoPE`, `DeepStack` multi-layer vision feature injection, and explicit textual timestamp tokens, backed by a multi-stage pretraining/post-training recipe that explicitly trains at 8K ‚Üí 32K ‚Üí 256K context windows (Sections 2‚Äì4; Table 1).

## 2. Context and Motivation

- **Problem / gap addressed**
  - Modern vision‚Äìlanguage models (VLMs) are expected to handle:
    - **Strong general multimodal reasoning** (STEM, visual math, puzzles).
    - **Long-context comprehension** over long documents and long videos.
    - **Agentic workflows** (GUI understanding, tool use).
  - A key tension highlighted is that **adding multimodal training must not degrade the underlying language model‚Äôs (LLM‚Äôs) text ability**; VLMs should ‚Äúmatch or surpass‚Äù their text-only counterparts on language benchmarks (Section 1).

- **Why this matters**
  - Many real workflows require *interleaved multimodal context* at long horizons: multi-page PDFs with figures/tables, videos up to hours long, and multi-step tool-augmented tasks (Section 1; Section 4.2.1).
  - Long-context + multimodal is particularly challenging because:
    - Positional encoding must represent **space (image layout)** and **time (video)** without collapsing at long ranges.
    - Training mixtures must balance text-only vs multimodal data without one dominating optimization (Section 1; Section 2.1; ‚Äúsquare-root reweighting‚Äù in Abstract/Section 1).

- **Prior approaches and shortcomings (as positioned here)**
  - The report frames Qwen3-VL as an evolution over prior Qwen VLMs:
    - `MRoPE` existed (Qwen2-VL), and Qwen2.5-VL used a unified positional encoding scheme for text + vision, but the earlier temporal/horizontal/vertical dimension chunking is described as causing **imbalanced frequency spectrum** and hurting long-video understanding (Section 1; Section 2.1).
    - Video temporal alignment via positional encodings in Qwen2.5-VL is described as producing **large/sparse temporal position IDs** for long videos and requiring costly training data coverage across frame rates (Section 2.3).

- **How Qwen3-VL positions itself**
  - It positions Qwen3-VL as a ‚Äúfoundational engine‚Äù that covers:
    1) stronger text-only understanding,
    2) robust 256K long-context interleaved multimodality,
    3) stronger multimodal reasoning (single/multi-image/video),
    enabled by architecture upgrades + long-context pretraining + bifurcated post-training into ‚Äúthinking‚Äù and ‚Äúnon-thinking‚Äù variants (Abstract; Section 1; Sections 3‚Äì4).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **three-part multimodal model** that converts images/videos into tokens and feeds them, interleaved with text tokens, into a large language model decoder for generation and reasoning (Section 2; Figure 1).
- It solves **long-context multimodal understanding** by combining (a) improved positional encodings for space/time, (b) stronger vision‚Üílanguage feature fusion, and (c) an explicit long-context training curriculum up to `256K` tokens (Sections 2‚Äì3; Table 1).

### 3.2 Big-picture architecture (diagram in words)

- **Input (text + images + video frames)**  
  ‚Üí **Vision encoder** (dynamic native resolution ‚Üí variable-length visual features/tokens)  
  ‚Üí **Vision‚Äìlanguage merger (MLP projection/compression)**  
  ‚Üí **LLM decoder backbone (Qwen3)** with:
  - `interleaved-MRoPE` positional encoding for multimodal tokens (Section 2.1),
  - `DeepStack` multi-layer vision token injection into early LLM layers (Section 2.2; Figure 1),
  - explicit **textual timestamp tokens** for video temporal grounding (Section 2.3).

### 3.3 Roadmap for the deep dive

- First explain the **base 3-module architecture** (vision encoder ‚Üí merger ‚Üí LLM) because everything else plugs into this pipeline (Section 2; Figure 1).
- Then cover the **three architecture upgrades**:
  1) `interleaved-MRoPE` (positional encoding fix for long video),
  2) `DeepStack` (multi-level vision features into LLM layers),
  3) **timestamp tokens** (explicit time grounding) (Sections 2.1‚Äì2.3).
- Next explain the **training curriculum**:
  - four-stage pretraining with increasing context length (Table 1; Section 3.1),
  - three-stage post-training (SFT ‚Üí distillation ‚Üí RL) and thinking vs non-thinking bifurcation (Section 4.1).
- Finally summarize **evaluation methodology and key results**, since the report provides many tables/benchmarks and a few targeted ablations (Section 5; Tables 2‚Äì12; Figure 3).

### 3.4 Detailed, sentence-based technical breakdown

**Framing:** This is primarily an *empirical systems + modeling report* that introduces architecture modifications and a large-scale training/evaluation pipeline for long-context multimodal foundation models (Sections 1‚Äì6).

#### 3.4.1 System/data pipeline diagram in words (what happens first, second, third)

1. **First, raw multimodal inputs are prepared and tokenized into an interleaved sequence** of text tokens plus visual tokens (images and video frames) that will fit within a target context length (up to `256K` tokens natively) (Abstract; Section 1; Table 1; Section 5.9 describes evaluation token/frame caps).
2. **Second, each image/video frame is encoded by a Vision Transformer (ViT)-style vision encoder** (initialized from SigLIP-2 checkpoints and continued-trained with dynamic resolutions), producing feature maps whose length depends on the input resolution (Section 2, ‚ÄúVision Encoder‚Äù).
3. **Third, a vision‚Äìlanguage ‚Äúmerger‚Äù projects/compresses vision features into LLM-compatible ‚Äúvisual tokens,‚Äù** using a two-layer MLP that compresses `2√ó2` visual features into one token aligned to the LLM hidden size (Section 2, ‚ÄúMLP-based Vision-Language Merger‚Äù).
4. **Fourth, the LLM decoder (Qwen3 backbone) consumes the interleaved token stream** (text tokens + visual tokens + timestamp tokens for video) and generates outputs for tasks like VQA, document parsing, grounding (boxes/points), code generation, or agent decisions (Sections 1‚Äì2; evaluation prompts in Appendix B).
5. **Fifth, during training, the loss is computed with a token-level reweighting scheme** (described as moving from per-sample to ‚Äúsquare-root-normalized per-token loss‚Äù) to balance text-only vs multimodal contributions (Section 1; Abstract).

#### 3.4.2 Model family and scaling choices (dense vs MoE, thinking vs non-thinking)

- **Dense variants:** `2B/4B/8B/32B` parameters (Section 1; Section 2).
- **MoE variants:** `30B-A3B` and `235B-A22B` (Section 1; Section 2).
  - In a `Mixture-of-Experts (MoE)` model, only a subset of parameters (‚Äúexperts‚Äù) are activated per token. The report specifies that the flagship `Qwen3-VL-235B-A22B` has `235B` total parameters with `22B` activated per token (Section 2). This is a standard latency/quality trade-off lever: total capacity is large, but compute per token is closer to the activated parameters.
- **Non-thinking vs thinking variants:** Post-training is ‚Äúbifurcated‚Äù into non-thinking vs thinking models, where thinking variants are trained with Chain-of-Thought (CoT) style data and are intended to perform stronger multimodal reasoning (Abstract; Section 1; Section 4.1).

#### 3.4.3 Vision encoder: dynamic resolution SigLIP-2 continuation training

- The report uses **SigLIP-2** as the base vision encoder architecture and continues training it with **dynamic input resolutions**, starting from official pretrained checkpoints (Section 2, ‚ÄúVision Encoder‚Äù).
- To handle variable resolutions:
  - It uses `2D-RoPE` (2D rotary position embedding) and **interpolates absolute position embeddings** based on input size, following `CoMP` (Section 2, ‚ÄúVision Encoder‚Äù).
- It states a default choice: **SigLIP2-SO-400M**, and **SigLIP2-Large (300M)** for small-scale LLMs (`2B` and `4B`) (Section 2, ‚ÄúVision Encoder‚Äù).
- **Missing details:** The excerpt does not provide ViT depth, patch size, embedding dimension, or exact training hyperparameters for the vision encoder (optimizer/LR/batch), so those cannot be reconstructed here.

#### 3.4.4 Merger: compressing vision features into ‚Äúvisual tokens‚Äù

- The merger is a **two-layer MLP** that compresses `2√ó2` visual features into a single token, mapped to the LLM hidden dimension (Section 2, ‚ÄúMLP-based Vision-Language Merger‚Äù).
- This serves two practical purposes:
  - **Compute/context control:** reduces the number of visual tokens that occupy context length.
  - **Interface alignment:** produces tokens that match the LLM‚Äôs internal representation size.

#### 3.4.5 Upgrade #1 ‚Äî `interleaved-MRoPE` for balanced spatial‚Äìtemporal positional encoding

- **What `MRoPE` is (paper-specific explanation):** `MRoPE` is a multimodal rotary positional embedding scheme that assigns positional rotation frequencies across multiple axes‚Äîtemporal (`t`), horizontal (`h`), and vertical (`w`)‚Äîso that the model can represent both text order and 2D/3D structure for images/video (Section 2.1).
- **Problem described:** Earlier `MRoPE` partitions embedding dimensions into separate `t/h/w` subspaces. The report argues this induces an **imbalanced frequency spectrum**, which harms long-video understanding (Section 2.1; also motivated in Section 1).
- **Solution:** `interleaved-MRoPE` **interleaves** the `t/h/w` components across embedding dimensions so each axis is represented across low- and high-frequency bands (Section 2.1).
- **Mechanism ‚Üí effect claim:** A more balanced spectrum ‚Äúmitigates spectral bias‚Äù and improves long-range positional modeling for video (Section 2.1). The report later connects long-context success to video understanding improvements (Section 5.9), though direct controlled experiments isolating interleaved-MRoPE are not shown in the provided ablations (the ablations shown are on Qwen3-ViT and DeepStack, plus Needle-in-a-Haystack).

#### 3.4.6 Upgrade #2 ‚Äî `DeepStack` cross-layer vision token injection

- **Goal:** improve **vision‚Äìlanguage alignment**, especially for fine-grained visual understanding (Section 1; Section 2.2; Table 12 commentary).
- **Baseline idea:** DeepStack (as referenced) is about ‚Äúdeeply stacking visual tokens‚Äù across layers, but Qwen3-VL adapts it differently (Section 2.2).
- **Qwen3-VL‚Äôs DeepStack variant:**
  - It extracts visual features from **intermediate ViT layers** (three distinct levels) instead of stacking multi-scale inputs (Section 2.2; Figure 1).
  - Dedicated merger modules project each level into visual tokens.
  - These multi-level visual tokens are then **added directly to the hidden states of the first three LLM layers** via residual-style injection (Section 2.2; Figure 1).
- **Why this might help (as argued here):** lower-level ViT features may retain edges/textures/layout; higher-level features capture semantics. Injecting multiple levels early could provide the LLM with a richer perceptual basis without increasing context length (Section 1; Section 2.2).

#### 3.4.7 Upgrade #3 ‚Äî explicit **text-based timestamp tokens** for video

- **Problem with positional-encoding-only time alignment:**
  1) Absolute-time positional IDs become ‚Äúexcessively large and sparse‚Äù for long videos, hurting understanding of long temporal contexts.
  2) Training would require broad sampling over frame rates, increasing data construction cost (Section 2.3).
- **Proposed approach:** prefix each temporal patch/group with a **timestamp expressed as a formatted text token**, e.g., `<3.0 seconds>` (Section 2.3).
  - Training uses both **seconds** and **HMS** format timestamps (hours:minutes:seconds) so the model learns multiple timecode styles (Section 2.3).
- **Trade-off:** This increases context length modestly, but aims to improve temporal grounding and timestamp-based tasks like video grounding and dense captioning (Section 2.3).

#### 3.4.8 Optimization / objective balancing ‚Äî square-root reweighting

- The report describes moving from **per-sample loss** to a **square-root-normalized per-token loss** so that text-only and multimodal data are better balanced during training (Section 1; Abstract).
- **Important missing detail:** The exact formula (symbols, normalization constants) is not included in the provided content, so the mechanism can only be described at a high level (reweight tokens to prevent one modality‚Äôs token mass from dominating gradients).

#### 3.4.9 Training pipeline: pretraining (4 stages) and post-training (3 stages)

**Pretraining (Section 3.1; Table 1)** is explicitly organized into four stages with increasing sequence lengths:

- **Stage S0 ‚Äî Vision‚ÄìLanguage Alignment (Merger-only training):**
  - Objective: bridge modality gap by training only merger parameters; vision encoder + LLM are frozen (Section 3.1).
  - Data: ~`67B` tokens of image-caption pairs, visual knowledge, OCR data (Section 3.1).
  - Sequence length: `8,192` tokens (Table 1).

- **Stage S1 ‚Äî Multimodal Pre-Training (All parameters unfrozen):**
  - Objective: end-to-end training on a large mixed corpus to preserve language ability (Section 3.1).
  - Token budget: ~`1T` tokens (Table 1).
  - Sequence length: `8,192` (Table 1).
  - Data mixture: VL + text-only; VL includes interleaved docs, grounding, VQA, STEM, small amount of video (Section 3.1).

- **Stage S2 ‚Äî Long-Context Pre-Training:**
  - Token budget: ~`1T` tokens (Table 1).
  - Sequence length: `32,768` tokens (Table 1).
  - Mixture shifts toward more text-only for long-form text, plus significantly larger video + agent instruction-following data (Section 3.1).

- **Stage S3 ‚Äî Ultra-Long-Context Adaptation:**
  - Token budget: `100B` tokens (Table 1).
  - Sequence length: `262,144` tokens (= 256K) (Table 1).
  - Data focuses on long-video and long-document understanding (Section 3.1).

**Post-training (Section 4.1)** has three phases:

1. **Supervised Fine-Tuning (SFT):**
   - Two phases: first at `32K`, then extended to `256K` focusing on long-document and long-video data (Section 4.1).
   - Data bifurcation:
     - non-thinking models use ‚Äústandard formats,‚Äù
     - thinking models use explicit Chain-of-Thought formats (Section 4.1).

2. **Strong-to-Weak Distillation:**
   - Distillation uses **text-only data** to fine-tune the LLM backbone, intended to improve reasoning across both text and multimodal tasks (Section 4.1).
   - The pipeline includes off-policy (teacher outputs) and on-policy distillation with KL divergence between student and teacher logits (Section 4.3).

3. **Reinforcement Learning (RL):**
   - Split into Reasoning RL and General RL (Sections 4.4.1‚Äì4.4.2).
   - Reasoning RL:
     - ~`30K` RL queries after filtering; sample 16 responses/query; filter out ‚Äúeasy‚Äù queries with >90% pass rate; mixed-task batches with tuned ratios (Section 4.4.1).
     - Uses `SAPO` (Soft Adaptive Policy Optimization) as the RL algorithm (Section 4.4.1).
   - General RL:
     - Multi-task RL over SFT domains; hybrid reward system combining rule-based and model-based judging (Section 4.4.2).
     - Explicit penalties for language mismatch (code-switching), repetition, formatting errors (Section 4.4.2).

#### 3.4.10 ‚ÄúThinking with Images‚Äù agentic training

- The report adds a specific two-stage training recipe for agent-like ‚Äúthink ‚Üí act ‚Üí analyze feedback ‚Üí answer‚Äù behavior (Section 4.5):
  - Stage 1: ~`10k` grounding examples, SFT on Qwen2.5-VL-32B + tool-integrated RL.
  - Stage 2: distill to ~`120k` multi-turn interactions, then apply similar cold-start SFT + tool-integrated RL for Qwen3-VL.
- Rewards include:
  - answer accuracy (judge: Qwen3-32B),
  - multi-turn reasoning quality (judge: Qwen2.5-VL-72B),
  - tool-calling reward to prevent reward hacking via single tool call (Section 4.5).

#### 3.4.11 Infrastructure and deployment

- Training infrastructure:
  - Alibaba Cloud PAI-Lingjun computing service (Section 4.6).
  - Hybrid parallelism with Megatron-LM: Tensor Parallelism (TP), Pipeline Parallelism (PP), Context Parallelism (CP), Expert Parallelism (EP), ZeRO-1 Data Parallelism (DP) (Section 4.6).
  - The report claims scaling ‚Äúup to 10,000 GPUs‚Äù with high utilization (Section 4.6).
- Deployment:
  - vLLM (PagedAttention) and SGLang for inference/evaluation, emphasizing throughput, memory efficiency, and structured generation (Section 4.6).

#### 3.4.12 Required ‚Äúcore configurations and hyperparameters‚Äù ‚Äî what is and isn‚Äôt specified here

The report provides some training/eval hyperparameters, but **many core training hyperparameters are not specified in the provided content**:

- **Specified:**
  - Pretraining stage token budgets + sequence lengths (Table 1).
  - Post-training SFT context lengths (32K then 256K) (Section 4.1).
  - RL sampling details (16 responses/query; 30K queries; filtering thresholds) (Section 4.4.1).
  - Text-centric evaluation sampling settings:
    - Instruct (large models 235B/32B/30B-A3B): temperature `0.7`, top-p `0.8`, top-k `20`, presence penalty `1.5` (Section 5.11).
    - Instruct (small models 8B/4B/2B): temperature `1.0`, top-p `1.0`, top-k `40`, presence penalty `2.0` (Section 5.11).
    - Thinking (MoE): temperature `0.6`, top-p `0.95`, top-k `20` (Section 5.11).
    - Thinking (dense): temperature `1.0`, top-p `0.95`, top-k `20`, presence penalty `1.5` (Section 5.11).
    - Max output length: `32,768` tokens; for AIME-25/HMMT-25/LiveCodeBench v6: `81,920` tokens (Section 5.11).
  - Video evaluation constraints:
    - cap `2,048` frames/video; total video tokens ‚â§ `224K`;
    - per-frame token cap: `768` for VideoMMMU/MMVU, `640` otherwise;
    - sampling fps: `4 fps` for Charades-STA, `2 fps` otherwise (Section 5.9).

- **Not specified (in the provided excerpt):**
  - Optimizer type (Adam/AdamW/etc.), learning rate schedule, weight decay.
  - Global batch size / micro-batch size, gradient clipping, training steps.
  - Tokenizer details, LLM architecture specifics (number of layers, hidden dimension, attention heads) for each model size.
  - Total compute budget in PF-days.
  
Given the instruction to avoid fabrication, these cannot be filled in beyond what is explicitly provided.

## 4. Key Insights and Innovations

1. **`Interleaved-MRoPE` to fix long-video positional encoding failure modes** (Section 2.1)
   - Novelty relative to the prior scheme described: instead of allocating separate embedding dimension blocks to `t/h/w`, it interleaves them to balance frequency coverage across axes.
   - Significance: directly targets a long-context degradation mechanism (‚Äúimbalanced frequency spectrum‚Äù) tied to long-video understanding.

2. **`DeepStack` adaptation: inject intermediate ViT layer features into early LLM layers** (Section 2.2; Figure 1; Table 12)
   - Novelty: uses intermediate ViT layers rather than multi-scale input token stacking; routes features to corresponding LLM layers via residual addition.
   - Significance: strengthens fine-grained perception and document VQA performance (Table 12 shows consistent gains in an ablation setting).

3. **Explicit textual timestamp tokens for video temporal grounding** (Section 2.3)
   - Novelty: replaces positional-encoding-only time alignment with human-readable timestamp tokens (seconds and HMS formats).
   - Significance: aims to reduce sparsity/scale issues of absolute time IDs and lower the burden of covering all fps regimes during training.

4. **Long-context training curriculum that actually trains at 256K tokens** (Table 1; Section 3.1)
   - Novelty (relative to many ‚Äúsupports long context‚Äù claims): the report explicitly includes a dedicated `262,144` sequence-length adaptation stage (S3) with `100B` tokens.
   - Significance: supports the paper‚Äôs emphasis on faithful long-range retrieval/cross-referencing in documents and videos (Abstract; Section 5.12.3).

5. **Post-training split into thinking vs non-thinking + increased post-training compute** (Abstract; Section 1; Section 4.1)
   - Novelty: operationalizes two product regimes (latency vs reasoning strength) while still training the same base family.
   - Significance: the evaluation tables repeatedly show different strengths for instruct vs thinking variants depending on task category (e.g., CharXiv reasoning subset favors thinking per Section 5.4 discussion; Table 2 shows many ‚Äúthinking‚Äù improvements but also cases where instruct is higher on specific tasks).

## 5. Experimental Analysis

### 5.1 Evaluation methodology (datasets, metrics, setup)

- The report evaluates across a broad suite of benchmarks spanning:
  - General VQA (MMBench, RealWorldQA, MMStar, SimpleVQA) (Section 5.1).
  - Multimodal reasoning/STEM/puzzles (MMMU, MathVista, MathVision, etc.) (Section 5.2; Appendix A).
  - Alignment/instruction following/hallucination (MM-MT-Bench, HallusionBench, MIA-Bench) (Section 5.3).
  - OCR & document understanding (DocVQA, InfoVQA, OCRBench, CC-OCR, OmniDocBench, CharXiv, MMLongBench-Doc) (Section 5.4; Table 2‚Äì4).
  - 2D/3D grounding + counting (RefCOCO, ODinW-13 with mAP@1.0 confidence, CountBench; Omni3D mAP@0.15) (Section 5.5).
  - Video understanding (VideoMME, MVBench, VideoMMMU, MMVU, LVBench, MLVU, Charades-STA) (Section 5.9).
  - GUI agent benchmarks (ScreenSpot Pro, OSWorld, AndroidWorld, etc.) (Section 5.10).
  - Text-centric LLM benchmarks (MMLU-Pro, GPQA, AIME-25, LiveCodeBench v6, etc.) (Section 5.11; Tables 5‚Äì10).

- **Prompts are explicitly listed** in Appendix B, showing task-specific formatting constraints (e.g., JSON output for DynaMath; boxed answer format for VisuLogic; Markdown conversion instruction for OmniDocBench).

- **Video evaluation constraints and fairness notes** are explicitly described (Section 5.9):
  - Their evaluation uses up to `2,048` frames/video, ‚â§ `224K` video tokens.
  - They note that comparisons to proprietary models may not be fully fair due to API/resource limitations (e.g., Gemini 512 frames, GPT-5 256 frames, Claude 100 frames).

### 5.2 Main quantitative results (with concrete numbers)

Below are representative highlights grounded in the provided tables/figures.

#### Flagship multimodal benchmark performance (Table 2)

- **General VQA (flagship 235B-A22B):**
  - `MMBench-EN`: `89.3` (Instruct) and `88.8` (Thinking) (Table 2).
  - `RealWorldQA`: `79.2` (Instruct) and `81.3` (Thinking) (Table 2).
  - `MMStar`: `78.4` (Instruct) and `78.7` (Thinking) (Table 2).

- **Multimodal reasoning / STEM examples (Table 2):**
  - `MMMU`: `78.7` (Instruct) and `80.6` (Thinking) (Table 2).
  - `MathVistamini`: `84.9` (Instruct) and `85.8` (Thinking) (Table 2).
  - `MathVision`: `66.5` (Instruct) and `74.6` (Thinking) (Table 2).

- **Alignment / hallucination (Table 2):**
  - `HallusionBench`: `63.2` (Instruct) and `66.7` (Thinking) (Table 2).
  - `MIA-Bench`: `91.3` (Instruct) and `92.7` (Thinking) (Table 2).

- **Document understanding / OCR (Table 2):**
  - `DocVQA_test`: `97.1` (Instruct) and `96.5` (Thinking) (Table 2).
  - `OCRBench`: `920` (Instruct) and `875` (Thinking) (Table 2).
  - `MMLongBench-Doc`: `57.0` (Instruct) and `56.2` (Thinking) (Table 2).

- **2D grounding / counting (Table 2):**
  - `RefCOCO-avg`: `91.9` (Instruct) and `92.1` (Thinking) (Table 2).
  - `ODinW-13` mAP (confidence fixed to 1.0 per Section 5.5): `48.6` (Instruct) and `43.2` (Thinking) (Table 2).
  - `CountBench`: `93.0` (Instruct) and `93.7` (Thinking) (Table 2).

- **Video understanding (Table 2):**
  - `MVBench`: `76.5` (Instruct) and `75.2` (Thinking) (Table 2).
  - `Video-MME (w/o subtitles)`: `79.2` (Instruct) and `79.0` (Thinking) (Table 2).
  - `MLVU-Avg`: `84.3` (Instruct) and `83.8` (Thinking) (Table 2).
  - `VideoMMMU`: `74.7` (Instruct) and `80.0` (Thinking) (Table 2).

These numbers support the report‚Äôs broader claim that the flagship model performs strongly across many multimodal axes, with ‚Äúthinking‚Äù often helping on reasoning-heavy tasks while ‚Äúinstruct‚Äù can be better on some tasks (e.g., OCRBench).

#### Text-centric performance vs text-only baselines (Tables 5‚Äì6)

- **Instruct comparison (Table 5):**
  - Qwen3-VL-235B-A22B-Instruct:
    - `MMLU-Pro 81.8`, `GPQA 74.3`, `AIME-25 74.7`, `LiveCodeBench v6 54.3`, `Arena-Hard v2 winrate 77.4` (Table 5).
  - The table also reports Qwen3 text-only counterpart and other baselines; notably, Qwen3 text-only is higher on some knowledge metrics (e.g., `MMLU-Pro 83.0` vs `81.8`) but Qwen3-VL is strong on reasoning tasks like AIME-25 (`74.7` vs Qwen3‚Äôs `70.3`) (Table 5).

- **Thinking comparison (Table 6):**
  - Qwen3-VL-235B-A22B-Thinking:
    - `AIME-25 89.7`, `HMMT-25 77.4`, `LiveCodeBench v6 70.1` (Table 6).
  - Qwen3 text-only thinking is higher on several metrics (e.g., AIME-25 `92.3`, HMMT-25 `83.9`, LiveCodeBench v6 `74.1`) (Table 6), suggesting that while multimodal thinking is strong, it does not universally exceed the best text-only counterpart on these particular benchmarks.

This directly relates to the paper‚Äôs motivation: preserving text ability during VL training. The provided tables show competitiveness, but not uniform dominance over text-only across all text benchmarks.

#### Medium and small model scaling (Tables 3‚Äì4)

- **Medium (Table 3):** Qwen3-VL-32B (Thinking) scores:
  - `MMBench-EN 89.5`, `MMStar 79.4`, `DocVQA_test 96.1`, `OCRBench 855`, `CountBench 94.1` (Table 3).
- **Small (Table 4):** Qwen3-VL-8B (Thinking) scores:
  - `MMBench-EN 85.3`, `MMStar 75.3`, `DocVQA_test 95.3`, `OCRBench 819`, `VideoMMMU 72.8` (Table 4).

These tables support the report‚Äôs ‚Äúscalability‚Äù narrative (Section 5.1), showing monotonic-ish improvements with size in many benchmarks (though not necessarily all).

### 5.3 Ablations and targeted analyses (Tables 11‚Äì12; Figure 3)

- **Vision encoder ablation: Qwen3-ViT vs SigLIP-2 (Table 11)**
  - In CLIP-style pretraining benchmarks, Qwen3-ViT is comparable or slightly better on ImageNet-1K (`84.6` vs `84.2`) and improves ObjectNet (`81.0` vs `79.9`) and an in-house ‚ÄúOmni‚Äù score (`45.5` vs `36.9`) (Table 11).
  - In VLM-stage benchmarks (paired with the same 1.7B Qwen3 LM and trained for 1.5T tokens), Qwen3-ViT improves several downstream numbers (e.g., AI2D `76.2` vs `74.1`, RLWDQA `66.1` vs `58.7`, InfoVQA `67.0` vs `65.3`, Omni `53.0` vs `50.1`) (Table 11).

- **DeepStack ablation (Table 12)**
  - Setting: internal `15B-A2B` LLM; pretrained on `200B` tokens; evaluated on validation sets with no post-training (Table 12).
  - `AVG` improves from `74.7` ‚Üí `76.0`.
  - Several benchmarks improve: OCRB `81.0` ‚Üí `83.6`, InfoVQA `71.9` ‚Üí `74.2`, ChartQA `81.5` ‚Üí `83.3`, DocVQA `89.5` ‚Üí `91.1`, MMMU `52.9` ‚Üí `54.1` (Table 12).

- **Needle-in-a-Haystack long-video test (Figure 3; Section 5.12.3)**
  - Task: insert a salient ‚Äúneedle‚Äù frame into long video, ask the model to locate and answer (Section 5.12.3).
  - Evaluation: videos sampled at `1 FPS`, resolution adjusted to keep a constant visual token budget (Section 5.12.3).
  - Reported result:
    - `100%` accuracy up to `30 minutes` video duration (corresponding to `256K` tokens),
    - `99.5%` accuracy up to `~1M` tokens (`~2 hours`) using ‚ÄúYaRN-based positional extension‚Äù (Section 5.12.3; Figure 3).
  - **Caveat:** The report references YaRN-based extension here, but the mechanism and training details for that extension are not included in the provided excerpt.

### 5.4 Do the experiments support the claims?

- **Supportive evidence present:**
  - Broad benchmark coverage across modalities and tasks (Section 5; Appendix A).
  - Concrete architectural ablation for `DeepStack` with multi-benchmark gains (Table 12).
  - Concrete long-context stress test demonstrating strong retrieval across long video contexts (Figure 3; Section 5.12.3).
  - Explicit long-context pretraining stage at 256K (Table 1) aligns with long-context performance claims.

- **Where evidence is weaker / less controlled (based on the provided content):**
  - The report introduces three architecture upgrades, but only **DeepStack** is directly ablated in the provided tables. There is no isolated ablation shown here for `interleaved-MRoPE`, timestamp tokens, or square-root loss reweighting.
  - Many datasets and some evaluations are **in-house** (e.g., multilingual OCR test set in Figure 2, OmniBench in Table 11), which can be useful but may reduce external comparability unless released and standardized.

## 6. Limitations and Trade-offs

- **Missing core training details (reproducibility limits)**
  - The provided content does not specify optimizer, learning rate schedule, batch sizes, gradient accumulation, tokenizer, or LLM architectural hyperparameters (layers/hidden size/heads). This makes full reproduction difficult beyond the high-level curriculum (Table 1; Section 5.11).
  - Compute budgets are described qualitatively (infrastructure up to 10k GPUs) but not quantified in PF-days or total GPU-hours (Section 4.6).

- **Limited ablation coverage for key claims**
  - Only DeepStack and the vision backbone receive explicit ablations (Tables 11‚Äì12).
  - The benefits of `interleaved-MRoPE`, timestamp tokens, and square-root reweighting are argued conceptually (Sections 2.1, 2.3, Section 1) but are not isolated experimentally in the provided excerpt.

- **Fairness and comparability constraints in video evaluation**
  - The report explicitly notes constraints on competitor frame counts due to API/resource limitations (Section 5.9). This can materially affect results on long-video tasks.

- **Context-length trade-offs**
  - Timestamp tokens ‚Äúincur a modest increase in context length‚Äù (Section 2.3). In a fixed window, any additional tokens reduce capacity for other content (more frames/text).

- **Dependence on teacher/judge models**
  - Distillation depends on strong teachers (Section 4.3).
  - RL and some evaluations depend on model-based judges (e.g., VideoMMMU evaluation uses a model-based judge; Section 5.9; plus RL uses Qwen3/Qwen2.5-VL judge models, Section 4.5). This can introduce judge bias and makes exact replication dependent on those judge implementations.

- **Use of internal datasets**
  - Several key datasets are internal (OCR data, multilingual OCR test set, internal documents, internal RL queries). Performance may not translate identically to other distributions (Sections 3.2.3, 4.2.1, Figure 2).

## 7. Implications and Future Directions

- **How this changes the landscape (as supported by the report)**
  - The report demonstrates that a VLM can be trained with a *native* `256K` interleaved context window (Table 1) and still remain competitive on text-centric benchmarks (Tables 5‚Äì6), pushing toward ‚Äúone model for text + multimodal + long-context.‚Äù
  - The architecture choices (balanced positional encoding + multi-level vision feature injection + explicit timestamps) emphasize that **temporal grounding and long-horizon retrieval** are now first-class concerns for VLM design (Sections 2.1‚Äì2.3; Figure 3).

- **Follow-up research directions suggested by the paper**
  - The conclusion explicitly points to:
    - interactive perception,
    - tool-augmented reasoning,
    - real-time multimodal control,
    - and exploration of unified understanding‚Äìgeneration architectures that include visual generation (Section 6).
  - Based on the technical sections, natural follow-ups include:
    - controlled ablations quantifying each of `interleaved-MRoPE`, timestamp tokens, and square-root reweighting independently,
    - more explicit study of how 256K training affects short-context tasks (Tables show competitiveness, but systematic trade-off curves are not provided).

- **Practical applications / downstream use cases (grounded in the report)**
  - Long technical document understanding (multi-page PDFs, textbooks) (Sections 3.2.3; 4.2.1; MMLongBench-Doc in Table 2).
  - Long-video tasks: grounding, dense captioning, long-horizon QA (Sections 2.3; 3.2.7; 5.9; Figure 3).
  - GUI agents and computer-use planning (Section 3.2.9; Section 5.10; Table 2 includes OSWorld/AndroidWorld-related metrics).
  - Multimodal code intelligence: UI screenshot ‚Üí HTML/CSS; image ‚Üí SVG; diagram/LaTeX transcription (Section 3.2.6; Table 2 includes Design2Code/ChartMimic/UniSVG).

- **Repro/Integration Guidance (when to prefer what)**
  - **Dense vs MoE selection (latency/quality trade-off):**
    - If you need maximum quality and can support MoE routing overhead, the flagship `235B-A22B` activates `22B` parameters/token (Section 2), targeting high capability with manageable per-token compute.
    - If deployment simplicity is key (or MoE infrastructure is unavailable), dense `32B/8B/4B/2B` variants provide a smoother scaling ladder (Section 2; Tables 3‚Äì4).
  - **Thinking vs non-thinking variants:**
    - Prefer **thinking** for heavy reasoning tasks (visual math, multi-hop reasoning, some video reasoning), as many such benchmarks are stronger under thinking settings (e.g., MMMU and MathVision in Table 2).
    - Prefer **instruct** where concise, format-following outputs and speed matter, and where thinking may not help (some OCR/document tasks show instruct ‚â• thinking; e.g., OCRBench `920` instruct vs `875` thinking in Table 2).
  - **Serving stack choices:**
    - The report suggests vLLM for memory-efficient high-throughput inference and SGLang for structured generation / complex prompts (Section 4.6), which maps naturally to workloads like document parsing (structured markdown/HTML) vs open-ended dialogue.

If you want, I can also produce a compact ‚Äúcheat sheet‚Äù mapping each architectural change (`interleaved-MRoPE`, `DeepStack`, timestamp tokens, square-root loss) to (a) the failure mode it targets, (b) where it appears in the pipeline, and (c) which evaluation sections most directly reflect it‚Äîwhile staying strictly within the provided report content.

# Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning

**ArXiv:** [2503.09516](https://arxiv.org/abs/2503.09516)

## ğŸ¯ Pitch

SEARCH-R1 pioneers a new way to train large language models to reason step-by-step while deciding how and when to use real-time web search, using reinforcement learning instead of static retrieval or simple prompting. By interleaving autonomous search actions and internal reasoning, and stabilizing RL with a novel retrieved-token loss masking trick, SEARCH-R1 delivers substantial gains on knowledge-intensive tasks. This approach overcomes the limits of previous retrieval-augmented strategies, enabling LLMs to provide more accurate, up-to-date answers essential for reliable AI in fast-evolving or long-tail domains.

---

## 1. Executive Summary
SEARCH-R1 trains a language model to interleave its own stepâ€‘byâ€‘step reasoning with live web search using reinforcement learning (RL), rather than relying only on prompting or static retrieval. It delivers sizable accuracy gains on seven questionâ€‘answering benchmarks by learning when and how to search, while keeping training stable through a simple outcome reward and a key stabilization trick called retrievedâ€‘token loss masking.

## 2. Context and Motivation
- Problem the work targets
  - Large language models (LLMs) often need upâ€‘toâ€‘date or longâ€‘tail facts to reason correctly, but their pretraining knowledge can be stale or incomplete. Existing retrieval methods either (a) run a oneâ€‘shot retrieval step before generation (standard Retrievalâ€‘Augmented Generation, RAG), or (b) prompt the model to â€œuse toolsâ€ like a search engine during inference.
- Why this matters
  - Accurate use of external knowledge is central to reliable assistants, enterprise search, scientific question answering, and other knowledgeâ€‘intensive tasks. Simply scaling parametric knowledge is insufficient for freshness and coverage.
- Gaps in prior approaches
  - RAG typically retrieves once using the user question as the query; the retrieved passages may be suboptimal for multiâ€‘step reasoning, leading to irrelevant or insufficient context (Section 2.1).
  - Prompted toolâ€‘use (e.g., ReAct/IRCoT) requires the model to already know how to search effectively; generalization is limited, and no learning occurs during deployment (Section 2.1).
  - Supervised toolâ€‘use training needs highâ€‘quality labeled trajectories (search queries and stepâ€‘byâ€‘step reasoning), which are expensive at scale (Section 2.1).
  - Search is nonâ€‘differentiable, so endâ€‘toâ€‘end gradient learning is not straightforward.
  - RL has recently proven effective for improving reasoning (e.g., DeepSeekâ€‘R1), but how to integrate real search, maintain stability, and design rewards is unclear (Section 1).
- How this work positions itself
  - SEARCH-R1 brings RL to the â€œsearchâ€‘whileâ€‘reasoningâ€ setting and addresses three core challenges: (1) an RL framework that treats search as part of the environment with stable optimization, (2) multiâ€‘turn, interleaved reasoning and search, and (3) simple but effective outcomeâ€‘only rewards (Section 1).

## 3. Technical Approach
SEARCH-R1 turns â€œsearch as a toolâ€ into a learnable behavior inside an RL training loop. The system alternates between generating tokens andâ€”when the model decidesâ€”calling a real search engine, then continues reasoning with the retrieved text.

- Highâ€‘level loop (Section 3.2; Algorithm 1; Table 1)
  1. The LLM receives a question wrapped in a minimal instruction template that enforces structure:
     - Think inside `<think>...</think>`.
     - If knowledge is missing, call search by generating `<search> query </search>`.
     - The system responds by injecting `<information> retrieved_passages </information>`.
     - Repeat â€œthink â†’ optionally searchâ€ any number of times.
     - When ready, output the final prediction inside `<answer>...</answer>`.
  2. This continues until an action budget is reached or the answer tag appears.
  3. The retrieved content becomes part of the next decoding context, so reasoning is conditioned on realâ€‘time search results.

- RL objective with search in the environment
  - Classical RL for LLMs optimizes the expected reward with a KL penalty to keep the policy close to a reference model. SEARCH-R1 extends this to include the search engine `R` as part of the environment:
    - Equation (1)/(6) defines the goal: maximize `E[rÏ•(x, y)] âˆ’ Î² D_KL(Ï€Î¸(Â·|x;R) || Ï€ref(Â·|x;R))`, where:
      - `Ï€Î¸` is the trainable â€œpolicy LLM,â€
      - `Ï€ref` is a frozen â€œreference LLM,â€
      - `y` is the whole trajectory containing both generated tokens and search insertions,
      - `R` indicates that generation is interleaved with retrieval (Section 3.1; Appendix A).
  - Why this matters: the policy is explicitly trained on trajectories that include real search calls and retrieved text, so the model learns to decide when to search, what to search for, and how to use results.

- Stabilization via retrievedâ€‘token loss masking (Section 3.1; Table 4; Figure 3)
  - During training, tokenâ€‘level losses (policy gradients and KL regularization) are computed only on LLMâ€‘generated tokens. Tokens copied from `<information>...</information>` are masked out and do not contribute to the loss.
  - Intuition: gradients should not encourage the model to â€œimitateâ€ the retrieved passages, which are exogenous to the model. Masking prevents spurious learning signals and improves stability.

- Two compatible RL optimizers (Section 3.1)
  - PPO (Proximal Policy Optimization): an actorâ€‘critic method with a â€œclippedâ€ objective (Equation (2)). Advantages are estimated with GAE (Generalized Advantage Estimation).
  - GRPO (Group Relative Policy Optimization): samples a group of responses for the same prompt and uses the groupâ€™s average reward as a baseline, avoiding a learned critic (Equation (3)).
  - Both use the same rollout mechanism and the same masking of retrieved tokens (including inside the KL term for GRPO).

- Reward function (Section 3.4)
  - Outcomeâ€‘only reward: at the end of a trajectory, extract the answer from `<answer>...</answer>` and compute a ruleâ€‘based correctness signal such as exactâ€‘match (EM) with the gold answer (Equation (4)).
  - No process rewards and no learned reward models are used. This keeps the system simple and avoids rewardâ€‘model brittleness.

- Experimental pipeline (Section 4.3; Appendix B)
  - Base models: `Qwen2.5-3B` and `Qwen2.5-7B` (both Base and Instruct variants).
  - Retrieval: 2018 Wikipedia dump; `E5` dense retriever; topâ€‘k = 3 by default (Appendix B).
  - Training set: merged NQ + HotpotQA training data; evaluation on 7 datasets (Section 4.1).
  - Default RL: PPO for 500 steps; batch size 512; sequence length 4096; max response 500 tokens; action budget B=4; training on 8Ã—H100 GPUs (Appendix B).

## 4. Key Insights and Innovations
- Treating search as part of the RL environment with interleaved generation (Sections 3.1â€“3.2)
  - Novelty: the trajectory `y` explicitly includes search calls and retrieved text; the policy is optimized on such trajectories, not just on pure text generations.
  - Significance: the model learns â€œwhen to searchâ€ and â€œwhat to queryâ€ as latent skills, improving generalization beyond handcrafted prompts.

- Retrievedâ€‘token loss masking (Section 3.1; Table 4; Figure 3)
  - Novelty: a simple but critical masking trick that excludes retrieved passage tokens from gradient updates and KL computations.
  - Significance: stabilizes RL and meaningfully boosts accuracy. For `Qwen2.5â€‘7Bâ€‘base` with PPO, average EM increases from 0.343 without masking to 0.431 with masking (Table 4).

- Minimal, outcomeâ€‘only rewards are enough (Section 3.4)
  - Novelty: avoids complex process supervision or neural reward models; relies solely on final correctness measured by EM.
  - Significance: simplifies training and still yields large gains across diverse QA tasks (Tables 2â€“3), showing that RL can teach effective search behavior using only final answers as feedback.

- Multiâ€‘turn, structured reasoning with explicit control tokens (Section 3.2; Table 1; Algorithm 1)
  - Novelty: a lightweight protocol based on four tagsâ€”`<think>`, `<search>`, `<information>`, `<answer>`â€”enables iterative reasoning, retrieval, and termination.
  - Significance: supports decomposition, selfâ€‘verification, and flexible numbers of search calls during training and inference (case studies in Appendix J).

- Empirical insights on RL choices and dynamics (Section 5)
  - PPO vs. GRPO: GRPO converges faster but can become unstable; PPO is steadier and often achieves the best final performance (Figure 2a; Table 3).
  - Base vs. Instruct: Instruct models learn faster initially, but base and instruct reach similar end performance after RL (Figure 2b; Appendix E).
  - Responseâ€‘length dynamics: responses first shorten (less filler), then lengthen as the model learns to search more and include retrieved evidence (Figure 2câ€“d).

## 5. Experimental Analysis
- Evaluation setup (Sections 4.1â€“4.3)
  - Datasets (7 total):
    - General QA: Natural Questions (NQ), TriviaQA, PopQA.
    - Multiâ€‘hop QA: HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle.
  - Metric: Exact Match (EM)â€”whether the predicted answer text matches a gold string (Section 4.3).
  - Baselines (Section 4.2):
    - No retrieval: Direct inference; Chainâ€‘ofâ€‘Thought (CoT).
    - With retrieval or tool use: RAG; IRCoT; Searchâ€‘o1.
    - Trainingâ€‘based: SFT; RL reasoning without search (R1); rejection sampling fineâ€‘tuning with search rollouts.
  - Fairness controls: same retriever, same number of retrieved documents (k=3), same corpora, same preâ€‘trained LLMs (Section 4.2; Appendix B).

- Main results (Table 2; Qwen2.5â€‘7B and 3B)
  - `Qwen2.5â€‘7Bâ€‘base`:
    - Average EM: 
      - RAG = 0.304, R1â€‘base = 0.276, Rejection Sampling = 0.348.
      - SEARCHâ€‘R1â€‘base = 0.431.
    - Example perâ€‘dataset gains:
      - On NQ: 0.480 (SEARCHâ€‘R1â€‘base) vs. 0.349 (RAG).
      - On HotpotQA (multiâ€‘hop): 0.433 vs. 0.299 (RAG).
  - `Qwen2.5â€‘3Bâ€‘instruct`:
    - Average EM: 0.325 (SEARCHâ€‘R1â€‘instruct) vs. 0.270 (RAG) and 0.224â€“0.229 (R1).
  - Takeaway: consistent improvements across both inâ€‘domain (NQ, HotpotQA) and outâ€‘ofâ€‘domain datasets (TriviaQA, PopQA, 2Wiki, Musique, Bamboogle). The abstract highlights average relative improvements of 24% (7B) and 20% (3B), which align with Table 2.

- PPO vs. GRPO (Table 3; Figure 2a)
  - For `Qwen2.5â€‘7Bâ€‘base`, average EM:
    - PPO: 0.431; GRPO: 0.350.
  - For `Qwen2.5â€‘3Bâ€‘instruct`, average EM:
    - PPO: 0.325; GRPO: 0.336.
  - Convergence behavior: GRPO often rises quicker but shows reward collapse later; PPO stays stable (Figure 2a; Appendix F).

- Ablations and robustness checks
  - Retrievedâ€‘token masking (Table 4; Figure 3):
    - `Qwen2.5â€‘7Bâ€‘base` PPO mean EM rises from 0.343 (no mask) to 0.431 (mask).
  - Number of retrieved passages (Appendix G; Figure 6; Table 7):
    - Topâ€‘k = 3 yields the best final average EM (0.431) vs. k=1 (0.375) and k=5 (0.400).
    - Interpretation offered: k=1 hurts recall; k=5 introduces noise that can degrade both learning and inference.
  - GRPO group size (Appendix H; Figure 7; Table 8):
    - Group size = 1 (REINFORCE) generalizes best on average (0.410) compared to size 3 (0.363) and 5 (0.350), despite slower convergence.
  - Training dynamics (Section 5.3; Figure 2câ€“d):
    - As steps progress, valid search calls increase, response length first decreases then increases as the model starts to incorporate more retrieved content.
  - Case studies (Appendix J):
    - Successes show multiâ€‘turn querying and selfâ€‘verification (Tables 10, 12, 13, 15, 18, 19).
    - Failures reveal queryâ€‘writing mistakes and susceptibility to misleading retrieval (Tables 11, 14, 16, 20).

- Do the experiments support the claims?
  - Yes, the breadth of datasets, consistent gain over strong RAG/toolâ€‘use baselines, and multiple ablations (masking, optimizer, topâ€‘k, group size) substantiate that learning to search through RL is both feasible and beneficial under the presented setup. The method remains competitive across model sizes and both base/instruct variants.

## 6. Limitations and Trade-offs
- Dependence on retrieval quality and corpus coverage
  - Experiments use the 2018 Wikipedia dump and an E5 retriever with k=3 by default (Appendix B). Different domains, nonâ€‘Wikipedia sources, or noisy web search could change outcomes.
- Reward design is taskâ€‘specific
  - Outcome reward is exactâ€‘match of a short answer string (Section 3.4). This is ideal for factual QA but less suited to tasks with long, freeâ€‘form answers, multiple valid phrasings, or subjective judgments.
- Compute and engineering costs
  - Training requires multiâ€‘GPU infrastructure (8Ã—H100), an online rollout server (vLLM), and repeated search calls (Appendix B). This is heavier than standard SFT/RAG pipelines.
- Stability vs. speed tradeâ€‘off
  - GRPO trains faster but can collapse; PPO is more stable but slower (Figure 2a; Appendix F). Tuning is nontrivial.
- Protocol reliance
  - The approach depends on special tokens (`<think>`, `<search>`, `<information>`, `<answer>`) and an action budget B (Algorithm 1). Porting to other tool APIs or multimodal settings requires reâ€‘engineering the protocol and the parser.
- Query decomposition remains fragile
  - Failure cases show that the model can write poor queries or be misled by irrelevant passages (Appendix J; Tables 14, 16, 20), indicating open challenges in query planning and evidence assessment.

## 7. Implications and Future Directions
- Field impact
  - SEARCH-R1 demonstrates that outcomeâ€‘only RL can teach LLMs nonâ€‘differentiable skills like querying a search engine in a multiâ€‘turn mannerâ€”without curated process supervision. This bridges the gap between â€œprompted tool useâ€ and â€œlearned tool use,â€ suggesting a viable path to train generalist agents that coordinate reasoning with tools.
- Research opportunities
  - Reward design: move beyond EM to semantic equivalence, citation faithfulness, or uncertaintyâ€‘aware rewards; explore process rewards that penalize spurious searches.
  - Retrieval strategy: dynamic topâ€‘k, query planning, query reformulation, and reranking; learning to decide when to stop searching.
  - Multiâ€‘tool/Multimodal: extend the same RLâ€‘withâ€‘environment abstraction to calculators, APIs, and vision tools; explore crossâ€‘modal retrieval and reasoning.
  - Offâ€‘policy data and scaling: combine SEARCH-R1 with logged trajectories (e.g., human or synthetic) and larger models (evidence in Appendix C shows consistent gains at 14B).
- Practical applications
  - Enterprise question answering with evolving knowledge bases; research assistants that cite upâ€‘toâ€‘date sources; factâ€‘checking and report generation with live references; customer support and analytics that require current policies or inventory data.

> Key quantitative takeaways:
> - On `Qwen2.5â€‘7Bâ€‘base`, SEARCHâ€‘R1 reaches average EM 0.431 vs. 0.304 (RAG) and 0.276 (R1 without search) across seven datasets (Table 2).
> - Retrievedâ€‘token masking is crucial: 0.431 (with mask) vs. 0.343 (without) on the same setup (Table 4).
> - PPO is more stable than GRPO (Figure 2a), and topâ€‘k=3 retrieval balances recall and precision best (Table 7).

Overall, SEARCH-R1 is a practical and conceptually clear framework for training LLMs to search while they think, delivering measured improvements and offering a foundation for broader toolâ€‘augmented reasoning with RL.

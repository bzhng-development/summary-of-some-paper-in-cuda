# 2 OLMo 2 Furious

**ArXiv:** [2501.00656](https://arxiv.org/abs/2501.00656)

## ðŸŽ¯ Pitch

OLMo 2 debuts a fully open suite of large language models (7B, 13B, 32B parameters) that not only outperform previous open models on the efficiency-performance frontier, but also release every artifact of their developmentâ€”model weights, code, training data, recipes, and logsâ€”for maximal transparency and reproducibility. By pioneering techniques for stable, efficient training and introducing a new two-stage 'mid-training' data curriculum, OLMo 2 demonstrates that state-of-the-art model performance can be achieved without sacrificing openness, enabling rigorous scientific research and empowering the community to build, analyze, and enhance advanced LLMs from the ground up.

---

## 1. Executive Summary
OLMo 2 introduces a fully open family of large language models (LLMs) at 7B, 13B, and 32B parameters, together with a training recipe that prioritizes stability and per-token efficiency. Beyond releasing weights, the work opens the entire development stackâ€”training data, code, logs, and intermediate checkpointsâ€”and demonstrates that, with careful architectural and training choices plus a targeted â€œmidâ€‘trainingâ€ data curriculum, open models can sit on the Pareto frontier of performance vs. compute (Figure 1) and yield competitive instructionâ€‘tuned variants (Tables 6â€“7).

## 2. Context and Motivation
- Problem addressed
  - Training large, dense LLMs is commonly unstable (loss/gradient spikes) and opaque (limited release of data/recipes). Even successful openâ€‘weights models rarely release full data and logs, limiting reproducibility and scientific study (Introduction, Â§1).
  - OLMoâ€‘0424â€”an earlier open modelâ€”suffered frequent loss spikes and growing gradient norms, which harmed scaling to larger models (Figure 2).
- Why it matters
  - Stability and transparency are prerequisites for reliable scaling, scientific analysis of training dynamics, and community contributions. Fully open artifacts enable research on training dynamics, memorization, safety, and efficient transfer (Introduction; Â§6).
- What existed before and limitations
  - Openâ€‘weights series (e.g., Llama 3.x, Qwen 2.5, Gemma 2) show strong capability but provide only partial openness (weights without full pretraining data/recipes).
  - Fully open projects (e.g., OLMo 1/0424, Pythia, Amber, DCLM) advanced openness but trailed the strongest openâ€‘weights models on several benchmarks (Introduction; Figure 1).
- Positioning
  - OLMo 2 aims for both rigor and competitiveness: it replaces instabilityâ€‘prone design choices, adds a twoâ€‘stage data curriculum (â€œmidâ€‘trainingâ€) to efficiently inject missing skills, and adopts a modern postâ€‘training pipeline (TÃ¼lu 3 + RLVR). It releases all artifactsâ€”from data to logsâ€”to be a platform for research (Â§2; Â§5; Â§6).

## 3. Technical Approach
This is a twoâ€‘stage baseâ€‘model training pipeline (pretraining â†’ midâ€‘training) followed by postâ€‘training (SFT â†’ preference tuning â†’ RLVR), supported by architectural and optimization choices aimed at stability and efficiency.

1) Model architecture and stability choices (Â§2.1; Â§3)
- Transformer backbone with several stabilityâ€‘oriented changes relative to OLMoâ€‘0424 (Table 1):
  - `RMSNorm` and postâ€‘norm residual layout: normalize outputs of attention and MLP instead of inputs:
    - h := x + RMSNorm(Attention(x))
    - hout := h + RMSNorm(MLP(h))  (Equations (1)â€“(2), Â§2.1)
    - Rationale: mitigates gradient/activation growth across depth; improves stability (Figure 7).
  - `QKâ€‘norm`: RMSNorm applied to queries and keys before attention score computation to prevent large logits and divergence (Â§2.1; Â§3.3.2).
  - `zâ€‘loss`: a small penalty on the logâ€‘partition Z of the softmax to keep logits in a healthy range (Â§3.3.3). The paper uses a small coefficient (Table 1 lists 1eâ€‘5; Â§3.3.3 experiments discuss 1eâ€‘4).
  - RoPE Î¸ increased to `5e5` to improve positional resolution (Â§2.1).
  - `GQA` (groupedâ€‘query attention) for the 32B variant to reduce KV cache and compute (Table 3).
  - Tokenizer: swaps to a larger, cl100kâ€‘style tokenizer with added PII masking tokens; yields small but consistent gains at 1B scale (Table 2).
- Initialization and optimizer adjustments (Â§3.2; Â§3.4)
  - Initialization: simple truncated normal with std 0.02 for all parameters instead of depthâ€‘scaled initialization; reduces gradient spikes and preserves gradient/activation scale across depth (Figures 4â€“6).
  - AdamW `Ïµ` lowered from 1eâ€‘5 to 1eâ€‘8 for larger effective early updates and faster stabilization (Figure 9).
  - No weight decay on token embeddings to avoid shrinking embedding norms (which amplifies early gradients via normalization); improves stability (Figure 10).
- Data sanitization targeting loss spikes (Â§3.1; Â§2.4)
  - Detect and filter documents containing long repeated nâ€‘gram runs (â‰¥32 repeats, span 1â€“13 tokens). Also mask such regions during data loading to avoid spikes at runâ€‘time (Figure 3).

2) Twoâ€‘stage base model training (Â§2.3â€“Â§2.4; Â§4)
- Stage 1â€”Pretraining with â€œOLMo 2 Mix 1124â€ (Table 4)
  - ~3.9T tokens, mostly highâ€‘quality web from DCLM Baseline (3.71T), plus code (StarCoder, filtered), academic corpora (peS2o; arXiv), mathâ€‘heavy web/proofs (OpenWebMath, Algebraic Stack), and Wikipedia.
  - Sequence length 4,096; batch sizes 1,024 (7B) or 2,048 (13B/32B); cosine LR schedule with 2k warmup (Table 3).
  - Total tokens: 4.05T (7B), 5.6T (13B), 6.6T (32B) (Â§2.3).
- Stage 2â€”Midâ€‘training (â€œannealingâ€) with â€œDolmino Mix 1124â€ (Tables 5, 10, 13; Â§4)
  - Purpose: in the scheduleâ€™s final phase, switch to upâ€‘sampled highâ€‘quality web + curated academic/encyclopedic content and mathâ€‘centric synthetic data to patch demonstrated capability gaps (esp. math).
  - Highâ€‘quality web is selected by combining a FastText quality filter with the FineWebâ€‘Edu classifier; plus decontaminated FLAN instructions, Wikipedia, peS2o, and a Q&A corpus from Stack Exchange (Table 5; Â§4.3).
  - Math subâ€‘mix combines multiple synthetic sources, including personaâ€‘conditioned math problems with solutions (`TuluMath`), rewrites of TinyGSM in natural language (`TinyGSMâ€‘MIND`), curated synthetic â€œtextbooksâ€ (`MathCoder2`â€‘style), filtered proofs (`Metamath`), and GSM8K train (Table 5; Â§4.4.1).
  - â€œMicroâ€‘annealsâ€: short, 50/50 experiments mixing small math subsets with general web to quickly assess data quality and mixture ratios before committing full runs (Â§4.4.2; Table 12).
  - Learningâ€‘rate study: higher peak LRs initially look better but converge to nearly the same final loss and downstream scores once linearly annealed to zero; i.e., performance is largely LRâ€‘insensitive when you finish the schedule (Figure 11; Table 8).
  - â€œCheckpoint soupsâ€: run the same mix multiple times with different data orderings and simply average the resulting checkpoints; this consistently beats the best single run (Table 14).

3) Postâ€‘training (instruction tuning) based on TÃ¼lu 3 (Â§5)
- Supervised fineâ€‘tuning (`SFT`): curated, permissively licensed instruction data; includes personaâ€‘based synthetic prompts; careful filtering (e.g., removing â€œdate cutoffâ€ patterns) and majority voting on synthetic math SFT to avoid training on incorrect answers (Â§5 â€œSFTâ€; Table 17).
- Preference tuning (`DPO`): collect onâ€‘policy generations from OLMoâ€‘2 SFT checkpoints and offâ€‘policy from a pool of permissively licensed models; score responses with GPTâ€‘4o as an LM judge; construct (chosen, rejected) pairs; train with DPO (Â§5 â€œPreFTâ€; Table 27).
- Reinforcement Learning with Verifiable Rewards (`RLVR`): PPO on tasks with automatically checkable answers (e.g., GSM8K/MATH, constraintâ€‘satisfaction prompts). Rewards are 1/0 based on verifiable correctness; value function initialized from a learned reward model (7B/13B). The 32B model uses `GRPO` (group relative policy optimization), which removes the reward model step (Â§5 â€œRLVRâ€; Figures 13â€“15; Table 18).

4) Infrastructure and implementation practices (Â§6)
- Training on two clusters (Cirrascale â€œJupiterâ€ and Google Cloud â€œAugustaâ€) with highâ€‘bandwidth interconnect and storage; workload orchestration via Beaker.
- Throughput optimizations: Torch `compile`, detect/avoid hostâ€“device syncs, asynchronous checkpointing/logging on a separate backend, and synchronized Python GCâ€”all to reduce stalls (Figure 16).

## 4. Key Insights and Innovations
- Stability recipe that scales
  - A specific combinationâ€”postâ€‘norm `RMSNorm` + `QKâ€‘norm` + simple 0.02 Gaussian initialization + lower AdamW `Ïµ` + no embedding weight decay + repeated nâ€‘gram filteringâ€”turns a previously spiky run (OLMoâ€‘0424) into a smooth training process (Figure 2), with each component justified by targeted analyses (Figures 3â€“10). This is more than a single trick; itâ€™s a synergistic package that generalizes across sizes.
- Midâ€‘training (â€œannealingâ€) as curriculum injection
  - Rather than a single pretraining mix, OLMo 2 splits compute: finish the cosine schedule earlier, then linearly anneal to zero on carefully chosen highâ€‘quality and domainâ€‘specific sources (Tables 5, 9, 11). This raises math and readingâ€‘comprehension scores dramatically without full retraining.
- â€œMicroâ€‘annealsâ€: cheap, sourceâ€‘level data selection
  - Small, controlled anneals diagnose which math sources and ratios help mostâ€”e.g., rewriting codeâ€‘style TinyGSM into natural language (`MIND`) flips GSM8K gains from negative to large positive (Table 12, Experiment 3). This is a practical methodology for dataâ€‘mix design at low cost.
- Learningâ€‘rate invariance at scale
  - When the LR is annealed to zero during midâ€‘training, a wide range of peak LRs produce nearly the same end performance (Table 8), echoing smallerâ€‘scale observations but now at trillionâ€‘token regimes (Figure 11). This reduces hyperparameter sensitivity and simplifies scaling.
- Checkpoint soups as a default
  - Simple weight averaging across 3 runs with different data orders consistently equals or beats the best single run on multiple mixes (Table 14). This is a robust, cheap improvement.

## 5. Experimental Analysis
- Evaluation protocol (Â§2.5; Appendix A)
  - Base models: OLMES suite with multipleâ€‘choice and generative tasks; explicit separation between â€œdevelopmentâ€ tasks and â€œheldâ€‘outâ€ tasks to avoid overfitting (Appendix A.1; Table 20).
  - Instruct models: categories for knowledge recall, reasoning, math, instruction following, and safety, with standardized fewâ€‘shot and prompting settings as per the TÃ¼lu 3 evaluation regime (Table 15; Appendix A.2).
- Main quantitative results (all numbers trace to specific tables/figures)
  - Pareto frontier: performance vs. training FLOPs (Figure 1) shows OLMo 2 on the frontier among models of comparable size and openness levels.
  - Baseâ€‘model comparisons (Table 6):
    - 7B: â€œAvgâ€ across dev tasks = 62.9 (FLOPs ~1.8Ã—10^23), beating DCLMâ€‘7B (56.9) and OLMoâ€‘0424â€‘7B (50.7).
    - 13B: 68.3 (FLOPs ~4.6Ã—10^23).
    - 32B: 73.3 (FLOPs ~13.0Ã—10^23), competitive with Qwen 2.5â€‘32B (74.9) despite greater transparency and fewer assumed tokens.
  - Effect of midâ€‘training (Table 9):
    - 7B avg improves from 53.0 â†’ 62.9 (+9.9), with GSM8K 24.1 â†’ 67.5 (+43.4), NQ 29.0 â†’ 36.9 (+7.9), DROP 40.7 â†’ 60.8 (+20.1).
    - 13B avg improves 58.9 â†’ 68.3 (+9.4); GSM8K 37.3 â†’ 75.1 (+37.8).
    - 32B avg improves 66.3 â†’ 73.3 (+7.0); GSM8K 56.2 â†’ 78.8 (+22.6).
  - Instruct models (Table 7):
    - OLMoâ€‘2â€‘7Bâ€‘Instruct average = 56.5; GSM8K = 85.1; BBH = 51.4; IFEval = 72.3.
    - OLMoâ€‘2â€‘13Bâ€‘Instruct average = 63.5; GSM8K = 87.4; MATH = 82.6.
    - OLMoâ€‘2â€‘32Bâ€‘Instruct average = 68.8; GSM8K = 87.6; BBH = 70.6; MATH = 85.6; competitive with Qwenâ€‘2.5â€‘32Bâ€‘Instruct (avg 68.1).
  - Tokenizer swap ablation at 1B (Table 2): OLMES increases 59.8 â†’ 60.6; MMLU 34.8 â†’ 35.2.
  - LR sweeps (Table 8): across peak LRs and anneal lengths, OLMES dev average varies within ~2 points, confirming LR insensitivity once midâ€‘training anneals to zero.
  - Microâ€‘anneals (Table 12):
    - Even 10â€“35% math proportion yields large GSM* gains, with modest impact on MMLU.
    - Rewriting TinyGSM to natural language (â€œMINDâ€) turns GSM* from 25.0 (inline code) â†’ 65.5; duplicating to 2Ã— raises it to 70.0.
  - Checkpoint soups (Table 14): e.g., â€œMix Fâ€ 7B, Best single vs. 3Ã— soupâ€”Avg 77.1 â†’ 77.9, GSM* 73.5 â†’ 74.5.
  - Training stability figures:
    - Loss and gradient spikes vanish with the revised stack (Figure 2).
    - Repeated nâ€‘gram filtering reduces gradient spikes (Figure 3).
    - Initialization and normalization choices maintain nearâ€‘zero growth exponents across depth (Figures 5â€“6) and suppress spikes (Figure 4).
  - RLVR learning curves (Figures 13â€“15): reward increases correlate with higher GSM8K/MATH/IFEval and average scores; multiâ€‘stage RLVR on 13B (GSM8Kâ€‘oriented pass followed by MATHâ€‘oriented pass) raises both math metrics and overall average (Â§5; Figure 13).
- Environmental accounting (Table 19):
  - Estimated pretraining energy: 131 MWh (7B) and 257 MWh (13B); emissions ~52 and ~101 tCO2eq respectively, using measured power, site PUEs, and regional carbon intensities.

Assessment
- The experiments are thorough and aligned with the paperâ€™s claims:
  - Stability: multiple targeted ablations/plots make the causal story plausible (Figures 3â€“10).
  - Curriculum: both aggregate improvements (Tables 9, 11) and fineâ€‘grained dataâ€‘source diagnostics (Table 12) support midâ€‘training efficacy.
  - Competitiveness: standardized, open evaluation (OLMES) with heldâ€‘out tasks (Table 6, Appendix A) gives credibility. Instructionâ€‘tuned models compare favorably against similarly sized openâ€‘weights (Table 7).
- Areas where evidence is suggestive rather than definitive:
  - LR invariance is shown for several LRs and token budgets, but the paper notes cost limited a full sweep to map plateaus/boundaries (Â§4.1).
  - Safety metrics decrease modestly from SFT/DPO to RLVR in some cases (e.g., 32B safety: 93.8 â†’ 85.9 in Table 16), suggesting a tradeâ€‘off that deserves deeper analysis.

## 6. Limitations and Trade-offs
- Scope and focus
  - Englishâ€‘centric pretraining and evaluation; nonâ€‘English capability is not a goal here (Â§5 excludes multilingual SFT variants as nonâ€‘beneficial at this time).
  - Coding was not a primary optimization target in the Instruct suite (Table 15 excludes code), so code performance is not emphasized.
- Data and contamination
  - While decontamination is applied (e.g., FLAN; Â§4.3), full contamination auditing across all sources is intractable at this scale; heldâ€‘out tasks mitigate but cannot eliminate concerns (Â§2.5).
- Compute and design search constraints
  - LR sweeps and anneal lengths are explored but not exhaustively mapped due to cost (Â§4.1). Other knobs (e.g., proportion of math/web in Dolmino) are tuned via microâ€‘anneals but still heuristic.
- Safety and rewardâ€‘hacking risk in RLVR
  - Instruction models see slight safety metric drops at the final RLVR stage (Table 16); using verifiable rewards focuses the model on pass/fail signals, which can narrow general caution or calibration if not balanced with broader safety objectives.
- Smallâ€‘model scaling challenges
  - The 1B variant struggles to use trillions of tokens efficiently during base pretraining (Appendix B). It benefits greatly from postâ€‘training, pointing to capacity limits at very small scales (Tables 22â€“23).
- Implementation subtleties
  - zâ€‘loss coefficient differs across sections (Table 1 vs. Â§3.3.3), indicating that best values may be setupâ€‘dependent; fused implementations can diverge in backward pass (Figure 8).
  - Postâ€‘training tokenizer mismatch in early â€œpreviewâ€ models required retraining to maintain consistency (Appendix C.3; Figure 17).

## 7. Implications and Future Directions
- What changes in the landscape
  - OLMo 2 demonstrates that fully open LLMs can be both competitive and reproducible when the entire pipelineâ€”data, code, logs, intermediate checkpointsâ€”is released. The stability recipe plus midâ€‘training curriculum give a replicable path to strong base models (Figure 1; Tables 6, 9).
- Practical applications
  - Researchers and practitioners can:
    - Reproduce largeâ€‘scale training runs and study dynamics (e.g., spike mitigation, initialization scaling).
    - Tailor midâ€‘training curricula for domain skills (e.g., biomedical, legal) using the microâ€‘anneal methodology (Â§4.4.2).
    - Apply RLVR and constraintâ€‘based reward shaping to tasks with verifiable outcomes (e.g., program synthesis with unit tests, math, data cleaning).
- Followâ€‘up research enabled/suggested
  - Data curriculum design: extend microâ€‘anneals to multilingual or codeâ€‘first curricula; explore principled mixture optimization (beyond duplications/filters) using signals gathered during microâ€‘anneals (Table 12).
  - Stability at longer contexts: with RoPE Î¸ = 5e5, study scaling to much longer sequence lengths with the same stability stack (Â§2.1).
  - Safetyâ€‘aware RL: integrate safetyâ€‘specific verifiable rewards or multiâ€‘objective RL to avoid the observed safety dips at the final stage (Table 16).
  - Learningâ€‘rate plateaus: systematically map the parameter/token regimes where LR invariance holds, and relate to theory (Â§4.1; Table 8).
  - Transparency benchmarks: OLMESâ€™s dev/heldâ€‘out split is a concrete step; broader community adoption could standardize honest reporting and reduce overfitting (Appendix A).

> Overall, OLMo 2â€™s core message is methodological: training stability is a firstâ€‘class objective; data curricula can be engineered efficiently; and full openness is feasible at modern scales. The result is a family of base and instruction models that are computeâ€‘efficient (Figure 1), show large capability gains from targeted midâ€‘training (Table 9), and offer a transparent foundation for research and deployment.

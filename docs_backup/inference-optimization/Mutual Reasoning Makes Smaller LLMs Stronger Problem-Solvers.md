# Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers

**ArXiv:** [2408.06195](https://arxiv.org/abs/2408.06195)
**Authors:** Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang
**Institutions:** Microsoft Research Asia

## üéØ Pitch

rStar introduces a groundbreaking "self-play mutual reasoning" approach that transforms small language models (SLMs) into significantly improved problem solvers without requiring fine-tuning or stronger teacher models. By integrating enhanced human-like search with Monte Carlo Tree Search and peer verification, rStar showcases substantial accuracy gains in math and commonsense benchmarks, notably boosting affordable and accessible AI solutions for cost-sensitive applications.

---

## 1. Executive Summary (2-3 sentences)
This paper proposes rStar, a ‚Äúself‚Äëplay mutual reasoning‚Äù procedure that turns small language models (`SLMs`, e.g., 3‚Äì8B parameters) into much stronger problem solvers at inference time‚Äîwithout fine‚Äëtuning or a stronger teacher model. rStar couples a richer, human‚Äëlike search with Monte Carlo Tree Search (MCTS) and a second, similarly capable SLM acting as a verifier; together they generate and mutually validate multi‚Äëstep reasoning, yielding large gains on math and commonsense benchmarks (e.g., GSM8K jumps from 12.51% to 63.91% for `LLaMA2-7B`; Table 2).

## 2. Context and Motivation
- Problem addressed
  - Smaller models are attractive for cost and latency but are notably worse at multi‚Äëstep reasoning than larger models. Even with standard prompting like Chain‚Äëof‚ÄëThought (`CoT`), 7‚Äì8B models struggle (e.g., Mistral‚Äë7B reaches only 36.5% accuracy on GSM8K with few‚Äëshot CoT; Section 1).
  - Improving reasoning without a stronger, supervising LLM (no fine‚Äëtuned value model, no GPT‚Äë4 teacher) remains hard.

- Why it matters
  - Practical: Many organizations deploy SLMs for cost/privacy; better reasoning extends their usefulness to math and logic tasks without expensive retraining.
  - Scientific: Tests whether inference‚Äëtime search and peer verification, rather than more parameters or supervised data, can unlock latent reasoning ability.

- Prior approaches and gaps
  - Single‚Äëround prompting (CoT, few‚Äëshot) improves transparency but often underperforms on complex reasoning (Sections 1‚Äì2).
  - Multi‚Äëround sampling (Self‚ÄëConsistency, SC) improves over single traces by majority vote, but requires that many sampled traces are correct; SLMs rarely meet that condition (Section 3.1).
  - Tree search methods (e.g., ToT, RAP) often use a narrow action space (one way to extend a trace), limiting exploration; reward signals can be unreliable when SLMs self‚Äëevaluate (Appendix A.1 shows near‚Äërandom self‚Äëevaluation in RAP when its r1 is randomized, Table 6).
  - Training separate value/reward models can overfit and needs labeled data (Section 2).

- Positioning
  - rStar expands the search space with five ‚Äúhuman‚Äëlike‚Äù reasoning actions and replaces self‚Äëjudgment with mutual verification by a second, equally capable SLM (‚Äúmutual consistency‚Äù). It thereby tackles both exploration (better candidate solutions) and selection (more reliable identification of good ones) without external supervision (Fig. 2, Section 3).

## 3. Technical Approach
rStar is a two‚Äëstage, inference‚Äëtime procedure: (1) generate multiple candidate reasoning trajectories with a richer MCTS, and (2) verify them via mutual consistency using a second SLM.

Key terms used below:
- `SLM`: a small language model (‚âà3‚Äì8B parameters).
- `Trajectory`: a full reasoning path from the original problem to a final answer, composed of intermediate steps `s1, s2, ‚Ä¶, sd` (Section 3.1).
- `Action space`: the set of available moves for extending a trajectory during search.
- `Mutual consistency`: a verification scheme where a second SLM, given partial steps from a trajectory, completes the reasoning; if it reaches the same answer, the trajectory is considered validated (Section 3.3).

Step-by-step:

A) Generate candidate trajectories with richer MCTS (Section 3.2, Fig. 3)
- Core idea
  - Use MCTS to grow a tree from the question (root) through intermediate steps to terminal nodes (complete solutions). Each edge corresponds to choosing an action that prompts the SLM to produce the next step.
- Rich, human‚Äëlike action space (five actions; Section 3.2)
  - `A1` One‚Äëstep thought: propose exactly the next reasoning step.
  - `A2` Remaining steps: ‚Äúfast think‚Äù to complete all remaining steps directly (standard CoT‚Äëstyle continuation).
  - `A3` Next sub‚Äëquestion + answer: decompose the problem into a smaller sub‚Äëquestion, then answer it (least‚Äëto‚Äëmost prompting).
  - `A4` Re‚Äëanswer sub‚Äëquestion: if a sub‚Äëquestion may be wrong or brittle, answer it again with few‚Äëshot CoT to improve reliability.
  - `A5` Rephrase the question: rewrite the problem into explicit conditions to reduce misunderstandings.
  - Some actions have ordering constraints (e.g., `A4` can only follow `A3`; `A5` applies only to the root; Section 3.2).
  - Why this design: Single‚Äëaction search (e.g., only decompose or only step‚Äëforward) often gets stuck; the five actions mimic human flexibility‚Äîdecompose when helpful, otherwise compute directly, revisit mistakes, or clarify the statement (Section 3.2, Fig. 3).
  - Evidence: An ablation on GSM8K (200 samples) shows accuracy increases as more actions are enabled: from 70.5% with only `A3` (RAP‚Äëlike) to 75.0% with all five (Table 1).

- Reward function tailored to SLMs (Section 3.2)
  - Challenge: SLM self‚Äëevaluation of intermediate steps is unreliable.
  - Design:
    - Initialize `Q(s, a) = 0` for unexplored nodes.
    - When a terminal node is reached, compute its reward `Q(sd, ad)` as the confidence from self‚Äëconsistency majority voting (i.e., the likelihood the final answer is correct across sampled completions).
    - Back‚Äëpropagate this terminal reward to every node along the path: `Q(si, ai) ‚Üê Q(si, ai) + Q(sd, ad)` for i=1..d‚àí1.
  - Why this design: It rewards actions by their empirical contribution to correct final answers (AlphaGo‚Äëstyle credit assignment), avoiding direct self‚Äëjudgment on intermediate steps (Section 3.2).
  - Node selection uses UCT (Upper Confidence Bound for Trees):
    - Equation: UCT(s, a) = Q(s,a)/N(s,a) + c * sqrt(ln N_parent(s) / N(s,a)) (Section 3.2).
    - Interpretation: Prefer actions with high average reward but still explore less‚Äëtried actions.

- Rollout details (Section 4.1)
  - 32 rollouts per problem; max depth `d=5` for most datasets, `d=8` for MATH.
  - Branching: up to 5 children per depth for `A1` and `A3`, 1 for others.
  - Output: a set of candidate trajectories (and their rewards/confidences).

B) Verify trajectories with mutual consistency (Section 3.3, Fig. 2 and Fig. 4)
- Problem: Picking the best single trajectory based only on MCTS reward is hard; many SLM‚Äëgenerated traces are partially wrong.
- Mechanism:
  - Introduce a second SLM `MÃÇ` (similar capability) as a discriminator.
  - For a candidate trajectory `t = x ‚äï s1 ‚äï ‚Ä¶ ‚äï sd`, pick a random split point `i < d`.
  - Provide `MÃÇ` with the question and the prefix `x ‚äï s1 ‚äï ‚Ä¶ ‚äï si‚àí1` as ‚Äúpartial hints,‚Äù and ask it to complete the remaining reasoning (Section 3.3, Fig. 4).
  - If `MÃÇ`‚Äôs completed answer matches the original trajectory‚Äôs answer, label `t` as ‚Äúvalidated.‚Äù
- Why partial hints: They reduce difficulty and variance, increasing the chance that `MÃÇ` can correctly finish the reasoning and thus provide informative feedback (Section 3.3).
- Final selection: Among validated trajectories, choose the one with the highest product of (i) the MCTS terminal reward and (ii) the terminal confidence from rollouts (Section 3.3).

C) Implementation specifics (Section 4.1)
- Models: five SLMs‚Äî`Phi3-mini (3.8B)`, `LLaMA2‚Äë7B`, `Mistral‚Äë7B`, `LLaMA3‚Äë8B`, `LLaMA3‚Äë8B‚ÄëInstruct`.
- Discriminator: `Phi3-mini-4k` by default (3.8B), run in parallel for efficiency; when `Phi3` is the generator, it self‚Äëdiscriminates (Section 4.1).
- Discriminator hinting: random split between 20% and 80% of the steps are given as prefix (Section 4.1).

Analogy: Think of two students solving a problem‚Äîone explores multiple solution outlines using different tactics (break down, compute directly, rephrase, retry a subpart), while the other, shown partial work, tries to finish the solution. If both independently reach the same answer, confidence increases (Section 3.3‚Äôs ‚Äúpeer verification‚Äù rationale).

## 4. Key Insights and Innovations
- Rich action space for reasoning search (fundamental)
  - What‚Äôs new: Five complementary actions (`A1‚ÄìA5`, Section 3.2) replace the typical single action (e.g., only decomposing or only stepping).
  - Why it matters: Better exploration generates higher‚Äëquality candidates. Table 1 shows monotonic gains as actions are added (70.5% ‚Üí 75.0% on GSM8K subsample).

- Mutual consistency verification with a peer SLM (fundamental)
  - What‚Äôs new: Instead of self‚Äëverification or majority voting across random samples, rStar cross‚Äëchecks a candidate with a second SLM that receives partial hints and must independently complete to the same answer (Section 3.3, Fig. 4).
  - Why it matters: It provides supervision‚Äëfree yet informative feedback that is more robust than self‚Äëjudgment and avoids training reward models (Table 5 left shows consistent gains over majority voting and self‚Äëverification).

- AlphaGo‚Äëstyle credit assignment without intermediate self‚Äëgrading (incremental but impactful)
  - What‚Äôs new: Rewards flow from successful terminals back to earlier steps; intermediate self‚Äëevaluation is avoided (Section 3.2).
  - Why it matters: Appendix A.1 (Table 6) suggests SLMs‚Äô self‚Äëratings can be near random; rStar‚Äôs design sidesteps this pitfall.

- Strong gains without a stronger teacher model (pragmatic innovation)
  - What‚Äôs new: Both generator and discriminator are SLMs; no GPT‚Äë4 teacher is required (though it can be used; Table 5 right).
  - Why it matters: Makes the approach broadly usable in constrained settings.

## 5. Experimental Analysis
- Evaluation setup (Section 4.1)
  - Datasets: four math word‚Äëproblem datasets‚Äî`GSM8K`, `GSM‚ÄëHard`, `SVAMP`, `MATH‚Äë500`‚Äîand one commonsense dataset `StrategyQA`.
  - Metrics: accuracy (percent of correctly answered questions).
  - Baselines (Sections 4.1‚Äì4.2): Zero-shot and few-shot CoT; Self‚ÄëConsistency (SC@8/64/128); tree‚Äësearch methods `ToT` and `RAP`.
  - Rollouts: 32 per question; depth `d=5` (most) and `d=8` (MATH‚Äë500).

- Main quantitative results (Tables 2‚Äì3)
  - Across models and tasks, rStar substantially improves over all baselines.
  - GSM8K (Table 2):
    - `LLaMA2‚Äë7B`: few‚Äëshot CoT 12.51% ‚Üí rStar 63.91% (‚Äú+51.4 points‚Äù).
    - `Mistral‚Äë7B`: 36.46% ‚Üí 81.88%.
    - `LLaMA3‚Äë8B‚ÄëInstruct`: 74.53% ‚Üí 91.13%.
    - Quote: ‚ÄúrStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2‚Äë7B, from 36.46% to 81.88% for Mistral‚Äë7B, from 74.53% to 91.13% for LLaMA3‚Äë8B‚ÄëInstruct‚Äù (Abstract; detailed in Table 2).
  - GSM‚ÄëHard (Table 2):
    - `Mistral‚Äë7B`: 13.57% (few‚Äëshot CoT) ‚Üí 37.91%.
    - `LLaMA3‚Äë8B‚ÄëInstruct`: 25.63% ‚Üí 37.53%.
  - SVAMP (Table 2):
    - `LLaMA3‚Äë8B`: 76.90% (few‚Äëshot) ‚Üí 90.00%.
    - `LLaMA2‚Äë7B`: 48.10% ‚Üí 74.90%.
  - StrategyQA (Table 2):
    - Modest but consistent gains: e.g., `LLaMA3‚Äë8B`: 64.05% (few‚Äëshot) ‚Üí 67.69%.
  - MATH‚Äë500 (Table 3):
    - `LLaMA3‚Äë8B‚ÄëInstruct`: rStar 42.94% vs. best baseline SC@128 at 33.80% (+9.14 points).
    - `Phi3‚Äëmini‚Äë4k`: rStar 48.60% vs. SC@128 at 45.60%.

- Generator vs. discriminator contributions
  - Generator alone (majority voting) is strong: on GSM8K, `LLaMA3‚Äë8B‚ÄëInstruct` improves to 88.70%‚Äîalready better than ToT and RAP (Table 2).
  - Adding the discriminator further lifts accuracy: e.g., `LLaMA3‚Äë8B‚ÄëInstruct` 88.70% ‚Üí 91.13% (Table 2).
  - Discriminator robustness:
    - Against trajectories from different generators, rStar‚Äôs discriminator outperforms majority voting and self‚Äëverification (Table 5, left).
    - Using different models as the discriminator barely changes accuracy; `GPT‚Äë4 (2024‚Äë05‚Äë01)` gives 92.57% vs. `Phi3‚ÄëMini‚ÄëInstruct` 91.13% on GSM8K (Table 5, right).

- Ablations and diagnostics
  - Action space ablation: Adding actions systematically improves accuracy (Table 1).
  - Generator comparison: rStar‚Äôs generator outperforms RAP and SC, both with majority voting and with rStar‚Äôs discriminator (Table 4).
  - Self‚Äëevaluation vs. rStar reward: Adding self‚Äëevaluation (Ours+Self‚Äëeval) reduces performance compared with rStar‚Äôs back‚Äëprop reward (Table 4).
  - Sensitivity to rollouts: rStar improves accuracy with as few as 2 rollouts and keeps improving with more; RAP saturates or declines after 4 in some settings (Fig. 5).
  - Self‚Äërewarding unreliability: Randomizing RAP‚Äôs intermediate self‚Äëscore `r1` barely changes results; randomizing the terminal confidence `r2` hurts (Appendix A.1, Table 6).

- Efficiency and cost
  - Inference‚Äëtime overhead is substantial: on GSM8K, average ‚âà 149‚Äì167 model calls and ‚âà 349k‚Äì367k generated tokens per question for `Mistral‚Äë7B` and `LLaMA2‚Äë7B` (Appendix A.2, Table 7).
  - Computation scales linearly with the number of rollouts; verification can be parallelized (Section 4.1; Appendix A.2).

- Overall assessment
  - The experiments are comprehensive: five models, five datasets, strong baselines, and extensive ablations. The gains are large in math tasks and consistent in commonsense. The evidence convincingly supports the claims that (i) richer search improves candidate quality, and (ii) mutual consistency selects better solutions without a teacher.

## 6. Limitations and Trade-offs
- Compute overhead at inference time
  - Many rollouts and long traces: ‚âà150‚Äì170 calls and ‚âà350k tokens per problem (Table 7). This may be too expensive for real‚Äëtime or large‚Äëscale deployments without batching/parallelization.

- Agreement ‚â† correctness
  - Mutual consistency can validate wrong answers when both SLMs follow the same flawed hint or bias. While results show strong net gains, the method does not guarantee correctness (Section 3.3 rationale; Table 5 shows strong but not perfect verification).

- Domain and tool use
  - The approach assumes reasoning can be expressed in natural language steps. Tasks requiring external tools (symbolic solvers, retrieval, calculators) are not integrated here. The method also avoids trained reward models or external supervision, which may cap performance on some domains (Sections 2‚Äì3).

- Hyperparameters and design choices
  - Performance depends on depth `d`, the mix of actions, and rollout count (Section 4.1; Fig. 5). Tuning may be dataset‚Äëspecific.

- Intermediate reward signal
  - Terminal confidence comes from self‚Äëconsistency on the final answer. If majority voting is weak (few correct completions), reward estimates may be noisy‚Äîthough the second‚Äëstage discriminator mitigates this (Section 3.2; Table 5).

## 7. Implications and Future Directions
- How this changes the landscape
  - rStar demonstrates that SLMs already contain latent reasoning ability that can be unlocked by better search and peer verification, challenging the notion that dramatic reasoning leaps require bigger models or supervised fine‚Äëtuning (Fig. 1; Table 2).

- Practical applications
  - Math tutoring, automated grading, data cleaning with logic constraints, operations planning, and lightweight on‚Äëdevice assistants that need stronger reasoning without server‚Äëside large models.

- Follow‚Äëup research
  - Smarter, cheaper search:
    - Adaptive rollouts based on early confidence; learned action priors to prune the tree; caching and reusing sub‚Äëtraces.
  - Stronger verification:
    - Multi‚Äëpeer or committee verification; integrating external tools for step‚Äëchecking; hybrid symbolic‚Äëneural validators.
  - Task‚Äëaware actions:
    - Extending the action set for domains like code reasoning, scientific QA, or spatial/diagram problems.
  - Semi‚Äësupervised refinement:
    - Use validated trajectories as training data for lightweight fine‚Äëtuning, potentially closing the loop between inference‚Äëtime gains and model updates‚Äîwithout a stronger teacher.
  - Safety and robustness:
    - Diagnose when mutual agreement is misleading; introduce adversarial peers to reduce confirmation bias.

In sum, rStar‚Äôs combination of a diversified action space for MCTS and mutual peer verification offers a compelling, supervision‚Äëfree path to boost SLM reasoning. The approach trades inference cost for accuracy and reliability, but it opens clear avenues to make small models far more capable where compute at test time is acceptable.

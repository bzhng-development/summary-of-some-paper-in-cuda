# FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION KERNELS

**ArXiv:** [2412.05496](https://arxiv.org/abs/2412.05496)

## ğŸ¯ Pitch

FlexAttention introduces a powerful programming model and compiler that lets researchers implement most attention variants in just a few lines of PyTorch code, automatically generating fused and highly efficient kernels. By bridging the gap between flexibility and performance, FlexAttention empowers experimentation with new and combined attention mechanismsâ€”without sacrificing speedâ€”thus accelerating innovation in large language models and eliminating the need for specialized kernel engineering.

---

## 1. Executive Summary (2-3 sentences)
FlexAttention introduces a programming model and compiler pipeline that lets users write most attention variants as a few lines of PyTorch code and automatically generates highâ€‘performance fused kernels. It closes the longâ€‘standing gap between flexibility and speed in attention by (a) expressing variants as simple score and mask functions and (b) lowering them into optimized Triton templates with blockâ€‘sparse execution, yielding performance competitive with handâ€‘written FlashAttention kernels while supporting combinations (e.g., Sliding Window + ALiBI + Paged Attention).

## 2. Context and Motivation
- Problem and gap
  - Attention is a core operation in Transformers, but fast implementations (e.g., FlashAttention) are optimized for only a small set of variants. Trying new masking patterns or score tweaks often forces researchers into slow, unfused baselines or costly kernel engineeringâ€”the â€œsoftware lotteryâ€ where ideas only thrive if they happen to fit existing kernels (Introduction, Â§1; Table 1).
- Importance
  - Real systems increasingly rely on nonâ€‘standard variants: Sliding Window for long context efficiency, ALiBI for extrapolation, document masking for packing variableâ€‘length sequences, soft capping for stability, and paged attention for KVâ€‘cache memory efficiency (Introduction, Â§1; Â§2.1).
  - Lack of fast, general kernels slows research and production: materialized masks blow up memory; kernel rewrites are brittle and timeâ€‘consuming; general ML compilers struggle to fuse attentionâ€™s specific algebra (Background, Â§2.2â€“2.3).
- Prior approaches and shortcomings
  - FlashAttention v2/v3: extremely fast, but limited variant coverage; adding new patterns usually needs new kernels; columnâ€‘sparse extensions (FlashMask) still impose overhead and lack scoreâ€‘mod flexibility (Background, Â§2.2).
  - General compilers (torch.compile/Inductor, TVM, Mirage): good at single matmuls; attention needs two coupled matmuls plus numericalâ€‘stable â€œonline softmax,â€ and often block sparsityâ€”difficult for generic compilers to discover and schedule (Background, Â§2.3).
- Positioning
  - FlexAttention sits between â€œhandâ€‘tuned kernelâ€ and â€œgeneric compiler.â€ It keeps the frontâ€‘end flexible (idiomatic PyTorch functions that modify attention logits or masks) while lowering those functions into a small set of highâ€‘performance, handâ€‘crafted attention templates that already embody the right algorithmic tricks (online softmax, tiling, fusion) (Approach overview, Â§1.1; Backend, Â§4.1; Fig. 2).

## 3. Technical Approach
FlexAttentionâ€™s core idea: most attention variants can be expressed as two tiny pieces of logic applied to the (not materialized) score matrix before softmax:
- `mask_mod(b, h, q_idx, kv_idx) -> bool`: return whether a score entry should be âˆ’âˆ (i.e., masked out). Examples: causal, sliding window, document boundaries (Frontâ€‘end, Â§3.1; Fig. 1).
- `score_mod(score, b, h, q_idx, kv_idx) -> T`: modify a scalar score (e.g., add a positional bias, apply tanh softâ€‘cap). Examples: ALiBI, softcapping (Eq. 1; Â§3.1).

These two user functions are captured with `torch.compile`, fused, and injected into a small set of optimized attention templates.

Stepâ€‘byâ€‘step mechanism
1. Unified abstraction (Frontâ€‘end, Â§3.1; Eq. 1, Fig. 1)
   - Standard attention computes softmax(QK^T / sqrt(d_k))V (Eq. 2).
   - FlexAttention replaces the logits with `mod(QK^T / sqrt(d_k))` (Eq. 1), where `mod` is the combination of `score_mod` and `mask_mod`. This isolates variant logic to the pointwise stage before softmax.
   - The paper distinguishes `mask_mod` from `score_mod` intentionally:
     - Converting a mask into multiplication adds compute and memory traffic.
     - A mask reveals sparsityâ€”entire tiles can be skippedâ€”which is a critical optimization (Â§3.1 â€œWhy mask_mod?â€; Â§4.2).

2. Logical fusion of variants (Frontâ€‘end, Â§3.2; Fig. 1 right)
   - Multiple masks can be composed with boolean â€œand/orâ€ to express combinations (e.g., PrefixLM = â€œprefix fully visible OR causalâ€).
   - Multiple score modifications can be nested (e.g., ALiBI + softcap), enabling composability without kernel rewrites.

3. Templateâ€‘based lowering (Backend, Â§4.1; Fig. 2)
   - `torch.compile` captures the Python of `mask_mod` and `score_mod` and lowers them to Triton IR via TorchInductor.
   - FlexAttention owns three handâ€‘written attention templates (forward, backward, decoding). These templates already implement:
     - Fused matmuls (QK^T and SV) without ever materializing the full score matrix (FlashAttention pattern),
     - Online softmax (numerically stable softmax computed tileâ€‘byâ€‘tile),
     - GPU occupancy management and partitioning/broadcasting,
     - Grouped Query Attention (`GQA`) specialization (Â§4.1).
   - The compiled `mask_mod`/`score_mod` code blocks are inlined at the right place in these templates, preserving fusion and register locality (Fig. 2 bottom).

4. Blockâ€‘sparse execution via `BlockMask` (Backend, Â§4.2; Fig. 3)
   - Concept: Treat the (logical) score matrix as a grid of tiles. If an entire tile is masked, skip it altogether.
   - Data structure:
     - `kv_num_block` (shape BÃ—HÃ—Num_Row): number of nonâ€‘masked tiles per queryâ€‘row,
     - `kv_indices` (shape BÃ—HÃ—Num_RowÃ—Num_Col): column indices of the nonâ€‘masked tiles (Â§4.2 â€œConcretely, BlockMaskâ€¦â€).
   - Generation: a `create_block_mask` utility (with `torch.vmap`) computes `BlockMask` from the userâ€™s `mask_mod` during compilation, not at runtime (Â§1.1; Â§4.2).
   - Full vs. partial tiles (Â§4.2 â€œFull Block Optimizationâ€):
     - Full tiles: all entries visible â†’ skip `mask_mod` checks entirely, apply only `score_mod`.
     - Partial tiles: some entries masked â†’ apply `mask_mod` elementwise.
     - Reported benefit: ~15% speedup on common patterns such as causal (Â§4.2).
   - Indirect access and pipelining:
     - The kernel iterates over the nonâ€‘masked tiles per queryâ€‘row using `kv_indices` (indirect addressing) (Fig. 3, Â§4.2 â€œGuided Indirect Memory Accessâ€).
     - A prefetch pipeline brings the next KV tile while computing the current one (Fig. 4), enabled by removing perâ€‘element branching on masks.
   - Memory footprint:
     - `BlockMask` overhead is O(ceil(Q/BS)Ã—ceil(KV/BS)) for block size `BS` (128 by default), much smaller than storing an itemized BÃ—HÃ—QÃ—KV mask (Â§4.2 â€œOverhead Analysisâ€).

5. Paged attention without kernel rewrites (Case study, Â§5.1; Fig. 5)
   - Setting: To reduce KVâ€‘cache fragmentation at inference, paged attention stores KV in a single physical buffer and uses a â€œpage tableâ€ to map perâ€‘sequence logical indices to physical locations.
   - Challenge: The extra indirection typically requires bespoke kernels.
   - FlexAttention solution:
     - Merge the pageâ€‘table indirection with `BlockMask`â€™s own indirect indexing by converting `kv_indices` from logical to physical indices at runtime (Fig. 5b).
     - Keep `kv_num_block` unchanged (the sparsity pattern is the same), only remap indices.
     - For `mask_mod` and `score_mod`, provide converted versions that map physical KV indices back to logical ones (a maintained O(1) map) and then call the original user functions (Fig. 5c; Â§5.1 â€œmask_mod and score_mod Conversionâ€).

6. Training vs. decoding API alignment (Case study, Â§5.2; Fig. 6)
   - Decoding attends one new query token at a time and needs an `offset` (how many tokens already processed). FlexAttention offers a decorator to transform trainingâ€‘time `mask_mod/score_mod` into decodingâ€‘time versions that incorporate the offset (Fig. 6aâ€“b).

Analogy for intuition
- Imagine the score matrix as a city map. `mask_mod` declares entire neighborhoods you never need to visit; `BlockMask` records only the neighborhoods worth visiting. `score_mod` says what to do when you get to a block (e.g., add a toll for distance). The compiler embeds your rules into a highâ€‘speed bus route (the templates) that skips blocked streets entirely and prefetches the next neighborhoodâ€™s data.

## 4. Key Insights and Innovations
- A simple, expressive frontâ€‘end that matches the structure of attention variants (Fundamental)
  - Novelty: Reducing the design space to two tiny callables on the score matrixâ€”`mask_mod` for visibility and `score_mod` for pointwise logit transforms (Frontâ€‘end, Â§3.1, Eq. 1).
  - Why it matters: It captures most real variants (ALiBI, sliding window, prefix LM, document masking, softcapping) and composes naturally (logical fusion, Â§3.2; Fig. 1), avoiding a combinatorial explosion of bespoke kernels.

- Templateâ€‘based compilation that preserves FlashAttentionâ€‘class optimizations while remaining flexible (Fundamental)
  - Novelty: Use `torch.compile` only for the small user codelets, then splice them into handâ€‘optimized Triton attention templates with online softmax and fused matmuls (Backend, Â§4.1; Fig. 2).
  - Significance: Keeps near stateâ€‘ofâ€‘theâ€‘art performance without sacrificing programmability; supports automatic backward via PyTorch autograd.

- BlockMask: a blockâ€‘sparse execution plan shared by all masks (Fundamental)
  - Novelty: A compact index structure (`kv_num_block`, `kv_indices`) generated from `mask_mod` that drives skipping entire tiles, distinguishes full vs. partial tiles, and enables indirect memory access/prefetch (Backend, Â§4.2; Fig. 3â€“4).
  - Significance: Delivers large compute and memory savings with negligible overhead; reported ~15% speedup from fullâ€‘block optimization on causal masks (Â§4.2).

- Zeroâ€‘rewrite support for paged attention by composing indirections (Incremental but impactful)
  - Novelty: Convert the `BlockMask` tile indices using the page table; wrap `mask_mod/score_mod` with physicalâ†’logical index remapping (Case study, Â§5.1; Fig. 5).
  - Significance: Avoids the typical 20â€“26% overhead seen in other systems and removes the need to maintain separate paged kernels; measured <1% overhead on average (Fig. 12a).

## 5. Experimental Analysis
- Evaluation setup (Evaluation, Â§6.1)
  - Hardware: NVIDIA H100 (limited to 650W, 2.4 TB/s), A100 (330W), A6000.
  - Variants tested: `noop`, `causal`, `alibi`, `sliding_window`, `prefixLM`, `softcap`, `document_mask`; multiâ€‘head attention (MHA) and Grouped Query Attention (GQA).
  - Baselines: SDPA (math, memoryâ€‘efficient, cuDNN), FlashAttention v2 (FAv2), v3 (FAv3, experimental), and FlashDecoding (FAKV) (Table 1).
  - Integration tests: torchtune (training with document mask/jagged packing) and gpt-fast (inference with long contexts) (Â§6.3).

- Kernel performance highlights
  - Training (forward/backward, variable sequence lengths; Fig. 7 top)
    - With causal masking, FlexAttention achieves:
      - Forward: 1.00Ã—â€“1.22Ã— speedup over FAv2,
      - Backward: 0.86Ã—â€“1.05Ã— relative to FAv2,
      - Across lengths 1kâ€“64k and with/without GQA.
  - Variants at 16k tokens (Fig. 7 bottom)
    - For variants supported by FAv2, FlexAttention is 0.68Ã—â€“1.43Ã— of FAv2 (i.e., sometimes faster, sometimes slower but close).
    - For variants unsupported by FA and typically run via SDPA with itemized masks, FlexAttention is 5.49Ã—â€“8.00Ã— faster by avoiding materialized masks and exploiting `BlockMask` (Â§6.2).
  - Decoding (1â€‘token queries; Fig. 8)
    - FlexAttention vs. FAKV: 0.93Ã—â€“1.45Ã— throughput, except a notable win:
      - GQA + ALiBI combination: FlexAttention is 5.37Ã— faster because FAKV falls back to a slower path (Fig. 8 right; Â§6.2 â€œInference Performanceâ€).

- Accuracy (Fig. 9)
  - Rootâ€‘meanâ€‘square error (RMSE) of bf16/fp16 outputs vs fp64 is on par with baselines (no added numerical error). Box plots show comparable distributions across backends.

- Endâ€‘toâ€‘end speedups
  - Training: torchtune on Llamaâ€‘3â€‘8B with document masking (Fig. 10)
    - SDPA throughput drops as length grows due to the quadratic boolean mask traffic (BÃ—NÃ—N).
    - FlexAttention uses `BlockMask` + perâ€‘token document IDs (size BÃ—N) and sustains higher throughput; narrative summary reports 2.4Ã— speedup (Â§6.3).
  - Inference: gpt-fast on Llamaâ€‘3.1â€‘8B (1Ã—H100) and 70B (4Ã—H100) (Fig. 11)
    - 8B: 1.22Ã—â€“2.04Ã— tokens/s over SDPA as context increases.
    - 70B: 0.99Ã—â€“1.66Ã— over SDPA; speedup grows with context length as attention dominates compute (Â§6.3).

- Paged attention overhead (Fig. 12)
  - Across sequence lengths and page sizes, FlexAttention with paged attention adds <1% overhead on average compared to without paging; in some longâ€‘context regimes it even outperforms FAv2 without paging (Â§6.4).
  - Varying page sizes (16â€“256) shows little impact (Fig. 12b). Note: experiments keep the KV cache in GPU memory (no host swapping).

- Additional case: Neighborhood Attention (Appendix A.1; Fig. 13â€“14)
  - Complex 2D locality patterns can be encoded in <10 lines of `mask_mod`, and changing the tiling/mapping (e.g., Morton curve) improves block sparsity and speed. Fig. 14 shows substantial throughput gains over SDPA with itemized masks on A6000 as canvas/kernel sizes grow.

- Overall assessment
  - The experiments are well targeted at the claimed goals:
    - Competitiveness with FA on supported variants,
    - Large wins where FA lacks optimized coverage or SDPA uses itemized masks,
    - Endâ€‘toâ€‘end benefits in realistic training/inference stacks,
    - Minimal overhead integration of paged attention.
  - Ablations/supporting analyses:
    - Performance benefit of â€œfull vs partial blockâ€ optimization (~15% on causal, Â§4.2),
    - Accuracy parity (Fig. 9),
    - Sensitivity to page size (Fig. 12b).
  - Remaining desiderata:
    - More microâ€‘ablations on how `BlockMask` density and block size affect speed/accuracy across hardware,
    - Expanded comparisons against FA v3 across more settings (FAv3 is limited/experimental in Table 1 and Fig. 7).

## 6. Limitations and Trade-offs
- Expressiveness boundary
  - The model assumes variants can be expressed as positionâ€‘based `mask_mod`/`score_mod`. Variants that require nonâ€‘local, dataâ€‘dependent logic beyond positions, or that alter the softmax reduction semantics in nonâ€‘compatible ways, may not fit without extending the templates (Â§3.1; Eq. 1).
- Block sparsity assumptions
  - Performance hinges on tileâ€‘level sparsity. If a mask yields many â€œpartialâ€ tiles or highly irregular perâ€‘element visibility, the benefit over itemized masks may shrink because `mask_mod` must be applied elementwise within many tiles (Â§4.2).
- Template dependency
  - Speed comes from fixed, handâ€‘tuned templates (forward/backward/decoding). New hardware features or exotic compute patterns still require template evolution (Backend, Â§4.1). FA v3â€™s latest asynchrony/precision tricks may not be fully matched yet in all regimes (Table 1; Fig. 7).
- Backend and hardware scope
  - Implementation targets PyTorch/Triton on NVIDIA GPUs (H100/A100/A6000). The paper does not evaluate other accelerators (e.g., AMD, TPU) or CPUs (Â§6.1).
- Paged attention scope
  - The study keeps the KV cache in GPU memory; no hostâ€‘disk swapping is addressed (footnote in Â§5.1), so the performance with outâ€‘ofâ€‘core paging is unknown.
- Developer experience tradeâ€‘offs
  - While `mask_mod`/`score_mod` are simple, users must still reason about positional indexing (`b, h, q_idx, kv_idx`) and ensure their functions are sideâ€‘effectâ€‘free and compilable. Debugging compiled kernels may require Triton/Inductor familiarity (Backend, Â§4.1).

## 7. Implications and Future Directions
- How it changes the landscape
  - It provides a practical â€œkernelâ€‘quality speed with Pythonâ€‘level programmabilityâ€ pathway for attention, reducing the cost of exploring novel variants or combinations. This can accelerate research on longâ€‘context models, efficient inference, and domainâ€‘specific attention patterns (Introduction, Â§1; Â§3.2).
- Enabled followâ€‘ups
  - Richer variant composition: mix softcapping, ALiBI, sliding windows, and document or prefix constraints without bespoke kernels.
  - Automated search over masks/score mods: since variants are small Python callables, AutoML or superoptimization (Ã  la Mirage) could search discrete masks and continuous biases, while FlexAttention guarantees a fused kernel at the end (Â§2.3).
  - Extended sparsity models: dynamic block sizes, hierarchical tiling, or learned block masks to better match content or 2D/3D structures (Appendix A.1 hints that mapping mattersâ€”Morton vs. tiled).
  - Broader system integration: combine with paged attention plus pipeline/TPU/CPU offload strategies; explore hostâ€‘paged KV caches; integrate with parameterâ€‘efficient finetuning stacks (gpt-fast, torchtune).
- Practical applications
  - Serving LLMs with long contexts and diverse positional schemes (ALiBI, soft caps) at nearâ€‘Flash speeds (Fig. 8, Fig. 11).
  - Training packed/batched variableâ€‘length corpora with document masking efficiently (Fig. 10).
  - Vision attention patterns (Neighborhood Attention) where mask geometry is complex yet blockâ€‘sparse (Appendix A.1; Fig. 13â€“14).

Key citations and anchors for quick lookup
- Abstraction and examples: Â§3.1â€“3.2; Eq. 1â€“2; Fig. 1.
- Lowering pipeline: Â§4.1; Fig. 2.
- BlockMask and execution: Â§4.2; Fig. 3â€“4.
- Paged attention integration: Â§5.1; Fig. 5; conversion API in Fig. 6.
- Variant coverage and baselines: Table 1.
- Performance: Fig. 7â€“8 (kernels), Fig. 10â€“11 (endâ€‘toâ€‘end), Fig. 12 (paged), Fig. 9 (accuracy).
- Neighborhood Attention case: Appendix A.1; Fig. 13â€“14.

Quoted findings
- â€œFlexAttention delivers 0.68Ã—â€“1.43Ã— the performance of FAv2 and 0.93Ã—â€“1.45Ã— of FAKV for decodingâ€¦ [and] is 5.37Ã— faster than FAKV when using GQA with ALiBIâ€ (Fig. 7â€“8; Â§6.2).
- â€œFlexAttention boosts endâ€‘toâ€‘end â€¦ inference by 1.22Ã—â€“2.04Ã— â€¦ and training by up to 2.4Ã—â€ (Fig. 10â€“11; Â§6.3).
- â€œFullâ€‘block optimization yields â‰ˆ15% performance improvement on common patterns such as causal masksâ€ (Â§4.2).
- â€œPaged attention adds less than 1% runtime overhead on averageâ€ (Fig. 12a; Â§6.4).

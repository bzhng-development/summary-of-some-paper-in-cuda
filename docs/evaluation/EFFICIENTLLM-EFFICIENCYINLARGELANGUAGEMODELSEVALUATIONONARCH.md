# EFFICIENTLLM: EFFICIENCY IN LARGE LANGUAGE MODELS EVALUATION ON ARCHITECTURE PRETRAINING, FINE-TUNING, AND BIT-WIDTH QUANTIZATION

**ArXiv:** [2505.13840](https://arxiv.org/abs/2505.13840)

## ğŸ¯ Pitch

EfficientLLM introduces the first comprehensive, large-scale empirical benchmark evaluating LLM efficiency across architecture design, fine-tuning, and inference quantization, using over 100 modelâ€“technique pairs on real-world, production-grade hardware. By jointly measuring memory, compute, latency, throughput, energy, and compression, it reveals that all efficiency methods involve nuanced trade-offsâ€”with optimal solutions depending on task, model scale, and deployment constraintsâ€”empowering practitioners to make data-driven decisions for building faster, cheaper, and more sustainable large AI systems.

---

## 1. Executive Summary (2â€“3 sentences)
EfficientLLM is a large-scale, end-to-end benchmark that measures how to make Large Language Models (LLMs) more efficient across three critical stages: architecture pretraining, fine-tuning, and inference via quantization. It runs over 100 modelâ€“technique pairs on production-class GPUs and introduces fine-grained metrics (memory, compute, latency, throughput, energy, compression) to quantify real-world trade-offs; key findings include that no single method is Pareto-optimal, optima depend on task and scale, and many techniques transfer to vision and visionâ€“language models (Figures 2â€“5; Sections 2, 5â€“6).

## 2. Context and Motivation
- Problem addressed
  - There is no comprehensive, empirical, end-to-end benchmark that evaluates LLM efficiency techniques under realistic deployment conditions. Prior work tends to study a single technique in isolation, at limited scale, often without energy measurement or cross-stage comparability (Section 1).
- Why this matters
  - Training and serving LLMs is extremely expensive in compute, memory, energy, and money. For instance, GPTâ€‘3â€™s training required ~3,640 PF-days and an estimated >$4.6M (Introduction). Deployment costs also scale with model and context size; energy and carbon footprints are substantial (Sections 1, 5.1.2).
- Prior approaches and their gaps
  - Architecture: Many â€œefficient attentionâ€ variants, Mixtureâ€‘ofâ€‘Experts (MoE), and positional encodings exist, but their measured trade-offs on modern accelerators and at multiple scales are rarely compared headâ€‘toâ€‘head.
  - Fineâ€‘tuning: Parameterâ€‘Efficient Fineâ€‘Tuning (PEFT) methods (LoRA and variants) are abundant, yet it is unclear which method is best for which model size or latency/energy budget.
  - Inference: Quantization and serving optimizations are common, but their energy/latency/memory/accuracy trade-offs are not consistently reported across families and scales (Sections 3â€“4).
- How this paper positions itself
  - It introduces a unified taxonomy (architecture pretraining, fineâ€‘tuning, inference) and runs a hundredâ€‘scale study on a modern cluster (48Ã— GH200; 8Ã— H200) with standardized metrics (AMU, PCU, AL, TT/ST/IT, AEC, MCR) to enable applesâ€‘toâ€‘apples comparisons (Figure 1; Sections 1, 5.1, 5.2).
  - It extends beyond text to Large Vision Models (LVMs) and Visionâ€‘Language Models (VLMs), testing transferability of efficiency techniques (Section 6).

## 3. Technical Approach
This is an empirical benchmark with carefully controlled hardware, software, and metrics, evaluating techniques across three lifecycle stages.

- Hardware and software
  - Pretraining studies: 48Ã— NVIDIA GH200 (96 GB) with NVLink/InfiniBand; 3D parallelism via Megatronâ€‘Core (tensor, pipeline, data parallel) (Section 5.3 â€œHardware and Training Frameworkâ€).
  - Fineâ€‘tuning studies: 8Ã— NVIDIA H200 (141 GB) using LlamaFactory; DeepSpeed ZeROâ€‘3 Offload for full fineâ€‘tuning when needed (Section 5.4).
  - Inference studies: GH200 nodes; serving on optimized inference servers (Section 5.5).

- Metrics (all formally defined in Section 5.1)
  - `AMU` (Average Memory Utilization): timeâ€‘averaged memory used over total device memory (Eq. 1).
  - `PCU` (Peak Compute Utilization): average actual GPU utilization / theoretical peak; reported only where it varies meaningfully (PEFT; Eq. 2 and footnote).
  - `AL` (Average Latency): average perâ€‘iteration/request time including compute and communication (Eq. 3).
  - Throughput: `TT` (tokens/second/parameter for pretraining; Eq. 4), `ST` (samples/second/parameter for fineâ€‘tuning; Eq. 5), `IT` (tokens/second for inference; Eq. 6).
  - `AEC` (Average Energy Consumption): average power over time, from integrated energy (Eqs. 7â€“8).
  - `MCR` (Model Compression Rate): size reduction adjusted by performance retention (Eq. 9).
  - A composite â€œEfficiency Scoreâ€ used in some visualizations is a weighted harmonic combination of normalized resource metrics (Appendix, Eq. 12; Figures 4, 7c). Normalization recipes are in Appendix (Eqs. 10â€“11).

- Architecture pretraining evaluations (Sections 4.4 and 5.3)
  - Efficient attention
    - `MQA` (Multiâ€‘Query Attention): share key/value across heads; only queries are perâ€‘head. Reduces KVâ€‘cache footprint and speeds decoding (Section 4.4.2).
    - `GQA` (Groupedâ€‘Query Attention): share K/V within groups of headsâ€”intermediate between MHA and MQA (Section 4.4.2).
    - `MLA` (Multiâ€‘Head Latent Attention): compress the KV cache into a lowâ€‘rank latent (dimension `dc << d`), then upâ€‘project per head on the fly: `c_t = h_t W_DKV` and `k_i = c_t W_UK_i`, `v_i = c_t W_UV_i`. This shrinks KV memory while keeping perâ€‘head expressivity (Section 4.4.2).
    - `NSA` (Native Sparse Attention): a triâ€‘branch designâ€”global compression, selective global attention, and local sliding windowâ€”with learned gates `g_cmp, g_slc, g_win` blending the three (Section 4.4.2).
  - Positional encoding (Section 4.4.3)
    - Compares Rotary (`RoPE`), absolute (fixed and learnable), a relative scheme (â€œRelateâ€), and â€œNoneâ€ (no PE).
  - Sparse modeling via `MoE` (Mixtureâ€‘ofâ€‘Experts) (Section 4.4.4)
    - Conditional computation: a router activates only topâ€‘k experts per token (topâ€‘2 in experiments), which reduces FLOPs per token while increasing total parameters. Tradeâ€‘off: extra routing and memory to store all experts.
  - Attentionâ€‘free alternatives (Section 4.4.5)
    - `Mamba` (stateâ€‘space model with selective updates): linearâ€‘time sequence modeling, high speed and low memory.
    - `RWKV` (recurrent variant that trains parallely and infers recurrently).
    - `Pythia` is used here as an attentionâ€‘lite baseline for comparison (Table 6).

  - Experimental design
    - Base backbone for pretraining sweeps uses Qwen2.5â€‘style decoder (0.5B, 1.5B, 3B), trained on FineWebâ€‘Edu (350B tokens)â€”an educational subset designed to improve reasoning/factuality (Sections 5.2, 5.3).

- Training and tuning efficiency evaluations (Section 4.5.2; Section 5.4)
  - `PEFT` (Parameterâ€‘Efficient Fineâ€‘Tuning): adapt only small addâ€‘on modules or selected weights.
    - `LoRA`: keep `W0` frozen and learn `Î”W = Î± A B` with low rank `r << min(m,n)`; merge after training (Section 4.5.2).
    - `LoRAâ€‘plus`: different learning rates for `A` and `B` to improve optimization (Section 4.5.2).
    - `RSLoRA`: rankâ€‘stabilized scaling (`Î± = 1/âˆšr`) to make higher ranks stable (Section 4.5.2).
    - `DoRA`: decompose weights into magnitude and direction; update direction via LoRAâ€‘like term and learn a magnitude vector (Section 4.5.2).
    - `PiSSA`: initialize lowâ€‘rank factors from principal singular vectors/values of `W0`; train `W = A B + R` (Section 4.5.2).
    - `Freeze`: freeze most parameters (e.g., initial layers) for minimal latency/compute.
    - `Full*`: full fineâ€‘tuning with DeepSpeed ZeROâ€‘3 Offload; batch sizes halved to fit memory (Table 7 note).
  - Data: `OpenO1â€‘SFT` (77k English/Chinese instructionâ€‘reasoning samples) and a domain dataset `Medicalâ€‘o1â€‘reasoningâ€‘SFT` (Sections 5.2, 5.4).

- Inference efficiency via bitâ€‘width quantization (Section 5.5)
  - Precisions evaluated: `bfloat16` (bf16), `float16` (fp16), and postâ€‘training `int4` (4â€‘bit weights). `int8` is excluded due to instability/unsupported kernels on GH200 in their setup (Section 5.5 â€œNote on Int8 Quantizationâ€).
  - Metrics include aggregate task score across MMLUâ€‘Pro, BBH, GPQA, IFEval, MATH, MuSR; `IT` (tokens/s), `AMU`, `AEC`, `MCR` (Table 9; Appendix Table 16 for perâ€‘benchmark scores).

- Crossâ€‘modal extensions (Section 6)
  - LVMs: insert efficient attention and MoE into DiTâ€‘style diffusion backbones (DiTâ€‘XL/2, DiTâ€‘L/8, DiTâ€‘B/4) and evaluate FID (FrÃ©chet Inception Distance; lower is better) and efficiency metrics (Tables 10â€“11).
  - VLMs: PEFT on LLaVAâ€‘1.5 (7B), Qwen2.5â€‘VLâ€‘7B, InternVLâ€‘3â€‘38B, QvQâ€‘72B (Table 12).
  - LVM fineâ€‘tuning: PEFT and full FT for Wan 2.1â€‘1.3B (video) and Stable Diffusion 3.5â€‘Medium (Table 13).

## 4. Key Insights and Innovations
1) A truly endâ€‘toâ€‘end, hardwareâ€‘grounded efficiency benchmark for LLMs  
- Whatâ€™s new: A unified evaluation across architecture pretraining, PEFT, and inference quantization, run on modern GH200/H200 clusters with energy tracking and modalityâ€‘agnostic metric collection (Figure 1; Sections 1, 5â€“6).  
- Why it matters: It enables evidenceâ€‘based decisions across the LLM lifecycle rather than isolated, anecdotal choices.

2) A principled metric suite that captures the real bottlenecks  
- Whatâ€™s new: Fineâ€‘grained metricsâ€”`AMU`, `PCU`, `AL`, `TT/ST/IT`, `AEC`, `MCR`â€”with explicit formulas (Section 5.1).  
- Why it matters: They quantify memory saturation, compute utilization, latencyâ€“throughput tradeâ€‘offs, energy cost, and compression in a way FLOPs/params alone cannot.

3) Quantified, crossâ€‘stage tradeâ€‘offs and scale dependence  
- Fundamental finding: â€œNo single technique achieves Pareto optimalityâ€ (Figure 2; Section 2.1).  
  - Example: MoE improves accuracy and reduces perâ€‘token FLOPs but inflates memory by ~40% and adds routing overhead; in their measurements, a `1.5BÃ—8` MoE has `AMU = 76.53 GB` vs `44.82 GB` for a dense `1.5B` (Table 5).  
  - Example: `int4` reduces memory/energy up to ~3.9Ã— with a ~3â€“5% average task score drop (Table 9; Abstract).  
- Scale dependence: `RSLoRA` surpasses `LoRA` on models â‰¥14B (Table 7); freezing layers yields the lowest latency across sizes (Tables 7â€“8).

4) Transferability to LVMs and VLMs  
- Whatâ€™s new: The same efficiency tricks validated on LLMs often help in vision (Tables 10â€“13).  
- Why it matters: It suggests common efficiency principles across modalities; e.g., `MQA/GQA` improve FID in DiT backbones (Table 10), and `RSLoRA/PISSA` scale well for large VLMs (Table 12).

## 5. Experimental Analysis
- Evaluation setup (Sections 5.2â€“5.5)
  - Models: LLaMAâ€‘3 series, Qwenâ€‘2.5 (7B/14B/32B), DeepSeekâ€‘R1 distill variants (1.5B/8B/14B), Phiâ€‘3.5/4, Yiâ€‘34B; LVMs (Stable Diffusion 3.5â€‘Medium, Wan 2.1); VLMs (LLaVAâ€‘1.5, Qwenâ€‘VLâ€‘7B, InternVLâ€‘3â€‘38B, QvQâ€‘72B) (Tables 2, 15).
  - Datasets: FineWebâ€‘Eduâ€‘350B (pretraining sweeps), OpenO1â€‘SFT (PEFT), Medicalâ€‘o1â€‘reasoningâ€‘SFT (medical PEFT), ChatQA (VLMs), plus LVM training corpora (Sections 5.2, 6.1â€“6.3; Table 13).

- Main quantitative results and what they mean

  Architecture pretraining (Section 5.3; Figures 3aâ€“c; Tables 3â€“6)
  - Efficient attention (Table 3, 1.5B scale)
    > `MLA` achieves the best language quality (PPL 7.79), but uses more memory/latency (52.93 GB; 0.2537 s/iter).  
    > `MQA` minimizes memory/latency (AMU 42.24 GB; AL 0.1298 s/iter) with slightly higher PPL (8.23).  
    > `NSA` has the lowest average energy (AEC 598 W) albeit with higher latency (0.5962 s/iter).  
    > `GQA` is a middle ground (PPL 8.09; AL 0.1283 s; AMU 44.87 GB; AEC 652.7 W).

    Interpretation: Choose `MLA` for qualityâ€‘critical pretraining, `MQA` for memoryâ€‘constrained or latencyâ€‘sensitive settings, `NSA` for powerâ€‘constrained training (Figure 3a).

  - Positional encoding (Table 4, 1.5B)
    > `RoPE` gives the lowest perplexity (PPL 8.09).  
    > A relative scheme (â€œRelateâ€) yields the best efficiencyâ€”lower latency (0.1246 s/iter), highest throughput (TT 8.98Ã—10â»Â²), and lower `AMU` (43.94 GB)â€”with a small PPL tradeâ€‘off (8.29).

  - Mixtureâ€‘ofâ€‘Experts vs dense (Table 5)
    > `MoE 1.5BÃ—8 (topâ€‘2)` improves PPL to 7.10 vs dense 1.5Bâ€™s 8.09 and even dense 3Bâ€™s 7.58, and boosts throughput (TT 1.25Ã—10â»Â¹).  
    > But it inflates memory (AMU 76.53 GB) and energy (AEC 692.45 W) and slightly increases latency.

    Takeaway: MoE can be computeâ€‘efficient per token and more accurate, at a memory/energy premium (Figure 3c).

  - Attentionâ€‘free alternatives (Table 6)
    > `Mamba` reduces memory and energy by ~25% (e.g., 1.5B: AMU 30.25 GB; AEC 510.64 W) and improves latency (0.1025 s), but with worse PPL (9.48 vs 8.09 baseline).  
    > `RWKV` and `Pythia` show mixed patternsâ€”moderate efficiency gains but larger PPL penalties.

    Tradeâ€‘off: attentionâ€‘free models can be attractive for strict memory/power budgets, but today typically underperform dense Transformers on PPL.

  PEFT (fineâ€‘tuning) on OpenO1â€‘SFT (Table 7; Figure 4)
  - Small models (1â€“3B)
    > `LoRAâ€‘plus` often has the lowest loss under similar memory, e.g., LLaMAâ€‘3.2â€‘1B loss 0.7442; LLaMAâ€‘3.2â€‘3B loss 0.5791.  
    > `Freeze` gives the lowest latency by ~3Ã— (e.g., 1B: 0.2542 s/iter vs ~1.16â€“2.15 for others) with good loss (0.6425), making it ideal when interactivity matters.

  - Mid/large models (â‰¥14B)
    > `RSLoRA` outperforms `LoRA` in both loss and latency; e.g., Qwenâ€‘2.5â€‘14B loss 0.4126 vs LoRA 0.4795 (Table 7).  
    > `DoRA` tends to have high latency (e.g., up to 8.93 s/iter) despite stable lossâ€”best suited to batch fineâ€‘tuning, not interactive settings.

  - Diminishing returns of full FT at scale
    > Full* FT becomes less attractive as parameters grow (e.g., Mistralâ€‘Smallâ€‘24B loss 1.2805 vs PEFT 0.3757â€“0.3975) and is more energy/memory intensive (Table 7).

  Domain PEFT (Medicalâ€‘o1; Table 8)
  - `Freeze` again gives best latency (e.g., 8B: 0.4632 s/iter) and strong loss (1.0120).  
  - `LoRAâ€‘plus` and `RSLoRA` are competitive in loss across sizes; `DoRA` remains latencyâ€‘heavy.

  Inference quantization (Table 9; Figure 5; Appendix Table 16)
  - Memory/energy/throughput
    > `int4` substantially reduces memory and often increases throughput: e.g., Qwenâ€‘2.5â€‘32B `AMU` 48.30 GB vs 71.33 GB (bf16) and `IT` 19.20 vs 17.54 tok/s.  
    > Compression ratios (`MCR`) approach 3.7â€“3.9Ã— for several models (e.g., DeepSeekâ€‘R1â€‘14B: 3.6965; Phiâ€‘4: 3.9157).

  - Accuracy impact
    > Average task score typically drops modestly (~3â€“5 pp): e.g., DeepSeekâ€‘R1â€‘14B 0.4719 (bf16) â†’ 0.4361 (int4). Appendix Table 16 shows perâ€‘benchmark variations (e.g., MATH is more sensitive).

  - bf16 vs fp16
    > On GH200/H200, bf16 often has lower latency/energy than fp16 (e.g., DeepSeekâ€‘R1â€‘1.5B AEC 144.39 W bf16 vs 158.96 W fp16; Figure 5).  
    > Notably, a few models (e.g., Phiâ€‘4, Yiâ€‘34B) show higher AEC under int4â€”serving stack and kernel maturity matter (Table 9).

  Crossâ€‘modal transfer (LVMs and VLMs; Section 6)
  - Efficient attention for LVMs (Table 10)
    > `MQA/GQA` consistently improve image generation quality: DiTâ€‘XL/2 FID drops from 19.47 (MHA) to 8.93 (MQA) and 8.71 (GQA).  
    > NSA/MLA show mixed results depending on model size and efficiency target.

  - MoE for LVMs (Table 11)
    > Improves FID and throughput while raising AMU/AECâ€”e.g., DiTâ€‘B/4 FID 68.38 â†’ 45.62 and TT 1.39eâ€‘5 â†’ 2.09eâ€‘5, with AMU 15.51 â†’ 18.95 GB.

  - PEFT for VLMs (Table 12; Figure 7c)
    > `LoRAâ€‘plus` is best for LLaVAâ€‘1.5 (loss 0.9716).  
    > `PISSA` leads for Qwenâ€‘VLâ€‘7B (loss 0.3156) and Internâ€‘VLâ€‘3â€‘38B (0.3635).  
    > `RSLoRA` wins at 72B scale (QvQâ€‘Preâ€‘72B loss 0.1434), echoing the LLM trend that `RSLoRA` scales better.

  - Fineâ€‘tuning LVMs (Table 13)
    > Full FT gives the best loss on Wan 2.1â€‘1.3B (0.104) and SD3.5â€‘Medium (0.204), but `GLORA`/`LoHA` provide strong tradeâ€‘offs with much lower AMU/latency.

- Do the experiments support the claims?
  - Yes, the study repeatedly demonstrates quantifiable tradeâ€‘offs and scale dependence across stages and modalities (Figures 2â€“5, 7; Tables 3â€“13). The use of energy and memory metrics, in addition to latency/throughput/accuracy, strengthens realâ€‘world relevance.
  - Ablations/comparisons exist across families, sizes, and techniques; perâ€‘metric radar and bar plots (Figures 3â€“5, 7) align with tabled numbers.

- Failure cases and caveats
  - Int8 inference is excluded due to GH200 kernel instability (Section 5.5).  
  - Some int4 cases show higher energy (Phiâ€‘4, Yiâ€‘34B), highlighting that quantization benefits depend on kernels and serving stack (Table 9).

## 6. Limitations and Trade-offs
- Coverage limits (Section 8.1)
  - The study focuses on three efficiency axes (architecture, PEFT, quantization). It omits, e.g., longâ€‘context KVâ€‘cache strategies, retrieval and alignment (RLHF) cost/quality tradeâ€‘offs, speculative decoding, and advanced serving schedulers.
- Hardware specificity
  - Results are on GH200/H200 clusters; behavior on TPUv4/TPUâ€‘v5p, consumer GPUs, or heterogeneous clusters may differ (Section 8.1).
- Scale in pretraining sweeps
  - Architecture pretraining results are at 0.5Bâ€“3B; conclusions may shift at â‰¥10B during pretraining (Table 3). The MoE memory/power overhead could scale differently for larger systems.
- Metrics and economics
  - Metrics are averaged; they donâ€™t capture transient spikes or tail latencies in multiâ€‘tenant serving (Section 8.1). Economic cost models (cloud pricing, amortization) are not included.
- PCU scope
  - `PCU` is meaningfully reported for PEFT; pretraining/inference showed nearâ€‘constant utilization on their stack (footnote in Section 5.1), so PCU comparisons there are intentionally limited.

## 7. Implications and Future Directions
- How this changes the landscape
  - EfficientLLM provides a common yardstick to reason about LLM efficiency tradeâ€‘offs across the entire lifecycle. It moves the discussion from isolated improvements to multiâ€‘objective, hardwareâ€‘aware optimization (Figures 2â€“5; Sections 2, 5â€“6).
- Practical guidance distilled from the benchmark
  - Architecture pretraining (Table 3; Figure 3a):  
    - Use `MQA` for memory/latencyâ€‘bound settings; `MLA` for qualityâ€‘first; `NSA` when power is the primary constraint.  
    - Consider relative PE schemes for efficiency (Table 4), and MoE when memory budget allows (Table 5).
  - Fineâ€‘tuning (Tables 7â€“8; Figure 4):  
    - `LoRAâ€‘plus` is a solid default for â‰¤3B; `RSLoRA` for â‰¥14B; `Freeze` for the lowest latency; avoid `DoRA` for interactive use.  
    - Full FT yields diminishing returns at very large scales.
  - Inference (Table 9; Figure 5):  
    - Prefer `int4` when memory/throughput dominate and small accuracy drops are acceptable; otherwise `bf16` is a strong default on Hopper GPUs.
- Research directions (Section 8.2)
  - Vectorâ€‘valued scaling laws balancing loss with latency/memory/energy across compute budgets.  
  - Memoryâ€‘aware MoE routing and unified theory for computeâ€“memory tradeâ€‘offs.  
  - Robust postâ€‘training quantization for long contexts (activation outliers), including joint weightâ€“activation schemes.  
  - Hardwareâ€‘aware autoâ€‘schedulers that jointly optimize data/tensor/pipeline/expert parallelism in heterogeneous clusters.  
  - Crossâ€‘modal PEFT designs that generalize across language, vision, audio, and tooling.

- Downstream applications
  - Costâ€‘constrained deployments (onâ€‘device, edge), green AI initiatives (minimizing AEC), rapid domain adaptation (PEFT recipes per scale), and capacity planning (predictable memory/latency profiles for a chosen technique mix).

> Bottom line: The benchmark shows efficiency is a multiâ€‘objective optimization problem with contextâ€‘dependent optima. With standardized metrics and crossâ€‘stage evidence, EfficientLLM turns folklore into data-driven guidance for building faster, cheaper, and greener foundation models.

# FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

**ArXiv:** [2205.14135](https://arxiv.org/abs/2205.14135)

## ğŸ¯ Pitch

FlashAttention introduces a novel, IO-aware exact attention algorithm for Transformers that dramatically reduces memory traffic between GPU high-bandwidth memory (HBM) and on-chip SRAM by tiling computations and avoiding the need to store the full NÃ—N attention matrix in slow memory. This breakthrough enables Transformers to process longer sequences much faster and with far less memoryâ€”achieving up to 7.6Ã— speedups and unlocking new practical capabilitiesâ€”making it a foundational step towards truly scalable and efficient large-scale model training and inference.

---

## 1. Executive Summary (2â€“3 sentences)
FlashAttention is an exact (not approximate) attention algorithm for Transformers that reorganizes the computation to minimize slow memory traffic between GPU high-bandwidth memory (`HBM`) and on-chip static RAM (`SRAM`). By tiling the computation, incrementally computing softmax, and recomputing intermediates during backpropagation, it avoids materializing the full NÃ—N attention matrix in `HBM`, yielding large, demonstrated wallâ€‘clock speedups and memory savings across models and sequence lengths (e.g., up to 7.6Ã— speedup on GPTâ€‘2 attention in Figure 1 right; 15% endâ€‘toâ€‘end BERTâ€‘large speedup in Table 1).

## 2. Context and Motivation
- Problem addressed
  - Standard attention requires computing S = QKáµ€ (size NÃ—N), applying softmax to get P, and then O = PV. Common implementations materialize S and P in GPU `HBM` (Algorithm 0), which causes both quadratic memory usage and a large number of slow memory reads/writes. Section 2.2 and Algorithm 0 detail this pipeline and its `HBM` traffic.
- Why it matters
  - On modern GPUs, computation is fast but memory movement is comparatively slow; many deep learning operations are memoryâ€‘bound rather than computeâ€‘bound (Section 2.1, â€œPerformance characteristics,â€ and Figure 1 left). For attention, this means wallâ€‘clock time often scales with memory input/output (IO), not with FLOPs.
  - Long sequences are increasingly important (language, vision, longâ€‘document tasks). Quadratic `HBM` traffic limits feasible context length and training speed.
- Prior approaches and shortcomings
  - Approximate attention methods (sparse or lowâ€‘rank) reduce FLOPs to nearâ€‘linear but often do not yield wallâ€‘clock speedups and may reduce model quality (Abstract; Section 1). A common reason: they focus on FLOP reduction while ignoring IO/memory movement overheads.
  - Some optimized attention kernels fuse a few steps but still read/write large intermediates to `HBM` (Section 2.1 â€œKernel fusionâ€ and Appendix E.4 on Apex FMHA).
- Positioning
  - FlashAttention reframes exact attention as an IOâ€‘aware algorithmâ€”explicitly optimizing the number and pattern of reads/writes between `HBM` and onâ€‘chip `SRAM` (Figure 1 left). It uses two techniquesâ€”tiling and recomputationâ€”to compute the same mathematical result while drastically reducing `HBM` traffic (Section 3.1, Algorithm 1).

## 3. Technical Approach
At a high level, FlashAttention changes â€œwhenâ€ and â€œwhereâ€ attentionâ€™s sub-steps are computed so that data live on the fast onâ€‘chip memory (`SRAM`) when they are needed, and the large NÃ—N intermediate matrices are never stored in `HBM`.

Key concepts defined as first used:
- `HBM` (High Bandwidth Memory): GPUâ€™s large but slower offâ€‘chip memory.
- `SRAM`: small, onâ€‘chip memory that is much faster (A100 example bandwidths in Figure 1 left).
- IOâ€‘aware: an algorithm designed to minimize reads/writes between memory levels.
- Tiling: split large matrices into blocks and compute with a small subset in `SRAM` at a time.
- Recomputation: during backpropagation, recompute intermediates onâ€‘chip instead of reading them from `HBM`.
- Fused kernel: implement the entire attention pipeline inside a single GPU kernel to avoid extra IO.

3.1 What the standard pipeline does and why it is slow (Algorithm 0; Section 2.2)
- Compute S = QKáµ€ and write S (NÃ—N) to `HBM`.
- Read S, compute P = softmax(S), write P (NÃ—N) to `HBM`.
- Read P and V, compute O = PV, write O to `HBM`.
- Result: Î˜(Nd + NÂ²) `HBM` accesses in forward alone (Theorem 2).

3.2 FlashAttention forward: tile and incrementally normalize (Algorithm 1, expanded in Algorithm 2)
- Partition Q into row blocks of size `B_r Ã— d` and K,V into column blocks of size `B_c Ã— d` (Algorithm 1, lines 1â€“4).
- Outer loop: load one K_j, V_j block from `HBM` to `SRAM` (line 6).
- Inner loop: for each Q_i block (line 8),
  1) Compute S_ij = Q_i K_jáµ€ on chip (`SRAM`) (line 9).
  2) Apply masking and compute a numerically stable softmax â€œpartiallyâ€ for the current block using per-row max and sum, `mÌƒ_i j` and `â„“Ìƒ_i j` (line 10; mask in Algorithm 2 line 11).
  3) Merge these per-block stats into global running stats for the same rows: `m_new = max(m, mÌƒ)` and `â„“_new = exp(mâˆ’m_new)â„“ + exp(mÌƒâˆ’m_new)â„“Ìƒ` (line 11; same formulas appear in Section 3.1 under â€œTiling,â€ and in Algorithm 2 lines 12â€“13). This exactly reproduces the softmax as if computed over the full row by using algebraic aggregation.
  4) Update the running output block O_i with properly rescaled contributions from this K,V block: O_i â† diag(â„“_new)^{-1} (diag(â„“) e^{mâˆ’m_new} O_i + e^{mÌƒâˆ’m_new} PÌƒ_ij V_j) (line 12; Algorithm 2 line 15).
  5) Save the updated O_i, â„“_i, m_i to `HBM` (lines 12â€“13) before moving to the next Q_i.
- Why it works
  - The key trick is decomposing softmax across concatenated blocks using per-row max `m` and sum `â„“` (Section 3.1 â€œTilingâ€; the derivation shows how to merge partial softmax results exactly).
  - Correctness is proven in Theorem 1: Algorithm 1 returns O = softmax(QKáµ€)V, with O(NÂ²d) FLOPs and O(N) extra memory.

3.3 FlashAttention backward: recompute instead of reading large intermediates (Algorithm 4; Appendix B.2â€“B.4)
- Store only O, the per-row softmax stats (`â„“`, `m`), and the PRNG state for dropout in the forward pass (Algorithm 2, lines 1, 19). Do not store NÃ—N attention matrices.
- During backward:
  - Recreate the same dropout mask from the stored PRNG state (Algorithm 4, lines 1, 14).
  - Recompute the needed P blocks on the fly using Q_i, K_j, `â„“_i`, `m_i` (Algorithm 4, lines 11â€“15).
  - Compute gradients without ever forming full NÃ—N matrices:
    - dV accumulates from (P_dropped)áµ€ dO (Algorithm 4, lines 16, 24).
    - dP = dO Váµ€, and the softmax Jacobian is applied in blocked form to obtain dS = P âˆ˜ (dP âˆ’ D) with D_i = sum(dO_i âˆ˜ O_i) (lines 17â€“20; see the scalar derivation in Appendix B.2, Eqs. (3)â€“(6)).
    - dQ accumulates as dQ_i â† dQ_i + Ï„ dS_ij K_j; dK similarly from dSáµ€Q (lines 21â€“22, 24).
- Benefit
  - Recomputation increases FLOPs slightly but slashes `HBM` IO and thus runtime (Figure 2 left: GFLOPs increase from 66.6â†’75.2, but `HBM` read/write drops from 40.3 GBâ†’4.4 GB; runtime drops 41.7â†’7.3 ms).

3.4 Complexity analysis (Section 3.2; Theorem 2 and proof)
- Let `N` = sequence length, `d` = head dimension, `M` = usable onâ€‘chip `SRAM` size.
- Standard attention: Î˜(Nd + NÂ²) `HBM` accesses.
- FlashAttention: It loads each K,V block once and makes T_c passes over Q,O, where T_c â‰ˆ N d / M (proof, â€œWe then have: T_c = Î˜(N d / M)â€). Total `HBM` accesses become Î˜(N d Â· T_c) = Î˜(NÂ² dÂ² / M).
- Lower bound: Proposition 3 shows no exact attention algorithm can asymptotically beat this form across all `M` in [d, N d]; i.e., up to constants FlashAttention is IOâ€‘optimal over a wide memory range.
- Empirical confirmation: Figure 2 middle varies block size (affecting `HBM` accesses) and shows runtime drops until arithmetic becomes the bottleneck.

3.5 Blockâ€‘sparse FlashAttention (Section 3.3; Algorithm 5; Proposition 4)
- Idea: If attention has a known blockâ€‘sparse pattern (only some Qâ€‘K block pairs interact), skip zero blocks entirely but keep the same IOâ€‘aware fused implementation.
- Complexity: Î˜(N d + (NÂ² dÂ² / M) Â· s) `HBM` accesses, where `s` is the fraction of nonâ€‘zero blocks (Proposition 4). This yields proportional IO and runtime gains (Figure 2 right).

3.6 Implementation details (Section 3.1 â€œImplementation details: Kernel fusionâ€; Section 2.1; Appendix E.4)
- One fused CUDA kernel handles â€œmatmul â†’ mask â†’ softmax â†’ dropout â†’ matmul,â€ reading inputs once per tile and writing only the final O (Figure 1 left).
- Memory hierarchy on A100: `SRAM` â‰ˆ 20 MB per GPU at â‰ˆ19 TB/s vs. `HBM` â‰ˆ 40â€“80 GB at 1.5â€“2.0 TB/s (Figure 1 left), motivating aggressive onâ€‘chip reuse.
- Compared with Nvidia Apex FMHA (which still stores the NÃ—N attention matrix in forward), FlashAttention is comparable or faster for short sequences and scales to much longer ones with far lower memory (Appendix E.4, Table 7).

## 4. Key Insights and Innovations
- IOâ€‘aware reformulation of exact attention
  - Novelty: Puts memory trafficâ€”not FLOPsâ€”at the center of algorithm design for attention. Unlike prior â€œfusedâ€ kernels, it avoids ever writing the NÃ—N attention matrix to `HBM` by computing and merging perâ€‘tile softmax statistics (`m`, `â„“`) (Algorithm 1; Section 3.1).
  - Impact: Reduces `HBM` accesses from Î˜(Nd + NÂ²) to Î˜(NÂ² dÂ² / M) (Theorem 2), directly predicting large wallâ€‘clock speedups on memoryâ€‘bound hardware (confirmed in Figure 2 left).
- Exact incremental softmax with algebraic aggregation
  - Whatâ€™s new: A practical tiling scheme that maintains exactness via the perâ€‘row max/sum trick for softmax (Section 3.1 â€œTilingâ€ equations). This enables exact attention without seeing all keys at once.
  - Why it matters: Prior â€œmemoryâ€‘efficientâ€ attention ideas avoided storing intermediates but still incurred quadratic reads/writes; here the incremental normalization makes a fused, IOâ€‘light pass possible.
- Backward recomputation with analytical simplifications
  - Whatâ€™s different: Instead of generic gradient checkpointing, the derivation uses O(N) statistics (`m`, `â„“`) and perâ€‘row dot products D_i = âŸ¨dO_i, O_iâŸ© to avoid NÃ—N tensors entirely (Appendix B.2â€“B.4; Algorithm 4).
  - Benefit: Keeps the backward pass IOâ€‘optimal and fast despite extra FLOPs (Figure 2 left).
- Blockâ€‘sparse extension as a firstâ€‘class primitive
  - Contribution: A dropâ€‘in IOâ€‘aware kernel that respects a block sparsity mask, achieving further linear improvements in IO proportional to sparsity (Section 3.3; Proposition 4; Figure 2 right).
  - Significance: Outperforms prior approximate methods in runtime at long lengths while often maintaining exactness for the nonâ€‘zero pattern.

These are fundamental innovations (changing the algorithmic objective to IOâ€‘minimization and proving nearâ€‘optimal IO bounds), not just incremental engineering.

## 5. Experimental Analysis
Evaluation setup (Section 4; Appendix E):
- Hardware: Primarily Nvidia A100 GPUs; additional tests on RTX 3090 and T4 (Appendix E.5).
- Models/datasets:
  - BERTâ€‘large pretraining on Wikipedia, MLPerf 1.1 setup (Table 1).
  - GPTâ€‘2 small/medium on OpenWebText (Table 2), with context lengths up to 4K (Table 4).
  - Long Range Arena (LRA) suite (sequence lengths 1Kâ€“4K) for accuracy and speed (Table 3).
  - Long document classification: MIMICâ€‘III and ECtHR (Table 5).
  - Pathâ€‘X (16K tokens) and Pathâ€‘256 (64K) tasks (Table 6).
- Baselines: Standard PyTorch attention, Megatronâ€‘LM attention, Apex FMHA (Appendix E.4), and multiple approximate/sparse methods (Linformer, Performer, Reformer, Local, BigBird, Longformer, Smyrf, LSFormer), with fair settings and, when needed, FP32 for methods lacking FP16 support (Appendix E.6).
- Metrics: Wallâ€‘clock time, memory usage, throughput, accuracy/perplexity. Where relevant, dropout and masking are included (Table series 9â€“21).

Main quantitative results
- Microbenchmarks: attention kernel speed and IO
  - GPTâ€‘2 medium (N=1024, d=64, 16 heads, bs=64) on A100: FlashAttention reduces `HBM` read/write from 40.3 GB to 4.4 GB and runtime from 41.7 ms to 7.3 ms, with a slight FLOP increase (66.6â†’75.2 GFLOPs). Quote (Figure 2 left):
    > â€œHBM R/W 40.3 GB â†’ 4.4 GB; Runtime 41.7 ms â†’ 7.3 ms; 7.6Ã— speedup on the attention computation.â€
  - Runtime decreases as block size increases (fewer passes over Q), until arithmetic becomes the bottleneck (Figure 2 middle).
  - Blockâ€‘sparse FlashAttention runs faster by a factor proportional to sparsity (Figure 2 right).
- Endâ€‘toâ€‘end training speed
  - BERTâ€‘large (MLPerf 1.1 target): Table 1 shows
    > â€œ20.0 Â± 1.5 min (Nvidia MLPerf) vs. 17.4 Â± 1.4 min (FlashAttention)â€ â€” a 15% speedup.
  - GPTâ€‘2 on OpenWebText: Table 2 shows
    > GPTâ€‘2 small: 9.5 days (HuggingFace) â†’ 2.7 days (FlashAttention), 3.5Ã— speedup; 4.7 days (Megatron) â†’ 2.7 days, 1.7Ã—.
    > GPTâ€‘2 medium: 21.0 days (HF) â†’ 6.9 days (FlashAttention), 3.0Ã—; 11.5 days (Megatron) â†’ 6.9 days, 1.7Ã—.
  - LRA throughput: Table 3 reports up to 2.4Ã— speedup for FlashAttention over standard Transformer, and 2.8Ã— for blockâ€‘sparse FlashAttention; accuracy is on par with baselines.
- Longer context with exact attention
  - GPTâ€‘2 small with FlashAttention at 4K context is still faster than Megatron at 1K and improves perplexity (Table 4):
    > 1K Megatron: 18.2 ppl, 4.7 days; 4K FlashAttention: 17.5 ppl, 3.6 days (1.3Ã— speedup vs Megatron 1K).
- Downstream quality gains from longer sequences
  - Longâ€‘document classification (Table 5):
    > MIMICâ€‘III microâ€‘F1: 52.8 (512) â†’ 57.1 (16K), +4.3 points vs 512; ECtHR: 72.2 (512) â†’ 80.7 (8K), +8.5 points vs 512.
- New capabilities at very long lengths
  - Pathâ€‘X (16K) and Pathâ€‘256 (64K): Table 6
    > First betterâ€‘thanâ€‘chance Transformer results: FlashAttention 61.4% on Pathâ€‘X; blockâ€‘sparse FlashAttention 63.1% on Pathâ€‘256.
- Runtime and memory scaling vs alternatives (Figure 3; Tables 9â€“21)
  - Forward+backward runtime: FlashAttention is up to 3Ã— faster than PyTorch attention for Nâ‰¤2K and remains competitive with many approximate/sparse methods up to a crossover between 512â€“1024 (Figure 3 left).
  - Memory: FlashAttention uses memory linear in N and is up to 20Ã— more memoryâ€‘efficient than exact attention baselines; it reaches N=64K where most baselines OOM (Figure 3 right). Table 21 shows, e.g., at N=8192 the memory is 1672 MB for FlashAttention vs 6784 MB for Local Attention and 24134 MB for Reformer.
- Comparison to Apex FMHA (Appendix E.4, Table 7)
  - For Nâ‰¤512, forward is slightly faster and backward slightly slower; net is comparable or slightly better. Crucially, FlashAttention scales to long sequences and reduces memory footprint because it does not store NÃ—N attention in forward.

Ablations, robustness, and additional checks
- Block size vs runtime (Figure 2 middle).
- Hardware sensitivity: speedups reported on A100, RTX 3090, and T4; trends match IO analysis (Appendix E.5, Figures 5â€“8).
- Numerical stability: validation perplexity curves for GPTâ€‘2 small/medium match HuggingFace across training (Appendix E.2, Figure 4).
- Broad baselines and consistent measurement protocols across dropout/masking settings (Tables 9â€“21).

Assessment
- The experiments convincingly support the core claims: reducing `HBM` IO leads to large, consistent wallâ€‘clock speedups and substantial memory savings without sacrificing exactness. Results span microâ€‘benchmarks, endâ€‘toâ€‘end training, and downstream tasks.

## 6. Limitations and Trade-offs
- Still quadratic FLOPs in sequence length
  - FlashAttention is exact attention; it avoids quadratic `HBM` traffic but not quadratic compute. Very large N may still be computeâ€‘limited once IO is minimized (Figure 2 middle shows a regime where runtime plateaus as compute dominates).
- Singleâ€‘GPU algorithm and kernel specialization
  - Design and proofs focus on a single GPUâ€™s `SRAM`/`HBM` hierarchy (Theorem 2). Multiâ€‘GPU sharding and interâ€‘GPU bandwidth are not addressed (Section 5 â€œMultiâ€‘GPU IOâ€‘Aware Methodsâ€).
  - Requires custom CUDA kernels tuned to GPU architectures (Section 5 â€œCompiling to CUDAâ€), increasing engineering burden and potential portability issues.
- Tuning block sizes and head dimensions
  - Effective block sizes depend on `SRAM` size and head dimension `d`; for larger `d`, blocks must shrink (Appendix E.5, Figure 6), reducing speedups.
- Applicability of blockâ€‘sparse extension
  - Blockâ€‘sparse FlashAttention assumes a block sparsity mask (Algorithm 5). Where sparsity is not available or mismatched to the task, the dense kernel is used.
- Dropout and masking assumptions
  - Backward relies on reâ€‘generating the same dropout mask from saved PRNG state (Algorithm 4, line 1). This is standard, but it is an assumption that the random state is preserved correctly across frameworks/runs.

## 7. Implications and Future Directions
- Broader impact on efficient deep learning
  - Demonstrates that IOâ€‘aware algorithm design can unlock large, practical speedups even for â€œfixedâ€ architectures like attention. This reframes a common optimization goal from FLOPs to data movementâ€”likely applicable to other layers that are memoryâ€‘bound (Section 5 â€œIOâ€‘Aware Deep Learningâ€).
- Enabling longerâ€‘context Transformers
  - FlashAttentionâ€™s linearâ€‘inâ€‘N memory enables training/evaluating with much longer contexts, improving quality in language modeling and longâ€‘document tasks (Tables 4â€“5) and enabling previously outâ€‘ofâ€‘reach benchmarks (Table 6).
- Compiler/DSL opportunities
  - A compelling target is a highâ€‘level compiler that emits IOâ€‘aware kernels from declarative attention specs, akin to Halide for image processing (Section 5 â€œCompiling to CUDAâ€).
- Multiâ€‘GPU and systemâ€‘level IO analysis
  - Extending IO analysis to include interâ€‘GPU (and even CPUâ€‘GPU) communication could yield new parallel attention algorithms optimized for modern clusters (Section 5 â€œMultiâ€‘GPU IOâ€‘Aware Methodsâ€).
- Beyond attention
  - The same principles may accelerate memoryâ€‘bound components like normalization layers, certain sequence models, or kernel machines (Appendix D.2 â€œKernel machine learningâ€). Blockâ€‘sparse ideas could also be applied to MLPs and other modules if IOâ€‘aware sparsity implementations are devised (Appendix D.2 â€œSparse MLP layersâ€).

Overall, FlashAttention is a fundamental reframing of exact attention around IOâ€‘efficiency, backed by both theory (Theorem 2; Proposition 3) and extensive empirical validation (Figures 1â€“3; Tables 1â€“7, 9â€“21). It provides immediate practical benefits and opens a pathway for IOâ€‘aware design across the deep learning stack.

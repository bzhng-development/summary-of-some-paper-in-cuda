# Relax: Composable Abstractions for End-to-End Dynamic Machine Learning

**ArXiv:** [2311.02103](https://arxiv.org/abs/2311.02103)

## ðŸŽ¯ Pitch

Relax introduces a unified compiler abstraction that bridges the gap between high-level computation graphs, loop-level tensor programs, and vendor operator libraries, while pioneering first-class symbolic shape annotations to enable end-to-end optimization of dynamic-shape machine learning workloadsâ€”including large language models. This cross-level approach empowers ahead-of-time (AOT) compilation with dynamic-shapeâ€“aware operator fusion, memory planning, and deployment to a wide range of backends (GPUs, mobiles, browsers), delivering performance and portability previously out of reach for dynamic models and supporting the next generation of machine learning systems.

---

## 1. Executive Summary
Relax is a new compiler abstraction and end-to-end compilation framework that makes dynamicâ€‘shape machine learning models (especially LLMs) run efficiently and portably. It unifies three traditionally separate layersâ€”graph IR, loopâ€‘level tensor programs, and vendor librariesâ€”into a single crossâ€‘level representation, and adds firstâ€‘class symbolic shape annotations so the compiler can optimize across levels even when tensor sizes are only known at runtime (Â§Â§3â€“4, Fig. 1).

The result is a practical AOT (aheadâ€‘ofâ€‘time) pipeline that delivers competitive performance to stateâ€‘ofâ€‘theâ€‘art systems across NVIDIA, AMD, and Apple GPUs, enables CUDA Graphs for dynamic models, cuts memory with dynamicâ€‘shapeâ€“aware planning, and deploys modern LLMs to emerging backends like mobile and WebGPU (Â§5, Figs. 14â€“20, Table 2, Table 3).

## 2. Context and Motivation
- Problem addressed
  - Modern ML workloads, notably large language models (LLMs), contain dynamic shapes: dimensions (e.g., batch size, sequence length, KV-cache length) that are unknown until runtime. This breaks many classic compiler optimizations such as static memory planning and crossâ€‘operator fusion that assume fixed sizes (Â§1, Â§2).
  - Existing ML compilers typically use multiple IR layersâ€”graph IR, loopâ€‘level tensor IR, and vendor librariesâ€”and â€œsingleâ€‘shotâ€ lowering between them. Each layer tends to treat the others as opaque, which prevents analyses or transformations that require information to flow across layers (Â§1, Â§2, Fig. 1).
- Why this matters
  - Real systems need to deploy models onto heterogeneous backends (servers, laptops, phones, embedded devices, browsers). JIT approaches that trace shapes at runtime often donâ€™t fit constrained or sandboxed environments (e.g., mobile, WebGPU). AOT compilation with wholeâ€‘program optimization is needed (Â§1, Â§2).
  - Without dynamicâ€‘shape reasoning, systems fall back to general-purpose runtime allocators, pay kernelâ€‘launch overheads, and miss fusion opportunitiesâ€”hurting latency, throughput, and memory usage (Â§1, Â§4.3â€“Â§4.5).
- Prior approaches and their gaps
  - Graph IRs with unknown shapes: Relay and several MLIR dialects represent dynamic dimensions as â€œunknown,â€ which loses relations between dimensions (e.g., output has size 4Ã—n) and limits optimization (Â§1, Â§3.2, Fig. 3).
  - JIT tracing: PyTorchâ€™s compiler tracks shapes per traced function, sidestepping crossâ€‘function shape tracking, but limiting AOT portability to environments like mobile/WebGPU (Â§1).
  - Loopâ€‘level work: Halide, DietCode, CoRA, SparseTIR optimize tensor programs and sometimes call libraries from within kernels, but they do not solve the crossâ€‘level, wholeâ€‘program optimization with dynamic shapes (Â§1, Â§6).
- Positioning
  - Relax introduces a crossâ€‘level program abstraction that integrates graphâ€‘level, loopâ€‘level (TensorIR), and external libraries in one IR, plus firstâ€‘class symbolic shapes tracked across function boundaries. This enables AOT, wholeâ€‘program, shapeâ€‘aware optimizations that span levels (Â§Â§3â€“4, Fig. 1).

## 3. Technical Approach
Relax consists of two pivotal ideas and a concrete optimization/lowering pipeline.

A) Firstâ€‘class symbolic shapes with interprocedural tracking (Â§3.2, Fig. 3; Table 1)
- What: Represent dynamic sizes as symbolic expressions over integer variables (e.g., `n`, `4*n`, `n+1`). These annotations live on values (tensors, tuples, shapes), can be passed as firstâ€‘class values, and are preserved through transformations.
- Why: This preserves relations like â€œflatten of `(n, 2, 2)` has length `4*n`,â€ enabling the compiler to reason about equality, reuse memory, and validate fusion even when `n` is not known at compile time (Fig. 3).
- How:
  - Forward symbolic deduction: Each operator has a shape rule that computes the output annotation from input annotations/values. Relax performs efficient forward propagation across the program and across function boundaries (Â§4.1).
  - `match_cast`: For dataâ€‘dependent shapes (e.g., `unique`), the compiler allows inserting a checked assertion that a value conforms to a symbolic annotation, with lightweight runtime checks (Â§3.2, Fig. 3).
  - Function signatures carry shape relations: Functions (including subgraphs) declare parameter/return annotations so callers can infer output shapes without seeing the callee body (Â§4.1, Fig. 7).
  - Symbolic expressions in parameter types: Fusions can pass additional shape arguments (e.g., `shape(n)`) when fused bodies refer to `2*n` to keep fused functions wellâ€‘typed (Fig. 8).

B) Crossâ€‘level abstraction with foreign calls in a single IR (Â§3.3, Figs. 4â€“5)
- What: Graphâ€‘level functions can call:
  - Loopâ€‘level tensor programs using `call_tir`, and
  - External vendor/library functions using `call_dps_library`.
- Why: Unifies the three levels so the compiler can partially lower some regions, analyze or transform others, and feed analyses back across levels (Fig. 6).
- How:
  - Destinationâ€‘passing style (DPS): Many lowâ€‘level functions accept an output buffer to write into. Relax models `call_tir` so the graph IR allocates the destination, passes it to the lowâ€‘level kernel, and returns the tensor view (Fig. 5). This keeps lowâ€‘level code simple and lets the graph level own memory management.
  - Shapeâ€‘aware calls: `call_tir` carries an explicit output annotation and optional symbolic arguments so TensorIR can specialize on static dimensions and keep only truly dynamic ones (Fig. 4).

C) Algorithms and crossâ€‘level optimizations (Â§4, Figs. 9â€“13; Algs. 1â€“3)
1) Dynamicâ€‘shapeâ€‘aware operator fusion (graph + tensor levels) (Â§4.2, Fig. 9)
   - Computeâ€‘pattern analysis on tensor programs (Alg. 1) classifies kernels (e.g., `ElementWise`, `Injective`, `Reduction`, `OutputEwiseFusible`).
   - Graph partitioning/fusion (Alg. 2, â€œFuseOpsâ€) groups calls (including custom tensor programs) into subgraph functions based on patterns (e.g., fusing `matmul` [OutputEwiseFusible] with following elementwise ops).
   - Crossâ€‘level â€œFuseTensorIRâ€ merges the tensor programs called in the subgraph into a single TensorIR function, preserving symbolic shapes (Fig. 9).
   - Example: Fusing a custom â€œquantization decodeâ€ loop with a matmul, no bespoke highâ€‘level operator required (Fig. 9, leftâ†’right).

2) Dynamicâ€‘shapeâ€‘aware memory planning (Â§4.3, Fig. 10; Alg. 3)
   - Step 1: Lower `call_tir` and `call_dps_library` to explicit allocations and DPS calls so allocations are visible (Fig. 5).
   - Step 2: Perform liveness analysis; maintain a storage pool that compares required sizes using symbolic equality (Alg. 3, lines 7â€“13).
   - Step 3: Reuse storage if shapes match symbolically; otherwise allocate new. Optionally use userâ€‘provided upper bounds (e.g., max context length) to preâ€‘allocate once, even with dynamic shapes (Â§4.3).

3) Crossâ€‘level workspace lifting (Â§4.4, Fig. 11)
   - Detect global workspace allocations inside tensor programs (e.g., Streamâ€‘K matmulâ€™s partial accumulations).
   - Lift these to graph level as explicit buffers passed into `call_tir` and then include them in global memory planning (Fig. 11).

4) CUDA Graph offloading for dynamic models (Â§4.5)
   - CUDA Graphs reduce perâ€‘kernel launch overhead by capturing a graph of launches, but require fixed, preâ€‘allocated memory.
   - After static planning, detect subgraphs that satisfy capture constraints; insert runtime builtins to â€œcapture on first run, replay thereafterâ€ (Â§4.5). This extends CUDA Graphs to dynamicâ€‘shape models by making memory static at capture time.

5) Partial lowering and operator optimization (Â§4.6, Fig. 12)
   - Patternâ€‘match and partially lower subgraphs to vendor libraries (e.g., matmul with specific epilogues to cuBLAS/CUTLASS) while compiling the rest to TensorIR.
   - Complement with schedule rules for TensorIR (and optional autotuning for hard cases) so library and codegen approaches compose (Â§4.6).

D) Endâ€‘toâ€‘end pipeline (Â§4.7, Fig. 13)
- Order: partial library lowering â†’ generate TensorIR for remaining highâ€‘level ops â†’ fusion â†’ workspace lifting â†’ memory planning â†’ CUDA Graph offloading â†’ build runnable module.
- Build: erase annotations, compute runtime values of symbolic expressions via a compact â€œsymbol tableâ€ tensor, generate GPU code for TensorIR, and package with a small VM that issues lowâ€‘level calls (Â§4.7).

Key definitions used above:
- `symbolic shape`: an expression like `(n, 256)` where `n` is a runtimeâ€‘known variable tracked symbolically.
- `dataflow block`: a sideâ€‘effectâ€‘free straightâ€‘line region that simplifies transformation (Fig. 2).
- `TensorIR`: TVMâ€™s loopâ€‘level IR for writing/scheduling kernels with explicit loops and buffers.
- `destinationâ€‘passing style (DPS)`: a callee writes results into a callerâ€‘provided buffer instead of allocating internally.
- `CUDA Graph`: a CUDA feature that records a sequence of GPU operations and replays them to reduce launch overhead; requires fixed memory allocations during capture.

## 4. Key Insights and Innovations
1) Crossâ€‘level program abstraction (fundamental) (Â§3.3, Fig. 4)
   - Whatâ€™s new: A single IR where graph nodes can directly call loopâ€‘level kernels (`call_tir`) and vendor libraries (`call_dps_library`), all while the compiler can analyze/transform across these calls.
   - Why it matters: Enables partial lowering, analysis feedback (infer op properties from kernel loops), and crossâ€‘level transformations like workspace lifting (Fig. 6).
   - Difference vs prior: Previous systems largely treated other levels as opaque during graph optimization or did singleâ€‘shot lowering, limiting composition and feedback.

2) Firstâ€‘class, interprocedural symbolic shapes (fundamental) (Â§3.2, Â§4.1, Figs. 3, 7â€“8; Table 1)
   - Whatâ€™s new: Symbolic shapes with arithmetic expressions propagate across function boundaries, subgraphs, and foreign calls, with runtime checks only when unavoidable (`match_cast`).
   - Why it matters: Retains equalities/relations (e.g., `2*n` reappearing after `transpose`) that enable memory reuse, fusion, and specialization. Forward deduction is fast and sufficient for most cases (Â§4.1).

3) Dynamicâ€‘shapeâ€“aware memory and execution planning (significant) (Â§4.3â€“Â§4.5, Figs. 10â€“11; Alg. 3)
   - Memory: Uses symbolic equality to reuse buffers and can preâ€‘allocate using upper bounds to make memory static even when shapes vary (Fig. 10, Alg. 3).
   - Workspace lifting: Moves large temporary buffers out of kernels into the graph so they can participate in global planning (Fig. 11).
   - CUDA Graphs: With static allocations, capture/replay becomes possible for dynamic models (Â§4.5).

4) Composable partial lowering and fusion with custom tensor programs (significant) (Â§4.2, Â§4.6, Fig. 9, Fig. 12)
   - Fusion works even when some ops exist only as custom loop kernels (e.g., quantization decode), because pattern kinds are inferred from TensorIR (Alg. 1) and shape compatibility is preserved symbolically (Fig. 9).
   - Library and codegen approaches compose; the compiler can choose bestâ€‘ofâ€‘both depending on batch size and backend (Fig. 12, Â§5.1).

## 5. Experimental Analysis
Setup (Â§5)
- Models and tasks:
  - LLM decoding latency across batch sizes {1, 16, 32, 64} for `Llama3â€‘8B`, `Gemma1.1â€‘7B`, `Qwen2â€‘7B` with float16 weights/activations (Figs. 14â€“16).
  - Additional models: `Whisperâ€‘largeâ€‘v3` ASR (30â€‘second transcription time), `LLaVA` (32â€‘token generation for an image) (Figs. 19â€“20).
- Hardware:
  - NVIDIA RTX 4090, AMD Radeon 7900 XTX, Apple M2 Ultra; plus emerging platforms (iPhone 14 Pro, Samsung S23, Orange Pi 5, Steam Deck, Jetson Orin, WebGPU on M3 Max) (Table 3).
- Baselines:
  - HuggingFace Transformers (PyTorch eager and `torch.compile`), vLLM, llama.cpp, Fasterâ€‘Whisper, WhisperX (where supported) (Â§5.1, Â§5.4).
- Metrics:
  - LLMs: perâ€‘token decode latency (ms/token).
  - Mobile/embedded/WebGPU: throughput (tokens/sec), Table 3.
  - Memory: allocated activation memory over varying prefill/decode shapes (Table 2).
- Implementation:
  - Relax on Apache TVM; models compiled once for arbitrary batch sizes and sequence lengths (Â§5.1).

Main quantitative results
- LLM decode latency (NVIDIA RTX 4090, Fig. 14)
  - Relax is consistently competitive and sometimes best. Quote:
    > â€œRelax â€¦ reduces the decode token latency by up to 27%.â€ (Fig. 14)
  - Example: For several batch sizes, Relax outperforms HF Transformers and vLLM; llama.cpp is less competitive on NVIDIA GPUs (Â§5.1).
- LLM decode latency (AMD 7900 XTX, Fig. 15)
  - Relax maintains leading or competitive performance; at batch size 1, it reports up to 1.50Ã— improvement over some baselines (caption of Fig. 15).
- LLM decode latency (Apple M2 Ultra, Fig. 16)
  - Relax is competitive with llama.cpp (which is strong on Apple) and substantially ahead of HF Transformers in many settings (Fig. 16).
- Ablation: where the speedups come from (RTX 4090, Llama3â€‘8B, Fig. 17)
  - Starting from â€œno fusion, no partial library, no CUDA Graph,â€ then add:
    - + operator fusion â†’ noticeable gains.
    - + partial library lowering â†’ the largest jump; up to 27% improvement at larger batch sizes by mapping big matmuls to cuBLAS.
    - + CUDA Graph offloading â†’ an additional ~1â€“2% by reducing driver launch overhead (Â§5.2, Fig. 17).
- Memory savings (Table 2)
  - Static memory planning (with upper bounds) vs. runtime allocator:
    > Prefill: 192.7 MiB â†’ 149.7 MiB (âˆ’22%); Decode: 150.0 MiB â†’ 88.2 MiB (âˆ’40%).
  - This reuse holds even as input shapes change over time (Â§5.2, Table 2).
- Emerging platforms (Table 3; Fig. 18)
  - Relax enables GPUâ€‘accelerated LLMs on platforms where baselines donâ€™t run or run only on CPU.
  - Throughput (tokens/sec) examples from Table 3:
    > Steam Deck Vulkan: Llama3â€‘8B 14.0 tok/s; Jetson Orin CUDA: 32.0 tok/s; WebGPU (M3 Max): 37.8 tok/s.
  - On Samsung S24, Relax beats llama.cpp by up to 55% throughput for 4â€‘bit LLMs (Fig. 18).
- Other models
  - Whisperâ€‘largeâ€‘v3 transcription time:
    > Relax is 14% faster than HF Transformers on RTX 4090 and competitive on Apple M2 Ultra (Fig. 19).
  - LLaVA image+text:
    > Relax achieves competitive optimized generation time on both RTX 4090 and Apple M2 Ultra (Fig. 20).

Assessment
- Do experiments support the claims?
  - Yes, the ablation (Fig. 17) shows that composabilityâ€”library + fusion + CUDA Graphâ€”explains the performance, not just one component.
  - Portability claims are demonstrated across three GPU vendors and in Table 3 for mobile/embedded/WebGPU.
  - Memory planning benefits are quantified and tied to the symbolicâ€‘shape mechanism (Table 2, Â§4.3).
- Caveats
  - Torch `compile` mode baselines are omitted for some models (e.g., Qwen2â€‘7B on RTX 4090; Fig. 14 caption notes â€œlack of supportâ€), and PyTorch/llama.cpp lack Apple GPU or Android GPU support. Where baselines are weaker or absent, absolute competitiveness is clear but relative advantage depends on availability.
  - CUDA Graph gains are modest but consistent (~1â€“2%; Fig. 17), which matches expectations for launchâ€‘overheadâ€‘dominated regimes.

## 6. Limitations and Trade-offs
- Shape reasoning requires symbolic equality; otherwise the compiler falls back
  - Memory reuse depends on proving symbolic equality (Alg. 3). If expressions differ or are dataâ€‘dependent, reuse may be missed. `match_cast` inserts runtime checks but cannot infer relations not stated (Â§3.2, Â§4.3).
- Dataâ€‘dependent operators need dynamic checks
  - Ops like `unique` produce shapes depending on values. Relax accepts this using `match_cast`, but it introduces runtime validation and possible failure if assumptions are violated (Fig. 3, Â§3.2).
- Pipeline order and AOT focus
  - The fixedâ€‘order pipeline is engineered for AOT portability (Fig. 13). It avoids JITâ€‘style profiling/adaptation that might yield extra gains in server settings. Choosing the order (e.g., when to fuse vs. when to dispatch to libraries) can limit some opportunities (Â§4.7).
- Developer effort shifts to crossâ€‘level patterns and TensorIR
  - While analysis feedback (Alg. 1) reduces perâ€‘op annotations, some expertise is still needed to:
    - Write custom TensorIR for nonâ€‘standard ops (e.g., quantization decode).
    - Register partial lowering patterns for libraries and keep them backendâ€‘aware (Â§4.6, Fig. 12).
- CUDA Graphs need static allocations and steady subgraphs
  - Capture applies only when memory is stable; if a deployment frequently changes upper bounds or toggles graph structure, the benefit may diminish (Â§4.5).
- Training not covered
  - The paper focuses on inference. Some mechanisms (e.g., gradient memory planning, optimizer state) are unaddressed.
- Precision/layout and library coverage
  - Performance depends on library availability and precision (e.g., FP16). Where vendor libraries lag (new dtypes/layouts), TensorIR must shoulder more optimization work (Â§4.6, Â§5.1).

## 7. Implications and Future Directions
- Changes to the landscape
  - Relax demonstrates that dynamicâ€‘shape models can be compiled AOT with performance comparable to specialized systems, while remaining portable (NVIDIA/AMD/Apple GPUs, mobile, WebGPU). The combination of crossâ€‘level IR and symbolic shapes is a compelling template for nextâ€‘generation ML compilers (Â§Â§3â€“5).
- What this enables
  - Unified deployment stack: Frameworks can target Relax to run one model binary across desktops, phones, embedded devices, and browsers without perâ€‘backend handâ€‘kernels (Table 3).
  - Richer dynamicâ€‘shape optimizations: Crossâ€‘level transformations (fusion, workspace lifting) that were previously awkward or impossible become routine because shape relations persist across levels (Figs. 9â€“11).
  - CUDA Graphs for dynamic models: By statically planning memory via upper bounds, dynamic models can now benefit from capture/replay (Â§4.5).
- Research directions
  - Stronger shape reasoning: Integrate constraint solvers to infer more complex relations when forward deduction is insufficient, while balancing compileâ€‘time cost (Â§3.2, Â§4.1).
  - Automated selection among codegen vs. libraries: Learnâ€‘toâ€‘optimize dispatch that chooses perâ€‘subgraph whether to â€œcompileâ€ or â€œcall a library,â€ conditioned on runtime ranges of shapes and target hardware (Â§4.6).
  - Training support: Extend symbolicâ€‘shape planning and crossâ€‘level optimizations to backprop, optimizer states, and distributed training.
  - Broader backends: Apply the same approach to new static execution graph features on emerging GPUs or NPUs (cf. CUDA Graphs; Â§4.5).
- Practical applications
  - Latencyâ€‘sensitive LLM serving with dynamic batching and context lengths (Figs. 14â€“17).
  - Onâ€‘device inference (mobile, embedded) where memory budgets are tight and JIT is infeasible (Table 3).
  - Webâ€‘native ML via WebGPU, enabling privacyâ€‘preserving or lowâ€‘latency inâ€‘browser assistants (Table 3).

> In short, Relaxâ€™s crossâ€‘level IR and firstâ€‘class symbolic shapes turn dynamic shape from an obstacle into a resource the compiler can reason about. This delivers concrete performance and memory wins (up to 27% decode latency reduction and 40% memory reduction in the reported settings; Figs. 14, 17; Table 2) while expanding the set of viable deployment targets (Table 3).

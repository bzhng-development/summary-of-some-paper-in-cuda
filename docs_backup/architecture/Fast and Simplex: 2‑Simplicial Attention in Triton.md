# Fast and Simplex: 2â€‘Simplicial Attention in Triton

**ArXiv:** [2507.02754](https://arxiv.org/abs/2507.02754)
**Authors:** Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil
**Institutions:** 

## ğŸ¯ Pitch

This paper presents 2-simplicial attention, an innovative mechanism that evaluates triplets of tokens rather than pairs, enhancing token efficiency and achieving improved parameter scaling for reasoning-intensive tasks. By addressing the data scarcity challenge, it provides a scalable solution that enables large language models to perform better with limited tokens, offering significant potential for advances in reasoning, math, and coding applications.

---

## 1. Executive Summary
This paper introduces 2â€‘simplicial attentionâ€”an attention mechanism that scores triplets of tokens rather than pairsâ€”and makes it practical with an efficient Triton implementation and a slidingâ€‘window design. In fixedâ€‘token training regimes, similarly sized models with interleaved 2â€‘simplicial layers achieve lower negative logâ€‘likelihoods on reasoning, math, and coding benchmarks and exhibit a steeper parameter scaling exponent than standard dotâ€‘product attention (Tables 2â€“3), suggesting improved token efficiency.

## 2. Context and Motivation
- Problem addressed
  - Modern scaling laws model loss as a power law in both parameters and tokens: L(N,D) = E + A/N^Î± + B/D^Î² (Equation 1, Section 3). Computeâ€‘optimal scaling suggests increasing both parameters and tokens together (Hoffmann/Chinchilla).
  - In practice, highâ€‘quality data (tokens) is becoming the bottleneck. Many architectural tweaks mostly shift the loss offset E but do not change the exponent Î± or Î², so they do not help when tokens are limited (Section 1, citing Kaplan, Shen, Hestness; summary discussion in Section 3).
- Why this matters
  - If an architecture can increase Î± (the parameter scaling exponent) for the same token budget, the model improves faster with scale under data scarcityâ€”directly addressing a dominant practical constraint.
- Prior approaches and shortcomings
  - Linearâ€‘time or sparse attentions reduce compute but often degrade quality (Section 2).
  - Higherâ€‘order attentions (e.g., 2â€‘simplicial, triangle attention in proteins) exist but lacked scalable, generalâ€‘purpose implementations and evidence of better token efficiency for language tasks (Section 2).
- Positioning
  - This work: (1) reâ€‘instantiates 2â€‘simplicial attention with a carefully optimized Triton kernel and a slidingâ€‘window design to control cubic costs (Sections 6â€“7), (2) proposes a rotationâ€‘invariant trilinear form to enable position encoding analogous to RoPE (Section 5), and (3) empirically shows improved tokenâ€‘efficiency and larger Î± than standard attention for reasoningâ€‘heavy tasks (Sections 8, Tables 2â€“4).

## 3. Technical Approach
At a high level, 2â€‘simplicial attention extends the pairwise interaction in dotâ€‘product attention to a triple interaction. Instead of comparing every query `q_i` to each key `k_j`, a query is compared to pairs of keys `(k_j, k'_k)`, producing a 3D tensor of logits.

- Standard dotâ€‘product attention refresher (Section 4)
  - Compute Q, K, V via linear projections.
  - Logits: A = QK^T / âˆšd (Equation 2); weights via rowâ€‘wise softmax (Equation 3); output vÌƒ_i = âˆ‘_j S_ij v_j (Equation 4).

- 2â€‘simplicial attention (Section 4)
  - Additional projections Kâ€² and Vâ€².
  - Trilinear logits:
    - A^(2s)_i j k = âŸ¨q_i, k_j, kâ€²_kâŸ© / âˆšd = (1/âˆšd) âˆ‘_{l=1}^d Q_il K_jl Kâ€²_kl (Equation 5).
    - Softmax across both j and k axes (Equation 6).
  - Output combines values multiplicatively:
    - vÌƒ^(2s)(i) = âˆ‘_{j,k} S^(2s)_i j k (v_j âˆ˜ vâ€²_k) (Equation 7), where âˆ˜ is elementâ€‘wise product.
  - Intuition: The score for token i depends on a triangular relation among tokens (i, j, k). This can capture constraints that are inherently tripletâ€‘based (e.g., transitive relations, simple logical/matching structures) that pairwise attention struggles with.

- Position encoding via a rotationâ€‘invariant trilinear (Section 5)
  - Issue: the naive trilinear âŸ¨a,b,câŸ© is not invariant under a shared rotation, which breaks positional schemes like RoPE that rely on rotational invariance of the scoring function.
  - Solution: use a determinantâ€‘based trilinear. Chunk each vector into 3â€‘dim blocks and sum 3Ã—3 determinants:
    - A^(det)_{i j1 j2} = âˆ‘_{l=1}^p det([q_i^(l), k_{j1}^(l), kâ€²_{j2}^(l)]) (Equation 9).
  - Why it works: det([a,b,c]) is invariant to a shared rotation of a, b, c. By Sarrusâ€™ rule, each determinant becomes a sum/difference of dotâ€‘products (Equation 8), so it remains implementable with standard tensor ops.
  - Expressivity: Theorem 5.1 shows a single 7â€‘dim head with this determinantâ€‘based attention can implement a modular â€œMatch3â€ predicate (existence of j1, j2 such that x_i + x_{j1} + x_{j2} â‰¡ 0 mod M). The constructive proof (Appendix A) embeds inputs into sinusoidal features so that the summed determinants realize cos(Î¸_i + Î¸_j + Î¸_k), peaking exactly when the modular sum constraint holds.

- Making 2â€‘simplicial attention practical (Sections 6â€“7)
  - Complexity control with sliding windows (Section 6):
    - Global 2â€‘simplicial is O(n^3). The paper uses local windows of widths `w1` for K and `w2` for Kâ€² so each query attends only to a rectangle of size w1Ã—w2 (Figure 2, left).
    - Complexity becomes O(n w1 w2) with a constant 6 from the trilinear/einsum arithmetic (Section 6: O(A^(2s)) = 6 n w1 w2).
    - The paper explores several (w1,w2) pairs and picks `(512,32)` as a latency/quality compromise (Table 1).
  - Head sharing and tiling for throughput (Section 6â€“7):
    - Adopts high `GQA` (groupedâ€‘query attention) ratio of 64 so many queries share K/Kâ€²/V/Vâ€², allowing tiling along the head dimension and dense computation without expensive masks.
  - Triton kernel with online softmax (Section 7; Appendix B/C):
    - 2D tiling trick: preâ€‘multiply two inputs elementwise (e.g., Qâˆ˜K or Vâˆ˜Vâ€²) so the remaining contraction is a matrix multiply; this lets the pipeline overlap CUDAâ€‘core elementwise work with Tensorâ€‘core matmuls (Figure 2, right).
    - Uses online softmax as in FlashAttention to keep memory traffic low.
    - Achieves up to ~520 TFLOPS in Triton, comparable to highâ€‘end FlashAttention v3 Triton kernels; potential to gain more with CUTLASS (Section 7).
  - Backward pass without atomic bottlenecks (Section 7; Algorithm 2; Appendix C):
    - Splits grad computation into two kernelsâ€”one for (dK, dV) and another for (dKâ€², dVâ€², dQ)â€”to avoid excessive atomics across three reduction orders (Equations 10â€“16).
    - For small `w2`, uses a twoâ€‘stage â€œeven/odd tileâ€ sweep (Algorithm 2) to compute dQ jointly with dKâ€²/dVâ€² without atomics.

## 4. Key Insights and Innovations
- 2â€‘simplicial attention with practical efficiency
  - Novelty: Local rectangular windows and a Triton kernel that fuses the trilinear contraction into matmulâ€‘friendly tiles (Figure 2, Section 7). This turns a conceptually cubic operator into a nearâ€‘quadraticâ€‘like cost at long contexts.
  - Significance: Enables routine inclusion of 2â€‘simplicial layers in large LMs rather than restricting them to niche tasks (Sections 6â€“7).
- Rotationâ€‘invariant trilinear form enabling relative positions (Section 5)
  - Novelty: Uses a sum of 3Ã—3 determinants over 3â€‘dim chunks (Equation 9). This preserves invariance under shared rotations, a property needed to generalize RoPE to trilinear attention.
  - Significance: Makes 2â€‘simplicial attention compatible with widely used positional schemes and provides a clean analytical object (determinant) with geometric meaning (signed volume).
- Theoretical expressivity result (Theorem 5.1; Appendix A)
  - Novelty: A single 7â€‘dim head can realize a tripletâ€‘matching predicate modulo M through the determinant form, with a constructive sinusoidal embedding.
  - Significance: Shows that 2â€‘simplicial attention can solve classes of triplet constraints in one layer that would be awkward or deep for pairwise attention, aligning with prior theory that higherâ€‘order attention broadens the representable function class.
- Empirical scalingâ€‘law change, not just a constant shift (Sections 3 and 8; Tables 2â€“4)
  - Novelty: When trained with the same number of tokens, models with interleaved 2â€‘simplicial layers show larger parameter exponents Î± than dotâ€‘product baselines (e.g., GSM8k Î±: 0.1683 vs 0.1420; +18.5%, Table 3).
  - Significance: If Î± is genuinely higher, one can increase parameters faster than tokens and still see returnsâ€”valuable in tokenâ€‘scarce regimes.

## 5. Experimental Analysis
- Setup (Section 8)
  - Models: Mixtureâ€‘ofâ€‘Experts (MoE) LMs with â€œactive parametersâ€ (the part used per token) ranging 1B, 2B, 3.5B; total parameters 57B, 100B, 176B respectively. Every fourth layer is a 2â€‘simplicial layer; the rest use standard attention. This interleaving balances pipeline stage compute (Section 8).
  - Training: AdamW, peak LR 4eâ€‘3, wd 0.0125, 4k warmup, cosine decay to 0.01Ã— peak (Section 8).
  - Evaluation: Negative logâ€‘likelihood (NLL) on GSM8k (5â€‘shot), MMLU, MMLUâ€‘pro, MBPP (Section 8). NLL is a pretrainingâ€‘aligned metric; lower is better.
  - Baseline: Same MoE sizes trained with purely dotâ€‘product attention; token budget is the same across conditions so the D term in Equation 1 can be treated as constant when fitting Î± (Section 8, Equations 17â€“20).
- Main quantitative results (Table 2)
  - 1B active params: 2â€‘simplicial is roughly neutral to slightly worse.
    - Example: GSM8k NLL 0.3302 vs 0.3277 (+0.79%).
  - 2B active params: consistent improvements.
    - GSM8k: 0.2942 vs 0.2987 (âˆ’1.51%); MMLU: 0.5862 vs 0.5932 (âˆ’1.19%).
  - 3.5B active params: bigger gains.
    - GSM8k: 0.2718 vs 0.2781 (âˆ’2.27%); MMLUâ€‘pro: 0.7689 vs 0.7858 (âˆ’2.15%).
  - The improvements concentrate on reasoningâ€‘heavy benchmarks (GSM8k, MMLUâ€‘pro).
- Scalingâ€‘law analysis (Section 8; Tables 3â€“4)
  - With D fixed, fit âˆ’log L(N) â‰ˆ Î± log N + Î² (Equations 18â€“20).
  - Reported Î± gains:
    - GSM8k: 0.1683 vs 0.1420 (+18.5%).
    - MMLUâ€‘pro: 0.1083 vs 0.0901 (+20.2%).
  - Goodness of fit is very high (R^2 â‰ˆ 0.997â€“0.9999, Table 4), but note the fit uses just three model sizes.
- Kernel performance (Figure 3; Section 7)
  - 2â€‘simplicial forward achieves up to ~520 TFLOPS in Triton and is competitive with FlashAttention v3 (FAv3) on long sequence lengths; execution time grows similarly with sequence length.
- Latency vs window sizes (Table 1; Section 6)
  - Investigated several (w1, w2) settings; chose (512,32) as a balanced point. For example, at 16k context:
    - (512,32): ~55 ms; (128,128): ~59 ms; (1024,16): ~55.1 ms.
- Assessment of evidence
  - Strengths:
    - Same token budget across models isolates the effect on Î± (Section 8).
    - Consistent NLL gains at 2B and 3.5B across multiple benchmarks (Table 2).
    - A clear, reproducible recipe for making 2â€‘simplicial layers efficient (Sections 6â€“7; Appendices Bâ€“C).
  - Caveats:
    - Only three model sizes for the scaling fit make Î± estimates sensitive (Tables 3â€“4).
    - Metrics are NLL rather than task accuracy/pass@k; conversion to downstream accuracy is not reported.
    - Ablations are limited: e.g., the frequency of 2â€‘simplicial layers, choice of (w1,w2), or head dimensions are not systematically tied to quality outcomes (Table 1 reports latency only).

## 6. Limitations and Trade-offs
- Computational and memory costs
  - Even with windows, complexity is O(n w1 w2) (Section 6). Choosing large windows can approach quadraticâ€‘toâ€‘superâ€‘quadratic costs and increases memory bandwidth needs.
  - Doubling key/value streams (K/Kâ€² and V/Vâ€²) increases memory traffic and storage versus standard attention.
- Kernel/engineering maturity
  - The current Triton kernels are â€œefficient for prototypingâ€ but â€œfar away from being used in productionâ€ (Section 9). More lowâ€‘level optimization (e.g., CUTLASSâ€‘based) may be required for peak deployment performance.
- Modelâ€‘size sensitivity
  - Gains appear at 2B and 3.5B active parameters; 1B shows no improvement and sometimes slight regressions (Table 2). This suggests a scale threshold before 2â€‘simplicial layers pay off.
- Evaluation scope
  - Pretrainingâ€‘style NLL is reported; no endâ€‘task accuracy or generative metrics (e.g., chainâ€‘ofâ€‘thought correctness, code execution pass@k).
  - No robustness checks reported (e.g., sensitivity to data distribution, longâ€‘context extrapolation quality beyond latency plots).
- Theoryâ€“practice gap
  - The expressivity theorem (Theorem 5.1) is for the determinant form; experiments use the simpler trilinear form for backprop derivations (Section 5 end). While the two are argued to be comparably expressive, the empirical models do not directly use the determinantâ€‘based logits.

## 7. Implications and Future Directions
- Impact on the field
  - Provides a practical template for higherâ€‘order attention in LLMs and shows that architectural changes can affect the scaling exponent Î± for reasoningâ€‘heavy tasks (Tables 2â€“3). This directly addresses tokenâ€‘scarce scaling.
- Research directions
  - Systematic ablations:
    - Frequency and placement of 2â€‘simplicial layers; head dimension and number of heads; alternatives to the Hadamard combination of V and Vâ€².
    - Window schedules (adaptive w1,w2 by layer or by token), and globalâ€‘plusâ€‘local hybrids.
  - Positional schemes and invariances:
    - Deploy and evaluate the determinantâ€‘based rotationâ€‘invariant form at scale; compare to vanilla trilinear with RoPE variants.
  - Theory:
    - Formalize when and why Î± increases for certain task families; extend Match3â€‘style results to richer compositional reasoning tasks and to deeper networks.
  - Systems:
    - Hardwareâ€‘aware kernels (CUTLASS/CUDA custom kernels) and accelerator coâ€‘design for triple contractions; memoryâ€‘saving layouts for Kâ€²/Vâ€².
  - Applications:
    - Reasoningâ€‘centric pretraining (math, code, logic) where token supply is precious; retrievalâ€‘augmented setups where triplet relations (query, document, snippet) are natural; protein/graph domains where triangle structures are intrinsic.
- Practical takeaway
  - When data is the bottleneck, inserting a modest fraction of 2â€‘simplicial layersâ€”implemented with sliding windows and efficient kernelsâ€”can yield better returns per token than scaling standard attention alone. As Section 10 concludes:
    > â€œ2â€‘simplicial attention â€¦ changes the exponent corresponding to parameters in the scaling law â€¦ leading to more favorable scaling under token constraints,â€ with larger Î± gains on harder benchmarks like GSM8k and MMLUâ€‘pro (Table 3).

# Holistically Evaluating the Environmental Impact of Creating Language Models

**ArXiv:** [2503.05804](https://arxiv.org/abs/2503.05804)
**Authors:** Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, Jesse Dodge
**Institutions:** 

## ğŸ¯ Pitch

This paper presents the first comprehensive lifecycle analysis of open language models, quantifying often-overlooked environmental impacts such as water consumption and development phase emissions, alongside hardware manufacturing and training costs. By providing a holistic, measurement-based assessment, it reveals significant unreported environmental burdens and high intra-run power variability, emphasizing the need for transparent reporting and infrastructure optimization to mitigate the ecological footprint of AI systems.

---

## 1. Executive Summary
This paper quantifies, end to end, the environmental impact of creating a family of open language models (OLMo) by measuring electricity use, carbon emissions, and water consumption across hardware manufacturing, model development, final training, and simulated deployment. It shows that impacts commonly omitted from reportsâ€”especially model development and water useâ€”are large: total emissions reach 493 metric tons CO2e and 2.769 million liters of water, and GPU power draw fluctuates sharply during training (â‰ˆ15%â€“85% of max), which complicates grid planning (Abstract; Â§4.1; Â§4.3; Fig. 2).

## 2. Context and Motivation
- Problem addressed
  - Public reporting for foundation models usually covers only the last, successful training run and only CO2 from electricity; it rarely includes development runs, embodied impacts from hardware manufacturing, or water consumption, and almost never provides highâ€‘resolution power data (Â§1; Â§2).
- Why it matters
  - Data centers could consume up to 11.7% of U.S. electricity by 2030; AI training and inference are major drivers (Â§1).
  - Without full accounting, stakeholders underestimate environmental costs, misplan grid capacity, and miss opportunities to reduce impacts (Â§1; Â§5).
- Gaps in prior work
  - Emissions often computed with simplifying assumptions such as â€œGPUs draw 100% of nameplate powerâ€ and without development or water accounting (e.g., Llama series reports; Â§2).
  - Embodied manufacturing impacts for GPUs are opaque; water use is underreported and often speculative (Â§2).
- Positioning
  - This work provides a holistic, measurementâ€‘based lifeâ€‘cycle view for a suite of LLMs (20Mâ€“13B active parameters; 1.7â€“5.6T tokens), including:
    - Operational impacts for development, training, and simulated inference (Â§3.1, Â§4.1â€“Â§4.2).
    - Embodied manufacturing impacts (Â§3.2).
    - Highâ€‘resolution (subâ€‘second) power traces revealing intraâ€‘run variability (Â§3.1; Â§4.3).

Definitions (used throughout):
- `PUE` (Power Usage Effectiveness): How much extra facility power (cooling, overhead) is needed per unit of IT power; lower is better.
- `WUE` (Water Usage Effectiveness): Liters of water consumed per kWh; includes onsite cooling and offsite water used by power plants (Â§3.1; Eq. 3).
- Scope 2 emissions: Emissions from purchased electricity; Scope 3: upstream/downstream impacts, e.g., manufacturing and userâ€‘side inference (not fully covered here, but partially via embodied and inference estimates; Â§3).

## 3. Technical Approach
The paper quantifies impacts using a modular pipeline that mirrors how models are actually created and used.

1) Operational impacts (development, training, inference)
- What is measured
  - GPU power draw sampled at subâ€‘second intervals for a representative node during each run, then extrapolated to all nodes in the job (Â§3.1).
  - The approach records only GPU power; CPU/network/storage overhead is not included, so operational numbers are lower bounds (Â§3.1).
- How electricity becomes CO2 and water
  - CO2 emissions:
    - Plain explanation: multiply electricity consumed by (a) how much extra facility power is needed and (b) the carbon intensity of the local grid.
    - Notation (Eq. 2): CO2e = P Â· PUE Â· CI, where `P` is IT power (kWh), `PUE` is a scalar (e.g., 1.2), and `CI` is kg CO2/kWh based on the local utility (Â§3.1).
  - Water consumption:
    - Plain explanation: each kWh causes water use at the data center (cooling) and at the power plant (steam/hydro losses).
    - Notation (Eq. 3): Water = P Â· PUE Â· (WUE_onsite + WUE_offsite). The paperâ€™s data center uses closedâ€‘loop cooling, so WUE_onsite = 0; offsite WUE is regionâ€‘dependent (Â§3.1).
- Where measurements were taken
  - Two H100 GPU clusters: â€œJupiterâ€ (Texas; Austin Energy: CI = 0.332 kg CO2/kWh; PUE = 1.2; WUE_offsite = 1.29 L/kWh) and â€œAugustaâ€ (Iowa; CI = 0.351 kg CO2/kWh; PUE = 1.12; WUE_offsite = 3.10 L/kWh) (Â§3.1; Â§4.1).

2) Embodied manufacturing impacts
- Rationale: GPUs carry â€œupstreamâ€ embodied CO2 and water from manufacturing and materials extraction.
- Method
  - Adopt prior estimate of 3,700 kg CO2e per 8Ã—GPU node (463 kg/GPU) from Luccioni et al. (2023) (Â§3.2; Â§4.1).
  - Water for fabrication approximated using TSMC process data: 12.33 L/cmÂ² leading to â‰ˆ100.4 L per H100; add rareâ€‘earth mining (assume 0.1% by mass), adding â‰ˆ2.2 L and 0.013 kg CO2e per GPU (Â§4.1 â€œHardware manufacturingâ€).
  - Amortize over a 4â€‘year GPU life to get perâ€‘GPUâ€‘hour factors: â‰ˆ0.013 kg CO2e and 0.003 L water per GPUâ€‘hour; multiply by total GPUâ€‘hours used (Â§4.1).

3) Models, data, and hardware (empirical context)
- Dense transformers spanning 20Mâ€“13B active parameters; subâ€‘billion models trained on 1.7T tokens; 1B on 3T; several 7B variants on 2â€“4T; 13B on 5.6T (Â§3.3).
- One mixtureâ€‘ofâ€‘experts (MoE) model with 1B active/7B total parameters trained on 5T tokens (active = parameters used per token; MoE routes tokens to a subset of â€œexpertsâ€) (Â§3.3).
- Standard HGX servers with 8Ã—NVIDIA H100; 2â€“128 nodes/run (Â§3.3).

4) Simulated deployment and inference
- Why simulate: the models were not deployed as a public service; to estimate downstream use, the authors emulate common chat usage (Â§3.4).
- Setup
  - Requests are sampled from the ShareGPT dataset (2,400 prompts) and fed to `SGLang` on a single H100 (Â§3.4).
  - Three arrival patterns: â€œbatch all at onceâ€ (âˆ req/s), 8 req/s, and 1 req/s; the latter two mimic Poisson arrivals used in online serving studies (Â§3.4).
  - Energy measured with CodeCarbon, crossâ€‘checked against the same power logging used for training; only active GPU processes are counted (no idle/listening overhead) (Â§3.4; Table 3 note).

5) Power variability analysis
- Subâ€‘second GPU power traces show onâ€‘off cycles during training, with sharp dips during checkpointing events (Â§4.3; Fig. 2).

## 4. Key Insights and Innovations
- First holistic accounting of LLM â€œcreationâ€ costs beyond the last training run
  - Novelty: separates and quantifies hardware manufacturing, development (ablation/tuning), the final training runs, and inference (Â§Â§3â€“4).
  - Significance: development alone emits ~50% as much CO2 as final training (159 vs. 312 tCO2e; Table 1 vs. Table 2), a large component usually unreported.
- Inclusion of water as a firstâ€‘class metric
  - Novelty: water consumption is computed both â€œonsiteâ€ (cooling; zero here due to closedâ€‘loop) and â€œoffsiteâ€ (from power generation), plus embodied water in hardware (Â§3.1â€“Â§3.2).
  - Significance: total water reaches 2.769 million liters when all phases are counted (Â§4.1 â€œPutting it in perspectiveâ€).
- Highâ€‘resolution measurement of nonâ€‘steady GPU power
  - Novelty: subâ€‘second traces show power swings from â‰ˆ85% of H100 TDP during training to â‰ˆ15% during checkpointing (Fig. 2); prior public reports typically assume constant draw (Â§4.3).
  - Significance: frequent, synchronized dips across many GPUs can destabilize grid operations and reduce efficiency (Â§5.2).
- Deployment-aware â€œbreakâ€‘evenâ€ analysis
  - Novelty: for multiple models and serving regimes, the paper computes how many inferences are needed before inference CO2 equals training CO2 (Â§4.2; Table 3).
  - Significance: breakâ€‘even often occurs between hundreds of millions and tens of billions of inferences; production systems can reach this quickly (Â§4.2).

## 5. Experimental Analysis
- Evaluation methodology
  - Development and training
    - Runs grouped by model scale: <1B, 1B, 7B, 13B, and MoE (1B active/7B total). Each group includes many experiments for stabilization and hyperparameter sweeps before a final training run (Â§4.1; Table 1).
    - CO2 and water calculated from measured GPU power using Eq. (2) and Eq. (3), with dataâ€‘centerâ€‘specific PUE/WUE and local grid carbon intensities (Â§3.1; Â§4.1).
  - Embodied impacts
    - GPU manufacturing impacts amortized per GPUâ€‘hour (details in Â§3.2; Â§4.1 â€œHardware manufacturingâ€).
  - Inference
    - SGLang on 1Ã—H100, 2,400 ShareGPT prompts, three arrival rates; power tracked by CodeCarbon; conversion to CO2/water uses the same PUE/WUE/CI as the training cluster (Â§3.4; Table 3 note).
- Main quantitative results
  - Development (Table 1)
    - Total: 680k GPUâ€‘hours; 459 MWh; 159 tCO2e; 843 kL water.
    - Concentration: ~70% of development impact is from 7B and 13B scales (Table 1).
  - Final training runs (Table 2)
    - Total: 913 MWh; 312 tCO2e; 1,921 kL water.
    - Example runs:
      - `OLMo 2 13B` (5.6T tokens): 230 MWh, 101 tCO2e, 892 kL water.
      - `OLMo 2 7B` (4T tokens): 157 MWh, 52 tCO2e, 202 kL water.
      - `OLMoE 0924` (1B active/7B total, 5T tokens): 54 MWh, 18 tCO2e, 70 kL water.
      - One 7B model trained on a fully hydroelectric supercomputer (LUMI) records essentially zero operational CO2/water, illustrating location and energyâ€‘mix sensitivity (Table 2, footnote â€œ*â€).
  - Wholeâ€‘program perspective (development + training + manufacturing)
    - Emissions: â‰¥493 tCO2e; Water: â‰¥2,769 kL (Â§4.1 â€œPutting it in perspectiveâ€).
    - Interpretation: â€œequivalent to â€¦ energy use for 98.2 U.S. homes in one yearâ€ and â€œ24.5 years of water use by one average U.S. personâ€ (Â§4.1).
  - Inference costs and breakâ€‘even (Table 3)
    - The table reports energy/CO2/water for the 2,400â€‘prompt benchmark (see table caption) and latency per 100 requests.
    - Examples at 1 req/s (unsaturated, realistic latency):
      - `OLMo 2 7B`: 0.358 kWh, 118.9 g CO2e, 0.533 L; 100.54 s per 100 req; breakâ€‘even â‰ˆ1.05 billion inferences.
      - `OLMo 1B (3T)`: 0.165 kWh, 54.8 g CO2e, 0.246 L; breakâ€‘even â‰ˆ441 million.
      - `Llama 2 13B`: 0.401 kWh, 133.1 g CO2e, 0.597 L; breakâ€‘even â‰ˆ1.13 billion.
    - Saturated throughput (âˆ req/s) yields much lower perâ€‘request energy than sparse arrivals, but many real applications cannot batch to that degree (Table 3; Â§4.2).
- Power variability (Fig. 2; Â§4.3)
  - >600 W per H100 when actively training (~85% of 700 W TDP) dropping to ~100 W during checkpointing (~15%); periodic dips reveal nonâ€‘steady demand.
- Are claims supported?
  - The training/development impact claims are strongly grounded in direct measurements and transparent conversions (Eq. 2â€“3; clusterâ€‘specific PUE/CI/WUE).
  - Embodied impacts are necessarily approximate due to industry opacity (explicitly acknowledged in Â§5.1 â€œEmbodied emissions are still an enigmaâ€).
  - Inference results are careful lower bounds (GPU only; no CPU/host overhead), with caveats clearly stated (Table 3 note; Appendix A.2).

> Table 1 shows development emits 159 tCO2e and consumes 843 kL water, while Table 2 shows 312 tCO2e and 1,921 kL for final training runsâ€”i.e., development â‰ˆ50% of training in CO2 and â‰ˆ44% in water.

> Figure 2 shows GPU power cycling between ~85% and ~15% of max due to checkpointing, indicating significant intraâ€‘run variability that typical â€œ100% powerâ€ assumptions miss.

## 6. Limitations and Trade-offs
- Measurement scope
  - GPUâ€‘only power: excludes CPU, memory, networking, and idle/listening overhead; operational impacts are lower bounds (Â§3.1; Table 3 note).
  - Singleâ€‘node sampling extrapolated to whole job; if nodes are heterogeneous or desynchronized, extrapolation error can grow (Â§3.1).
- Embodied impacts uncertainty
  - Manufacturing CO2 and water rely on secondary sources and assumptions (e.g., 463 kg CO2e/GPU; 100.4 L water/GPU; 0.1% rareâ€‘earth mass fraction), amortized over a 4â€‘year life (Â§4.1 â€œHardware manufacturingâ€).
  - Other Scope 3 elements (transport, dataâ€‘center construction, endâ€‘ofâ€‘life) are not fully included (Â§4.1 â€œOther Costsâ€).
- Inference realism
  - Simulations use a single H100 with SGLang on 2,400 ShareGPT prompts; results do not capture diverse serving stacks, quantization, edge deployment, multiâ€‘GPU inference, or interactive/streaming patterns (Â§3.4; Appendix A.2).
  - Measured values exclude system overhead, leading to optimistic perâ€‘request energy (Â§3.4; Table 3 note).
- Generalizability
  - Results depend on region (grid carbon intensity, offsite WUE), dataâ€‘center efficiency (PUE), and training recipes (e.g., checkpoint frequency), limiting direct transfer to other sites (Â§3.1; Table 2).
  - Observed nearâ€‘linear training cost trends with model size may not hold in decentralized or multiâ€‘datacenter training with higher communication overhead (Appendix A.2).

## 7. Implications and Future Directions
- How this changes the landscape
  - Moves environmental reporting from â€œfinal training CO2 onlyâ€ toward a lifecycle assessment that includes development, water, and power dynamics. This raises the bar for transparency and provides concrete numbers to guide procurement, scheduling, and model design (Â§5.1â€“Â§5.2).
- Practical takeaways
  - Track and publish development costs: development can be â‰ˆ50% of training in CO2 (Table 1 vs. Table 2), so optimizing early-stage experiments (e.g., simulation, better scalingâ€‘law ladders) can yield large savings (Â§4.1; Â§5.1).
  - Choose location and timing strategically: switching regions or energy sources can drive impacts near zero (e.g., 7B on LUMI hydro power in Table 2). Demandâ€‘response scheduling could also reduce marginal grid intensity (Â§5.2).
  - Reduce power variability: parallelized/asynchronous checkpointing and better job orchestration can minimize highâ€‘frequency load swings that strain grids (Fig. 2; Â§5.2).
  - Account for water: offsite WUE dominates in closedâ€‘loopâ€‘cooled facilities; grid mix (e.g., thermoelectric vs. wind/solar) materially affects water use (Â§3.1; Â§5.1).
  - Deployment optimization matters as much as training efficiency: breakâ€‘even can be reached in 10^8â€“10^10 inferences; unsaturated serving (1 req/s) is far less efficient than batched scenarios (Table 3; Â§4.2).
- Policy and reporting
  - The analysis underscores the need for standardized, auditable disclosures (e.g., EU AI Act reporting; U.S. Artificial Intelligence Environmental Impacts Act proposals) and for GPU manufacturers to publish embodied impact factors (Â§5.1).
- Research directions
  - Better embodied-impact models for chips and systems; public datasets of perâ€‘component CO2/water.
  - Scheduling and systems research to smooth power demand (e.g., checkpointâ€‘aware schedulers, gridâ€‘integrated datacenters).
  - Methods to estimate realâ€‘world inference footprints, including system overhead, edge devices, and mixed workloads.
  - Training pipelines that minimize development runs (e.g., transferâ€‘efficient methods, computeâ€‘efficient scalingâ€‘law ladders; see Â§3.1 Eq. 1 and Â§5.1).

In sum, this paper provides a rigorous, transparent template for holistic environmental accounting of LLMs, reveals substantial unreported costs (development and water), and identifies systemsâ€‘level issues (power variability) that require attention from both ML practitioners and infrastructure planners.

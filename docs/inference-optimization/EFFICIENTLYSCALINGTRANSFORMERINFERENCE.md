# EFFICIENTLY SCALING TRANSFORMER INFERENCE

**ArXiv:** [2211.05102](https://arxiv.org/abs/2211.05102)

## üéØ Pitch

This paper presents a principled, hardware-aware framework for scaling generative inference in very large Transformer models, introducing analytical models and optimal partitioning strategies that maximize efficiency and minimize latency on distributed accelerators like Google TPU v4. By combining multi-dimensional tensor partitioning, advanced memory management, and specific attention mechanisms, the authors set a new Pareto frontier‚Äîdemonstrating fast, cost-effective inference for models exceeding 500 billion parameters, an achievement crucial for deploying large language models in both latency-sensitive and high-throughput production environments.

---

## 1. Executive Summary
This paper develops a principled, hardware-aware method for running very large Transformer models fast and cheaply at inference time by choosing the right way to split (‚Äúpartition‚Äù) model tensors across many accelerator chips and by optimizing memory traffic. It delivers an analytical model and a set of concrete partitioning layouts that together set a new latency‚Äìefficiency Pareto frontier on Google TPU v4 for 500B+ parameter models, achieving, for PaLM 540B, 29 ms/token decode latency with int8 weights and up to 76% model FLOPS utilization (MFU) in large-batch prefill (Abstract; Figure 1; Tables 2‚Äì3).

## 2. Context and Motivation
- Problem addressed
  - Generative inference for very large Transformers must process one token at a time (a ‚Äúdecode step‚Äù), so it has much less parallelism than training and is therefore latency- and bandwidth-sensitive (Section 1).
  - Large models do not fit on a single chip; they require multi-chip partitioning and careful handling of the persistent attention key/value (‚ÄúKV cache‚Äù) whose size grows with batch and context length (Section 2; Section 3.3).
- Why it matters
  - Real applications span interactive chat (tight latency) and offline processing (high throughput/low cost). Getting both to work at 100B‚Äì500B scale determines the practical utility of LLMs in production (Section 1; Section 2.1).
- Prior approaches and gaps
  - Training-time parallelism systems (e.g., Megatron, GSPMD, Alpa) provide general sharding but don‚Äôt explain which layout is best for inference‚Äôs unique bottlenecks or for different phases (prefill vs. decode) (Related Work, Section 6).
  - Existing inference suites such as FasterTransformer rely on limited forms of tensor and pipeline parallelism and hit communication bottlenecks as tensor-parallel degree grows (Figure 9; Section 5).
  - Multiquery attention (MQA) reduces KV cache size but loses its memory benefits if sharded over heads in the obvious way, because K/V must be replicated across devices (Section 3.3; Figure 4b).
- Positioning of this work
  - Provides an analytical framework that predicts communication time for different layouts and shows how to select the best one as a function of model size, batch size, sequence length, and chip count (Sections 2‚Äì3; Appendix A).
  - Introduces an attention partitioning for MQA that shards across batch during decode to realize KV-cache savings (Section 3.3; Figures 4c, 5b).
  - Combines these with low-level scheduling/communication overlap and int8 weight quantization (Sections 3.5‚Äì3.6).

Key terms used throughout (defined once):
- `prefill`: the initial pass over the full input context where the model processes B√óLinput tokens in parallel (Section 2.2).
- `decode`: the autoregressive phase that generates tokens one step at a time; each step depends on previous outputs (Section 2.2).
- `KV cache`: per-layer tensors that store past attention keys and values for every sequence; needed to attend to prior tokens without recomputing (Section 2).
- `MFU (Model FLOPS Utilization)`: observed throughput divided by the theoretical peak FLOPS of the hardware configuration; higher MFU means better hardware efficiency (Section 2).
- `reduce-scatter` / `all-gather` / `all-reduce` / `all-to-all`: standard multi-chip collectives to sum, shard, or reshuffle tensors across devices (Section 3.1; Figure A.1).
- `weight-stationary` vs. `weight-gathered` layouts: strategies that either keep weights fixed on each chip and move activations, or keep activations fixed and move weights (Sections 3.2.1‚Äì3.2.3).

## 3. Technical Approach
The paper builds an end-to-end system by aligning high-level partitioning choices with the actual sources of time: compute, memory traffic (weights + KV cache), and communication.

A) Cost model and phases (Section 2)
- Latency has two parts:
  - Prefill: parallel over the entire input sequence (good parallelism).
  - Decode: a loop of Lgen steps; each step is a full forward pass on the newest token (poor parallelism).
- Compute cost: a decoder-only model with N parameters performs about 2N floating-point operations per token (Kaplan scaling; Section 2).
- Memory cost: weights and KV cache must be loaded from on-device HBM to compute cores each forward pass; at large batches/long contexts, KV cache dominates memory time (Section 2; 2.1).
- Communication cost: depends on which tensor dimensions are sharded and which collectives are required (Section 3.1; Appendix A.1).
- Objective: pick partitioning to minimize total latency (or maximize MFU) subject to hardware limits and application goals.

B) Partitioning the feedforward (FFN) layers (Section 3.2)
The FFN dominates FLOPs, so its sharding determines most of the compute/communication.

1) 1D weight‚Äëstationary (Section 3.2.1; Figure 2a)
- What it does: shard each E√óF weight by one dimension (typically F, the intermediate dimension), keep the shard local, all‚Äëgather inputs and reduce‚Äëscatter outputs between the two matmuls in the FFN ‚Äúblock‚Äù.
- Communication behavior: per forward pass, activations of shape `BLE` are aggregated; communication time scales roughly as Tcomm ‚âà 2BLE / (network bandwidth), independent of chip count nchips (Section 3.2.1; Appendix A.1).
- Limitation: as nchips grows, memory and compute scale down, but this constant communication term becomes the bottleneck.

2) 2D weight‚Äëstationary (Section 3.2.2; Figure 2b)
- What it does: shard weights along both model dimension E and FFN dimension F so each device holds a smaller, closer-to-square chunk; alternate the aggregation axis across the two FFN matmuls so no chip ever needs the full activation (mechanically, one reduce-scatter/all-gather happens along E, the other along F).
- Key result: with optimal split X along E and Y√óZ along F, communication time scales as
  - Tcomm = 8BLE / (‚àönchips √ó network bandwidth), minimized by choosing X = 0.5‚àönchips and YZ = 2‚àönchips when F ‚âà 4E (Appendix A.2.1).
- Why it matters: unlike 1D, communication decreases with more chips (‚àù 1/‚àönchips), so latency keeps dropping as we scale up.

3) Weight‚Äëgathered layouts (Section 3.2.3; Figure 2c; Figure A.2)
- When to use: at very large tokens-per-batch BL (e.g., prefill with many sequences), the activation outputs become larger than the weights, so moving weights can be cheaper than moving activations.
- How it works:
  - Keep activations stationary on each chip; all‚Äëgather weights over N chips just-in-time for the two FFN matmuls.
  - Three variants differ in how widely weights are gathered: X‚Äëonly, XY, or XYZ (full) gathering; activations are partitioned to match (Figure A.2).
- Optimal choice and cost:
  - Choose N (the number of chips to all‚Äëgather weights over) to balance weight vs. activation traffic: N = ‚àö(BL¬∑nchips / F).
  - Communication time becomes Tcomm = 4E¬∑‚àö(BLF) / (‚àönchips √ó network bandwidth) (Appendix A.2.2).
- Design choice: store weights on device in the same ExFyz layout as 2D weight‚Äëstationary so the system can switch layouts between prefill (often weight‚Äëgathered) and decode (weight‚Äëstationary) without re-sharding the parameters (Section 3.2.3).

C) Partitioning the attention layer with multiquery attention (MQA) (Section 3.3; Figures 4‚Äì5)
- Background: MQA emits multiple query heads but shares a single key and a single value head across all queries; this reduces KV cache size by a factor of nheads (Section 3.3).
- Pitfall: If we shard attention over heads (as is typical for multi-head attention), MQA‚Äôs single K/V head must be fully replicated on each chip, erasing the memory benefit (Figure 4b).
- Proposed layout for decode (Figures 4c and 5b):
  - Shard by batch (B) across devices so each chip only loads and uses the slice of the KV cache for its subset of sequences.
  - Pay a small extra all‚Äëto‚Äëall to reshuffle the much smaller Q/K/V inputs and outputs.
  - Why it works: during decode, KV cache (many past tokens) is orders of magnitude larger than per-step Q/K/V (one token per sequence), so trading a small communication on Q/K/V for a large reduction in KV memory loads is beneficial (Section 3.3).
- Prefill exception: during prefill, Q has many tokens and reuses the same K/V; the amortized KV load is not the bottleneck, so the head-sharded layout remains preferable (Section 3.3).

D) Block structure: parallel attention/FFN layers (Section 3.4)
- Use the ‚Äúparallel‚Äù Transformer block (as in PaLM) where attention and FFN start from the same layer-normed input and are fused:
  - Benefits: one layer norm instead of two; larger fused matmuls; one fewer all‚Äëreduce per layer along the E/F axis (Section 3.4).

E) Low-level and numerical optimizations (Section 3.5‚Äì3.6)
- Looped CollectiveEinsum: schedule reduce‚Äëscatter/all‚Äëgather overlapping with the corresponding matmuls, improving end-to-end performance by ~1.4√ó vs. a naive compiler schedule (Section 3.5).
- Prefer reduce‚Äëscatter into hidden dims (E/F) to expose more overlap opportunities (Section 3.5).
- Miscellaneous kernels: faster sampling, softmax/swish, incremental prefill, better in-memory layouts (Section 3.5).
- Quantization: convert bfloat16 weights to int8 using AQT, reducing weight loading time (especially impactful at small batch sizes); matmuls still use bfloat16 activations (Section 3.6).

F) Implementation and hardware (Section 4; Section 4.4; Section 5)
- Framework: JAX + XLA; derived from T5X codebase (Section 4).
- Hardware: TPU v4 (275 TFLOPS bfloat16, 32 GiB HBM @1200 GB/s, 270 GB/s interconnect in 3D torus) (Section 4).
- Practical tweak: pad PaLM 540B heads from 48 to 64 to enable cleaner partitioning on 64+ chips (adds ~3% parameter overhead, recovered by better partitioning) (Section 4).

## 4. Key Insights and Innovations
1) Closed-form, phase-aware partitioning strategy for FFN (Sections 3.2 and A.2)
   - Novelty: explicit formulas for communication time for 1D vs. 2D weight‚Äëstationary and multiple weight‚Äëgathered variants, with the optimal sharding splits derived analytically.
   - Significance: enables principled selection of layout as batch size (BL), model dims (E, F), and chip count vary. In practice this flips prefill from weight‚Äëstationary to weight‚Äëgathered at large BL, while decode stays 2D weight‚Äëstationary (Figure 7).

2) Batch‚Äësharded multiquery attention for decode (Section 3.3; Figures 4‚Äì5)
   - Novelty: for MQA, shard attention over batch (not heads) during decode so each device loads only its KV-cache slice; use all‚Äëto‚Äëall to reshuffle small Q/K/V tensors.
   - Significance:
     - Dramatically lowers memory time for long contexts and large batches, enabling much longer contexts: up to 32‚Äì64√ó longer maximum context length than head-sharded variants on 64 chips (Table 1).
     - Yields growing latency wins as context length increases (Figure 8).

3) Use of parallel attention/FFN with collective-fused scheduling (Sections 3.4‚Äì3.5)
   - Novelty: systematically align block-level fusion (parallel block) with collective scheduling (Looped CollectiveEinsum) and reduction axes to hide communication.
   - Significance: ~1.4√ó speedup vs. naive schedules (Section 3.5), and fewer all‚Äëreduces per layer (Section 3.4).

4) End-to-end Pareto frontier at 500B scale with int8 weights (Figure 1; Tables 2‚Äì3)
   - Innovation is integrative: combining the theoretical partitioning choices, MQA batch sharding, fusion, overlap, and int8 weights.
   - Significance:
     - Low-latency interactive setup with PaLM 540B: 29 ms/token decode (int8 weights) and the ability to process 64-input + 64-output tokens with a 1920‚Äëtoken history in 1.9 s total on 64 chips (Figure 1; Section 4).
     - Large-batch prefill MFU of 76% (Figure 7; Table 2).

## 5. Experimental Analysis
Evaluation setup
- Models: PaLM family (8B, 62B, 540B) using parallel block and multiquery attention (Section 4); a Megatron-like model is also evaluated for cross-suite comparison (Appendix D, Table D.1).
- Hardware: up to 256 TPU v4 chips; most headline results on 64 chips (Section 4).
- Metrics:
  - Latency (prefill per forward pass; decode per generated token).
  - Cost measured as chip‚Äëseconds per token = nchips √ó time / (B√óL), directly proportional to dollars; inversely proportional to MFU (Section 4.4).
  - MFU to normalize across different hardware when compared to FasterTransformer on NVIDIA A100 (Section 5).
- Workloads:
  - Context length 2048 for main Pareto plots (Figure 1).
  - Prefill vs. decode analyzed separately due to distinct parallelism/memory patterns (Sections 2.2, 3.2‚Äì3.3).

Main quantitative results
- Overall Pareto frontiers (Figure 1)
  - Decode: ‚Äúminimum latency is ~3√ó lower than batch‚Äë512 latency,‚Äù showing the latency‚Äìcost trade-off as batch decreases (Figure 1, left).
  - Prefill: high MFU and low cost even at moderate batch sizes due to weight‚Äëgathered layouts; batch‚Äë512 prefill cost is ~2√ó lower than batch‚Äë512 decode (Figure 1, right; Section 4.4).
- Concrete PaLM 540B configurations (Table 2)
  - Low-latency setting on 64 chips (int8 weights):
    - Prefill 2048 tokens at batch 1 with 2D weight‚Äëstationary FFN: MFU 43%, 0.29 s.
    - Decode 64 tokens at batch 64 with 2D weight‚Äëstationary FFN and batch‚Äësharded attention: MFU 14%, 1.82 s.
  - High-throughput setting on 64 chips (bfloat16 weights):
    - Prefill 2048 tokens at batch 512 using XYZ weight‚Äëgathered FFN: MFU 76%, 85.2 s for the entire batch.
    - Decode 64 tokens at batch 512: MFU 33%, 6.0 s for the entire batch (Table 2).
- FFN partitioning behavior
  - 2D vs. 1D weight‚Äëstationary during decode: latency per token improves with chip count for 2D, while 1D flattens due to communication bottlenecks (Figure 6).
  - Prefill layout switch: as tokens per batch increase, MFU transitions from 2D weight‚Äëstationary to weight‚Äëgathered, peaking at 76% MFU at very large batches (Figure 7).
- MQA partitioning and long-context capability
  - Max context length on 64 chips with 30% memory reserved for KV cache (Table 1):
    - Multihead (dhead=128): 1320 (B=128) and 330 (B=512).
    - Baseline MQA, head-sharded (dhead=256): 660 and 165.
    - Optimized MQA, batch-sharded: 43,000 and 10,700.
  - Latency vs. context length (decode, 8-layer proxy): batch-sharded MQA increasingly outperforms as context grows; at long contexts, attention becomes only 8‚Äì31% of runtime (Figure 8; Section 4.2).
- Comparison to FasterTransformer (Figure 9; Appendix D)
  - MFU vs. latency for a 60‚Äëinput, 20‚Äëoutput benchmark: PaLM implementation on 64 TPU v4 achieves the best absolute latency and higher MFU at most points; notably, at 64‚Äëway tensor parallelism it maintains ~44% MFU, whereas FasterTransformer‚Äôs 32‚Äëway setup peaks near 33% MFU and degrades at 32‚Äëway vs. 16‚Äëway (Figure 9; Section 5).
  - Full numeric tables for several input/output settings show the same MFU‚Äìlatency pattern (Tables D.2‚ÄìD.4).

Do the experiments support the claims?
- The phase-aware layout selection is directly corroborated by MFU and latency trends (Figures 6‚Äì7).
- The MQA batch-sharding design both increases max context length by up to 32‚Äì64√ó (Table 1) and lowers decode step time at long contexts (Figure 8).
- The end-to-end Pareto plots and example configs quantify low-latency and high-throughput operating points on a 540B model (Figure 1; Table 2).
- Cross-suite comparisons normalize differences via MFU and show scalability advantages at higher parallel degrees (Figure 9; Section 5). The paper notes hardware differences explicitly.

Ablations and robustness
- Ablations are analytical + empirical:
  - 1D vs. 2D vs. weight‚Äëgathered FFN (Figures 6‚Äì7).
  - Head‚Äësharded vs. batch‚Äësharded MQA (Figures 4‚Äì5, 8; Table 1).
  - Parallel vs. serial block: serial increases decode latency by ~14% at batch 512 on 64 chips (Section 4.3).
- Failure modes:
  - Without batch‚Äësharded MQA, memory limits prevent long contexts at 540B scale (Figure 8, dotted line; Table 1).
  - Some weight‚Äëgathered layouts exhaust memory unless communication/computation overlap is carefully implemented (Section 3.5).

## 6. Limitations and Trade-offs
- Hardware specificity vs. generality
  - The derivations assume access to fast collective primitives and a high-bandwidth, low-diameter interconnect (3D torus on TPU v4). While the formulas for collective costs are general (Appendix A.1), absolute numbers and optimal splits may differ on other topologies (Section 7 notes generalization, but no direct GPU result beyond MFU comparisons).
- Latency vs. cost
  - Extremely low latency requires many chips and small batch sizes, which reduces MFU and raises cost per token (Figure 1 left; Section 2.1).
- Activation quantization not used
  - Only weights are int8. Large-batch scenarios remain compute‚Äëbound with bfloat16 matmuls; activation quantization could further reduce cost but is not implemented (Section 3.6; Section 4.4).
- Scope: dense, decoder-only models
  - The analysis targets dense models; mixture-of-experts or encoder‚Äìdecoder specifics are not explored (Section 7 suggests this as future direction).
- Operational complexity
  - Switching layouts between prefill and decode, and using different attention sharding per phase, adds system complexity and requires careful orchestration (Sections 3.2‚Äì3.3; 4.1).
- Quality considerations for MQA
  - The work assumes MQA (as in PaLM) without re-evaluating quality trade-offs vs. multihead in this paper; results focus on efficiency and memory (Section 4; Table 1 discusses capacity limits, not task accuracy).

## 7. Implications and Future Directions
- Practical guidance for serving very large LLMs
  - Use 2D weight‚Äëstationary for decode on many chips to avoid a communication plateau (Figure 6).
  - For prefill at large BL, switch to weight‚Äëgathered (often XYZ) to push MFU into the 70%+ range (Figure 7).
  - With MQA models, shard attention by batch during decode to unlock long contexts and lower memory time (Figures 4‚Äì5; Table 1; Figure 8).
  - Prefer parallel attention/FFN blocks and fuse collectives with matmuls to hide communication (Sections 3.4‚Äì3.5).
  - Apply int8 weight quantization for low-latency regimes (Section 3.6; Figure 1 left).
- Impact on the field
  - Shifts inference design from ‚Äúuse as much tensor parallelism as possible‚Äù to ‚Äúchoose phase- and tensor-dimension-aware partitioning with provable communication scaling,‚Äù enabling 500B‚Äëclass models to meet both interactive and offline needs on multi-chip clusters.
- Research directions
  - Activation quantization and communication compression to further reduce cost at large batch sizes (Section 3.6; Section 7).
  - Extending the analytical framework to sparse/MoE models and adaptive computation to reduce FLOPs per token (Section 7).
  - Porting the batch‚Äësharded MQA approach and 2D layouts to different interconnects (multi-node NVLink, Ethernet fabrics) and validating optimal splits under those bandwidth/latency profiles (Section 7).
- Downstream applications
  - Long-context interactive systems (summarization, chat with long histories) become feasible at 500B scale due to the 32√ó‚Äì64√ó context length expansion (Table 1).
  - High-throughput offline scoring/distillation pipelines benefit from weight‚Äëgathered prefill and high MFU (Table 2; Figure 1 right).

Selected, grounded highlights
- ‚Äú29 ms per token during generation (using int8 weight quantization) ‚Ä¶ on the PaLM 540B model‚Äù (Abstract; Figure 1 left; Table 2 decode).
- ‚Äú76% MFU during large-batch-size processing of input tokens [prefill]‚Äù (Abstract; Figure 7; Table 2 prefill).
- ‚ÄúOptimized multiquery attention enables up to 32√ó larger context lengths‚Äù (Table 1; Section 4.2).
- ‚Äú2D weight‚Äëstationary communication scales as 1/‚àönchips‚Äù with optimal X,Y,Z (Appendix A.2.1).
- ‚ÄúWeight‚Äëgathered communication scales with ‚àö(BL)‚Äù and becomes preferable at large tokens per batch (Appendix A.2.2; Figure 3).
- ‚ÄúOur 64‚Äëway tensor parallelism maintains ~44% MFU, whereas FasterTransformer‚Äôs 32‚Äëway peaks near 33% MFU‚Äù (Figure 9; Section 5).

# A Survey of Reinforcement Learning for Large Reasoning Models

**ArXiv:** [2509.08827](https://arxiv.org/abs/2509.08827)

## ðŸŽ¯ Pitch

This comprehensive survey establishes the first unified framework for applying reinforcement learning (RL) to Large Reasoning Models (LRMs)â€”advanced language and multimodal models designed for complex, multi-step reasoning. By synthesizing recent advances in reward design, policy optimization, scalable infrastructure, and open problems, it distinguishes RL for reasoning from alignment-focused approaches like RLHF/DPO and clarifies how to scale LLMs into verifiable, agentic, and generalist problem solvers. This work matters because it lays the methodological foundation for transforming LLMs into AI systems capable of planning, tool use, and scientific reasoning, offering a roadmap for the next wave of practical and scientific breakthroughs in AI.

---

## 1. Executive Summary (2-3 sentences)
This survey systematizes the fastâ€‘moving field of reinforcement learning (RL) for Large Reasoning Models (LRMs)â€”language and multimodal models whose core capability is multiâ€‘step reasoning rather than only instruction following. It builds a unified framework for how to design rewards, optimize policies, sample data, and evaluate progress; clarifies open controversies (e.g., whether RL â€œdiscoversâ€ new skills or mainly â€œsharpensâ€ existing ones); and compiles the resources and infrastructure required to scale RL beyond alignment toward verifiable, agentic, and multimodal reasoning (Figures 1â€“2, Sections 3â€“7).

## 2. Context and Motivation
- Problem the paper addresses
  - There is no consolidated playbook for turning general LLMs into LRMs that reliably plan, reason, and act under long horizons. Prior postâ€‘trainingâ€”RLHF and DPOâ€”focused on aligning behavior to preferences, not on incentivizing correct reasoning (Figure 2). This survey answers how to design and scale RL specifically for reasoning across tasks such as math, code, tools, agents, robotics, and medicine (Sections 1â€“2, 6).

- Why this matters
  - Practical impact: Verifiable tasks such as competition math and coding benefit from automated rewards, enabling rapid capability gains and reducing reliance on scarce human labels (Sections 3.1.1, 6.1). Agentic and toolâ€‘use systems need turnâ€‘byâ€‘turn feedback to learn to plan, call tools, and correct themselves (Section 6.2).
  - Theoretical significance: RL introduces a new scaling axisâ€”trainâ€‘time interactions and testâ€‘time â€œthinkingâ€ budgetâ€”that is orthogonal to data/parameter scaling (Section 1; Figure 2).

- Limitations of prior approaches
  - RLHF/DPO rely on learned reward models or preferences; these are noisy outside wellâ€‘specified domains and susceptible to reward hacking (Sections 1, 3.1.1, 4.5).
  - Supervised fineâ€‘tuning (SFT) often memorizes surface patterns and can cause catastrophic forgetting when distribution shifts (Section 4.2).

- Positioning relative to existing work
  - The survey centers RL for reasoning, not just alignment. It formalizes how LMs fit into the RL loop (Figure 3; Section 2.1), maps the algorithmic space (Tables 2â€“3; Section 3), distills foundational debates (Section 4), curates training resources (Tables 4â€“6; Section 5), and inventories applications in coding, agents, multimodality, multiâ€‘agent, robotics, and medicine (Figure 6; Section 6). It closes with research roadmaps (Section 7).

## 3. Technical Approach
This is a structured survey. Its â€œmethodâ€ is an organizing framework and precise formalization of RL for LRMs.

- Mapping LMs to the RL loop (Section 2.1; Figure 3)
  - State `s_t`: the prompt plus tokens generated so far, i.e., `(x, a_1: t-1)`.
  - Action `a_t`: a next token, a segment, or an entire sequence; the â€œgranularityâ€ matters for reward and credit assignment (Table 2).
  - Transition: deterministic string concatenation `s_{t+1} = [s_t, a_t]` until EOS.
  - Reward: can be sequenceâ€‘level (sparse) or token/step/turnâ€‘level (dense) (Table 2).
  - Objective: maximize expected return J(Î¸) over the data distribution (Eq. (1)).

- Reward design taxonomy (Section 3.1)
  - Verifiable rewards (Section 3.1.1): ruleâ€‘based correctness/format checksâ€”e.g., boxed math answer equality or unit tests for code. â€œVerifierâ€™s Lawâ€ highlights that tasks are easiest to train when feedback is automatically checkable.
  - Generative rewards (Section 3.1.2): learned â€œjudgesâ€ that reason before scoring. Two families:
    - Modelâ€‘based verifiers for verifiable tasks to handle formatting brittleness.
    - Assessmentâ€‘based generative reward models (GenRMs) for subjective tasks using chainâ€‘ofâ€‘thought (CoT) critiques or rubricâ€‘guided evaluation; can coâ€‘evolve with the policy.
  - Dense rewards (Section 3.1.3): step/token/turnâ€‘level signals via process reward models (PRMs), Monteâ€‘Carlo attribution, or explicit perâ€‘turn supervision for toolâ€‘use/agents (Table 2).
  - Unsupervised rewards (Section 3.1.4): no human labelsâ€”derive signals from model consistency, internal confidence/entropy, selfâ€‘generated knowledge (â€œselfâ€‘rewardingâ€), or dataâ€‘centric heuristics and corpora.
  - Reward shaping (Section 3.1.5): combine verifiers with reward models and structure advantages at the group/set level (e.g., group baselines in GRPO; aligning to Pass@K).

- Policy optimization landscape (Section 3.2)
  - General PPOâ€‘style objective (Eq. (5)): clipped ratio and advantage estimate.
  - Criticâ€‘based algorithms (Section 3.2.2): PPO with value models and GAE (Eqs. (6)â€“(9)) provides tokenâ€‘level signals but adds compute/instability under long horizons.
  - Criticâ€‘free algorithms (Section 3.2.3): REINFORCE (Eq. (10)); GRPO (Eqs. (11)â€“(12)) replaces tokenâ€‘level values with groupâ€‘relative sequence advantagesâ€”simple, scalable with verifiable rewards.
  - Offâ€‘policy optimization (Section 3.2.4; Eq. (13)): learn from replay/asynchronous data and offline corpora; also hybrid SFT+RL loss or data mixing.
  - Regularization (Section 3.2.5): KL penalties to a reference or old policy (Eq. (14)); entropy regularization to avoid entropy collapse (Eq. (15)); length penalties to control thinking cost.

- Sampling strategy (Section 3.3)
  - Dynamic sampling (Section 3.3.1): focus rollouts on mediumâ€‘difficulty or underâ€‘mastered items; curriculum and prioritized replay; encourage exploration where uncertainty/entropy is high.
  - Structured sampling (Section 3.3.1): tree sampling (MCTSâ€‘like) for nodeâ€‘level process signals; sharedâ€‘prefix/segment rollouts to reuse compute.
  - Hyperparameters (Section 3.3.2): tuning temperature, entropy targets, clipping bounds, and staged context lengthening (e.g., 8kâ†’32k) to balance exploration and cost.

- Empirical scaffolding compiled by the survey
  - Frontier models timeline and coverage (Section 2.2; Figure 4; Table 1).
  - Static corpora (Section 5.1; Table 4) and dynamic environments (Section 5.2; Table 5).
  - RL infrastructure (Section 5.3; Table 6): training runtimes, serving, distributed rollouts.

## 4. Key Insights and Innovations
- A unified formal and practical map of RL for LRMs
  - Whatâ€™s new: The survey aligns notation (Figure 3; Eq. (1)), clarifies action/reward granularities (Table 2), and connects algorithm families and sampling/regularization choices (Sections 3.1â€“3.3; Table 3).
  - Why it matters: Practitioners can plugâ€‘andâ€‘play componentsâ€”verifier/GenRM, GRPO/PPO, dynamic samplingâ€”rather than reinvent pipelines.

- Verifierâ€‘centric scaling principle (â€œVerifierâ€™s Lawâ€)
  - Content: â€œThe ease of training AI systems to perform a task is proportional to the degree to which the task is verifiableâ€ (Section 3.1.1). Math and code reward pipelines succeed because they offer automatic, precise, and scalable feedback.
  - Significance: Explains why RLVR (ruleâ€‘based RL with verifiable rewards) has rapidly advanced math/code reasoning (Sections 1, 2.2, 6.1), and frames the challenge of openâ€‘ended tasks where only GenRMs or rubrics are viable (Section 3.1.2).

- Processâ€‘level credit assignment for longâ€‘horizon reasoning
  - Content: The survey organizes dense reward techniques (token/step/turn in Table 2 and Section 3.1.3) including PRMs, Monteâ€‘Carlo step attribution, turnâ€‘level evaluators for tool calls, and tree rollouts.
  - Significance: These are the mechanisms that reduce variance and improve sample efficiency relative to solely outcomeâ€‘level rewards (Section 3.1.3 â€œTakeawaysâ€).

- Clarity on contentious issues that guide scaling decisions (Section 4)
  - Sharpening vs. discovery (Â§4.1): Evidence that RL improves Pass@1 but can shrink exploration (Limitâ€‘ofâ€‘RLVR), alongside counterâ€‘evidence showing extended RL grows both Pass@1 and Pass@K and enables composition of new skills.
  - RL vs. SFT (Â§4.2): Empirical patternsâ€”â€œSFT memorizes, RL generalizesâ€â€”with important caveats and unified/alternating paradigms that often work best.
  - Model priors (Â§4.3): RL responsiveness differs across families (e.g., Qwen vs. Llama); midâ€‘training/annealing with math/code corpora can make weaker priors more RLâ€‘friendly.

- Complete ecosystem view
  - Datasets (Table 4), gyms/worlds (Table 5), and infra (Table 6) are cataloged so one can reproduce endâ€‘toâ€‘end RLVR/RLâ€‘agent training. Few surveys provide this breadth (Sections 5â€“6).

## 5. Experimental Analysis
This survey synthesizes results across many works rather than running new experiments. It still specifies evaluation norms, representative findings, and caveats.

- Evaluation methodology compiled
  - Tasks: math and code (verifiable); toolâ€‘use and agents (turnâ€‘level signals); multimodal images/videos/3D; robotics and medicine (Sections 6.1â€“6.6).
  - Metrics:
    - Pass@1 and Pass@K for problem solving; the survey notes that optimizing for Pass@1 can hurt broad exploration and introduces Pass@Kâ€‘aligned objectives and credit shaping (Section 3.1.5; Section 4.1).
    - For agents: perâ€‘turn success, tool correctness, and environmentâ€‘level completion (Sections 3.1.3 Turnâ€‘level; 6.2).
    - For generation: aesthetic/consistency/physics scores for images/videos; rubricâ€‘based ratings for subjective tasks (Sections 3.1.2, 6.3).
  - Setups: Onâ€‘policy RL (GRPO/PPO), criticâ€‘free vs criticâ€‘based, replay and asynchronous architectures (Sections 3.2â€“3.3; Table 6).

- Representative quantitative patterns reported from the literature
  - Smooth scaling with more RL compute and testâ€‘time thinking for o1/R1â€‘style systems (Section 1; Figure 2 discussion).
  - Groupâ€‘relative advantages (GRPO, Eq. (12)) stabilize criticâ€‘free training on verifiable tasks; PPO/GAE (Eqs. (6)â€“(9)) offer finer tokenâ€‘level control but are heavier and sensitive to noise (Sections 3.2.2â€“3.2.3; Table 3).
  - Evidence tension in Â§4.1:
    - Quote (sharpening view): â€œPass@K evaluations indicate that RL enhances Pass@1 performance, yet tends to underperform relative to base models when sampling broadly at largeâ€‘K Pass@Kâ€ (Section 4.1).
    - Quote (discovery view): â€œProRL v2 â€¦ demonstrates stronger results,â€ including improved Pass@1 and Pass@K via prolonged RL with engineering advances (Section 4.1); and â€œLLMs can learn new skills in RL through composition of existing capabilitiesâ€ (Section 4.1).
  - RL vs. SFT: â€œSFT memorizes, RL generalizesâ€ across textual and vision settings (Section 4.2 summarizing GeneralPoints/Vâ€‘IRL); however, SFT warmâ€‘ups and unified SFT+RL objectives often yield the best stability and transfer (Section 4.2).

- Ablations/failure modes highlighted
  - Reward hacking and â€œreasoning illusionsâ€ when rewards are poorly specified or when learned reward models are brittle (Sections 3.1.2, 4.5).
  - Entropy collapse without either explicit entropy terms (Eq. (15)) or explorationâ€‘promoting tricks such as asymmetric clipping and advantage shaping (Section 3.2.5; 3.3.2).
  - Modelâ€‘family dependence and midâ€‘training sensitivity (Section 4.3).
  - Lengthâ€‘performance tradeâ€‘offs and staged context curricula to prevent wasteful long CoT (Section 3.2.5 Length Penalty; Section 3.3.2).

- Does the evidence support the claims?
  - The survey refrains from a single leaderboard; instead, it triangulates mechanisms and consistent patterns across many sources and explicitly flags points of contention (Section 4). When claims are conditional (e.g., RL helps more on verifiable domains), the conditions and counterâ€‘examples are provided.

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Verifiability assumption: many successes hinge on tasks with automatic checkers; subjective domains still need GenRMs/rubrics and are harder to scale (Sections 3.1.1â€“3.1.2).
  - Stationarity and onâ€‘policy sampling: PPO/GRPO assume manageable drift; large asynchrony, quantized inference, or offâ€‘policy data requires careful importance weighting and replay design (Section 3.2.4).

- Scenarios not fully addressed
  - Openâ€‘ended creativity and safety alignment under sparse or conflicting human preferences remain challenging; rubric/GenRM solutions are evolving (Section 3.1.2).
  - Crossâ€‘domain, longâ€‘horizon, multiâ€‘step tasks without clear intermediate verifiers still lack robust process credit assignment (Section 3.1.3 â€œTakeawaysâ€).

- Computational and data constraints
  - RL rollouts add a heavy training inference cost; long CoT and large group sizes increase memory/latency (Sections 3.3.2, 4.4).
  - Critic models (PPO/GAE) add compute overhead and instability under noisy rewards (Section 3.2.2).
  - Dynamic environments and tool ecosystems require significant engineering (Tables 5â€“6).

- Open questions and weaknesses called out
  - Exact role of KL regularization for reasoning RLâ€”some pipelines remove it entirely while others find it essential (Section 3.2.5).
  - Whether RL primarily â€œsharpensâ€ or truly â€œdiscoversâ€ depends on model prior, reward design, and training scale (Section 4.1).
  - Evaluation gaps: inconsistent OOD benchmarks; distinguishing generalization from data contamination (Section 4.2).

## 7. Implications and Future Directions
- How this work changes the landscape
  - Provides a blueprint to build LRMs with RL beyond alignment: choose verifiable or generative rewards; pick criticâ€‘free GRPO for scalable verifiable tasks or criticâ€‘based PPO/PRM when dense tokenâ€‘level signals are needed; use dynamic sampling and length control; exploit replay/asynchrony when helpful (Sections 3â€“5).
  - Gives practitioners a catalog of environments, datasets, and frameworks to launch largeâ€‘scale projects quickly (Tables 4â€“6).

- Concrete followâ€‘ups and research opportunities (Section 7)
  - Continual RL for LRMs (Â§7.1): lifelong, multiâ€‘stage training that balances stability vs plasticityâ€”experience replay and policy reuse tailored to language agents.
  - Memoryâ€‘based RL (Â§7.2): turn perâ€‘task memory into an experience substrate shared across tasks; learn policies that manage and compose memory.
  - Modelâ€‘based RL (Â§7.3): build world models (text/vision) to simulate environments and generate robust state/reward signals.
  - Efficient reasoning (Â§7.4): learn computeâ€‘allocation and halting policiesâ€”instanceâ€‘adaptive reasoning depth rather than uniform long CoT; formalize costâ€‘performance tradeâ€‘offs.
  - Latentâ€‘space reasoning (Â§7.5): move from tokenâ€‘space CoT to continuous latent reasoning, then design reward/advantage signals for latent trajectories.
  - RL for preâ€‘training (Â§7.6): reframe nextâ€‘token prediction as RL with corpusâ€‘derived rewards; explore unsupervised/selfâ€‘rewarding at scale (Eq. (13) context).
  - RL for diffusionâ€‘based LLMs (Â§7.7): address ELBO/likelihood estimation challenges and trajectoryâ€‘level rewards during denoising.
  - Scientific discovery (Â§7.8): couple RL with simulators and domainâ€‘specific verifiers to replace slow/expensive wetâ€‘lab feedback.
  - Architectureâ€“algorithm coâ€‘design (Â§7.9): treat routing/sparsity/expert activation as actions and jointly optimize capability and hardware efficiency.

- Practical applications
  - Coding and program repair with unitâ€‘test/verifier rewards (Section 6.1).
  - Web, search, and toolâ€‘use agents using turnâ€‘level rewards and asynchronous rollouts (Section 6.2).
  - Multimodal reasoning and generation with visual/temporal verifiers or rubricâ€‘guided GenRMs (Section 6.3).
  - Multiâ€‘agent collaboration with RLâ€‘trained language agents (Section 6.4).
  - VLA robotics trained with success/failure outcomes and replay in simulation (Section 6.5).
  - Medical reasoning with correctness, formatting, and rubric rewards across text and imaging (Section 6.6).

> Overall, Figures 1â€“2 frame the shift from alignment RL (RLHF/DPO) to reasoningâ€‘oriented RL (RLVR and beyond); Figure 3 and Tables 2â€“3 specify the mechanics; Tables 4â€“6 furnish the ecosystem; Sections 4 and 7 articulate the debates and the path forward.

# ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION

**ArXiv:** [1412.6980](https://arxiv.org/abs/1412.6980)

## ğŸ¯ Pitch

Adam introduces a novel optimization algorithm that adapts the learning rate for each parameter by maintaining moving averages of both the gradients and their squared valuesâ€”automatically adjusting to the noise and sparsity common in practical machine learning. By unifying the benefits of AdaGrad (good with sparse data) and RMSProp (stable in non-stationary settings), while adding crucial bias correction and strong theoretical convergence guarantees, Adam has become a robust, easy-to-use default for training deep neural networks and other large-scale models with stochastic objectives.

---

## 1. Executive Summary
Adam is an optimization algorithm for training models with stochastic (noisy) gradients that adapts the learning rate separately for every parameter by maintaining moving averages of first and second moments of the gradients. It matters because it combines the strengths of two influential methods (AdaGradâ€™s handling of sparsity and RMSPropâ€™s stability on nonâ€‘stationary problems) while adding crucial biasâ€‘correction and a convergence guarantee, making it a robust default optimizer for large, noisy, highâ€‘dimensional problems.

## 2. Context and Motivation
- Problem addressed
  - Many learning problems optimize an objective that is observed through noisy estimates (e.g., miniâ€‘batches, dropout). Choosing effective learning rates is hard, especially when gradients vary across parameters or over time.
  - Existing firstâ€‘order methods either require handâ€‘tuned schedules or handle only part of the problem (sparsity vs. nonâ€‘stationarity).

- Why important
  - Practical: determines whether large neural networks and other models train quickly and stably on GPUs/large datasets.
  - Theoretical: interest in provable guarantees for online convex optimization with adaptive methods.

- Prior approaches and gaps
  - SGD with momentum: strong baseline but sensitive to global learningâ€‘rate choice; no perâ€‘parameter adaptivity.
  - AdaGrad (Duchi et al., 2011): sums past squared gradients to set perâ€‘parameter step sizes; excels with sparse gradients but its learning rate decays monotonically, which can be too aggressive.
  - RMSProp (Tieleman & Hinton, 2012): uses an exponential moving average (EMA) of squared gradients to adapt step sizes for nonâ€‘stationary problems but lacks biasâ€‘correction; can be unstable when the EMA uses slow decay (Section 5 and Figure 4).
  - Quasiâ€‘Newton/SFO: stronger curvature use but heavy memory/time costs and difficulty with stochastic regularization (Section 6.2).

- Positioning
  - Adam is designed to â€œcombine the advantagesâ€ of AdaGrad and RMSProp (Section 1), add initialization biasâ€‘correction (Sections 2â€“3), provide bounded effective step sizes with invariance to gradient rescaling (Section 2.1), and offer an onlineâ€‘learning regret bound (Section 4). It also introduces a simple infinityâ€‘norm variant, AdaMax (Section 7.1).

## 3. Technical Approach
Adam performs perâ€‘parameter adaptive updates using two exponentially decayed moving averages; it then corrects their initialization bias and uses them to scale the step.

- Core update (Algorithm 1, Section 2)
  1. Initialize `m0 = 0`, `v0 = 0`, timestep `t = 0`. Hyperparameters: learning rate `Î±`, decay rates `Î²1` and `Î²2` in `[0,1)`, and a small `Îµ` for numerical stability. Defaults reported: `Î± = 0.001, Î²1 = 0.9, Î²2 = 0.999, Îµ = 1e-8`.
  2. At each step `t`:
     - Compute stochastic gradient `g_t = âˆ‡Î¸ f_t(Î¸_{t-1})`.
     - Update firstâ€‘moment EMA (an EMA is a weighted average that gives exponentially less weight to older values): `m_t = Î²1 m_{t-1} + (1 - Î²1) g_t`.
     - Update second raw moment EMA (uncentered variance): `v_t = Î²2 v_{t-1} + (1 - Î²2) g_t^2` (elementâ€‘wise square).
     - Biasâ€‘correct both EMAs to remove smallâ€‘t underestimation caused by zero initialization (Section 3): `mÌ‚_t = m_t / (1 - Î²1^t)`, `vÌ‚_t = v_t / (1 - Î²2^t)`.
     - Parameter update: `Î¸_t = Î¸_{t-1} - Î± Â· mÌ‚_t / (sqrt(vÌ‚_t) + Îµ)`.

- Why biasâ€‘correction?
  - With EMAs starting at zero, early estimates are biased low. Section 3 derives the expected value of `v_t`:
    - `E[v_t] = (1 - Î²2^t) E[g_t^2] + Î¶` (Equations 1â€“4), where `Î¶` is small if the second moment is slowly varying. Dividing by `(1 - Î²2^t)` removes the main bias term. An analogous correction applies to `m_t`.

- Effective step size properties (Section 2.1)
  - Ignoring `Îµ`, the effective step is `Î”_t = Î± Â· mÌ‚_t / sqrt(vÌ‚_t)`.
  - Bounded magnitude: 
    - If `(1 - Î²1) â‰¤ sqrt(1 - Î²2)`, then `|Î”_t| â‰¤ Î±` for each coordinate (upper bound); otherwise `|Î”_t| â‰¤ Î± Â· (1 - Î²1)/sqrt(1 - Î²2)`.
  - Invariance to gradient scaling: scaling all gradients by `c` scales `mÌ‚_t` by `c` and `vÌ‚_t` by `c^2`, which cancel in `mÌ‚_t / sqrt(vÌ‚_t)`.
  - â€œSignalâ€‘toâ€‘noise ratioâ€ (their terminology, Section 2.1): the ratio `mÌ‚_t / sqrt(vÌ‚_t)` shrinks near optima (low gradient magnitude relative to variance), naturally annealing steps.

- Theoretical guarantee (Section 4)
  - Framework: online convex optimization with regret `R(T) = âˆ‘_{t=1}^T [f_t(Î¸_t) - f_t(Î¸*)]`, where `Î¸*` is the best fixed point in hindsight.
  - Under assumptions of bounded gradients and bounded parameter diameter, with a decaying learning rate `Î±_t = Î± / sqrt(t)` and exponentially decaying momentum coefficient `Î²1,t = Î²1 Î»^{t-1}`, Theorem 4.1 proves
    - `R(T) = O(âˆšT)` and Corollary 4.2 gives average regret `R(T)/T = O(1/âˆšT)`.
  - Proof elements rely on Lemma 10.3 (bounding sums of `|g_t|/âˆšt`) and Lemma 10.4 (bounding terms involving biasâ€‘corrected moments) in the Appendix.

- Relation to prior methods (Section 5)
  - If `Î²1 = 0` and `Î²2 â†’ 1` with annealed `Î±_t = Î± / âˆšt`, Adam reduces to AdaGrad with perâ€‘coordinate step `Î± Â· g_t / âˆš(âˆ‘_{i=1}^t g_i^2)`â€”but only when using biasâ€‘correction; otherwise the limit is illâ€‘behaved.
  - RMSProp with momentum resembles Adam but lacks biasâ€‘correction and applies momentum to rescaled gradients rather than maintaining a separate firstâ€‘moment EMA.

- AdaMax variant (Section 7.1; Algorithm 2)
  - Generalizes Adamâ€™s `L2`â€‘based scaling to the `Lp` norm and takes `p â†’ âˆ`. The limiting update uses the exponentially weighted infinity norm:
    - `u_t = max(Î²2 Â· u_{t-1}, |g_t|)` and the update `Î¸_t = Î¸_{t-1} - [Î±/(1 - Î²1^t)] Â· m_t / u_t`.
  - No biasâ€‘correction is needed for `u_t`. The parameter step has a simple bound: `|Î”_t| â‰¤ Î±`.

- Temporal averaging (Section 7.2)
  - Optional EMA over parameters: `\bar{Î¸}_t = Î²2 Â· \bar{Î¸}_{t-1} + (1 - Î²2) Î¸_t`, with biasâ€‘corrected `\hat{Î¸}_t = \bar{Î¸}_t / (1 - Î²2^t)`; a standard technique to reduce the variance of the final iterate.

## 4. Key Insights and Innovations
- Biasâ€‘corrected adaptive moments for stability and correctness (Sections 2â€“3)
  - Novelty: explicitly corrects the initialization bias of both first and second moment EMAs via division by `(1 - Î²^t)`. RMSProp and many momentum variants omit this step.
  - Significance: enables using very slow decay (`Î²2 â‰ˆ 0.999`) needed for sparse gradients without exploding steps. Figure 4 shows that with `Î²2` close to 1, removing biasâ€‘correction leads to instabilities early in training, while biasâ€‘correction stabilizes and improves final loss.

- Bounded, scaleâ€‘invariant effective step sizes (Section 2.1)
  - Novelty: a clean analytical bound on each coordinateâ€™s step magnitude (â‰¤ `Î±` in common settings) and invariance to gradient rescaling.
  - Significance: easier hyperparameter selection and robust behavior across models with different gradient scales; the SNR view explains automatic annealing near optima.

- Unified view connecting AdaGrad and RMSProp (Section 5)
  - Novelty: shows Adam reduces to AdaGrad under a limit while behaving like a biasâ€‘corrected RMSProp with a separate firstâ€‘moment track.
  - Significance: clarifies when and why Adam inherits strengths: sparseâ€‘feature efficiency from AdaGrad and nonâ€‘stationary handling from RMSProp.

- Onlineâ€‘learning convergence with adaptive moments (Section 4; Appendix)
  - Novelty: an `O(âˆšT)` regret bound for Adam with decaying `Î±_t` and `Î²1,t`, under convexity and boundedness assumptions.
  - Significance: places Adam among methods with competitive theoretical guarantees while remaining practical.

- AdaMax: a simple, numerically stable infinityâ€‘norm variant (Section 7.1)
  - Novelty: derivation by taking the `Lp` limit; update uses `u_t = max(Î²2 u_{t-1}, |g_t|)`.
  - Significance: fewer numerical issues and a very simple bound on steps; reported default `Î± = 0.002` works well in tested problems.

## 5. Experimental Analysis
- Evaluation methodology
  - Datasets and tasks:
    - Logistic regression on MNIST digits and IMDB bagâ€‘ofâ€‘words sentiment (Section 6.1). IMDB features are highly sparse; dropout (50%) applied to input features.
    - Multilayer perceptrons (two hidden layers with 1000 ReLU units) on MNIST; both deterministic loss with `L2` weight decay and with dropout regularization (Section 6.2). Minibatch size 128.
    - Convolutional neural network on CIFARâ€‘10 with architecture `c64â€“c64â€“c128â€“1000` (three conv layers with 5Ã—5 filters and 3Ã—3 maxâ€‘pooling, then a 1000â€‘unit ReLU layer), with whitening and dropout on input and the fully connected layer (Section 6.3).
  - Baselines:
    - AdaGrad, RMSProp (for IMDB logistic regression), SGD with Nesterov momentum, AdaDelta, and SFO (a quasiâ€‘Newton optimizer) for the deterministic MLP.
  - Schedules/hyperparameters:
    - Logistic regression uses `Î±_t = Î± / âˆšt` to match the analysis in Section 4 (Section 6.1).
    - Minibatch size uniformly 128, consistent across comparisons (Sections 6.1â€“6.3).

- Main results
  - Logistic regression (Figure 1):
    - MNIST: Adam converges at roughly the same rate as SGD with Nesterov momentum and both are noticeably faster than AdaGrad. The left panel shows Adam and Nesterov curves dropping steeply in the first few fullâ€‘dataset iterations, reaching substantially lower training cost than AdaGrad by 45 iterations.
    - IMDB (sparse features with dropout): AdaGrad outperforms momentum SGD by a wide margin; Adam matches AdaGradâ€™s fast convergence. The right panel shows Adam+dropout and AdaGrad+dropout attaining the lowest training cost across 160 iterations.
  - Multilayer neural networks (Figure 2):
    - With dropout: Adam achieves the lowest training cost across ~200 passes, outperforming AdaGrad, RMSProp, Nesterov SGD, and AdaDelta (Figure 2a).
    - Deterministic objective vs. SFO: Adam reduces cost faster per iteration and per wallâ€‘clock time. SFO requires curvature updates that make each iteration â€œ5â€“10Ã— slowerâ€ and its memory scales with the number of minibatches (Section 6.2).
    - SFO fails to converge with stochastic regularization (dropout), whereas Adam remains stable (Section 6.2).
  - Convolutional neural networks (Figure 3):
    - Early phase (first 3 epochs): Adam and AdaGrad drop the training cost fastest (left panel).
    - Later phase (45 epochs): Adam and SGD with momentum converge substantially faster than AdaGrad, whose progress slows markedly (right panel). Section 6.3 explains that Adamâ€™s secondâ€‘moment estimate `vÌ‚_t` â€œvanishes to zeros after a few epochs and is dominated by Îµ,â€ making geometry estimation less helpful; firstâ€‘moment variance reduction dominates, favoring Adam/SGD over AdaGrad.
  - Biasâ€‘correction ablation (Figure 4):
    - With `Î²2` close to 1 (e.g., `0.999` and `0.9999`), removing biasâ€‘correction produces instability â€œespecially at first few epochs,â€ while biasâ€‘correction yields lower loss both after 10 and 100 epochs when training a VAE (Section 6.4). The red (corrected) curves are consistently better than green (uncorrected) across stepsizes.

- Do the experiments support the claims?
  - Yes, across convex and nonâ€‘convex tasks, Adam shows:
    - At least parity with the best baseline in each setting, and clear wins under dropout/stochastic regularization (Figures 2 and 4).
    - Robustness to sparse features (IMDB; Figure 1 right) and nonâ€‘stationarity (dropout; Figure 2a).
    - Practical efficiency relative to quasiâ€‘Newton SFO (Section 6.2).
  - Ablations: The biasâ€‘correction study (Figure 4) directly tests the necessity of Adamâ€™s correction and finds it critical when `Î²2` is large (a common practical choice).

- Numbers explicitly reported in the paper
  - Default hyperparameters used successfully: â€œÎ± = 0.001, Î²1 = 0.9, Î²2 = 0.999, Îµ = 10^{-8}â€ (Algorithm 1 caption).
  - SFO perâ€‘iteration cost: â€œ5â€“10Ã— slower per iteration compared to Adamâ€ (Section 6.2).
  - Theoretical rates: regret `O(âˆšT)` and average regret `O(1/âˆšT)` (Theorem 4.1 and Corollary 4.2).

## 6. Limitations and Trade-offs
- Theoretical assumptions vs. practice (Section 4)
  - Convergence proof requires convex losses, bounded gradients/parameter distances, and decays `Î±_t = Î±/âˆšt` and `Î²1,t = Î²1 Î»^{t-1}`. Many practical deepâ€‘learning uses keep `Î²1` constant and optimize highly nonâ€‘convex objectives; the proof does not apply there.
- Sensitivity to `Î²2` without correction (Section 6.4)
  - Large `Î²2` (slowly decaying second moment), which is desirable for sparsity, needs biasâ€‘correction; otherwise training may diverge early (Figure 4).
- Secondâ€‘moment usefulness can diminish (Section 6.3)
  - In the presented CNN experiment, `vÌ‚_t` â€œvanishes to zeros after a few epochsâ€ and `Îµ` dominates, reducing curvature adaptivity; performance then resembles momentumâ€‘based methods.
- Computational/storage profile
  - Adam stores two extra vectors (`m_t`, `v_t`), doubling memory over vanilla SGD. This is relatively small, but nonâ€‘negligible for extremely large models.
- Hyperparameter defaults are strong but not universally optimal
  - The paper provides defaults, but optimal `Î±` can still vary; Section 2.1 gives bounds that help, but tuning may still be required.

## 7. Implications and Future Directions
- Field impact
  - By offering an adaptive, stable, and largely tuningâ€‘free optimizer with theoretical grounding and strong empirical results, Adam provides a robust default for training a broad range of models, especially when gradients are noisy, sparse, or nonâ€‘stationary (Sections 1, 2, 6).
  - The biasâ€‘correction mechanism clarifies how to safely use slow EMA decaysâ€”a design element now standard in many optimizers.

- Followâ€‘up research enabled or suggested by this work
  - Extending theoretical guarantees to nonâ€‘convex settings and to constant `Î²1` schedules common in practice (Section 4 conditions).
  - Understanding when secondâ€‘moment information becomes uninformative (as in Section 6.3) and designing variants that adaptively downâ€‘weight or replace it.
  - Exploring richer preconditioners beyond diagonal `vÌ‚_t` while keeping memory affordable (Section 5 notes links to natural gradient and quasiâ€‘Newton ideas).
  - Investigating temporal parameter averaging (Section 7.2) in combination with Adam across tasks for improved generalization.

- Practical applications
  - Training large neural networks under heavy stochastic regularization (dropout), NLP models with sparse features (IMDB BoW experiment), and general deep architectures where perâ€‘parameter adaptivity and simple tuning are valued.
  - The AdaMax variant (Algorithm 2) offers a numerically stable alternative when `Lâˆ`â€‘based scaling or tighter step bounds (`|Î”_t| â‰¤ Î±`) are preferred.

> Key takeaways grounded in the paper:
> - â€œGood default settings â€¦ Î± = 0.001, Î²1 = 0.9, Î²2 = 0.999 and Îµ = 10^{-8}â€ (Algorithm 1 caption).
> - Effective steps are approximately bounded by `Î±` and invariant to gradient rescaling (Section 2.1).
> - With decaying schedules, â€œAdam achieves â€¦ O(âˆšT) regretâ€ (Theorem 4.1) and average regret `O(1/âˆšT)` (Corollary 4.2).
> - Biasâ€‘correction is crucial at high `Î²2`; without it, training shows â€œinstabilities â€¦ especially at first few epochsâ€ (Section 6.4; Figure 4).

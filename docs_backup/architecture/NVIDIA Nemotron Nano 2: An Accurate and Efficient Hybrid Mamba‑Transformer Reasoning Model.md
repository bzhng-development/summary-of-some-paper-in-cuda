# NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mambaâ€‘Transformer Reasoning Model

**ArXiv:** [2508.14444](https://arxiv.org/abs/2508.14444)
**Authors:** Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adi Renduchintala, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewskaâ€‘DziubiÅ„ska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran Elâ€‘Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen
**Institutions:** NVIDIA

## ğŸ¯ Pitch

Nemotron Nano 2 introduces the `NVIDIA-Nemotron-Nano-9B-v2` model, a 9B-parameter hybrid Mamba-Transformer architecture that achieves state-of-the-art reasoning accuracy with significantly enhanced throughputâ€”up to 6.3Ã— fasterâ€”on commodity GPUs. This advancement transforms economic feasibility for long-form applications such as math tutoring and legal document analysis by combining efficient memory use with a streamlined distillation and compression strategy.

---

## 1. Executive Summary (2-3 sentences)
Nemotron Nano 2 introduces `NVIDIA-Nemotron-Nano-9B-v2`, a 9B-parameter hybrid Mambaâ€“Transformer model distilled from a 12B base that keeps stateâ€‘ofâ€‘theâ€‘art reasoning accuracy while delivering 3Ã—â€“6Ã— higher inference throughput for long â€œthinking traceâ€ generations on a single 22â€¯GiB A10G GPU (Figure 1, Â§4). It achieves this through an architecture with few attention layers, FP8 pretraining on 20T tokens, a 512k longâ€‘context extension, a multiâ€‘stage alignment pipeline (SFT â†’ DPO/GRPO/RLHF), and a pruningâ€‘plusâ€‘distillation compression strategy adapted to memory/throughput constraints (Â§2â€“Â§4).

## 2. Context and Motivation
- Problem addressed
  - Reasoningâ€‘style LLM use (math/code/tool use) often requires generating long chainâ€‘ofâ€‘thought (â€œthinking tracesâ€), which makes standard Transformers slow and memoryâ€‘intensive due to attentionâ€™s `KV cache` growth with sequence length. The goal is to keep or improve accuracy on difficult reasoning tasks while dramatically increasing throughput and fitting 128k context inference on a 22â€¯GiB GPU (Â§1, Â§4.2).
  - `KV cache` (keyâ€“value cache) stores attention keys/values for each generated token so later tokens can attend to earlier ones; its memory grows linearly with sequence length and number of attention heads, which becomes a bottleneck for long contexts and long generations.

- Why it matters
  - Realâ€‘world: Faster longâ€‘form reasoning reduces latency and cost for math tutoring, coding assistants, and longâ€‘document QA, enabling deployment on lowerâ€‘cost hardware (A10G) (Â§1, Â§4.2).
  - Scientific: Tests whether hybrid architectures that replace most attention with `Mambaâ€‘2` (a structured stateâ€‘space model layer with linearâ€‘time sequence processing) can keep accuracy while unlocking throughput and context length (Â§2.1).

- Prior approaches and limitations
  - Pure Transformers maintain strong accuracy but incur large KV caches and quadratic attention costs for long contexts.
  - Prior hybrids (e.g., Jamba; cited in Â§1) demonstrate feasibility but leave open how to: (a) sustain SoTA reasoning accuracy at small/medium scale, (b) deliver 128k context on 22â€¯GiB, and (c) provide rigorous data/recipe releases.

- Positioning
  - Builds on `Nemotronâ€‘H` hybrid design (few attention layers, many Mamba layers) but introduces new data, FP8 pretraining to 20T tokens, a 512k longâ€‘context extension, alignment with budgeted thinking, and a compression pipeline targeted to the A10G memory budget (Â§1, Â§2, Â§3, Â§4).

## 3. Technical Approach
This section walks through the full pipeline: architecture â†’ pretraining â†’ longâ€‘context extension â†’ alignment â†’ compression/distillation â†’ budgeted thinking.

- Hybrid architecture (Figure 2, Table 1, Â§2.1)
  - Layering: 62 total layers with â‰ˆ8% attention (6 attention layers evenly dispersed), 28 Mambaâ€‘2 layers, and 28 FFN layers. The small attention fraction preserves some capabilities that benefit from attention (e.g., exact token interactions), while Mambaâ€‘2 handles most sequence modeling at lower memory/compute.
  - Key dims: model hidden 5120; FFN hidden 20480; groupedâ€‘query attention (GQA) with 40 query heads and 8 keyâ€‘value heads; Mambaâ€‘2 uses 8 groups, state dim 128, head dim 64, conv window 4. No positional embeddings; RMSNorm; squared ReLU activations (Â§2.1).
  - Why Mambaâ€‘2? Mambaâ€‘2 (an SSMâ€‘based layer) processes sequences in linear time without caching past token keys/values, reducing memory pressure and improving throughput for long generations (Â§2.1). Attention is retained sparingly to preserve longâ€‘range tokenâ€‘wise interactions that SSMs may not fully replace.

- Pretraining data and curriculum (Â§2.2â€“Â§2.3)
  - Data scale and diversity: 20T tokens from curated web (Nemotronâ€‘CCâ€‘v2), code, math (Nemotronâ€‘CCâ€‘Math), multilingual, academic, and synthetic SFTâ€‘style data targeting math/code/general reasoning. Three blend phases progressively emphasize higherâ€‘quality data (Figure 3).
  - Two notable synthetic components:
    - `Fundamental Reasoning` SFTâ€‘style data targeting logical/analytical reading comprehension (LSAT, LogiQA, AQuAâ€‘RAT) improves MMLUâ€‘Pro by +12.1 points in an 8B ablation (Table 3, Â§2.3.2).
    - Multilingual `DiverseQA` shows in ablation that translating English DiverseQA to many languages (â€œDiverseQAâ€‘crawlâ€) yields the best Globalâ€‘MMLU scores (Table 2, Â§2.3.1).
  - Training numerics: FP8 (E4M3) for tensors with FP32 master weights; first/last four linears kept in BF16; optimizer state FP32. They keep weights in FP8 to do distributed allâ€‘gathers in FP8 (Â§2.4). LR schedule: `Warmupâ€‘Stableâ€‘Decay`, stable LR 4.5eâ€‘4, min 4.5eâ€‘6; seq length 8192; global batch 768; Adam Î²1=0.9, Î²2=0.95; weight decay 0.1 (Â§2.5).

- Longâ€‘context extension to 128k+ (Â§2.6)
  - After pretraining, continuous pretraining with sequence length 512k (not 128k/256k) using contextâ€‘parallelism (8â€‘way tensor parallel + 16â€‘way context parallel) and a small global batch to keep token count per batch unchanged. Added â‰ˆ18.9B tokens in this phase.
  - Synthetic longâ€‘document QA: chunk academic documents >32k tokens and generate QA pairs, appending to the source document to teach longâ€‘range dependencies. Ablation on an 8B model shows that training at 512k with synthetic longâ€‘doc QA reaches the highest RULERâ€‘128k (81.04) versus 128k/256k setups (Table 4).

- Alignment pipeline (Figure 4, Â§3)
  - Stage 1 SFT: â‰ˆ80B tokens of promptâ€“response with reasoning traces; 10% of prompts have responses with the trace removed to enable â€œreasoningâ€‘offâ€ directâ€‘answer mode. Samples are concatenated up to ~128k to maintain longâ€‘range behaviors (Â§3.2).
  - Stage 2 SFT: focused on tool calling without concatenation (Stage 1 concatenation hurt learning toolâ€‘calling patterns). Data comes from curated toolâ€‘calling corpora and simulated multiâ€‘turn/multiâ€‘step calls with verification (Â§3.1, Â§3.2).
  - Stage 3 SFT: reinforces longâ€‘context + introduces `truncated traces`â€”reasoning is cut after 1â€“2k tokens but the final answer remains. This teaches the model to finish cleanly when a thinking budget is exhausted (Â§3.2, Â§3.4).
  - Preference/RL phases:
    - IFEval RL: reward is how strictly instructions are followed; improves instruction following but may slightly move other metrics, so checkpoint selection matters (Â§3.2).
    - DPO on toolâ€‘calling: uses the `WorkBench` environment to verify multiâ€‘step calls against database state, generating onâ€‘policy positive/negative trajectories (Â§3.2).
    - GRPO (a groupâ€‘relative policy method) and RLHF (chat helpfulness) on HelpSteer3â€‘style data (Â§3.2).
  - Checkpoint interpolation: weightâ€‘space linear merge (Î±â‰ˆ0.5) of a reasoningâ€‘strong and a chatâ€‘strong RL checkpoint recovers a balanced capability set (Â§3.2).

- Compression for 22â€¯GiB and high throughput (Â§4)
  - Constraint: fit 128k context, batchâ‰¥1 in â‰¤19.66â€¯GiB (22.06â€¯GiB minus framework buffer and room for a vision encoder), and maximize throughput on 8k input/16k output in vLLM (Â§4.2).
  - Importance scoring (Â§4.1)
    - `Layer importance`: iteratively remove one candidate layer at a time, compute logits MSE vs original; prune the least impactful layer, repeat (Â§4.1).
    - `FFN/embedding importance`: activationâ€‘based scoring (mean/L2 over outputs) to drop lowâ€‘importance FFN neurons and embedding channels (Â§4.1).
    - `Mamba head importance`: groupâ€‘aware head scoring following Taghibakhshi etâ€¯al. 2025, but at the modest compression ratios used here, head pruning gave limited benefit (Â§4.1, Â§4.4).
  - Lightweight NAS (Â§4.2)
    - Step 1: pick depth. After 6B tokens of distillation, average reasoning accuracy improves markedly from 52â†’54â†’56 layers (44.92 â†’ 47.35 â†’ 51.48; Table 9); fix depth at 56 with 4 attention layers (â‰ˆ7â€“8% attention).
    - Step 2: width search within the memory budget. Evaluate top candidates with short KD (19B tokens) and throughput measurement. The chosen `Candidate 2` uses hidden 4480, FFN 15680, 128 Mamba heads, totals 8.89B params with the best accuracy among topâ€‘3 and competitive throughput (Table 10).
  - Distillation schedule (Â§4.3)
    - Loss: forward KL (teacher logits â†’ student), i.e., match the teacherâ€™s tokenâ€‘level probability distribution.
    - Reasoning model: depthâ€‘only KD (60B) @8k â†’ widthâ€‘pruned KD (50B @8k, 25B @49k, 1B @262k) â†’ DPO â†’ GRPO â†’ KD (0.4B @262k) to recover drops â†’ RLHF â†’ final checkpoint merge (Figure 6).
    - Dataset mix for KD: a 70% reasoningâ€‘SFT + 30% pretraining blend maximizes math accuracy after ~6B KD (Table 11).
    - Base model KD: 120B (depthâ€‘only) + 360B (width) @8k + 2.5B @524k with 100% pretraining data (Â§4.3).

- Budgeted thinking mechanism (Â§3.4)
  - Protocol: the model emits a `<think>` token to start its reasoning trace. The runtime counts â€œthinking tokensâ€; at the budget limit, it tries to inject a closing `</think>` after the current sentence (or forcibly by +500 tokens if no newline appears). Training with truncated traces makes outputs â€œwellâ€‘formedâ€ (exactly one closing tag) and prevents the model from compensating by writing longer final answers (Figure 5a vs 5b).

## 4. Key Insights and Innovations
- Hybrid Mambaâ€‘heavy depth with minimal attention for long generations (Â§2.1; Figure 2, Table 1)
  - Novelty: an explicit design where only ~8% of layers are attention while most are Mambaâ€‘2. This slashes KVâ€‘cache costs yet preserves some attention for tokenâ€‘exact interactions.
  - Significance: enables 128k context and high throughput on 22â€¯GiB hardware while maintaining accuracy on hard reasoning tasks (Figure 1, Â§4.4).

- Longâ€‘context extension at 512k with synthetic longâ€‘doc QA (Â§2.6; Table 4)
  - Novelty: train at 512k even though the target is 128k to reduce doc splitting during pretraining and attach generated QA to real long documents.
  - Significance: improves RULERâ€‘128k substantially (up to 81.04 in ablation), without harming other benchmarks (Â§2.6).

- Alignment for â€œbudgeted thinkingâ€ and toolâ€‘calling reliability (Â§3.2â€“Â§3.4; Figure 5)
  - Novelty: mix concatenated 128k SFT, reasoningâ€‘off samples (empty trace), and deliberately truncated traces; DPO in a verifiable multiâ€‘step toolâ€‘calling environment.
  - Significance: model obeys thinking budgets with wellâ€‘formed outputs and avoids compensating by bloating the final answer; toolâ€‘calling strengthened through verified onâ€‘policy preferences.

- Compression under explicit memory/throughput constraints using Minitronâ€‘style NAS + KD (Â§4)
  - Novelty: importanceâ€‘guided pruning across layers/FFN/embeddings under a 19.66â€¯GiB cap, with staged longâ€‘sequence distillation and final RLHF/merging.
  - Significance: the final 9B student matches or beats similarly sized baselines while delivering up to 6.3Ã— higher throughput for 8k/16k reasoning workloads (Figure 1; Table 10; Â§4.4).

- Data curation showing what matters for multilingual and highâ€‘difficulty reasoning (Â§2.2â€“Â§2.3; Tables 2â€“3)
  - Finding: translated DiverseQA outperforms curated crawl on Globalâ€‘MMLU; specialized â€œFundamental Reasoningâ€ SFT boosts MMLUâ€‘Pro by +12.1 on an 8B ablation.

## 5. Experimental Analysis
- Evaluation setup (Â§2.7, Â§3.3, Â§4.2)
  - Harness: based on lmâ€‘evaluationâ€‘harness with math grading via Mathâ€‘Verify; code via EvalPlus; ARC presented all options; RULER for longâ€‘context (Â§2.7).
  - Throughput: vLLM measuring output tokens/s/GPU at ISL/OSL 8k/16k; single A10G GPU bfloat16; relative throughput reported (Figure 1, Â§4.2).
  - Metrics: task accuracies (or pass@k for coding/math), IFEval strictness, BFCL v3 toolâ€‘calling score, Arenaâ€‘Hard winâ€‘rate style metric (Â§3.3).

- Main quantitative results
  - Throughput vs Qwen3â€‘8B (reasoning workloads; A10G, BF16)
    > â€œup to 6.3Ã— higherâ€ at 8k input / 16k output; â‰ˆ3.3Ã— at 1k/8k (Figure 1).
  - Aligned 12B reasoning model vs Qwen3â€‘8B/14B (Table 8)
    - Math/Science/Code: AIMEâ€‘24 85.42 vs 75.83; AIMEâ€‘25 76.25 vs 69.31; MATHâ€‘500 97.75 vs 96.30; GPQAâ€‘Diamond 64.48 vs 59.61; LiveCodeBench 70.79 vs 59.50.
    - Tool use and instruction: BFCL v3 66.98 vs 66.34; IFEvalâ€‘Strict 89.81 vs 89.39.
    - Longâ€‘context: RULERâ€‘128k 83.36 vs 74.13.
    - Chat: Arenaâ€‘Hard 74 vs 78.4 (Qwen3â€‘8B) and 87.7 (Qwen3â€‘14B)â€”Nano 2 focuses on reasoning/throughput, not topping chat.
    - Mixed: SciCode is lower (18.75 vs 24.65 for Qwen3â€‘8B), showing a domain where it lags.
  - Base model comparisons (Table 5)
    - `12B Base` vs Qwen3â€‘8B Base and Gemma3â€‘12B Base:
      > MMLU 78.24 (vs 76.44, 73.61); MMLUâ€‘Proâ€‘5shot 63.98 (vs 56.27, 45.12); GSM8K CoT 91.66 (vs 84.00, 74.45); MATH 83.54 (vs 55.40, 42.40); AIMEâ€‘24 pass@32 56.67 (vs 20.00, 16.67); HumanEval+ avg@32 61.03 (vs 57.55, 36.68).  
      Commonsense is comparable or better; RULERâ€‘128k 84.74 (Gemma3â€‘12B 80.70).
    - `9B Base` (the pruned student before alignment) retains strong scores: MMLUâ€‘Pro 59.43, GSM8K 91.36, MATH 80.50, RULERâ€‘128k 82.22.
  - Multilingual (Table 6)
    - `12B Base` averages: Globalâ€‘MMLUâ€‘Lite 75.13 (vs Qwen3â€‘8B 72.81), MGSM 85.94 (vs 80.93).
    - `9B Base` remains competitive (Globalâ€‘MMLUâ€‘Lite 69.94; MGSM 84.67).

- Ablations and pipeline diagnostics
  - Multilingual data choice: Within continuous pretraining of a 1B model, DiverseQAâ€‘crawl leads (avg 47.0) vs curated crawl (37.0) and FineWebâ€‘2 (35.1) on Globalâ€‘MMLU (Table 2).
  - Fundamental Reasoning SFT: On an 8B model, MMLUâ€‘Pro jumps 44.24 â†’ 56.36 and average math rises by â‰ˆ+1.8 (Table 3).
  - Longâ€‘context training length/synthetic data: RULERâ€‘128k improves from 70.19 (256k, no synthetic) to 81.04 (512k, synthetic) (Table 4).
  - Depth effect under KD: 56 layers best (Table 9).
  - Architecture candidates postâ€‘pruning: Candidate 2 achieves the best accuracy (63.02) with 8.89B params and strong throughput (156 toks/s) under the 8k/16k, batchâ€‘8 test (Table 10).
  - Distillation/policy stages: DPO and GRPO boost toolâ€‘calling (BFCL v3) and instruction following (IFEval), but temporarily depress MMLUâ€‘Pro; a short KD recovers it. RLHF improves Arenaâ€‘Hard, then modelâ€‘merge balances tradeâ€‘offs (Figure 6).
  - Budget control: Truncation training eliminates â€œcompensate by longer final answerâ€ behavior and yields singleâ€‘closure wellâ€‘formedness across budgets (Figure 5b vs 5a).

- Do the experiments support the claims?
  - Yes for the central claims: (1) comparable or better accuracy vs similarly sized baselines across reasoning/math/code/multilingual (Tables 5â€“6, 8); (2) substantial throughput gains for longâ€‘generation scenarios on A10G (Figure 1); (3) 128k context with strong RULER scores (Tables 5, 8).
  - Tradeâ€‘offs are candidly shown: SciCode lag; chat (Arenaâ€‘Hard) behind Qwen3â€‘8B/14B; temporary metric drops during RL phases and recovery via KD/merging (Figure 6).

## 6. Limitations and Trade-offs
- Scope and assumptions
  - Optimization target is specifically a 22â€¯GiB A10G GPU with vLLM; memory budget and throughput measurements are calibrated to this setting (Â§4.2). Benefits transfer to other GPUs but exact ratios may differ.
  - The hybrid design fixes â‰ˆ7â€“8% attention layers; other ratios might work better for different tasks but are not exhaustively explored (Â§4.2.2).

- Data dependencies and potential biases
  - Heavy use of synthetic data (multilingual DiverseQA, STEM Q/A, reasoning SFT) generated by external large models (Qwen, DeepSeek) (Â§2.2). Quality and bias of these upstream models influence the final model.
  - Most postâ€‘training data are singleâ€‘turn promptâ€‘response with reasoning traces; true multiâ€‘turn, multiâ€‘step interaction breadth may be narrower than in real deployments (Â§1).

- Performance tradeâ€‘offs
  - While reasoning and toolâ€‘calling are strong, some codingâ€‘research tasks (SciCode) and openâ€‘ended chat (Arenaâ€‘Hard) trail stateâ€‘ofâ€‘theâ€‘art chatâ€‘optimized models (Table 8).
  - DPO/GRPO/RLHF phases can cause temporary regressions on knowledge/understanding metrics; extra KD or model merging is required to rebalance (Figure 6).

- Compression boundaries
  - Mamba head pruning brought limited benefits at the modest compression ratios here; larger reductions might require more sophisticated SSMâ€‘aware pruning or retraining (Â§4.2.2).
  - Final model is 9B parameters in BF16; further memory reductions (e.g., INT8/FP8 inference quantization) are not studied.

- Budgeted thinking mechanism
  - Relies on special `<think>` tags and runtime heuristics for closing the trace; behavior with different decoders or thirdâ€‘party stacks is untested (Â§3.4).

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that Mambaâ€‘heavy hybrids can deliver topâ€‘tier reasoning accuracy with much higher throughput for long reasoning traces, even at smallâ€‘toâ€‘mid scales and within tight memory budgets. This shifts deployment economics for longâ€‘context, longâ€‘generation applications (Â§1, Figure 1, Tables 5â€“6, 8).

- Practical applications
  - Costâ€‘efficient math/code tutors and graders; verifiable multiâ€‘step tool workflows; legal/technical document analysis over 100k+ tokens; batch reasoning where outputâ€‘token throughput dominates cost.
  - Onâ€‘prem or edge deployments constrained to 22â€¯GiBâ€‘class GPUs (A10G) with 128k input contexts.

- Research directions
  - Architecture: explore dynamic attentionâ€“Mamba allocations (per layer or per token), and principled ways to place attention layers within Mamba stacks (Â§2.1).
  - Compression: stronger groupâ€‘aware SSM pruning at higher compression ratios; joint KD + RL objectives that avoid postâ€‘RL regressions; quantization for inference.
  - Alignment: richer multiâ€‘turn/multiâ€‘tool curricula; improved reward modeling beyond IFEval strictness; more systematic budgeted thinking controls (e.g., tokenâ€‘adaptive stopping).
  - Data: expand highâ€‘fidelity math/code sources; controllable reasoningâ€‘trace styles; multilingual longâ€‘document QA beyond Wikipedia; better contamination checks at scale (Â§2.2â€“Â§2.3).
  - Evaluation: broader robustness checks (adversarial longâ€‘context, toolâ€‘calling under noisy APIs), and deeper failure analysis on tasks like SciCode.

Overall, Nemotron Nano 2 provides a complete, reproducible recipeâ€”from FP8 pretraining and 512k longâ€‘context extension to alignment and Minitronâ€‘style compressionâ€”that yields an open, 9Bâ€‘parameter reasoning model with strong accuracy and dramatically higher throughput for longâ€‘generation workloads on commodity datacenter GPUs (Figures 1, 5â€“6; Tables 4â€“6, 8, 10; Â§2â€“Â§4).

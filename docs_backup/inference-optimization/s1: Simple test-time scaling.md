# s1: Simple test-time scaling

**ArXiv:** [2501.19393](https://arxiv.org/abs/2501.19393)
**Authors:** Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Feiâ€‘Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, Tatsunori Hashimoto
**Institutions:** Stanford University, University of Washington, Allen Institute for AI, Contextual AI

## ğŸ¯ Pitch

This paper revolutionizes inference-time reasoning in language models by introducing 'budget forcing,' a method that enables scalable, controllable accuracy using a small curated dataset. This advancement not only empowers users to optimize accuracy versus computation cost but also provides a transparent framework that demystifies test-time scaling, setting a new benchmark for reproducibility and efficiency in language model research.

---

## 1. Executive Summary
This paper introduces a very simple recipe for making a language model reason better by spending more computation at test time (â€œtest-time scalingâ€). The recipe has two parts: (1) a tiny, carefully selected dataset of 1,000 problems with reasoning traces (`s1K`), and (2) a decoding-time control method called `budget forcing` that reliably lengthens or shortens how long the model â€œthinks.â€ Finetuning Qwen2.5â€‘32B on `s1K` and applying budget forcing yields `s1â€‘32B`, which shows clear, controllable accuracy gains as more thinking tokens are allowed (Figure 1; Â§3â€“4), and reaches strong sampleâ€‘efficiency and competitiveness with closed models (Table 1; Figure 2).

## 2. Context and Motivation
- Problem gap
  - â€œTest-time scalingâ€ means using more compute at inference to get better answers. While OpenAIâ€™s o1 family demonstrated this effect, details were not public, prompting numerous replication attempts that often relied on reinforcement learning (RL) and large private datasets (Â§1; related work Â§6).
  - A missing piece has been a minimal, openly described method that both: (a) reliably exhibits test-time scaling curves and (b) reaches competitive reasoning performance without vast training data.

- Why it matters
  - Practically: gives users a knob to trade latency for accuracy. For example, the model can spend more tokens when a question is hard and fewer when it is easy (Figure 1).
  - Scientifically: provides clean, controllable baselines and metrics to study inference-time scaling (Â§3.2), decoupled from opaque RL pipelines.

- Prior approaches and limits
  - RL-based systems (e.g., DeepSeekâ€‘R1) achieve high reasoning scores but use â€œmillions of samples and multiple training stagesâ€ (Â§1, Â§6.1).
  - Multi-agent and tree-search approaches (e.g., MCTS, REBASE) can be powerful but add complexity and extra models (reward models) (Â§6.2).
  - Many replication attempts did not openly reproduce a clear, monotonic scaling curve with test-time compute (Â§1).

- Positioning of this work
  - The paper pursues the simplest feasible route: supervised finetuning (SFT) on just 1,000 high-quality reasoning traces, plus an inference-time control that needs no extra training or models. It then defines metrics to evaluate test-time scaling methods (Â§3.2), and shows the methodâ€™s controllability and gains across math and science benchmarks (Â§4â€“5).

## 3. Technical Approach
The pipeline has three pillars: data curation (to build `s1K`), a lightweight SFT, and an inference-time controller (`budget forcing`).

1) Reasoning data curation â†’ `s1K` (Â§2)
- Start with 59,029 questions across 16 sources focused on quality, difficulty, and diversity (Table 7; Â§2.1). Examples include NuminaMATH, MATH, OmniMath, OlympicArena, AGIEval, and two new sets: Stanford PhD probability (`s1-prob`) and hard trading brainâ€‘teasers (`s1-teasers`).
- For each question, obtain a reasoning trace and final answer by calling Gemini 2.0 Flash Thinking and extracting its hidden chain of thought and solution (Â§2.1).
- Clean-up and filtering:
  - Remove API/formatting issues (down to 51,581), deduplicate, and decontaminate against evaluation sets using 8â€‘gram overlaps (Â§2.1, Â§C.5).
  - Difficulty filter: discard questions that Qwen2.5â€‘7B or Qwen2.5â€‘32B can already solve (to keep challenging items) and use generated trace length as a proxy for difficulty (Â§2.2).
  - Diversity filter: label questions with Mathematics Subject Classificationâ€‘style domains using Claude 3.5, then sample problems across 50 domains with a lengthâ€‘weighted sampler to favor longer traces (Â§2.2; Algorithm 1 in Â§C.4).
- Outcome: `s1K` has 1,000 diverse, hard questions with reasoning traces (Figure 2 left; Table 6). Notably, traces need not be always correct; the grader reports 53.6% correctness for `s1K` and 63.0% for a later `s1Kâ€‘1.1` update (Â§2.2; Â§A).

2) Supervised finetuning (`s1â€‘32B`) (Â§4.1; Â§D)
- Base model: `Qwen2.5â€‘32Bâ€‘Instruct`.
- Training data: `s1K` (1,000 triples: question, reasoning trace, answer).
- Formatting: the training target is the reasoning trace then the answer, separated by special delimiters `<|im_start|>think` and `<|im_start|>answer` (Â§D).
- Hyperparameters: 5 epochs, batch size 16, bfloat16, AdamW, lr 1eâ€‘5 with 5% warmâ€‘up and cosine decay; 26 minutes on 16 H100 GPUs (Â§4.1; Figure 9; Â§D).
- Sequence length ablation (Table 8; Â§D.1): using a long training context (32k) reduces test-time â€œthinkingâ€ length and improves accuracy versus a short context (4096).
  - Example on AIME24: 50.0% accuracy with 6984 thinking tokens vs 30.0% with 20721 tokens for the shortâ€‘context model (Table 8).

3) Test-time compute control â†’ `budget forcing` (Â§3.1)
- Goal: deterministically set a maximum and/or minimum â€œthinkingâ€ budget without changing the model weights.
- Thinking/answer phases: because training teaches the model to â€œthink first, then answerâ€ using explicit delimiters, decoding can intercept the transition.
- Two control levers:
  - Enforce a maximum: when the running count of thinking tokens reaches a cap, force the transition to the answer by appending the endâ€‘ofâ€‘thinking delimiter (and optionally â€œFinal Answer:â€) (Â§3.1).
  - Enforce a minimum/extend thinking: when the model tries to stop thinking, suppress the delimiter and â€œnudgeâ€ the chain to continue by appending a short string such as â€œWaitâ€ (Â§3.1). This often triggers selfâ€‘checking and fixes earlier steps (Figure 3).
- Why this over alternatives? The paper compares budget forcing to (i) token/step/classâ€‘conditional prompting and (ii) rejection sampling, and finds budget forcing provides perfect control and the best accuracyâ€‘vsâ€‘compute scaling (Â§5.2; Table 3; Figure 6).

4) How scaling is measured (Â§3.2)
- A method is evaluated at several compute points (different thinking budgets), producing a piecewiseâ€‘linear curve of accuracy vs tokens (see Figure 1 and Figure 4).
- Three metrics:
  - `Control` (Eq. 1): fraction of runs that meet the prescribed compute budget (100% is perfect).
  - `Scaling` (Eq. 2): average slope across all budget pairs; positive means accuracy rises as compute increases.
  - `Performance` (Eq. 3): best accuracy achieved over the tested budgets.

5) Parallel scaling for comparison (Â§4.2; Figure 4 right; Figure 7)
- Majority voting: run the base model many times and pick the most frequent answer.
- REBASE: a treeâ€‘search guided by a separate process reward model; used here as a strong parallelâ€‘scaling reference (Figure 7).

## 4. Key Insights and Innovations
1) Budget forcing is a minimal, effective, and controllable test-time scaler (Â§3, Â§5.2)
- Whatâ€™s new: it exploits the trained â€œthinkâ†’answerâ€ delimiter to intervene at decode time. No extra models or RL.
- Why it matters:
  - Perfect compute control (100% `Control`, Table 3) and positive scaling (slope 15) with the best AIME24 `Performance` among the tested methods (56.7%).
  - Works both to cap compute and to extend it in small increments; the tiny â€œWaitâ€ token often induces useful reflection (Figure 3; Table 4).

2) Only 1,000 carefully chosen examples can unlock reasoning and scaling (Â§2, Â§4)
- Whatâ€™s new: instead of massive distillation/RL corpora, the paper shows that curating for `Quality + Difficulty + Diversity` is sufficient.
- Why it matters: `s1â€‘32B` substantially outperforms the base model with just 1k samples (Table 1), and sits on the sampleâ€‘efficiency frontier (Figure 2 right).

3) A clear evaluation framework for test-time scaling (Â§3.2, Â§5.2)
- Whatâ€™s new: explicit definitions of `Control`, `Scaling`, and `Performance`, applied to multiple test-time methods (Table 3).
- Why it matters: distinguishes methods that increase tokens but not accuracy (e.g., rejection sampling shows negative slope, Figure 6) from methods that truly scale.

4) Sequential scaling can beat naive parallel scaling at comparable budgets (Â§4.2; Figure 4)
- Insight: After SFT on `s1K`, sequentially extending a single, coherent reasoning trace (with budget forcing) yields better curves than many independent samples plus majority vote from the base model (Figure 4 right). It supports the hypothesis that â€œlater computations can build on intermediate resultsâ€ (Â§3.1).

Incremental but useful: small prompting tweaks for extrapolation
- Observation: different extrapolation strings matterâ€”â€œWaitâ€ is best among tried variants (Table 4).

## 5. Experimental Analysis
- Evaluation setup (Â§4.1):
  - Benchmarks:
    - `AIME24` (30 competition math problems; integer answers) with figure inputs provided via Asymptote (Â§4.1).
    - `MATH500` (500 competition math problems; OpenAIâ€™s subset) (Â§4.1).
    - `GPQA Diamond` (198 PhD-level science Qs; experts 69.7% per OpenAI) (Â§4.1).
  - Metric: accuracy (pass@1); default decoding temperature 0 unless noted (Â§4.1).
  - Infrastructure: lmâ€‘evaluationâ€‘harness; vLLM; notes on determinism issues and mitigations in Appendix B.

- Main quantitative results (Table 1; Figure 1; Figure 4)
  - `s1â€‘32B` vs base `Qwen2.5â€‘32Bâ€‘Instruct`:
    - AIME24: 56.7% vs 26.7% (+30.0 points).
    - MATH500: 93.0% vs 84.0% (+9.0 points).
    - GPQA Diamond: 59.6% vs 49.0% (+10.6 points).
  - Against `o1â€‘preview`:
    - AIME24: 56.7% vs 44.6%.
    - MATH500: 93.0% vs 85.5%.
    - GPQA: 59.6% vs 73.3% (here `o1â€‘preview` is stronger).
  - Testâ€‘time scaling curve: On AIME24, extending thinking multiple times by suppressing the stop and appending â€œWaitâ€ increases accuracy from ~50% (no extrapolation) to ~57% at higher budgets (Figure 1 middle; Figure 4 left). The curve â€œeventually flattens out,â€ and too many suppressions can cause loops (Â§4.2).
  - Parallel scaling comparisons:
    - Majority voting on the base model (up to 64 generations) fails to catch `s1â€‘32B` sequential scaling (Figure 4 right).
    - Adding REBASE on top of `s1â€‘32B` can scale further at very large budgets but requires an extra reward model pass per step (Figure 7).

- Test-time method ablations (Table 3; Â§5.2)
  - Budget forcing: `Control 100%`, `Scaling 15`, `Performance 56.7`.
  - Tokenâ€‘conditional control: poor control (40%), negative slope (âˆ’24); adding budget forcing improves control to 100% but not performance (40.0%).
  - Stepâ€‘conditional control: medium control (60%); still weak scaling and performance (â‰¤36.7%).
  - Classâ€‘conditional (â€œshortâ€ vs â€œlongâ€ prompts): some scaling (slope 25) but low control (50%) and low performance (36.7%).
  - Rejection sampling: perfect control (by construction) but inverse scaling (âˆ’35), i.e., longer traces sampled this way tended to be worse (Figure 6 and Â§E.2 case study).

- Data ablations (Table 2; Â§5.1)
  - Random 1k (â€œOnly Qualityâ€): much worse than `s1K` on AIME24 (36.7% vs 50.0%).
  - â€œOnly Diversityâ€ (uniform over domains): 26.7% on AIME24.
  - â€œOnly Difficultyâ€ (longest 1k traces): strong on GPQA (59.6%) but still below `s1K` overall.
  - Full 59k training: 53.3% AIME24, 92.8% MATH500, 58.1% GPQAâ€”close to `s1K` but requires ~394 H100 GPU hours vs ~7 for `s1K` (Â§5.1).

- Training ablation: sequence length (Table 8; Â§D.1)
  - Longer training context yields better accuracy and shorter thinking at inference.
  - The paper explains the mechanism: with longer sequences, the model more often sees complete examples where the answer follows the chain, which raises the likelihood of transitioning to the answer earlier (Â§D.1).

- Illustrative generations and selfâ€‘correction
  - Figure 3 shows a simple example where appending â€œWaitâ€ after an early stop pushes the model to â€œreâ€‘readâ€ and fix a counting mistake.
  - Figure 5 shows correct endâ€‘toâ€‘end outputs on one item each from AIME24, MATH500, and GPQA.

- Update s1.1 (Appendix A, Table 5)
  - Reâ€‘distilling the same 1,000 prompts with DeepSeekâ€‘R1 traces improves performance (e.g., `s1.1 w/o BF` AIME24 56.7%; AIME2025 50.0%). Table 5 gives a fuller matrix, including OpenAI o3â€‘mini baselines.

- Do the experiments support the claims?
  - Yes for controllable test-time scaling: the `Control/Scaling/Performance` metrics (Table 3) and the curves (Figure 1, Figure 4) consistently show that budget forcing both enforces budgets and increases accuracy with more tokens, within limits.
  - Yes for sample efficiency: Figure 2 (right) and Table 1 place `s1â€‘32B` near the best open models trained with vastly more reasoning data; ablating the selection criteria (Table 2) shows the 1k set is carefully chosen rather than arbitrary.

- Notable caveats disclosed
  - vLLM determinism issues can cause runâ€‘toâ€‘run differences; using full precision mitigates this (Appendix B).
  - Gemini API â€œrecitation errorâ€ complicated their own evaluation of Gemini; they manually evaluated AIME24 in the web UI, and left other cells N/A (Table 1; Â§4.1).

> â€œFinetuning took 26 minutes on 16 NVIDIA H100 GPUsâ€ (Â§4.1; Â§D).

> â€œSuppressing the endâ€‘ofâ€‘thinking token delimiter too often can lead the model into repetitive loopsâ€ (Â§4.2; Figure 4 left).

> Budget forcing ablation: `Control 100%`, `Scaling 15`, `Performance 56.7` (Table 3).

## 6. Limitations and Trade-offs
- Dependence on distillation quality
  - Reasoning traces come from proprietary models (Gemini; later R1 in `s1.1`). Some traces are incorrect (only ~54â€“63% correct, Â§2.2, Â§A). While the method still works, noise in traces may bound ceiling performance.
- Scaling limits and context windows
  - Sequential test-time scaling â€œeventually flattens out,â€ and excessive continuation induces loops (Figure 4 left). Long chains can exceed the modelâ€™s context window, hurting performance (Figure 7, where 12/30 AIME questions overflow at 512 steps).
- Benchmark scope
  - Focuses on math and science QA (AIME, MATH500, GPQA Diamond). Other domains (e.g., code generation, openâ€‘ended multiâ€‘turn tasks, multimodal reasoning) are not evaluated.
- Evaluation nonâ€‘determinism
  - vLLM batching, continuation, and tensor parallelism can change results (Appendix B). The paper addresses this but it remains a practical consideration.
- Compute at inference
  - Gains come from spending more thinking tokens. In production, this is a latency/cost tradeâ€‘off. The method helps control it, but does not remove it.
- Comparison to strongest closed systems
  - While `s1â€‘32B` beats `o1â€‘preview` on AIME24 and MATH500, it trails `o1` and `o3â€‘mini` on GPQA/MATH in Table 5. The focus here is simplicity and openness rather than stateâ€‘ofâ€‘theâ€‘art peak scores.

## 7. Implications and Future Directions
- How this changes the landscape
  - Establishes that a small, wellâ€‘curated SFT dataset plus a simple decoder-time controller is enough to produce clear, monotonic testâ€‘time scaling curvesâ€”no RL or giant corpora required. This lowers the barrier for researchers to study inference-time reasoning.
  - Provides practical control metrics (`Control/Scaling/Performance`) that other scaling methods can be judged against (Â§3.2; Table 3).

- Followâ€‘up research enabled
  - Better extrapolation without loops:
    - Explore dynamic strings instead of a fixed â€œWait,â€ add frequency penalties or temperature schedules to avoid repetition (Â§6.2, â€œLimits to further testâ€‘time scalingâ€).
    - Learn the extrapolation policy (small controller or rewardâ€‘free heuristics) while keeping the main model fixed.
  - Combine sequential and parallel scaling:
    - Use REBASE/tree-search selectively when approaching context limits (Figure 7).
    - Study hybrid schedules that interleave singleâ€‘trace revision (sequential) with branching exploration (parallel).
  - Data curation science:
    - Make the selection algorithm adaptive to target domains; test if the `Quality + Difficulty + Diversity` triad transfers to other modalities (visionâ€‘language), coding, or dialog planning.
  - RL interplay:
    - Apply budget forcing to RLâ€‘trained â€œreasoningâ€ models; assess whether RL improves extrapolation (suggested in Â§6.2). Use the paperâ€™s metrics to compare.

- Practical applications
  - Systems with userâ€‘tunable accuracy/latency: assistants that â€œthink moreâ€ when asked to doubleâ€‘check; onâ€‘device models that default to short thinking and escalate only for difficult items.
  - Education and scientific assistance: math/physics tutoring or problemâ€‘solving tools that can allocate more budget on tricky steps, and terminate early on easier ones.
  - Evaluation infrastructure: the `Control/Scaling/Performance` triad can become part of standard reports for any new inferenceâ€‘time algorithm.

In short, the paperâ€™s core achievement is making testâ€‘time scaling both simple and rigorous: a small, public dataset (`s1K`) plus a clean decoding trick (`budget forcing`) yields a controllable accuracyâ€‘vsâ€‘tokens curve and competitive results, while providing the community with reproducible metrics and ablations that clarify what works and why.

# ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION

**ArXiv:** [1412.6980](https://arxiv.org/abs/1412.6980)

## ğŸ¯ Pitch

Adam introduces a groundbreaking optimization method that adaptively computes individual learning rates for each parameter using estimates of first and second moments of the gradients, combined with bias correction. This approach makes training large, complex models more efficient, robust, and less sensitive to hyperparameter tuningâ€”particularly in settings with noisy, sparse, or non-stationary gradientsâ€”making Adam pivotal for advancing deep learning and large-scale machine learning.

---

## 1. Executive Summary
Adam is an optimization algorithm for training models with noisy or minibatch gradients. It adaptively tunes a separate learning rate for every parameter by tracking two running statistics of the gradient (its mean and uncentered variance) and corrects their early-iteration bias. This makes optimization fast, memoryâ€‘light, robust to nonâ€‘stationarity and sparse gradients, and theoretically sound with an online convex optimization regret bound of order O(âˆšT) (Section 4; Theorem 4.1).

## 2. Context and Motivation
- Problem addressed
  - Training modern models requires optimizing stochastic objectives where only noisy gradients are available (Section 1). Conventional firstâ€‘order methods like SGD can be sensitive to the learning rate, struggle with sparse gradients, and require manual schedule tuning.
- Why it matters
  - Practical: Faster, stabler training on large datasets and large parameter spaces with minimal tuning is crucial for deep learning and other highâ€‘dimensional problems.
  - Theoretical: A method that adapts perâ€‘parameter step sizes while keeping convergence guarantees helps bridge practice and theory.
- Prior approaches and their gaps
  - SGD with momentum: effective but needs careful global learning rate schedules and often different rates per layer or feature frequency.
  - AdaGrad (Duchi et al., 2011): accumulates squared gradients to adapt perâ€‘parameter rates and excels with sparse features, but its learning rate can decay too aggressively (Section 5).
  - RMSProp (Tieleman & Hinton, 2012): maintains an exponential moving average of squared gradients to cope with nonâ€‘stationarity, but (i) lacks the bias correction needed when decay is slow and (ii) applies momentum to the rescaled gradient instead of using an explicit estimate of the first moment (Section 5).
- Positioning
  - Adam combines the strengths of AdaGrad and RMSProp: it uses exponentially decayed moving averages (good for nonâ€‘stationarity) and explicit biasâ€‘corrected estimates of both the first and second moments (good for stability and sparse gradients). It also provides convergence analysis and an infinityâ€‘norm variant (AdaMax) with a simpler bound (Section 7.1).

## 3. Technical Approach
Adam estimates two statistics of the gradient at each timestep t and uses them to scale parameter updates.

Core idea in plain language
- Keep two running â€œmemoriesâ€:
  - m_t: an exponentially decayed average of recent gradients (an estimated mean).
  - v_t: an exponentially decayed average of recent squared gradients (an estimated uncentered variance).
- Because both are initialized at zero, early values are biased low. Adam fixes this by dividing each by a known factor so they become unbiased.
- Update each parameter by stepping in the direction of the estimated mean, scaled by the square root of the estimated variance. Intuition: trust directions that are consistently pointing the same way (large mean) and discount directions that fluctuate a lot (large variance).

Stepâ€‘byâ€‘step (Algorithm 1; defaults Î±=0.001, Î²1=0.9, Î²2=0.999, Îµ=1eâˆ’8)
1. Compute stochastic gradient g_t = âˆ‡_Î¸ f_t(Î¸_{tâˆ’1}).
2. Update firstâ€‘moment (mean) estimate:
   - m_t = Î²1 Â· m_{tâˆ’1} + (1âˆ’Î²1) Â· g_t.
3. Update second raw moment (uncentered variance) estimate:
   - v_t = Î²2 Â· v_{tâˆ’1} + (1âˆ’Î²2) Â· g_t^2 (elementâ€‘wise square).
4. Bias correction (Section 3):
   - mÌ‚_t = m_t / (1âˆ’Î²1^t),
   - vÌ‚_t = v_t / (1âˆ’Î²2^t).
   - Why needed: with zero initialization, early averages are biased toward zero; Section 3 derives the correction by taking expectations of the exponential average (Equations (1)â€“(4)).
5. Parameter update (elementâ€‘wise):
   - Î¸_t = Î¸_{tâˆ’1} âˆ’ Î± Â· mÌ‚_t / (sqrt(vÌ‚_t) + Îµ).

Design choices and their rationale
- Exponential moving averages: give more weight to recent gradients, handling nonâ€‘stationarity better than AdaGradâ€™s cumulative sum (Section 2).
- Bias correction: crucial when Î²2 is close to 1 (slow decay), as otherwise v_t is severely underestimated and steps can explode (Section 3 and Section 6.4; Figure 4).
- Perâ€‘parameter scaling: divides by sqrt(vÌ‚_t), shrinking updates for parameters with high gradient variance and expanding them for stable parameters (Sections 2â€“2.1).
- Îµ: prevents division by zero and bounds the denominator when vÌ‚_t becomes tiny; in practice set to 1eâˆ’8 (Algorithm 1).

How the update behaves (Section 2.1)
- Effective step per coordinate: Î”_t = Î± Â· mÌ‚_t / sqrt(vÌ‚_t) (assuming Îµâ‰ˆ0).
- Bounded step sizes: Section 2.1 shows |Î”_t| is approximately bounded by Î± (exact upper bound depends on Î²1, Î²2), which acts like a trust region: the algorithm rarely makes steps much larger than the base learning rate.
- Scaleâ€‘invariance: scaling all gradients by a constant c scales mÌ‚_t by c and vÌ‚_t by c^2, which cancels in Î”_t; thus updates are invariant to gradient rescaling (Section 2.1).
- Automatic annealing: the â€œsignalâ€‘toâ€‘noise ratioâ€ SNR â‰¡ mÌ‚_t / sqrt(vÌ‚_t) shrinks near optima (mean decreases faster than variance), automatically reducing step sizes without an explicit schedule (Section 2.1).

Computational and memory cost
- Memory: stores m_t and v_t for each parameter (two extra arrays the size of Î¸; Section 1).
- Compute: constant overhead per parameter update relative to SGD (Algorithm 1).

Theoretical framework (Section 4)
- Setting: online convex optimization. At each step t, a convex loss f_t is revealed after choosing Î¸_t.
- Goal: small cumulative regret R(T) = Î£_t [f_t(Î¸_t) âˆ’ f_t(Î¸*)], where Î¸* is the best fixed comparator in hindsight.
- Result: With decaying learning rate Î±_t = Î± / âˆšt and decaying momentum Î²1,t = Î²1 Î»^{tâˆ’1}, Adam achieves R(T) = O(âˆšT) (Theorem 4.1; Corollary 4.2).
- Intuition of the proof: combine the convexity inequality (Lemma 10.2) with bounds on sums involving the biasâ€‘corrected moments (Lemmas 10.3 and 10.4).

Connections and variants
- Relation to AdaGrad and RMSProp (Section 5): Adam reduces to AdaGrad when Î²1=0 and Î²2â†’1 with bias correction and Î±_t=Î±/âˆšt; it differs from RMSProp by using explicit biasâ€‘corrected first/second moments rather than momentum on rescaled gradients and by avoiding the instability seen when Î²2 is close to 1 without correction.
- AdaMax (Algorithm 2; Section 7.1): a pâ€‘norm generalization where pâ†’âˆ leads to u_t = max(Î²2Â·u_{tâˆ’1}, |g_t|). Update becomes Î¸_t = Î¸_{tâˆ’1} âˆ’ (Î±/(1âˆ’Î²1^t)) Â· m_t / u_t. No bias correction for u_t is required, and the update magnitude is bounded by Î±.

## 4. Key Insights and Innovations
- Biasâ€‘corrected moment estimates
  - What: Divide m_t and v_t by (1âˆ’Î²1^t) and (1âˆ’Î²2^t), respectively (Algorithm 1; Section 3).
  - Why itâ€™s novel/important: Corrects the initialization bias intrinsic to exponential moving averages started at zero. Section 3 derives the correction (Equations (1)â€“(4)). Empirically critical when Î²2 is close to 1 (sparse/noisy settings); Figure 4 shows training instability without correction and stable convergence with it.
- Perâ€‘coordinate, varianceâ€‘normalized updates with bounded magnitude
  - What: Use mÌ‚_t / sqrt(vÌ‚_t) to scale each coordinate; Section 2.1 proves step size bounds roughly by Î± and invariance to gradient rescaling.
  - Why it matters: Reduces the burden of tuning Î± and improves robustness across layers and parameter scales; supports consistent progress even with nonâ€‘stationary or heteroskedastic gradients.
- Theoretical regret bound that leverages adaptivity under sparsity
  - What: Theorem 4.1 shows O(âˆšT) regret with constants that depend on coordinateâ€‘wise accumulators. Under sparse gradients, the sums Î£_i ||g_{1:T,i}||_2 and Î£_i âˆšT vÌ‚_{T,i}^0.5 can be much smaller than dÂ·GâˆâˆšT, yielding tighter guarantees similar to AdaGradâ€™s improvements (Section 4, paragraph after Theorem 4.1).
  - Why it matters: Provides formal backing for observed gains on sparse features (e.g., IMDB BoW; Figure 1 right).
- AdaMax: an infinityâ€‘norm variant with simple state and bound
  - What: Replace sqrt(vÌ‚_t) with u_t = max(Î²2 u_{tâˆ’1}, |g_t|); no bias correction needed for u_t and |Î”_t| â‰¤ Î± (Section 7.1; Algorithm 2).
  - Why it matters: Numerically stable alternative with minimal bookkeeping and a clear update bound.

## 5. Experimental Analysis
Evaluation setup (Section 6)
- Datasets and models
  - Logistic regression on MNIST images (Figure 1 left) and on IMDB reviews represented as 10,000â€‘dimensional sparse bagâ€‘ofâ€‘words (Figure 1 right).
  - Multilayer perceptron (MLP): two hidden layers, 1000 ReLU units each, minibatch 128; with and without dropout; comparison to the Sumâ€‘ofâ€‘Functions Optimizer (SFO) for deterministic cost (Figure 2).
  - Convolutional neural networks (CNNs) on CIFARâ€‘10 with architecture c64â€‘c64â€‘c128â€‘1000: three 5Ã—5 conv layers, 3Ã—3 maxâ€‘pool (stride 2), fully connected layer with 1000 ReLUs; input whitening; dropout on input and fully connected layers; minibatch 128 (Figure 3).
- Baselines and hyperparameters
  - AdaGrad, RMSProp, SGD with Nesterov momentum, AdaDelta, and SFO (Figures 1â€“3). Learning rates and momenta are tuned on a grid; Adam uses defaults unless otherwise stated (Section 6).
  - For theoretical comparisons in logistic regression, Î±_t = Î±/âˆšt to match the analysis (Section 6.1).

Main results
- Logistic regression, MNIST (Figure 1 left)
  - Observation: Adam converges at least as fast as SGD with Nesterov momentum and faster than AdaGrad in training negative logâ€‘likelihood over 45 passes.
  - Takeaway: For dense features, Adam retains the speed of wellâ€‘tuned momentum methods without perâ€‘problem scheduling.
- Logistic regression, IMDB BoW with dropout (Figure 1 right)
  - Observation: Adagrad and Adam significantly outperform SGD with Nesterov momentum. The plotted training cost shows a marked gap where AdaGrad and Adam rapidly drop to â‰ˆ0.25â€“0.3 while SGD remains higher across 160 passes.
  - Takeaway: In sparse feature regimes, adaptivity by perâ€‘coordinate scaling is crucial; Adam matches AdaGrad while handling noise via exponential averaging.
- MLPs on MNIST (Figure 2)
  - With dropout (Figure 2a): Adam achieves the lowest training cost across iterations among firstâ€‘order methods (AdaGrad, RMSProp, AdaDelta, SGD+Nesterov).
  - Without dropout, comparison to SFO (Figure 2b): Adam reduces cost faster both per iteration and wallâ€‘clock time; SFO requires 5â€“10Ã— more time per iteration due to curvature updates and has memory linear in the number of minibatches (Section 6.2).
  - Additional note: SFO fails to converge when the objective includes stochastic regularization such as dropout (Section 6.2).
- CNNs on CIFARâ€‘10 (Figure 3)
  - Early epochs (left): Adam and AdaGrad both lower cost rapidly in the first three epochs.
  - Full training (right): Adam and SGD with momentum eventually converge considerably faster than AdaGrad. Section 6.3 reports that vÌ‚_t â€œvanishes to zeros after a few epochs and is dominated by Îµ,â€ making the secondâ€‘moment estimate a poor geometry proxy for this CNN, while the firstâ€‘moment term (variance reduction) is more important.
  - Takeaway: On CNNs, Adam provides marginal improvement over SGD with momentum and removes the need for layerâ€‘specific learning rates, but AdaGradâ€™s cumulative secondâ€‘moment can overâ€‘attenuate updates.
- Biasâ€‘correction ablation (Figure 4)
  - Setup: Train a variational autoencoder (single hidden layer of 500 softplus units; 50â€‘dim Gaussian latent) across grids of Î²1âˆˆ{0,0.9}, Î²2âˆˆ{0.99,0.999,0.9999}, log10(Î±)âˆˆ[âˆ’5,âˆ’1].
  - Result: Without bias correction, training is unstable for Î²2 close to 1, especially in early epochs; with correction, the loss curves are stable and generally better after 100 epochs.
  - Conclusion: Bias correction is not optional when slow decay is needed (e.g., sparse gradients).

Do the experiments support the claims?
- Yes, across three regimes:
  - Dense convex (MNIST logistic regression): fast convergence similar to wellâ€‘tuned momentum.
  - Sparse features (IMDB): clear advantage over momentum; matches AdaGrad.
  - Deep nonâ€‘convex (MLP/CNN): strong performance and robustness with dropout; competitive vs SGD and better than AdaGrad; SFO limitations with stochastic objectives are highlighted.
- Robustness checks
  - Biasâ€‘correction ablation (Figure 4) demonstrates the necessity of Adamâ€™s correction mechanism.
  - Multiple baselines and hyperparameter searches are used; however, quantitative numbers are shown primarily as curves rather than tables, so exact numeric margins are not reported.

## 6. Limitations and Trade-offs
- Theoretical assumptions (Section 4)
  - Convergence analysis requires convex losses, bounded gradients (||âˆ‡f_t(Î¸)||_2 â‰¤ G), and bounded parameter distances (||Î¸_mâˆ’Î¸_n||_2 â‰¤ D), with specific decay schedules Î±_t=Î±/âˆšt and Î²1,t=Î²1Â·Î»^{tâˆ’1}. These do not hold in general for deep nonâ€‘convex problems.
- Behavior under certain architectures (Section 6.3)
  - In the CNN setting, the secondâ€‘moment estimate vÌ‚_t can become so small that Îµ dominates, weakening geometry adaptation and limiting the benefit over SGD with momentum.
- Hyperparameter coupling
  - While default (Î±, Î²1, Î²2, Îµ) works broadly (Algorithm 1), performance can depend on decay choices; the proof also relies on decaying Î²1,t, which is not always used in practice.
- No secondâ€‘order curvature
  - Adam uses only firstâ€‘order information. On smooth deterministic problems with reliable curvature estimates, quasiâ€‘Newton methods may converge in fewer function evaluations (though at higher perâ€‘iteration cost).
- Evaluation scope
  - Results are reported on standard vision/NLP benchmarks with training cost curves; broader tasks, test accuracy comparisons, or largeâ€‘scale industrial systems are not detailed in this paper.

## 7. Implications and Future Directions
- Impact on the field
  - Adam becomes a standard default optimizer because it combines fast initial progress, resilience to noise and sparsity, minimal tuning, and a principled foundation. The biasâ€‘corrected moments and bounded, scaleâ€‘invariant steps generalize well across tasks.
- Followâ€‘up research enabled or suggested
  - Alternative decay schedules and adaptive Î²1,t strategies grounded in the theory of Section 4.
  - Diagnostics for when vÌ‚_t collapses (as seen in Section 6.3) and mechanisms to prevent Îµâ€‘domination, potentially by mixing AdaMaxâ€‘style norms (Section 7.1) or adaptive Îµ.
  - Richer averaging schemes (Section 7.2) and their effect on generalization with Adamâ€‘style updates.
  - Extensions that decouple regularization from adaptive scaling or combine Adam with curvature sketches while keeping O(d) memory.
- Practical applications
  - Training deep networks with dropout or other stochastic regularizers (Section 6.2), logistic models with sparse features (IMDB BoW; Section 6.1), and largeâ€‘parameter models where perâ€‘parameter adaptation and low memory footprint matter.
  - The AdaMax variant (Algorithm 2) offers a simple, stable alternative in scenarios with heavyâ€‘tailed gradients or when a hard bound |Î”_t| â‰¤ Î± is desirable.

Key takeaways grounded in the paper
- Adamâ€™s update rule (Algorithm 1) is simple to implement and computationally efficient.
- Bias correction (Section 3) is essential for stability, particularly with Î²2 â‰ˆ 1; Figure 4 empirically validates this.
- Step sizes are effectively bounded and invariant to gradient scaling (Section 2.1), improving robustness and ease of use.
- Convergence with O(âˆšT) regret in online convex settings (Theorem 4.1) aligns with the best known rates while benefiting from adaptivity, especially under sparsity.
- Empirical results across MNIST, IMDB, and CIFARâ€‘10 (Figures 1â€“3) show strong practical performance and clarify when Adam excels (sparse, noisy, nonâ€‘stationary) and when gains are modest (some CNN regimes).

# Not All LLM Reasoners Are Created Equal

**ArXiv:** [2410.01748](https://arxiv.org/abs/2410.01748)
**Authors:** ArianÂ Hosseini, AlessandroÂ Sordoni, DanielÂ Toyama, AaronÂ Courville, RishabhÂ Agarwal
**Institutions:** Mila, GoogleÂ DeepMind, MicrosoftÂ Research

## ðŸŽ¯ Pitch

The paper introduces Compositional GSM, a novel benchmark that reveals significant weaknesses in large language models by testing their ability to solve two simple math problems in sequence. This work is crucial as it highlights the gap between high performance on single-question benchmarks and actual multi-step reasoning skills, urging a reevaluation of model evaluation and training for tasks requiring sequential reasoning.

---

## 1. Executive Summary
This paper introduces Compositional GSM, a two-step (â€œtwo-hopâ€) version of the popular GSM8K gradeâ€‘school math benchmark, and a metric called the reasoning gap that measures how much a modelâ€™s performance drops when two easy problems must be solved together. Across 20+ open and closed LLMs, the study shows that many modelsâ€”especially small, costâ€‘efficient, and mathâ€‘specialized onesâ€”perform far worse than expected on this simple composition, largely due to distraction from extra context and failure on the second reasoning hop (Figures 3â€“4, 10â€“12).

## 2. Context and Motivation
- Problem addressed
  - Many LLMs score very highly on singleâ€‘question gradeâ€‘school math benchmarks like GSM8K. The open question is whether these models truly reason or whether they exploit patterns in question format. This work probes that gap by testing whether models can combine two familiar problems into one small composition without increasing math difficulty (Section 1, Figure 2).

- Why it matters
  - Real tasks often require multiâ€‘step reasoning: solving a subproblem and correctly carrying its result into the next step. If LLMs fail on such basic composition, their reliability on workflows, tutoring, planning, and tool use is overestimated by singleâ€‘question benchmarks.

- Prior approaches and shortcomings
  - Robustness work has explored testâ€‘set leakage, adversarial phrasing, or functional rewrites of math problems (Section 4). These show brittleness but do not directly isolate the skill of chaining two easy, familiar problems in one prompt.
  - Multiâ€‘hop reasoning analyses often use knowledge retrieval tasks; here the authors use a numerically precise, easily verifiable math setting to pinpoint where multiâ€‘hop breaks.

- Position relative to existing work
  - The paper positions Compositional GSM not as â€œjust another benchmarkâ€ but as a controlled case study of two-hop reasoning at the same difficulty level as GSM8K (Section 1). It pairs this with a simple expectedâ€‘performance model and a diagnostic metric (Equation 1) to quantify how far models fall below what should be achievable.

## 3. Technical Approach
- Core setup
  - GSM8K: A dataset of gradeâ€‘school math word problems with short, verifiable answers.
  - Compositional GSM: Each test item joins two GSM8K questions, Q1 and Q2, so that the numerical answer to Q1 becomes a variable `X` that must be substituted into Q2 (Figure 2). Both subproblems are gradeâ€‘school level; only composition is new.

- How the dataset is built (Section 2; Appendices A, E, F)
  - Start from 1,200 GSM8K test questions to serve as Q1.
  - Build a modified set of 1,200 Q2 questions by taking other GSM8K items and editing a single number in their codeâ€‘form solutions so the final answer changes, remains a positive integer, and stays close in magnitude to the original. The substitution location is chosen so Q2 remains sensible after replacing that number with `X` (Figure 2; Appendix A shows resulting answerâ€‘magnitude distributions remain similar to GSM8K).
  - Sanity checks: for each modified Q2, generate 16 candidate solutions using two strong models and keep or fix items that do not yield consistent correct answers (about 25% required manual edits).

- How accuracy and the â€œreasoning gapâ€ are defined (Section 2)
  - Measure three accuracies (each on 1,200 items):
    - `S1`: accuracy on the original GSM8K test split (used as Q1).
    - `S2`: accuracy on the modified GSM8K test split (the standalone Q2 variants).
    - `Scomp`: accuracy on the compositional set where Q1 and Q2 appear together and Q2 depends on `X`.
  - Expected compositional performance if the two steps were independent is `S1 Ã— S2`. The reasoning gap is
    - â€œReasoning gap: Î” = Scomp âˆ’ S1 Ã— S2â€ (Equation 1).
  - Intuition: If a model can solve Q1 and Q2 independently with probabilities `S1` and `S2`, it should solve both in composition with probability `S1Ã—S2`. Deviations indicate difficulty caused by composition itself.

- Evaluation protocol (Section 3; Appendices Bâ€“F)
  - Models: GPTâ€‘4o/mini, Gemini 1.0/1.5 (Flash/Pro), Llamaâ€‘3 (8B/70B, PT/IT), Gemma2 (9B/27B, PT/IT), Mistralâ€‘7B (PT/IT), Mixtralâ€‘8Ã—7B (PT/IT), Phiâ€‘2, Phiâ€‘3â€‘miniâ€‘IT, Mathstralâ€‘7B, NuminaMathâ€‘7Bâ€‘CoT, Qwen2.5â€‘Mathâ€‘7B/72Bâ€‘IT (Section 3).
  - Prompting: standardized 8â€‘shot prompts for all three splits; a short preamble is added if a model needs formatting guidance (Appendix B, D, E).
  - Decoding: temperature 0, `pass@1` (i.e., the first output must be correct; no sampling for multiple tries).
  - Two solution modes: naturalâ€‘language chainâ€‘ofâ€‘thought (`CoT`) vs Python code generation that explicitly defines `solve_q1()` and then calls it inside a `solution()` function for Q2 (Appendix F).

- Additional experiments to diagnose causes (Section 3.6)
  - Leakage check: Compare `S1` vs `S2` to see if modified standalone Q2s are harder or contaminated; they mostly line up on the x=y line (Figure 9).
  - Distraction check: Compare accuracy on a Q1 alone vs the same Q1 when embedded at the start of a compositional item (Figure 10).
  - Secondâ€‘hop check: Compare accuracy on a Q2 alone vs Q2 in composition conditional on Q1 being solved correctly (Figure 11).
  - Twoâ€‘questions capacity: Compare Q2 alone vs Q2 with Q1 in context but independent vs Q2 in composition (Figure 12).

- Design rationale
  - Two-hop composition focuses on the simplest form of compositional generalization: correctly solve an easy subproblem and carry its result forward.
  - The controlled codeâ€‘based editing ensures Q2 stays gradeâ€‘school, keeps answer magnitudes comparable to GSM8K (Appendix A), and lets the study attribute failures to composition rather than a shift in difficulty.

## 4. Key Insights and Innovations
- A simple, controlled test exposes big hidden weaknesses
  - Innovation: the Compositional GSM construction plus the reasoningâ€‘gap metric (Equation 1) isolates the cost of composition itself.
  - Significance: Many models that ace GSM8K drop sharply when asked to solve two easy steps in one prompt (Figures 1 and 3). This reveals a gap between benchmark performance and actual multiâ€‘step reliability.

- Size and cost matterâ€”in the wrong direction for deployment
  - Finding: Small and costâ€‘efficient models show much larger negative reasoning gaps than their larger counterparts, despite similar GSM8K scores (Figure 4).
  - Significance: For practitioners optimizing cost, singleâ€‘benchmark scores are misleading; multiâ€‘step reliability may collapse.

- Task formatting and training recipes interact with model size
  - Finding: Instruction tuning (â€œITâ€) substantially boosts GSM8K accuracy for small models but yields only modest gains on compositional GSM; for larger models the pattern is weaker or reversed (Figure 5).
  - Significance: The same IT recipe can overfit smaller models to standard formats while not improving compositional reasoning.

- â€œMathâ€‘specializedâ€ does not equal â€œcompositionalâ€
  - Finding: Models trained heavily on math (e.g., Qwen2.5â€‘Mathâ€‘7Bâ€‘IT) still show substantial gaps and even signs of overfitting to benchmark style (Figure 6).
  - Significance: Specialized training can raise singleâ€‘problem scores without transferring to simple twoâ€‘hop composition.

- Where the failure comes from: distraction and secondâ€‘hop errors
  - Evidence: Models often miss details in Q1 when Q2 is present (Figure 10) and frequently fail Q2 even when Q1 is correct (Figure 11). When Q2 is independent of Q1, simply adding Q1 to the context causes little harm (Figure 12).
  - Significance: The bottleneck is not handling two questions per se; it is correctly using Q1â€™s result inside Q2.

- Code helps, especially for small models
  - Finding: Switching from naturalâ€‘language CoT to code yields large relative gains on compositional GSM for smaller modelsâ€”for example, Llamaâ€‘3â€‘8B (+69%), Gemma2â€‘9B (+74%), and Mistralâ€‘7B (+149%)â€”with smaller effects for big models like Llamaâ€‘3â€‘70B (+2%) (Figure 8).
  - Significance: Externalizing intermediate computation into code scaffolds the second hop for weaker models.

## 5. Experimental Analysis
- Evaluation methodology
  - Datasets: Three 1,200â€‘item test sets (original GSM8K as Q1; modified GSM8K as standalone Q2; compositional GSM combining Q1â†’Q2) (Section 3).
  - Metric: exactâ€‘match accuracy; `pass@1`.
  - Prompting: 8â€‘shot exemplars for each split (Appendices Dâ€“F).
  - Models: A broad sweep of open and closed models, pretrained (PT), instructionâ€‘tuned (IT), and mathâ€‘specialized (Section 3).

- Main quantitative results
  - Overall reasoning gaps
    - Figure 1 plots compositional accuracy vs the geometric mean of `S1` and `S2` with a y=xÂ² expectation line; most points lie well below the curve, showing large negative Î”.
    - Figure 3 ranks models by Î”: small/costâ€‘efficient and mathâ€‘specialized models have the largest negative gaps.
  - Costâ€‘efficient vs highâ€‘end (Figure 4)
    - GPTâ€‘4o vs GPTâ€‘4o mini: mini shows a far larger negative Î” (â‰ˆâˆ’14 points vs â‰ˆâˆ’1).
    - Gemini 1.5 Pro vs 1.5 Flash: Flash has a much larger gap (â‰ˆâˆ’11 points vs â‰ˆâˆ’6).
    - Llamaâ€‘3â€‘70Bâ€‘IT vs 8Bâ€‘IT: 8Bâ€‘IT gap (~âˆ’27.5) dwarfs 70Bâ€‘IT (~âˆ’4.9).
    - Gemma2â€‘27Bâ€‘IT vs 9Bâ€‘IT: 9Bâ€‘IT gap (~âˆ’37.3) is much larger than 27Bâ€‘IT (~âˆ’18).
    - Quote: â€œAlthough the cheaper models perform similarly on the original GSM8K test, they show a significant decline in performance on the compositional GSM testâ€ (Figure 4 caption).
  - Instruction tuning across sizes (Figure 5)
    - Small models: IT boosts GSM8K a lot more than compositional GSM (e.g., Mistralâ€‘7B: +14.1 vs +4.3; Llamaâ€‘3â€‘8B: +25.1 vs +12.6; Gemma2â€‘9B: +22.8 vs +4.8).
    - Large models: pattern weak or reversed (e.g., Llamaâ€‘3â€‘70B: +8.6 GSM8K vs +19.0 compositional).
    - Quote: â€œFor smaller modelsâ€¦ instructionâ€‘tuning results in substantial improvements on the original GSM8K test set, but a much smaller improvement on the compositional GSM testâ€ (Figure 5 caption).
  - Mathâ€‘specialized models (Figure 6)
    - Large negative gaps remain: Numinaâ€‘7Bâ€‘CoT (~âˆ’12), Mathstralâ€‘7B (~âˆ’14), Qwen2.5â€‘Mathâ€‘7Bâ€‘IT (~âˆ’22), while 72Bâ€‘IT is closer to parity (~âˆ’3).
    - Text highlights: â€œQwen2.5â€‘Mathâ€‘7Bâ€‘ITâ€¦ solves less than 60% of the compositional gradeâ€‘school math problemsâ€ despite strong MATH performance (Section 3.3).
  - Finetuning and overfitting (Figure 7)
    - Fineâ€‘tuning Gemma2â€‘27B on GSM8K solutions (human or synthetic) improves GSM8K accuracy steadily, but compositional accuracy increases only up to ~100 steps, then drops by 400 steps.
    - Quote: â€œAfter 100 training steps, compositional GSM test performance drops while GSM8K test performance keeps improvingâ€¦ [suggesting] taskâ€‘specific overfittingâ€ (Figure 7 caption).
  - Natural language CoT vs code (Figure 8)
    - Relative gains for small models are striking: Mistralâ€‘7B (+149%), Llamaâ€‘3â€‘8B (+69%), Gemma2â€‘9B (+74%). Big models benefit less (e.g., Llamaâ€‘3â€‘70B +2%).
    - Quote: â€œSmaller models benefit more from generating code rather than natural language CoTâ€ (Figure 8 caption).

- Diagnostic analyses on causes (Section 3.6)
  - Leakage not the culprit (Figure 9)
    - Plot of `S1` (original) vs `S2` (modified) hugs the x=y line: â€œtest set leakage is not a major concern.â€
  - Distraction on Q1 (Figure 10)
    - Many models perform worse on the same Q1 when it appears at the start of a compositional item; responses often â€œoverlook important detailsâ€ when Q2 sits below.
  - Secondâ€‘hop difficulty (Figure 11)
    - Even when Q1 is correct, many models fail Q2 more often than when Q2 is asked alone; they have â€œbecome too specialized in handling GSM8Kâ€‘style questions.â€
  - Twoâ€‘question capacity (Figure 12)
    - When Q2 does not depend on Q1, adding Q1 barely harms performance. The failure is specifically on using Q1â€™s answer inside Q2.

- Do the experiments support the claims?
  - Yes. The expectedâ€‘performance model (`S1 Ã— S2`) plus the triad of plots (Figures 10â€“12) triangulate the mechanism: extra context alone is not the main problem; rather, composing the second hop using the first hopâ€™s result is the failure point. Size, cost, and training mode systematically modulate the gap (Figures 4â€“6), and code scaffolding mitigates it in smaller models (Figure 8).

## 6. Limitations and Trade-offs
- Assumptions in the expectedâ€‘performance model
  - The baseline expectation `S1 Ã— S2` treats Q1 and Q2 as independent subtasks. In practice, solving Q1 in the compositional setting might be easier or harder than alone (Figure 10 shows it is often harder), which builds the gap by design. That is a feature for diagnosis, but it also means Î” conflates secondâ€‘hop failure with Q1 distraction.

- Scope of tasks
  - The study focuses on gradeâ€‘school math. While chosen for verifiability and control, results may differ in other domains (e.g., commonsense, programming beyond arithmetic).

- Single prompting/decoding regime
  - All models use 8â€‘shot prompts and temperature 0 `pass@1`. Some models might improve under different prompting or sampling strategies (e.g., reruns, selfâ€‘consistency). The uniform regime strengthens comparability but may understate a modelâ€™s best achievable performance.

- Dataset construction choices
  - Q2s are edited via codeâ€‘form solution changes and curated with model agreement plus manual checks (Section 2). This yields a clean test but still relies on the availability and correctness of codeâ€‘form solutions and editorial choices that keep the final answer â€œnot too farâ€ from the original.

- Finetuning study breadth
  - Overfitting results are shown for one base model (Gemma2â€‘27B PT) and short training runs (50â€“400 steps). The pattern is suggestive but not exhaustive across models or curricula.

- Evolving closed models
  - Results for proprietary models (e.g., GPTâ€‘4o, Gemini 1.5) reflect specific versions and may shift as APIs update.

## 7. Implications and Future Directions
- Implications for evaluation and deployment
  - Singleâ€‘question math benchmarks can overestimate real reasoning reliability. A basic twoâ€‘hop composition reveals sizable weaknesses, especially in the small/costâ€‘efficient regime (Figures 3â€“4). Practitioners should include compositional tests when selecting models for workflows that chain steps, even if each step is simple.
  - Instruction tuning can overfit small models to familiar formats (Figure 5). Training and evaluation should explicitly include multiâ€‘step compositions to avoid brittle behavior.
  - Codeâ€‘based reasoning scaffolds (Figure 8) can notably boost small models; toolâ€‘use or programâ€‘aided agents may be preferable for lowâ€‘cost deployments.

- Research directions
  - Extend compositional testing beyond GSM8K: to harder math (MATH), nonâ€‘math reasoning, and multimodal settings (discussion in Section 5).
  - Develop training curricula that directly target secondâ€‘hop groundingâ€”learning to carry forward intermediate results faithfully rather than just producing fluent singleâ€‘problem solutions.
  - Diagnose and mitigate distraction: techniques that preserve focus across multiple subquestions (e.g., structured memory, explicit subâ€‘goal tracking, verifierâ€‘aided decomposition).
  - Explore scalable codeâ€‘oriented scaffolds and hybrid NLâ€‘code strategies that retain interpretability while improving reliability for small models.
  - Rethink progress metrics: incorporate expectationâ€‘based gaps like Î” and conditional plots (Figures 10â€“12) into standard evaluation suites to distinguish genuine reasoning from format familiarity.

- Practical applications
  - Education/tutoring systems that assign multiâ€‘part problems.
  - AI agents that must execute sequences of subgoals or tool calls where later steps consume outputs from earlier steps.
  - Quality assurance for costâ€‘sensitive deployments: compositional smoke tests can prevent overâ€‘reliance on high GSM8K scores.

> Bottom line: Even at gradeâ€‘school difficulty, twoâ€‘hop composition remains a stumbling block for many LLMs. Measuring and training for this capability is essential if we want models that not only answer isolated questions but also carry correct intermediate results through to the end.

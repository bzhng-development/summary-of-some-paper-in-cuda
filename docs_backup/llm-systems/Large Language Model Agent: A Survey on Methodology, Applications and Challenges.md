# Large Language Model Agent: A Survey on Methodology, Applications and Challenges

**ArXiv:** [2503.21460](https://arxiv.org/abs/2503.21460)
**Authors:** Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, Ming Zhang
**Institutions:** 

## ğŸ¯ Pitch

This paper introduces a unified framework for Large Language Model (LLM) agents, organized around constructing, collaborating, and evolving, thus creating a systematic lifecycle blueprint. By connecting fragmented research areas, it offers a comprehensive methodology essential for deploying robust, coordinated, and adaptive systems in high-stakes applications like healthcare and scientific discovery, ultimately advancing the field beyond ad hoc solutions.

---

## 1. Executive Summary
This survey proposes a unified, methodology-centered framework for understanding Large Language Model (LLM) agents, organized around three tightly connected dimensions: how agents are built (â€œConstructionâ€), how they work together (â€œCollaborationâ€), and how they improve over time (â€œEvolutionâ€). It complements this core taxonomy with coverage of evaluation benchmarks, development tools, realâ€‘world security/privacy/ethics issues, and application domains, giving researchers and practitioners a coherent map of a fastâ€‘moving field (Figures 1â€“2, Sections 2â€“5).

## 2. Context and Motivation
- Problem the paper addresses
  - Research on LLM agents has exploded, but contributions are fragmented across subtopics: profile/role design, memory, planning, tool use, multiâ€‘agent cooperation, selfâ€‘learning, safety, evaluation, and applications. Whatâ€™s missing is a coherent methodology that links these pieces into a lifecycle view of â€œhow agents are constructed, collaborate, and evolveâ€ (Figure 1; Sections 1 and 2).
- Why this matters
  - Realâ€‘world deployment needs more than clever prompts. It requires robust architectures, collaboration protocols, learning/evolution mechanisms, evaluation standards, and safety practices. Without a unifying lens, itâ€™s hard to compare systems, identify gaps, or transfer best practices to highâ€‘stakes domains such as scientific discovery and healthcare (Sections 1, 3, 4, 5).
- Prior approaches and their gaps
  - Earlier surveys typically focus on narrow slices: gaming agents [11, 12], deployment environments [13, 14], multimodality [15], or security [16]. Broader overviews exist but lack a detailed methodological taxonomy that ties individual agent internals to multi-agent systems and their evolution (Section â€œDistinction from Previous Surveysâ€ in 1).
- How this paper positions itself
  - It contributes a â€œBuildâ€“Collaborateâ€“Evolveâ€ taxonomy that deconstructs agents into fundamental componentsâ€”profile, memory, planning, actionâ€”and connects these to collaboration styles (centralized, decentralized, hybrid) and to evolution mechanisms (selfâ€‘learning, coâ€‘evolution, external knowledge/tools) (Figure 2; Section 2). It also systematizes evaluation (benchmarks, datasets), tools (for/with/by agents), and realâ€‘world issues (security, privacy, social impact) (Sections 3â€“4).

## 3. Technical Approach
The paperâ€™s â€œtechnical approachâ€ is a structured taxonomy that explains how to design and reason about LLM agents endâ€‘toâ€‘end. It is not a single algorithm, but a conceptual architecture backed by concrete exemplars, with a stepâ€‘byâ€‘step decomposition (Figure 2; Section 2).

A. Construction: defining a single agentâ€™s internals (Section 2.1)
- Profile definition (Section 2.1.1)
  - What it is: A profile encodes an agentâ€™s role, objectives, capabilities, and behavior constraints.
  - Two implementations:
    - Humanâ€‘curated static profiles: Manually specified roles and protocols yield predictable behavior and complianceâ€”e.g., role orchestration in `CAMEL`, `AutoGen`, `MetaGPT`, `ChatDev`, `AFlow` (Section 2.1.1).
    - Batchâ€‘generated dynamic profiles: Parameterized initialization stochastically creates a diverse â€œpopulationâ€ of agents with varied personas/values, useful for simulating societies or user cohortsâ€”e.g., `Generative Agents`, `RecAgent`, with optional optimization in `DSPy` (Section 2.1.1).
- Memory mechanism (Section 2.1.2)
  - Why memory is split: LLM context windows are limited, so agents need mechanisms to preserve and retrieve relevant information across time.
  - Shortâ€‘term memory: Maintains recent dialogue and intermediate thoughts; enables interactive reasoning but is transient and must be compressed or pruned (e.g., `ReAct`, `Graph of Thoughts`, `AFlow`) (Section 2.1.2).
  - Longâ€‘term memory: Converts ephemeral reasoning into persistent skills/knowledge via:
    - Skill libraries (e.g., `Voyager` in Minecraft; `GITM`) 
    - Experience repositories (e.g., `ExpeL`, `Reflexion`) 
    - Tool synthesis/self-expanding toolkits (e.g., `TPTU`, `OpenAgents`) (Section 2.1.2).
  - Knowledge retrieval as memory: Treats `RAG` (Retrievalâ€‘Augmented Generation) and graph retrieval (`GraphRAG`) as an externalized memory layer interleaved with reasoning (`IRCoT`, `Llatrieval`, `KGâ€‘RAR`, `DeepRAG`) (Section 2.1.2).
- Planning capability (Section 2.1.3)
  - Task decomposition strategies:
    - Singleâ€‘path chaining: planâ€‘andâ€‘solve or dynamic nextâ€‘step planning; robustness can be improved with multiple reasoning paths (selfâ€‘consistency, voting, discussion) (Section 2.1.3).
    - Treeâ€‘based search: `Treeâ€‘ofâ€‘Thought (ToT)` explores multiple branches with backtracking and feedback; can integrate Monte Carlo Tree Search for complex domains, including robotics/gameplay (Section 2.1.3).
  - Feedbackâ€‘driven iteration:
    - Sources of feedback: environment (embodied settings), humans (labels/guidance), model introspection (selfâ€‘critique), and other agents (collaboration) (Section 2.1.3).
    - Mechanism: Regenerate/refine plans in a loop until success criteria are met (e.g., `AdaPlanner`, `AIFP`) (Section 2.1.3).
- Action execution (Section 2.1.4)
  - Tool utilization: Two subproblemsâ€”when to use a tool (decision) and which tool to pick (selection). Systems simplify tool docs or leverage toolâ€‘use training to improve reliability (e.g., `EASYTOOL`, `GPT4Tools`, `AvaTaR`, `TRICE`) (Section 2.1.4).
  - Physical interaction: For embodied agents, translate plans to lowâ€‘level actions considering hardware, social norms, and multiâ€‘agent coordination (e.g., `DriVLMe`, `ReAd`, `Collaborative Voyager`) (Section 2.1.4).

B. Collaboration: organizing groups of agents (Section 2.2; Table 1)
- Centralized control (Section 2.2.1)
  - Explicit controllers: A central agent (or human) decomposes tasks, allocates subgoals, and integrates results (e.g., `Coscientist`, `LLMâ€‘Blender`, `MetaGPT`) (Section 2.2.1).
  - Differentiation-based control: A highâ€‘capacity model implicitly plays multiple subâ€‘roles via metaâ€‘prompts and then aggregates (e.g., `AutoAct`, `Metaâ€‘Prompting`, `WJudge`) (Section 2.2.1).
  - Tradeâ€‘off: Strong coordination and accountability vs. singleâ€‘point bottlenecks and reduced diversity (Section 2.2.1).
- Decentralized collaboration (Section 2.2.2)
  - Revisionâ€‘based: Agents iteratively edit/refine a shared output with limited direct discussion; often more deterministic (e.g., `MedAgents`, `ReConcile`, `METAL`, `DSâ€‘Agent`) (Section 2.2.2).
  - Communicationâ€‘based: Agents openly debate/critique with structured protocols to avoid â€œdegeneration of thoughtâ€ and reach consensus (e.g., `MAD`, `MADR`, `MDebate`, `AutoGen`) (Section 2.2.2).
  - Tradeâ€‘off: Flexibility and exploration vs. coordination overhead and convergence risks (Section 2.2.2).
- Hybrid architectures (Section 2.2.3)
  - Static hybrids: Predefine central vs. peerâ€‘toâ€‘peer patterns (e.g., `CAMEL` group roles; `AFlow`â€™s threeâ€‘tier planning; `EoT`â€™s BUS/STAR/TREE/RING topologies) (Section 2.2.3).
  - Dynamic hybrids: Learn collaboration graphs or adapt structures by task importance/complexity (e.g., `DiscoGraph`, `DyLAN`, `MDAgents`) (Section 2.2.3).
  - Tradeâ€‘off: Better fit to heterogeneous tasks vs. additional complexity and training/inference cost (Section 2.2.3).

C. Evolution: improving agents over time (Section 2.3; Table 2)
- Autonomous optimization and selfâ€‘learning (Section 2.3.1)
  - Selfâ€‘supervised adaptation (e.g., `SE`, evolutionary model merging);
  - Selfâ€‘reflection/correction (`SELFâ€‘REFINE`, `STaR`, `Vâ€‘STaR`, `Selfâ€‘Verification`);
  - Selfâ€‘rewarding/RL alignment (`Selfâ€‘Rewarding`, `RLCD`, `RLC`) (Section 2.3.1).
- Multiâ€‘agent coâ€‘evolution (Section 2.3.2)
  - Cooperative: Intent inference and shared policy improvement (`ProAgent`, `CORY`, `CAMEL`) (Section 2.3.2).
  - Competitive/adversarial: Debate and redâ€‘teaming to strengthen reasoning and robustness (`MDebate`, `MAD`, `Redâ€‘Team LLMs`) (Section 2.3.2).
- Evolution via external resources (Section 2.3.3)
  - Knowledgeâ€‘enhanced evolution (`KnowAgent`, `WKM`) to constrain planning and reduce hallucinations;
  - Feedbackâ€‘driven evolution via tools/executors (`CRITIC`, `STE`, `SelfEvolve`) (Section 2.3.3).

D. Evaluation and tooling (Sections 3.1â€“3.2; Figure 3)
- Evaluation frameworks span general agent capability benchmarking, domainâ€‘specific simulations (medicine, driving, data science), and multiâ€‘agent system assessment (Sections 3.1.1â€“3.1.3).
- Tools are organized as: used by agents (search, calculators, API callers), created by agents (tool creation pipelines), and used to deploy/operate agents (frameworks like `AutoGen`, `LangChain`, `LlamaIndex`, `Dify`; and the `Model Context Protocol`) (Section 3.2).

Definitions of uncommon terms used above:
- `RAG` (Retrievalâ€‘Augmented Generation): a technique where the agent retrieves relevant external documents/graphs during generation to supplement its internal parameters (Section 2.1.2).
- `Treeâ€‘ofâ€‘Thought (ToT)`: a treeâ€‘structured reasoning process that explores multiple branches, allows backtracking, and uses feedback to pick better paths (Section 2.1.3).
- `Multiâ€‘agent debate`: a structured dialogue among agents (or multiple runs of one agent) that alternates critique and defense to improve answers/consensus (Sections 2.2.2, 2.3.2).

## 4. Key Insights and Innovations
- A unified lifecycle view: Build â†’ Collaborate â†’ Evolve (Figures 1â€“2; Section 2)
  - Whatâ€™s new: Rather than listing techniques, the paper shows how profile, memory, planning, and action form a recursive loop for a single agent, and how collaboration and evolution sit on top of that loop (Sections 2.1â€“2.3).
  - Why it matters: It connects internal design choices (e.g., longâ€‘term memory) to systemâ€‘level properties (e.g., decentralized debate) and longâ€‘horizon improvement (e.g., selfâ€‘verification + tool feedback).
- â€œKnowledge retrieval as memoryâ€ (Section 2.1.2)
  - Whatâ€™s new: Treats RAG/GraphRAG not as â€œjust toolsâ€ but as an externalized memory tier with tight reasoning integration (`IRCoT`, `KGâ€‘RAR`, `DeepRAG`).
  - Why it matters: Clarifies architectural implications (e.g., when to store skills vs. when to fetch facts) and helps avoid conflating internal memories with retrieval pipelines.
- Fineâ€‘grained collaboration taxonomy (Section 2.2; Table 1)
  - Whatâ€™s new: Distinguishes centralized controllers (explicit vs. differentiationâ€‘based), decentralized modes (revisionâ€‘ vs. communicationâ€‘based), and hybrid systems (static vs. dynamic topology).
  - Why it matters: Provides a vocabulary to compare agent systems by coordination load, robustness, and scalability, not only by task performance.
- Security and privacy reframed for agentic settings (Figure 4; Sections 4.1â€“4.3; Tables 3â€“5)
  - Whatâ€™s new: Splits threats into agentâ€‘centric (adversarial, jailbreak, backdoor, collaboration attacks) vs. dataâ€‘centric (prompt injection, external source poisoning, interactionâ€‘level attacks), plus memorization/IP risks.
  - Why it matters: Security thinking shifts from singleâ€‘model prompts to systemâ€‘level attack surfaces (tools, memories, interâ€‘agent messages, topology).

These are fundamental framing contributions (not just incremental lists) because they change how we decompose, compare, and secure agent systems endâ€‘toâ€‘end.

## 5. Experimental Analysis
Because this is a survey, â€œexperimentsâ€ are curated benchmarks, datasets, and case studies that substantiate coverage and illustrate evaluation practices (Section 3; Figure 3). The paper reports concrete scales and task designs:

- General agent capability benchmarks (Section 3.1.1)
  - AgentBench: 8 interactive environments to test reasoning and acting (Section 3.1.1).
  - Mind2Web:
    > â€œthe first generalist agent for evaluating 137 realâ€‘world websites with different tasks spanning 31 domainsâ€ (Section 3.1.1).
  - MMAU: decomposes into five core competencies across 3,000+ tasks (Section 3.1.1).
  - VisualAgentBench: multimodal foundationâ€‘agent evaluation across GUI, visual design, etc. (Section 3.1.1).
  - Embodied Agent Interface and CRAB: fineâ€‘grained error classification and crossâ€‘platform embodied testing (Section 3.1.1).
  - Dynamic/selfâ€‘evolving evaluation:
    > â€œBENCHAGENTSâ€¦ automatically creates benchmarks through LLM agentsâ€ (Section 3.1.1);  
    > â€œSealâ€‘Tools (1,024 nested instances of tool calls)â€ and â€œCToolEval (398 Chinese APIs across 14 domains)â€ (Section 3.1.1).

- Domainâ€‘specific and realâ€‘world environments (Section 3.1.2)
  - Medicine: 
    > â€œMedAgentBenchâ€¦ tasks designed by 300 clinicians in an FHIRâ€‘compliant environmentâ€;  
    > â€œAI Hospitalâ€¦ simulates clinical workflows through multiâ€‘agent collaborationâ€ (Section 3.1.2).
  - Driving and desktop/web action:
    > â€œLaMPilotâ€¦ executable code generation benchmark for autonomous drivingâ€;  
    > â€œOSWorldâ€¦ 369 multiâ€‘application tasks across Ubuntu/Windows/macOSâ€ (Section 3.1.2).
  - Data science and ML engineering:
    - DSEval, DAâ€‘Code, DCAâ€‘Bench, MLAgentâ€‘Bench, MLEâ€‘Bench (Section 3.1.2).
  - Planning:
    > â€œTravelPlannerâ€¦ 1,225 planning tasks that require multiâ€‘step reasoning, tool integration, and constraint balancingâ€ (Section 3.1.2).
  - Dailyâ€‘life multimodal:
    > â€œEgoLifeâ€¦ a 300â€‘hour multimodal egocentric datasetâ€¦ with EgoLifeQA tasksâ€ (Section 3.1.2).
  - Toolsâ€‘inâ€‘theâ€‘wild:
    - GTA: general tool agents with realâ€‘world APIs and multimodal inputs (Section 3.1.2).

- Multiâ€‘agent system and collaboration evaluation (Section 3.1.3)
  - TheAgentCompany simulates a software company to test web interaction and code collaboration; MLRB and MLEâ€‘Bench evaluate research/engineering workflows (Section 3.1.3).

- Security robustness benchmarks (Sections 4.1â€“4.2; Tables 3â€“4)
  - AgentDojo:
    > â€œ97 realistic tasks and 629 security test casesâ€ (Section 4.1.1).
  - Agent security bench:
    > â€œacross 10 scenarios, 10 agents, 400+ tools, 23 attack/defense methods, and 8 metricsâ€ (Section 4.1).
  - AgentHarm:
    > â€œ440 malicious agent tasks in 11 hazard categoriesâ€ (Section 3.1.2 and 4.2.2).

- Scientific/medical dataset creation via agents (Sections 5.1.4â€“5.1.5)
  - PathGenâ€‘1.6M:
    > â€œ1.6 million pathology imageâ€‘text pairs generation through multiâ€‘agent collaborationâ€ (Section 5.1.4).

How convincing is the empirical coverage?
- Breadth: The survey spans general capability, domain simulations, embodied settings, tool use, and collaborationâ€”backed by concrete scales and scenarios (Sections 3.1.1â€“3.1.3).
- Depth: It does not attempt metaâ€‘analysis (e.g., pooled effect sizes) or headâ€‘toâ€‘head bakeâ€‘offs across all agents, which would be infeasible given scope; instead, it catalogs evaluation design patterns with examples and numeric scales, sufficient to guide practitioners choosing benchmarks.
- Robustness checks:
  - Security: Attack/defense typologies are tied to testbeds and numbers (Sections 4.1â€“4.2; Tables 3â€“4).
  - Ablations per method are out of scopeâ€”this is a curation, not a single systemâ€™s experiment.

Summary of key quantitative takeaways (selected):
- OSWorld covers â€œ369â€ tasks across three operating systems (Section 3.1.2).
- Mind2Web spans â€œ137 websitesâ€ and â€œ31 domainsâ€ (Section 3.1.1).
- TravelPlanner sets â€œ1,225â€ planning tasks (Section 3.1.2).
- Sealâ€‘Tools: â€œ1,024â€ multiâ€‘step toolâ€‘use instances; CToolEval: â€œ398 APIs across 14 domainsâ€ (Section 3.1.1).
- AgentDojo: â€œ629â€ adversarial test cases; AgentHarm: â€œ440â€ harmful tasks (Sections 4.1â€“4.2).

## 6. Limitations and Trade-offs
- As a survey:
  - Timeliness vs. completeness: The landscape is changing monthly. While the paper includes lateâ€‘2024/earlyâ€‘2025 resources (e.g., OSWorld, EgoLife), new agent forms and safety techniques will quickly emerge (Sections 3â€“4).
  - No crossâ€‘benchmark synthesis: It catalogs benchmarks and tools but does not normalize task difficulty or compare evaluation outcomes across suites (Sections 3.1â€“3.2).
- Taxonomy boundaries can blur:
  - â€œKnowledge retrieval as memoryâ€ is insightful, but in practice retrieval, tool calls, and longâ€‘term memory often intertwine, making strict categorization situational (Section 2.1.2).
  - Collaboration categories can overlap in complex pipelines (e.g., hybrid systems that adapt topologies midâ€‘task) (Section 2.2.3).
- Scalability and cost:
  - Dynamic hybrid collaboration (e.g., learned collaboration graphs) and extensive memory/retrieval introduce computational and engineering overhead (Sections 2.2.3, 6.1).
  - Multiâ€‘agent debate and multiâ€‘path reasoning improve reliability but increase token/latency costs (Sections 2.2.2, 6.1).
- Assumptions and unaddressed scenarios:
  - Many exemplars assume reliable tool APIs and stable environments; failure modes of flaky tools or adversarial APIs remain challenging (Sections 2.1.4, 4.2).
  - Safety defenses are cataloged, but there is no universal defenseâ€”e.g., jailbreak/backdoor/collaboration attacks target different layers and require different mitigations (Sections 4.1â€“4.2; Tables 3â€“4).
- Evaluation blind spots:
  - Despite growth, dynamic multiâ€‘turn, multiâ€‘agent evaluations with lifecycle tracking (learning over time) are early; benchmark selfâ€‘evolution is promising but nascent (Section 3.1.1; Section 6.4).

## 7. Implications and Future Directions
- How this changes the fieldâ€™s framing (Figures 1â€“2; Section 6)
  - The Buildâ€“Collaborateâ€“Evolve lens turns a list of techniques into an engineering methodology. It encourages explicit design of: (a) internal cognition loops (profileâ†’memoryâ†’planningâ†’action), (b) collaboration topology matched to task constraints, and (c) evolution channels (selfâ€‘reflection, debate, knowledge/tool feedback).
- What research it enables or suggests (Section 6)
  - Scalable coordination: Hierarchical/decentralized planning with learned or ruleâ€‘based scheduling for many agents (Section 6.1).
  - Longâ€‘term memory: Hierarchical (episodic/semantic) memory with autonomous compression to maintain identity and adapt over months (Section 6.2).
  - Reliability and rigor: Builtâ€‘in verificationâ€”knowledgeâ€‘graph crossâ€‘checks, citationâ€‘grounded responses, selfâ€‘consistency and audit logsâ€”for highâ€‘stakes domains (Section 6.3).
  - Dynamic evaluation: Benchmarks that evolve as models improve, measure multiâ€‘turn learning, and capture emergent collaboration patterns (Section 6.4).
  - Safety at system level: Topologyâ€‘aware defenses, interâ€‘agent message sanitization, toolâ€‘use firewalls, and constitutional governance for agent collectives (Sections 4, 6.5).
  - Roleâ€‘play fidelity: Better modeling of underrepresented roles and culturally diverse behaviors; richer dialogue diversity (Section 6.6).
- Practical applications and downstream use (Section 5; Table 7)
  - Science and engineering: Hypothesis generation, experimental planning/execution, and dataset creation (e.g., `SciAgents`, `Curie`, `ChemCrow`, `PathGenâ€‘1.6M`) (Sections 5.1.1â€“5.1.4).
  - Healthcare: Simulated hospitals, diagnostic agents, and medically grounded conversation (e.g., `AgentHospital`, `ClinicalLab`, `AIPatient`, `CXRâ€‘Agent`, `MedRAX`) (Section 5.1.5).
  - Productivity: Software engineering with roleâ€‘based multiâ€‘agent workflows (`ChatDev`, `MetaGPT`) and recommender systems where agents model users/items (`AgentCF`, `RecMind`) (Section 5.4).
  - Web/OS automation and embodied control: OSWorldâ€‘style assistants, web agents (Mind2Web), and embodied planning with feedback loops (Sections 3.1.2, 2.1.3â€“2.1.4).

In short, this survey provides an actionable blueprint for designing, evaluating, and securing LLM agents. Its lifecycle perspective helps teams move from ad hoc prompting to principled, evolvable, and safer agent systems, while pointing to key open problems in scalable coordination, longâ€‘term memory, dynamic evaluation, and systemâ€‘level safety.

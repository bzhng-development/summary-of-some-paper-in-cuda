# Inference Scaling for Longâ€‘Context Retrieval Augmented Generation

**ArXiv:** [2410.04343](https://arxiv.org/abs/2410.04343)
**Authors:** Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky
**Institutions:** University of Illinois Urbanaâ€‘Champaign, Google DeepMind, University of Massachusetts Amherst

## ðŸŽ¯ Pitch

This paper introduces the `DRAG` and `IterDRAG` strategies to optimize test-time compute for retrieval-augmented generation (RAG), using a computation-allocation model to predict the best use of resources for maximum accuracy. It transforms scalable inference into a reliable tool, empowering applications to trade compute resources for enhanced accuracy, ultimately advancing long-context language model capabilities in knowledge-intensive tasks.

---

## 1. Executive Summary
This paper shows how to turn the extra testâ€‘time compute offered by longâ€‘context LLMs into predictable, nearâ€‘linear gains for retrievalâ€‘augmented generation (RAG). It introduces two inference strategiesâ€”`DRAG` (demonstrationâ€‘based RAG) and `IterDRAG` (iterative demonstrationâ€‘based RAG)â€”and a simple computationâ€‘allocation model that predicts how to spend a fixed inference budget (documents, examples, iterations) to maximize accuracy.

## 2. Context and Motivation
- Problem the paper tackles
  - Longâ€‘context LLMs can ingest huge inputs (e.g., up to millions of tokens), but simply stuffing more retrieved text into prompts often plateaus or hurts RAG quality due to noise and distraction. Figure 1 (left) and related discussion in Section 1 highlight that standard RAG plateaus around 10^4 tokens; retrieving beyond soft thresholds (e.g., topâ€‘10) can degrade answers.
  - Two practical questions remain unanswered:
    1) If we scale inference compute wisely, how much can RAG actually improve?
    2) Given a testâ€‘time compute budget, can we predict the best way to spend it?

- Why this matters
  - Knowledgeâ€‘intensive applications (search assistants, enterprise question answering, analytics) depend on RAG quality. If testâ€‘time compute can buy reliable gains, teams can trade money/latency for accuracy with confidence.
  - Theoretically, a scaling law at inference time (not just model size) clarifies the role of context and retrieval for LLMs.

- Shortcomings of prior approaches
  - Prior â€œinference scaling for RAGâ€ mainly means â€œretrieve more/longer documentsâ€ (Related Work, Section 2.3), which:
    - Increases recall but also injects noise; performance often plateaus or drops (Section 1; Figure 5b; Appendix A Figure 7).
    - Doesnâ€™t teach the model how to use the extra information.

- How this paper positions itself
  - It expands the scaling dimensions beyond â€œmore documentsâ€ to include â€œmore demonstrationsâ€ and â€œmore generation steps,â€ and introduces:
    - `DRAG`: manyâ€‘shot inâ€‘context RAG with demonstrations that themselves include retrieved documents (Section 3.2).
    - `IterDRAG`: iterative query decomposition with interleaved retrieval and answering (Selfâ€‘Ask style), letting the model fetch targeted evidence for each subâ€‘question (Section 3.3).
    - A computationâ€‘allocation model that predicts the optimal mix of retrieval (`k` docs), examples (`m` shots), and iterations (`n`) for a given token budget, and explains the observed â€œinference scaling lawsâ€ (Sections 4â€“5).

## 3. Technical Approach
Key terms (defined once as used here):
- `RAG`: Retrievalâ€‘Augmented Generationâ€”retrieve external text passages and condition the LLMâ€™s answer on them.
- `Inâ€‘context learning (ICL)`: teach the model the task by showing inputâ€“output examples in the prompt.
- `Effective context length`: the total number of input tokens the LLM processes across all inference steps before the final answer (Section 3.1). Output tokens and retrieval compute are excluded.
- `Selfâ€‘Ask`: a prompting format where the model alternates between â€œFollow up:â€ subâ€‘questions and â€œIntermediate answer:â€ steps before â€œSo the final answer is:â€ (Section 3.3).

Stepâ€‘byâ€‘step:

1) Measuring and budgeting testâ€‘time compute
- The paper treats inference compute as a budget `L_max` of input tokens (â€œeffective context lengthâ€). For multiâ€‘step methods, it sums tokens across steps (Section 3.1).
- It then asks: given `L_max`, what configuration `Î¸` = (`k` documents per step, `m` inâ€‘context examples, `n` iterations) maximizes average task performance? This is formalized in Equation (1):
  > Maximize the average metric P over dataset X by searching `Î¸` subject to `l(x_i; Î¸) â‰¤ L_max` for all examples.

2) `DRAG`: Demonstrationâ€‘based RAG (Section 3.2)
- What it is:
  - A oneâ€‘call method (`n=1`) that combines: many retrieved documents (`k`) + many demonstrations (`m`). Each demonstration contains its own retrieved documents, a question, and its answer.
- How it works:
  - For each demonstration and the test query, a retriever selects topâ€‘`k` documents from a large corpus (Wikipedia, via Geckoâ€‘1B embeddings; Implementation, Section H).
  - Documents are reversed so higherâ€‘ranked items appear closer to the question (a known prompt ordering trick; Section 3.2).
  - The prompt includes many such â€œcontextâ€“questionâ€“answerâ€ demonstrations, followed by the test query and its retrieved context (Figure 15 and the prompt in Figure 16).
  - The LLM (Gemini 1.5 Flash) answers in a single pass. Increasing `k` and/or `m` scales compute up to the modelâ€™s context window (1M for Flash).

3) `IterDRAG`: Iterative demonstrationâ€‘based RAG (Section 3.3)
- What it is:
  - A multiâ€‘call method (`n > 1`) that learns to decompose the user query into subâ€‘queries. For each subâ€‘query, it retrieves more documents and produces an intermediate answer, then combines everything to produce the final answer (Figure 3 right; prompt in Figure 17).
- How it works:
  - Training the inâ€‘context format: they synthesize demonstrations that include subâ€‘queries and intermediate answers using constrained decoding to the Selfâ€‘Ask format (Section 3.3).
  - Inference loop (details in Section 3.3):
    1. Start with initial retrieved documents for the main question plus the demonstration set.
    2. The model emits either a â€œFollow up:â€ subâ€‘query, an â€œIntermediate answer:â€, or a final answer.
    3. If a subâ€‘query appears, the system retrieves additional documents and appends them to the running context.
    4. Repeat up to 5 iterations, after which the model must produce the final answer.
  - Compute scales with the number of iterations plus the extra retrieved context per step, so IterDRAG can exceed a single context window via multiple calls.

4) Finding optimal performance for a budget (Section 4.1)
- For each `L_max` in {16k, 32k, 128k, 1M, 5M}, the paper gridâ€‘searches `k`, `m`, and (for IterDRAG) `n` to find the best achievable accuracy on each datasetâ€”this gives the â€œoptimal performanceâ€ `P*(L_max)` used to characterize scaling (Table 1; Figure 4).

5) Modeling how to allocate compute (Section 5)
- Goal: Predict performance as a function of `Î¸` and identify the optimal `Î¸` under a budget without exhaustive search.
- Model (Equation (2)):
  - Transform the metric `P` by an inverse sigmoid `Ïƒ^-1` to account for mild saturation at very long contexts (>1M tokens; Section 4.3), then fit a linear model in logâ€‘space:
    > `Ïƒ^-1(P(Î¸)) â‰ˆ (a + b âŠ™ i)^T log(Î¸) + c`
  - `Î¸ = (k, m, n)`. `i = (i_doc, i_shot, 0)` measures taskâ€‘specific informativeness of documents vs. examples, computed from simple base configurations (Section 5.1).
  - `a, b, c` are learned per LLM (estimated by ordinary least squares, parameters reported in Appendix F Table 8). This separates modelâ€‘level behavior (`a, b, c`) from taskâ€‘level informativeness (`i`).
- Use: once fitted, the model predicts the best mix of `k, m, n` for a given `L_max` and task (Sections 5.2, Tables 3â€“4).

6) System components and data (Implementation, Section H)
- Retriever: Geckoâ€‘1B embeddings on Wikipedia (KILT) with rightâ€‘truncation to 1024 tokens per document.
- LLM: Gemini 1.5 Flash (1M token window). For >1M effective tokens, IterDRAG uses multiple calls.
- Prompt construction: demonstrations then test documents then test query; reverse ordering of retrieved documents; constrained decoding for Selfâ€‘Ask in IterDRAG.

## 4. Key Insights and Innovations
1) A practical â€œinference scaling lawâ€ for RAG
- Whatâ€™s new: When compute is optimally allocated across documents, demonstrations, and iterations, RAG accuracy improves almost linearly with the order of magnitude of effective context length (Sections 4.2â€“4.3).
- Evidence:
  - Figure 1 (right) and Figure 4: red dots (optimal configs) lie close to a straight line in logâ€‘scale for DRAG and IterDRAG; standard RAG plateaus early (~10^4 tokens).
  - Table 1: as the budget grows from 16k â†’ 1M â†’ 5M tokens, DRAG and especially IterDRAG keep improving, while baselines saturate.

2) Two complementary scaling strategiesâ€”`DRAG` and `IterDRAG`
- Whatâ€™s new: The paper shows that manyâ€‘shot ICL (DRAG) and iterative retrieval with query decomposition (IterDRAG) are both effective, but at different scales (Section 4.2).
  - DRAG dominates at smaller budgets (â‰¤32k).
  - IterDRAG takes over at larger budgets (â‰¥128k) by adding multiâ€‘step retrieval and reasoning.
- Significance: It turns extra tokens into guided computation rather than just more noise (Figures 5, 8).

3) A simple, predictive computationâ€‘allocation model
- Whatâ€™s new: A logâ€‘linear model (Equation (2)) with a taskâ€‘informativeness vector `i` that predicts RAG performance and optimal hyperparameters under a budget (Section 5).
- Why it matters:
  - High fit quality: `R^2 = 0.903`, `MSE = 0.085` for the full model with sigmoidal scaling (Table 2).
  - Generalizes across domains and lengths:
    - Domain generalization reaches 96.6% of oracle performance at 1M tokens (Table 3).
    - Length extrapolation is accurate up to 1M tokens (average 2.8% gap from 128k â†’ 1M; Table 4).

4) Iterative retrieval improves evidence quality, not just quantity
- Whatâ€™s new: IterDRAGâ€™s interleaved retrieval boosts ranking quality (not only recall), addressing a major pain point of longâ€‘context RAG (Appendix A Table 5).
- Evidence:
  > On 2WikiMultiHopQA with 50 docs and 2 shots, NDCG improves from 0.421 (DRAG) to 0.605 (IterDRAG), and MRR from 0.336 to 0.528.

## 5. Experimental Analysis
- Setup (Section 4.1 + Implementation H)
  - LLM: Gemini 1.5 Flash (1M token context).
  - Retriever: Geckoâ€‘1B over Wikipedia (KILT).
  - Budgets `L_max`: 16k, 32k, 128k, 1M, 5M tokens.
  - Search space: `k âˆˆ {0,1,2,5,10,20,50,100,200,500,1000}`, `m âˆˆ {0, 1, 2, 4, 8, 16, 32, 64, 128, 256}`, `n â‰¤ 5`.
  - Datasets: Multiâ€‘hop QAâ€”Bamboogle, HotpotQA, MuSiQue, 2WikiMultiHopQA; plus oneâ€‘hop TriviaQA and Natural Questions, and binary StrategyQA (Sections 4.1 and Appendix C).
  - Metrics: `EM` (exact match), `F1`, and `Acc` (whether the groundâ€‘truth string appears in the modelâ€™s prediction; Section 4.1).

- Baselines (Section 4.1):
  - `Zero-shot QA` (no retrieval, no demos),
  - `Many-shot QA` (demos only),
  - `RAG` (retrieval only).

- Main quantitative results
  - Scaling behavior
    - Figure 4: For both DRAG and IterDRAG, the optimal points (red dots) align with a nearâ€‘linear trend as effective context grows; standard RAG curves flatten early.
  - Endâ€‘toâ€‘end accuracy (Table 1)
    - At 128k tokens (typical long context):
      - On 2WikiMultiHopQA, `Acc`: RAG 48.4 â†’ DRAG 53.1 â†’ IterDRAG 74.6.
      - On Bamboogle, `Acc`: RAG 52.8 â†’ DRAG 54.4 â†’ IterDRAG 68.8.
      - On MuSiQue, `Acc`: RAG 16.8 â†’ DRAG 17.9 â†’ IterDRAG 24.5.
    - At 1M tokens:
      - On 2WikiMultiHopQA, `Acc`: DRAG 53.3 â†’ IterDRAG 76.4.
      - On MuSiQue, `Acc`: DRAG 18.2 â†’ IterDRAG 30.5.
    - At 5M tokens (via multiple IterDRAG steps):
      - On 2WikiMultiHopQA, `Acc` rises to 76.9; on HotpotQA, `Acc` to 56.4.
  - Takeaway:
    - DRAG is strongest at 16kâ€“32k; IterDRAG overtakes beyond 128k (Section 4.2).
    - The paper reports â€œup to 58.9% gains over standard RAGâ€ when optimally scaling compute (Abstract and Figure 2 summary).

- Parameterâ€‘specific insights (Section 4.4; Figure 5)
  - Documents vs. shots:
    - For DRAG, increasing documents `k` typically yields larger gains than increasing shots `m` (Figure 5b vs. 5c).
    - For IterDRAG, adding just one shot (`m: 0 â†’ 1`) often helps more than adding one document, because demonstrations teach query decomposition and evidence use.
  - Saturation and soft thresholds:
    - Gains diminish or reverse beyond certain `k`/`m` levels due to noise (Figure 5aâ€“c).

- Retrieval quality analysis (Appendix A)
  - More documents improve recall but not ranking quality; NDCG/MRR plateau around ~100 documents (Appendix A Figure 7).
  - IterDRAGâ€™s interleaved retrieval improves all retrieval metrics relative to oneâ€‘shot DRAG (Appendix A Table 5), e.g. for 2WikiMultiHopQA recall 0.722 â†’ 0.935; NDCG 0.421 â†’ 0.605.

- Comparison to chainâ€‘ofâ€‘thought (Appendix B, Table 6)
  - IterDRAG outperforms a Selfâ€‘Askâ€‘style CoT without interleaved retrieval:
    > On 2WikiMultiHopQA (k=5, m=4), `Acc`: CoT 36.7 vs. IterDRAG 72.3.

- Predictive model validation (Section 5.2)
  - Fit quality and ablations (Table 2):
    > Full model with sigmoidal scaling: `R^2 = 0.903`, `MSE = 0.085`; removing taskâ€‘informativeness term (`b âŠ™ i`) reduces `R^2` to 0.866.
  - Domain generalization (Table 3):
    > At 1M tokens, predicted configs achieve 96.6% of oracle performance; e.g., on 2WikiMultiHopQA `Acc` 76.4 (oracle) vs. 74.9 (predicted).
  - Length extrapolation (Table 4):
    > From 128k â†’ 1M tokens, predicted `Acc` is within 2.8% of oracle on average; 1M â†’ 5M is harder (5.6% gap).

- Failure analysis and robustness (Appendix G)
  - Four error types:
    1) Inaccurate/outdated retrieval,
    2) Incorrect or missing reasoning,
    3) Hallucination/unfaithful reasoning,
    4) Evaluation issues/refusals.
  - IterDRAG reduces (1) and (2) by targeting subâ€‘queries with fresh retrieval (Appendix G narrative).

- Do experiments support the claims?
  - Yes, across multiple datasets and budgets:
    - Nearâ€‘linear optimal scaling appears consistently (Figures 1, 4, 11).
    - DRAG and IterDRAG outperform baselines at their respective scales (Table 1, Figure 2).
    - The computeâ€‘allocation model predicts well and generalizes (Tables 2â€“4).
  - Caveat: improvements beyond ~1M tokens are smaller (Section 4.3), indicating remaining longâ€‘context limits.

## 6. Limitations and Trade-offs
- Dependence on retrieval quality
  - Recall improves with more documents, but NDCG/MRR saturate and noise grows (Appendix A Figure 7). Without reâ€‘ranking/filtering, adding documents can distract the model (Section 4.4).
- Longâ€‘context modeling limits
  - Gains become subâ€‘linear or plateau beyond ~1M tokens (Section 4.3). DRAGâ€™s perâ€‘step context seems to peak around 10^5 tokens, while IterDRAG benefits by spreading compute across steps (Discussion: Longâ€‘Context Modeling).
- Budget definition and realâ€‘world costs
  - Compute budget excludes output tokens and retrieval cost (Section 3.1). In production, retrieval latency/compute and outputâ€‘length constraints may matter.
- Demonstration quality and format
  - IterDRAG requires demonstrations formatted with Selfâ€‘Ask; the paper uses constrained decoding to create them (Section 3.3). Quality of these demos affects performance.
- Model and data scope
  - Most experiments use Gemini 1.5 Flash and Geckoâ€‘1B retriever; while Appendix D shows similar scaling with GTRâ€‘XXL, broader crossâ€‘model validation would strengthen generality.
- Evaluation scale and metrics
  - Datasets are subâ€‘sampled (1.2k per dataset; Section 4.1). â€œAccuracyâ€ metric is liberal (checks if ground truth string appears in output; Section 4.1), which favors verbose outputs; exact match still remains modest on harder datasets (Table 1).

## 7. Implications and Future Directions
- What changes for the field
  - Testâ€‘time compute becomes a reliable knob for RAG: with the right allocation policy, you can â€œbuyâ€ accuracy with tokens up to ~1M effective tokens (Figures 1, 4).
  - Iterative retrieval and manyâ€‘shot demonstrations are not just trainingâ€‘time ideasâ€”they are scalable inference strategies that unlock longâ€‘context LLMs.

- Practical guidance (from results throughout Sections 4â€“5)
  - Small budgets (â‰¤32k): prefer `DRAG` with moderate `k` and a handful of `m`.
  - Medium budgets (~128k): start transitioning to `IterDRAG` to benefit from interleaved retrieval.
  - Large budgets (â‰¥1M): rely on `IterDRAG` with multiple steps; expect diminishing returns above ~1M.
  - Documents vs. shots: for singleâ€‘step DRAG, prioritize raising `k`; for IterDRAG, even 1â€“2 demonstrations can markedly help query decomposition (Figure 5).
  - Use the computationâ€‘allocation model (Equation (2)) to predict `k, m, n` under a specified token budget and task (Tables 3â€“4).

- Followâ€‘on research
  - Retrieval refinement at scale: pair IterDRAG with learned reâ€‘rankers or selective reading to curb noise (Appendix A and Discussion).
  - Stronger longâ€‘context representations: improve modelsâ€™ ability to locate relevant spans in 10^5â€“10^6â€‘token contexts (Discussion: Longâ€‘Context Modeling).
  - Dynamic, perâ€‘query allocation: learn policies that adapt `k, m, n` online using uncertainty or earlyâ€‘exit signals, building on the allocation model.
  - Better evaluation: beyond EM/F1, add faithfulness and stepâ€‘level reasoning checks to diagnose hallucination and unfaithful chains (Appendix G).

- Downstream applications
  - Enterprise search and analytics, scientific assistants, educational tutors: anywhere RAG is used and latency/compute budgets vary, the approach offers a principled way to spend extra tokens for predictable gains.

> In short, the paper provides both an engineering recipe (DRAG/IterDRAG) and a planning tool (the computationâ€‘allocation model) to convert longâ€‘context budgets into steady, nearâ€‘linear improvements for RAG up to about one million effective tokens (Figures 1 and 4; Table 1), with validated predictions across domains and lengths (Tables 3â€“4).

# Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention

**ArXiv:** [2502.11089](https://arxiv.org/abs/2502.11089)
**Authors:** Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y.â€¯X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng
**Institutions:** DeepSeek (Peking University), Peking University, University of Washington

## ğŸ¯ Pitch

Native Sparse Attention (NSA) introduces a revolutionary, sparse attention mechanism designed to align with modern GPU architectures, transforming theoretical speed improvements into true performance gains. This method significantly reduces computational demands of long-context models, enabling efficient training and execution without compromising accuracy, thereby making advanced natural language processing applications feasible and cost-effective in real-world scenarios.

---

## 1. Executive Summary
The paper proposes NSA (Native Sparse Attention), a new attention architecture that is both endâ€‘toâ€‘end trainable and engineered to align with modern GPU hardware. It replaces full quadratic attention with a hierarchical, queryâ€‘dependent sparse mechanism that combines block compression, block selection, and a local sliding window, then implements custom Triton kernels to realize the theoretical savings in practice. NSA matches or exceeds fullâ€‘attention models on general, longâ€‘context, and reasoning benchmarks while delivering substantial speedups, especially at long sequence lengths (e.g., up to 11.6Ã— in decoding at 64k tokens; Figure 1, right; Table 4).

## 2. Context and Motivation
- Problem addressed
  - Longâ€‘context modeling is computationally expensive because standard attention scales quadratically with sequence length. At 64k contexts, attention dominates latency (estimated 70â€“80% of total latency in decoding; Section 1).
  - Many sparse attention methods reduce theoretical operations but fail to translate those savings into real wallâ€‘clock speedups, and most are not designed for (or compatible with) training (Sections 2.1â€“2.2).

- Why it matters
  - Real systems increasingly need long contexts for repositoryâ€‘level code, multiâ€‘turn agents, and extended reasoning (Section 1). If attention remains the bottleneck, these capabilities are impractical due to cost/latency.

- Shortcomings of prior approaches (Section 2)
  - Phaseâ€‘restricted sparsity: some methods speed up only prefilling (processing an input prompt to build the keyâ€‘value cache) or only decoding (generating token by token), leaving the other phase near fullâ€‘attention cost (Section 2.1).
  - Incompatibility with modern architectures: newer decodingâ€‘efficient designs such as `GQA` (Groupedâ€‘Query Attention) and `MQA` (Multipleâ€‘Query Attention) share key/value caches across heads. Several sparse methods pick different tokens per head, which forces loading the union of all headsâ€™ selections, negating memory benefits (Section 2.1).
  - â€œTrainabilityâ€ myths: approaches with discrete operations (e.g., kâ€‘means, hashing) are not differentiable, so gradients cannot shape selection; or they require tokenâ€‘granular random memory access that breaks fast attention kernels in training (Section 2.2).

- Positioning
  - NSA is designed â€œnatively sparseâ€ (sparsity present throughout pretraining and downstream training) and â€œhardwareâ€‘alignedâ€ (blockwise, contiguous memory access; groupâ€‘wise KV sharing) so theoretical savings convert to real speedups in all phases: prefilling, decoding, and backpropagation (Sections 3â€“4; Figures 1 and 6).

## 3. Technical Approach
NSA restructures attention around three parallel branches and a gating mechanism (Figure 2). For a query at position `t`, NSA replaces attending to all previous tokens with three compact representations of the past:

- Definitions (as used here)
  - `KV cache`: stored keys (`k`) and values (`v`) from preceding tokens used during decoding to avoid recomputation.
  - `Prefilling`: processing the input context to fill the KV cache before generation.
  - `Arithmetic intensity`: compute operations per byte transferred; higher values are computeâ€‘bound, lower ones are memoryâ€‘bound (Section 3.1).

Stepâ€‘byâ€‘step (Sections 3.2â€“3.3; Equations 1â€“12; Figure 2):

1. Start from standard attention
   - Equation (1)â€“(2) defines normal attention: the output for query `q_t` is a normalized weighted sum over all past values `v_1:t`, with weights from dotâ€‘products between `q_t` and keys `k_1:t`.

2. Replace the full key/value set with a compact, queryâ€‘specific set
   - NSA constructs â€œremappedâ€ keys/values `KÌƒ_t`, `VÌƒ_t` tailored to query `q_t` (Eq. 3), then computes attention over them (Eq. 4) rather than all tokens.
   - It combines multiple remappings `c âˆˆ {cmp, slc, win}` via a learned gate (Eq. 5), where `g_t^c âˆˆ [0,1]` weights each branchâ€™s contribution.

3. Three branches (C = {cmp, slc, win})
   - Compression (`cmp`): Coarseâ€‘grained summaries
     - Partition the past into overlapping blocks of length `l` with stride `d` (typically `d < l` to reduce information loss).
     - Each block is mapped to a single compressed key (and value) via a learnable MLP `Ï†` that can include intraâ€‘block positional encoding (Eq. 7). The result is a much shorter list of compressed tokens `KÌƒ_cmp`, `VÌƒ_cmp`.
     - Intuition: a quick global scan; cheap to compute, preserves blockâ€‘level semantics.

   - Selection (`slc`): Fineâ€‘grained tokens only from important blocks
     - Goal: recover fine detail lost by compression but only where it matters.
     - Compute attention scores between `q_t` and compressed keys: `p_cmp = Softmax(q_t^T KÌƒ_cmp)` (Eq. 8). These scores indicate which coarse blocks are relevant.
     - Translate those coarse scores to the selection blocksâ€™ scheme. If block sizes differ (`l` for compression; `l'` for selection), aggregate based on spatial overlap (Eq. 9). For `GQA/MQA`, sum importance across query heads in the same group to force a common selection and avoid redundant memory reads (Eq. 10).
     - Pick the topâ€‘`n` most important selection blocks (Eq. 11) and concatenate all tokens from them to form `KÌƒ_slc`, `VÌƒ_slc` (Eq. 12). The method also includes a small fixed set (e.g., the very first block and a couple of local blocks).
     - Intuition: compression branch cheaply proposes â€œwhere to lookâ€; selection branch zooms in there with full token granularity.

   - Sliding window (`win`): Always keep the most recent local context
     - Keep the latest `w` tokens (`KÌƒ_win = k_{t-w:t}`, `VÌƒ_win = v_{t-w:t}`), capturing strong local dependencies (Section 3.3.3).
     - NSA uses separate key/value projections per branch to avoid â€œshortcut learningâ€ where everything routes through the easy local branch. The three outputs are then combined with learned gates `g_t^c` (Eq. 5).

4. Hardwareâ€‘aligned kernel design (Section 3.4; Figure 3)
   - Why needed: training/prefilling are computeâ€‘bound, decoding is memoryâ€‘bound (Section 3.1). Speed requires contiguous, blockwise memory access with high Tensor Core utilization.
   - Custom Triton kernels for selection branch:
     - Groupâ€‘centric query loading: for each time step, load all query heads in a `GQA` group together so they share the same sparse KV blocks (reduces redundant KV fetches).
     - Shared KV fetching: load selected blocks contiguously into onâ€‘chip SRAM, process them there, then move outputs back to HBM.
     - Grid scheduling: outer loops over query positions on Tritonâ€™s grid; inner loops iterate over the contiguous selected KV blocks. This balances workloads across streaming multiprocessors.
   - Compression and slidingâ€‘window branches reuse FlashAttentionâ€‘2 style kernels since they access contiguous blocks naturally.

5. Design choices and rationale
   - Blockwise (not tokenwise) sparsity: matches GPU memory systems and Tensor Cores; also aligns with observed attention â€œblock clusteringâ€ (Figure 8).
   - Use compressedâ€‘attention scores to drive selection: avoids extra indexing networks or discrete, nonâ€‘differentiable preprocessing; keeps selection computation cheap (Eqs. 8â€“10).
   - Groupâ€‘wise selection for `GQA/MQA`: preserves their decoding advantages by preventing perâ€‘head scatter/gather (Section 2.1, Section 3.3.2).
   - Separate projections per branch + gating: reduce interference and stabilize training in the presence of a strong local prior (Section 3.3.3).

Hyperparameters in the main experiments (Section 4.1): compression block size `l=32`, stride `d=16`; selection block size `l'=64`, number of selected blocks `n=16` (with the first block and two local blocks always active); sliding window `w=512`. The backbone is a 27Bâ€‘parameter MoE transformer with 3B active params, `G=4` GQA groups, 64 heads total (`d_k=192`, `d_v=128`), trained on ~270B tokens at 8k then extended to 32k with YaRN (Section 4.1; Figure 4).

## 4. Key Insights and Innovations
1. Hierarchical sparse attention that is both global and local (Figure 2; Eqs. 5â€“12)
   - Whatâ€™s new: jointly using compressed coarse tokens to guide a fineâ€‘grained block selection, plus an explicit local window, with learned gating across branches.
   - Why it matters: preserves longâ€‘range awareness and tokenâ€‘level precision while remaining cheap enough to train endâ€‘toâ€‘end. This contrasts with many prior methods that either rely on fixed local windows or perform queryâ€‘aware selection without a cheap global scan.

2. Hardwareâ€‘aligned blockwise design with groupâ€‘wise KV sharing (Section 3.4; Figure 3)
   - Whatâ€™s new: selection is enforced at the GQA group level, and kernels load contiguous KV blocks into SRAM per group. This achieves high arithmetic intensity and avoids scattered memory access that kills decoding throughput.
   - Why it matters: turns theoretical sparsity into actual wallâ€‘clock speedups across forward, backward, and decoding. Many previous methods claimed FLOP reductions but lost the gains to memory and scheduling overhead (Section 2.1).

3. Native trainability without fragile auxiliary objectives (Sections 3.3â€“3.4; 6.1)
   - Whatâ€™s new: block importance comes â€œfor freeâ€ from compressedâ€‘attention scores (Eq. 8) rather than from separate predictors with extra losses or nonâ€‘differentiable algorithms (e.g., kâ€‘means, hashing).
   - Why it matters: enables fullâ€‘model pretraining with the sparse mechanism itself, avoiding the mismatch of training with full attention and pruning only at inference (Section 2.2). Figure 4 shows stable convergence with lower loss than the fullâ€‘attention baseline; Figure 7 shows alternative selection strategies have worse training loss on a 3B model.

4. Arithmeticâ€‘intensity awareness across phases (Section 3.1; Section 5)
   - Whatâ€™s new: the method and kernels are designed differently for training/prefilling (computeâ€‘bound) and decoding (memoryâ€‘bound). Table 4 connects reduced KV loads directly to expected decoding speedups.
   - Why it matters: yields increasing speedups with longer sequencesâ€”precisely where attention becomes a bottleneck.

## 5. Experimental Analysis
- Evaluation setup (Sections 4, 5)
  - Backbone: 27B MoE transformer (3B active), `GQA`, 30 layers, hidden size 2560, heads=64, `d_k=192`, `d_v=128`; MoE uses 72 experts (topâ€‘k=6) with first layer replaced by SwiGLU for stability (Section 4.1).
  - Training data: ~270B tokens at 8k context, then longâ€‘context adaptation at 32k with YaRN. NSA hyperparameters as above (Section 4.1).
  - Baselines: Full attention; inferenceâ€‘only sparse baselines H2O, InfLLM, Quest; and an â€œExactâ€‘Topâ€ upper bound that selects exact topâ€‘n tokens after computing full scores (Sections 4.2â€“4.3).
  - Kernels: NSA implemented in Triton and compared against Tritonâ€‘based FlashAttentionâ€‘2 (Section 5.1).

- General benchmark results (Table 1)
  - Benchmarks: MMLU, MMLUâ€‘PRO, CMMLU (knowledge); BBH, GSM8K, MATH, DROP (reasoning); MBPP, HumanEval (coding).
  - Summary: NSA outperforms the fullâ€‘attention baseline on 7/9 metrics; improvements include DROP (+0.042 F1: 0.545 vs. 0.503) and GSM8K (+0.034: 0.520 vs. 0.486). Average score improves from 0.443 to 0.456.
  - Interpretation: Despite heavy sparsity, NSA retains or improves capability; the hierarchical mechanism appears to help the model focus on salient information (Section 4.3).

- Longâ€‘context evaluation (Figure 5; Table 2)
  - Needleâ€‘inâ€‘aâ€‘Haystack (64k): NSA achieves perfect retrieval accuracy across all positions (Figure 5).
  - LongBench: To equalize sparsity, each method receives a 2560â€‘token budget including 128 leading and 512 local tokens (Section 4.3). NSA achieves the highest average (0.469) vs Full Attention (0.437) and Exactâ€‘Top (0.423). Notable gains: HPQ +0.087 (0.437 vs. 0.350), 2Wiki +0.051 (0.356 vs. 0.305), Passage Retrieval EN +0.075 (0.905 vs. 0.830), LCC (code) +0.069 (0.232 vs. 0.163).
  - Interpretation: The combination of compressed scanning and targeted selection preserves global recall and local precision better than fixed or heuristic patterns.

- Chainâ€‘ofâ€‘thought reasoning after SFT (Table 3)
  - Setup: Distillationâ€‘based SFT from DeepSeekâ€‘R1 on 10B tokens of 32k math traces; compare NSAâ€‘R vs Full Attentionâ€‘R on AIME â€™24 with 16 samples per question (temperature 0.7, topâ€‘p 0.95).
  - Results: NSAâ€‘R surpasses Full Attentionâ€‘R at 8k (0.121 vs. 0.046) and 16k (0.146 vs. 0.092).
  - Interpretation: NSAâ€™s sparse patterns do not hinder, and may even aid, extended reasoning sequences.

- Speed and efficiency (Figures 1, 6; Table 4)
  - Training/prefilling speed (Figure 6):
    - Forward speedups grow with context: 2.1Ã— (8k), 3.8Ã— (16k), 6.3Ã— (32k), 9.0Ã— (64k).
    - Backward speedups: 1.1Ã— (8k), 2.0Ã— (16k), 3.4Ã— (32k), 6.0Ã— (64k).
  - Decoding speed (Figure 1 right; Table 4):
    - KV tokens loaded per step: Full attention loads all tokens; NSA loads roughly â€œcompressed + selected + windowâ€.
    - Example expected speedups (Table 4): 4.0Ã— (8k), 6.4Ã— (16k), 9.1Ã— (32k), 11.6Ã— (64k).
  - Conclusion: NSA realizes substantial endâ€‘toâ€‘end speedups, especially at long sequences where attention is the bottleneck.

- Ablations/diagnostics (Section 6)
  - Alternative selection strategies:
    - Auxiliaryâ€‘lossâ€‘based block predictors and parameterâ€‘free heuristics (Questâ€‘style) both underperform NSA in training loss on a 3B model (Figure 7).
  - Attention visualization:
    - Fullâ€‘attention maps show blockwise clusteringâ€”nearby keys often share importance (Figure 8), justifying blockwise selection.

Overall, the experiments support the core claims: NSA maintains or improves accuracy while drastically reducing computation and memory traffic in all phases.

## 6. Limitations and Trade-offs
- Discrete selection remains nonâ€‘differentiable
  - The topâ€‘`n` block choice (Eq. 11) is a hard selection; gradients do not flow through the indices. NSA mitigates this by deriving block scores from a differentiable compressed branch (Eq. 8), but the selection threshold itself is not learned. This could, in principle, reduce adaptability around decision boundaries.

- Dependence on hardwareâ€‘aligned assumptions
  - Gains rely on blockwise, contiguous memory and `GQA/MQA` KV sharing (Section 3.4). On architectures without strong Tensor Core performance or with different memory hierarchies, speedups may diminish.

- Perâ€‘group shared selection vs. perâ€‘head specialization
  - Enforcing the same sparse blocks across all heads in a `GQA` group (Eq. 10) minimizes memory traffic but may limit diversity among heads within that group.

- Hyperparameter sensitivity and engineering overhead
  - Performance depends on block sizes (`l`, `l'`), stride `d`, selected block count `n`, and window `w` (Section 4.1). The paper gives strong defaults but limited exploration of the full tradeâ€‘off surface.
  - Custom Triton kernels and careful scheduling are required; portability to other accelerators/backends may require reâ€‘engineering.

- Sequence regimes where benefits shrink
  - At short contexts (e.g., 8k), speedups are smaller (Figure 6), and kernel overheads can reduce relative gains.

- Reporting discrepancies and external validity
  - The abstract mentions 260B tokens, while Section 4.1 mentions ~270B. Also, most results are on a single 27B MoE backbone; broader scaling and crossâ€‘model validation would strengthen generality.

## 7. Implications and Future Directions
- How this changes the landscape
  - NSA demonstrates that sparse attention can be made native to training and aligned with hardware, delivering both capability and speed. This reduces the longâ€‘context cost barrier and encourages training models that truly learn to use sparse patterns.

- Practical applications
  - Longâ€‘document question answering, repositoryâ€‘level code understanding, multiâ€‘turn agents, and any application that needs 32kâ€“64k (or longer) contexts with reasonable latency/cost. The perfect retrieval on 64k Needleâ€‘inâ€‘aâ€‘Haystack (Figure 5) and strong LongBench/code results (Table 2) are particularly encouraging.

- Research directions
  - Differentiable or soft block selection: relax the topâ€‘`n` step to enable gradient flow through selection while keeping hardware efficiency (e.g., sparseâ€‘continuous relaxations that still map to contiguous blocks).
  - Adaptive group granularity: explore dynamic grouping that balances memory sharing with head diversity.
  - Learned compression operators: investigate richer block encoders than an MLP `Ï†` (Eq. 7), including crossâ€‘block context or lightweight attention inside blocks.
  - Crossâ€‘hardware portability: replicate kernels on H100, AMD GPUs, and specialized accelerators; study the impact of memory hierarchies and scheduling.
  - Taskâ€‘aware sparsity curricula: curriculum schedules that adjust `n`, `l`, or `w` during training to progressively bias the model toward more efficient patterns without hurting accuracy.

In sum, NSA offers a concrete path for bringing sparse attention from theoretical complexity reductions to practical endâ€‘toâ€‘end speedups, without sacrificing accuracyâ€”and sometimes improving itâ€”on the tasks that matter for longâ€‘context language models.

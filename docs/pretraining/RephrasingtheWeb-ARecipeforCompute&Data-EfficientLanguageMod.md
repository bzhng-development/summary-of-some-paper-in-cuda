# Rephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling

**ArXiv:** [2401.16380](https://arxiv.org/abs/2401.16380)

## ğŸ¯ Pitch

This paper introduces WRAP (Web Rephrase Augmented Pre-training), a straightforward yet powerful approach that leverages an instruction-tuned language model to automatically rephrase noisy web data into cleaner, evaluation-aligned styles (like Wikipedia or QA format), and then pre-trains large language models on a balanced mix of real and rephrased text. WRAP dramatically boosts training efficiencyâ€”cutting the compute and data needed by up to 3Ã— and improving both generalization and zero-shot performanceâ€”without sacrificing the webâ€™s broad knowledge or incurring the cost and bias risks of fully-synthetic corpora. This signals a practical path toward faster, cheaper, and more robust language model pre-training as quality data becomes scarce and compute budgets tighten.

---

## 1. Executive Summary
This paper introduces WRAP (Web Rephrase Augmented Preâ€‘training), a simple data recipe: use an instructionâ€‘tuned language model to rephrase web pages into cleaner, taskâ€‘aligned styles (e.g., â€œWikipediaâ€‘likeâ€ or â€œquestionâ€“answerâ€), then preâ€‘train a model on a 1:1 mixture of original and rephrased text. Across 1.3Bâ€‘parameter models trained up to 300B tokens, WRAP cuts the compute or data needed for a given quality by roughly 3Ã—, reduces perplexity on outâ€‘ofâ€‘distribution corpora by up to ~50%, and improves zeroâ€‘shot QA accuracy by ~2â€“3% on average (Figures 1bâ€“c, 2; Tables 1â€“2).

## 2. Context and Motivation
- Problem addressed
  - Web scrapes (e.g., Common Crawl) are noisy, unstructured, and rarely formatted like downstream evaluations (e.g., QA). Standard practice is to preâ€‘train on this data, often with handcrafted filters, which still leaves style and quality mismatches.
  - Scaling laws (Chinchilla) suggest linearly increasing both compute and data with model size. This is becoming impractical due to cost and the scarcity of highâ€‘quality data (Introduction; Related Work).
- Why it matters
  - Preâ€‘training dominates the cost of building LLMs. If the same performance can be achieved with fewer tokens or steps, the savings are substantial.
  - Matching preâ€‘training data style to evaluation (e.g., QA format) could improve zeroâ€‘shot generalization without expensive instructionâ€‘tuning (Introduction; Section 3.1).
- Prior approaches and gaps
  - Data filtering and curated mixtures help but are opaque and often proprietary (Related Work).
  - Synthetic corpora (e.g., â€œtextbookâ€‘qualityâ€ or TinyStories) can work well for small models but are expensive to generate with large closed LLMs, may introduce topic/knowledge biases, and do not scale transparently (Related Work; Introduction).
- Positioning of this work
  - WRAP reuses the webâ€™s knowledge but modifies its style/clarity by rephrasing, avoiding topic selection bias and expensive â€œknowledge generation.â€ It uses smaller, open instruction models (e.g., `Mistralâ€‘7Bâ€‘Instruct`) to create synthetic data at lower cost while keeping the original information content (Sections 3.1, 6.2, 7.1).

## 3. Technical Approach
Stepâ€‘byâ€‘step overview (Section 3; Appendices Bâ€“C, G):
1. Source corpus
   - Use C4 (a cleaned Common Crawl subset; ~170B English tokens) as the main corpus to rephrase (Appendix A.1).
2. Rephrasing model and styles
   - Freeze an instructionâ€‘tuned model (default: `Mistralâ€‘7Bâ€‘Instruct`) and prompt it to rephrase each document chunk (â‰¤300 tokens) into one of four `styles` (Section 3.1; Appendix G):
     - `Easy`: very simple sentences and small vocabulary.
     - `Medium`: â€œhighâ€‘quality English like Wikipedia.â€
     - `Hard`: â€œterse and abstruseâ€ scholarly language.
     - `Q/A`: convert to â€œQuestion: â€¦ Answer: â€¦â€ conversational format.
   - 300â€‘token chunks are used because longer inputs led to information loss during rephrasing (Section 3.1).
   - Outputs are lightly filtered to remove generic prefaces (e.g., â€œHereâ€™s a paraphraseâ€¦â€), using sentence heuristics (Appendix B).
3. Training mixture (â€œWRAPâ€)
   - Preâ€‘train target LLMs on a 1:1 mixture of original C4 and synthetic rephrases sampled online (Section 3.1).
   - Rationale: retain exposure to â€œmessyâ€ realâ€‘web tokens (URLs, markup, typos) while gaining style diversity and clarity from rephrases (Section 3.1; Figures 2â€“3).
4. Models and optimization
   - Decoderâ€‘only GPTâ€‘style models at three sizes (Section 3.2):
     - `128M` (12 layers, 12 heads, d=768), `350M` (24L, 16H, d=1024), `1.3B` (24L, 16H, d=2048).
   - Train with Megatronâ€‘LM, context 1024, cosine LR schedule, Adam (Î²1=0.9, Î²2=0.999), weight decay 0.01, grad clip 1.0; batch size 1M tokens (Section 3.2).
   - Standard runs: 300k steps (=~300B tokens), unless otherwise noted (Section 3.2).
5. Evaluations and why not evaluate on C4 itself
   - Main generalization metric: perplexity on 21 Pile domains (first 10k documents per domain) rather than C4 (Section 4; Appendix A.2).
   - Reason: training on WRAP optimizes a different distribution from C4 alone (Equations (1) vs (2)), so C4 perplexity can increase slightly while generalization improves.
   - Zeroâ€‘shot QA: 13 tasks via the LMâ€‘Evaluationâ€‘Harness (Section 5.1; Tables 1â€“2).
6. Cost model (Section 7.1)
   - Rephrasing with `Mistralâ€‘7B` via vLLM yields ~3M tokens/hour per A100. Rephrasing 85B tokens â‰ˆ 25k GPU hours. Model training (1.3B for 300B tokens) on 64Ã—A100 at 0.5M tok/s â‰ˆ 6k GPU hours; for 13B, â‰ˆ30k GPU hours.
   - Throughput and speculative decoding can reduce generation cost; smaller rephrasers (e.g., `Qwenâ€‘1.8B`) run ~3Ã— faster with similar quality (Figure 5; Section 7.1).

How WRAP â€œworksâ€ conceptually
- It replaces â€œnoisyâ€ or mismatched styles with clear, evaluationâ€‘aligned styles while preserving the original information (semantic content). Evidence:
  - Cosine similarity of sentence embeddings between original and rephrased text is high (Figure 8aâ€“b; Appendix C.2), indicating content preservation.
  - Rephrases change readability and syntactic profiles (e.g., â€œMediumâ€ increases reading level and dependency depth closer to Wikipedia/academic text; â€œQ/Aâ€ lowers reading level, aligning with QA corpora) (Figures 10â€“11; Appendix C).

## 4. Key Insights and Innovations
- Rephrasing for style alignment, not knowledge synthesis
  - Novelty: instead of asking a large LLM to â€œgenerate new knowledge,â€ WRAP uses a smaller LLM to â€œrephraseâ€ existing web text into target styles. This keeps the topic distribution of the web while making the text cleaner and closer to evaluation formats (Section 3.1; Appendix G).
  - Significance: avoids expensive and opaque syntheticâ€‘corpus design and mitigates bias from topic curation. Leads to faster learning and better zeroâ€‘shot QA (Figures 1bâ€“c; Tables 1â€“2).
- Style is a powerful lever
  - The `Q/A` style consistently boosts zeroâ€‘shot QA accuracy; `Medium` (â€œWikipediaâ€‘likeâ€) improves perplexity over diverse domains (Figures 2â€“4; Tables 3, 6, 10â€“11).
  - No single style dominates everywhere; an oracle mixing styles per domain could reduce perplexity by 16% for small models (Figure 7), suggesting styleâ€‘diverse preâ€‘training is valuable.
- Synthetic data is not just â€œaugmentationâ€
  - Basic augmentations (synonym replacement, random deletion) fail to match rephrasing benefits (Figure 6). Gains come from style and structural changes that better match downstream distributions, not from trivial lexical variants.
- The rephraser can be small and open
  - Highâ€‘quality paraphrases from `Qwenâ€‘1.8B` or `Mistralâ€‘7B` match or beat those from a larger `Vicunaâ€‘13B` on downstream perplexity; a weak `T5â€‘base` rephraser hurts (Figure 5). This enables cheaper, reproducible pipelines.
- Compute/data efficiency without new knowledge
  - On â€œSpecialized Knowledgeâ€ tasks (e.g., MMLU, PubMedQA), performance scales with more real data exposure; WRAP helps but does not â€œcreateâ€ knowledge (Table 2). This clarifies the role of rephrasing: faster learning, not knowledge injection.

## 5. Experimental Analysis
- Setup
  - Datasets: Preâ€‘training on C4 or RefinedWeb (RW); Pile for perplexity evaluation across 21 domains (Section 4; Appendix A.2); 13 zeroâ€‘shot tasks split into â€œGeneral Understandingâ€ and â€œSpecialized Knowledgeâ€ (Section 5.1; Appendix A.3).
  - Metrics: macro tokenâ€‘level perplexity (Equation (3)); task accuracy for QA datasets (Section 5; Appendix D).
  - Baselines: C4 (85B or 170B tokens), RW (160B/320B tokens), `Pythiaâ€‘1.4B` (trained on Pile), `TinyLlama` (1T tokens) (Tables 1â€“2).
- Main results
  - Faster, better preâ€‘training
    - Zeroâ€‘shot learning curves (Figure 1b): WRAP reaches a target average accuracy with ~3Ã— fewer tokens; early checkpoints show up to ~15Ã— faster progress on Pile perplexity (Section 4).
    - Perplexity: For a 1.3B model trained 300B tokens, `C4 + QAâ€‘85B` beats `C4â€‘170B` across most Pile domains (Figure 2). The average Pile perplexity reduction is up to ~50% (Section 4).
    - Data efficiency: With 150B tokens total, 1.3B models trained with WRAP outperform models trained 300B tokens on C4 alone (Figure 1c; Figure 13).
  - Zeroâ€‘shot QA gains (Table 1)
    - On â€œGeneral Understandingâ€ tasks, `Synthetic (85B)` reaches an average 49.4% vs ~47% for strong realâ€‘data baselines; `Synthetic + C4 (85B)` also averages 49.4%.
    - Biggest win: TruthfulQA improves to 44.0% (`Synthetic`) vs 33.5â€“39% (realâ€‘data baselines). When mixing in real data, TruthfulQA drops slightly to 40.6%, indicating a tradeâ€‘off between style benefits and realâ€‘web noise exposure.
  - Specialized Knowledge tasks (Table 2)
    - Averages: WRAP variants (45.0â€“45.5%) are comparable to stronger realâ€‘data baselines (`Pythiaâ€‘Pile 300B` at 44.6%, `TinyLlama 1T` at 45.6%). Improvements saturate as total real tokens increase (RWâ€‘320B only +0.2% over RWâ€‘160B).
    - Interpretation: WRAP speeds learning but does not inject new domain facts; exposure to more real tokens still matters.
- Ablations and robustness
  - Real data is necessary: Using synthetic only degrades perplexity on webâ€‘like domains (e.g., OpenWebText2, HackerNews) that contain special tokens and noise (Figure 3). Adding real data improves both perplexity and QA in most settings (Tables 3â€“4).
  - Combining styles: Mixing `Q/A` and `Medium` yields small perplexity gains but does not outperform `Q/A` alone on zeroâ€‘shot QA (Tables 5â€“6; Figure 4).
  - Rephraser quality: `Qwenâ€‘1.8B` and `Mistralâ€‘7B` produce strong rephrases; a weak `T5â€‘base` hurts (Figure 5).
  - Not just augmentation: WRAP significantly outperforms synonym replacement or deletion (Figure 6).
  - Styleâ€“domain match: For 128M models, training with a style that matches a domain (e.g., `Q/A` for StackExchangeâ€‘like text) helps, but no single style dominates across all Pile domains (Figure 7).
  - Semantic fidelity and leakage check: High SimCSE cosine similarity between real and rephrased pairs shows content preservation, while differences with random real pairs/halves indicate rephrases do not add new content (Figure 8; Appendix C.2). Additional MRPC paraphrase analysis shows synthetic rephrases behave similarly to true paraphrases (Figure 9).
- Smallerâ€‘budget regimes
  - 350M models, 75B tokens: WRAP improves average QA by ~1.4 points (44.1% vs 42.7%) and Specialized Knowledge by ~2.2 points (41.0% vs 38.8%) when adding `Q/A` rephrases to 15B real tokens (Tables 8â€“9; Figure 12).
  - 1.3B models, 150B tokens: WRAP (`QA+C4â€‘35B`) improves â€œSpecialized Knowledgeâ€ average to 45.0% vs 42.4â€“42.8% for C4 alone, and â€œGeneral Understandingâ€ to 48.4% vs 46.0â€“46.7% (Tables 10â€“11; Figure 13).
- Fewâ€‘shot leaderboard snapshot
  - On six OpenLLMâ€‘Leaderboard tasks, a 1.3B WRAP model matches or beats `Falconâ€‘RW 1.3B` and `Pythiaâ€‘1.4B` (e.g., ARCâ€‘C 36.4 vs 35.1; TruthfulQA 40.6 vs 36.0 and 38.7) (Table 12).

Do the experiments support the claims?
- Yes, for data/compute efficiency and zeroâ€‘shot QA. The paper controls for realâ€‘token exposure (85B vs 170B vs 300B), compares against competitive baselines, and provides detailed ablations (Figures 2â€“7; Tables 1â€“6, 8â€“12). The â€œno new knowledgeâ€ caveat is consistent with results on MMLU/PubMedQA (Table 2).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Rephrasing preserves semantics. Similarity analyses (Figure 8; Appendix C) support this, but subtle factual drift or hallucinations in rephrases are not deeply audited at scale.
  - The benefits are shown primarily for models up to 1.3B parameters and Englishâ€‘language web corpora. Behavior at 7Bâ€“70B scale and multilingual settings remains untested (implicit limitation).
- What WRAP does not address
  - It does not â€œcreateâ€ domain knowledge. On knowledgeâ€‘heavy benchmarks, performance still depends on exposure to more real tokens (Table 2).
  - Diversity of synthetic outputs: paraphrasers can reduce content diversity (Section 7.2). The pipeline does not explicitly enforce topical diversity or novelty.
- Computational and data costs
  - Rephrasing 85B tokens costs â‰ˆ25k GPU hours with `Mistralâ€‘7B`, though smaller/faster rephrasers and speculative decoding can reduce this (Section 7.1). This is a nontrivial upfront cost, offset when training multiple models.
  - Syntheticâ€‘only training hurts robustness to realâ€‘web artifacts (Figure 3); hence a real+synthetic mixture is required, which means managing two distributions during preâ€‘training.
- Open questions
  - Optimal style mixing schedules over training.
  - How many rephrases per document are beneficial?
  - Interaction with later instructionâ€‘tuning or RLHF.
  - Risk of longâ€‘term â€œmodel collapseâ€ when repeatedly training on modelâ€‘generated text (Related Work), even if WRAP reuses web semantics.

## 7. Implications and Future Directions
- How this changes the landscape
  - WRAP reframes â€œdata scalingâ€ as â€œstyle scalingâ€: preserving web knowledge while shaping text to match evaluation formats. This gives a practical, transparent alternative to opaque data curation or largeâ€‘LLM synthetic corpora.
  - It provides a computeâ€‘efficient path to better zeroâ€‘shot QA: train once on a real+`Q/A` mixture, then evaluate without extra instructionâ€‘tuning (Figures 1b, 2; Tables 1, 10â€“11).
- Enabled followâ€‘up research
  - Styleâ€‘aware curriculum schedules: start with â€œMediumâ€ for fast perplexity gains; blend in â€œQ/Aâ€ as training progresses for QA transfer.
  - Automatic style selection: learn perâ€‘domain or perâ€‘batch style weights (cf. DoReMiâ€‘like mixture optimization), leveraging the domainâ€‘style sensitivity in Figure 7.
  - Lightweight rephrasers: systematically search the smallest model that preserves semantics but improves style (Figure 5 suggests the bar can be low).
  - Robustness checks: factual drift detection between original and rephrase; semantic equivalence metrics beyond cosine similarity; human audits on sensitive domains.
  - Lowâ€‘resource and multilingual settings: rephrase small, noisy corpora into clearer registers to bootstrap strong base models (Section 7.1 motivation).
- Practical applications
  - Costâ€‘effective preâ€‘training for organizations with limited highâ€‘quality data access.
  - Targeted preâ€‘training for assistants (e.g., customer support) where `Q/A` style alignment is crucial.
  - Data governance: rephrasing can filter harmful formatting without discarding knowledge, potentially aiding safety and bias mitigation (discussed qualitatively in Conclusion).

> Core takeaway: Rather than hunting for ever larger or â€œcleanerâ€ web dumps, WRAP shows that rephrasing the web you already haveâ€”into styles that match how you evaluate and use modelsâ€”yields faster learning, better zeroâ€‘shot transfer, and comparable knowledge performance, all with smaller, open paraphrase models (Figures 1â€“6; Tables 1â€“2, 10â€“11).

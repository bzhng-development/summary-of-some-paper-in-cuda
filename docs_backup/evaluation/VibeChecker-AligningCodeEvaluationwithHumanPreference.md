# Vibe Checker: Aligning Code Evaluation with Human Preference

**ArXiv:** [2510.07315](https://arxiv.org/abs/2510.07315)

## üéØ Pitch

Vibe Checker redefines how we evaluate code from large language models by moving beyond simple functional tests to also measure whether generated code follows real-world, non-functional instructions‚Äîincluding style, documentation, and library usage‚Äîthat developers care about. By introducing VeriCode, a taxonomy of 30 verifiable code instructions with automated checkers, and augmenting existing benchmarks to jointly assess both function and form, this testbed reveals that models excelling at both dimensions align much more closely with human preferences. This approach paves the way for future models that deliver code not just that works, but that truly meets users' practical standards‚Äîmaking AI code generation more useful, trustworthy, and attuned to real developer workflows.

---

## 1. Executive Summary
This paper introduces Vibe Checker, a new way to evaluate code generated by large language models (LLMs) that goes beyond ‚Äúdoes it pass the tests?‚Äù to also measure whether the code follows non-functional instructions humans care about (style, structure, docs, library choices). It contributes two concrete assets: VeriCode, a taxonomy of 30 verifiable code instructions paired with deterministic checkers, and an augmented testbed (BigVibeBench and LiveVibeBench) that evaluates both functional correctness and instruction following; results over 31 LLMs show that a combined score matches human preferences best (Figure 5).

## 2. Context and Motivation
- Problem the paper tackles
  - Current code benchmarks largely measure only functional correctness using pass@k (does the code pass unit tests within k samples). That ignores many ‚Äúnon-functional‚Äù instructions programmers routinely request: adhere to style guides, minimize branching, add docstrings, prefer certain libraries, or keep edits minimal.
  - The paper terms the emerging workflow ‚Äúvibe coding,‚Äù where users iterate with an LLM until the solution ‚Äúpasses the vibe check‚Äù‚Äîit feels right, reads cleanly, preserves intent, and remains correct (Section 1; Figure 1).

- Why this matters
  - In practical tools (e.g., Copilot-like settings), people often choose between multiple candidate snippets based on both function and form. Rankings from human preference platforms (e.g., Copilot Arena) correlate weakly with functional-only scores, indicating missing signals (Section 1).
  - Optimization regimes (e.g., RL with verifiable rewards, often using pass@k) risk overfitting to an incomplete notion of code quality (Section 1).

- Prior approaches and their gaps
  - General instruction-following benchmarks exist, often synthetic and not code-specific; they use deterministic checkers or LLM-as-a-judge (Related Work, Section 5).
  - Code-focused benchmarks overwhelmingly test against unit tests (e.g., HumanEval, APPS, BigCodeBench) and do not track non-functional adherence at scale (Section 5).
  - Some coding benchmarks evaluate non-functional aspects (e.g., readability) via heuristics or LLM/human judgments, which are hard to scale and not verifiable (Section 5).

- How this work positions itself
  - It hypothesizes that ‚Äúinstruction following‚Äù (IF) for code‚Äîadhering to concrete, verifiable non-functional instructions‚Äîis the missing measurable component behind human vibe checks (Section 1).
  - It formalizes that hypothesis into a taxonomy with binary verifiers (VeriCode) and integrates these checks into established benchmarks to evaluate both functionality and IF together (Sections 2‚Äì3).

Key terms introduced:
- pass@k: the fraction of problems solved within the first k generated attempts (unit-test based).
- instruction following (IF): here, the verifiable adherence to explicit, non-functional constraints (e.g., ‚Äúlimit line length to 79,‚Äù ‚Äúuse Google-style docstrings‚Äù).
- vibe check: the human preference signal that a solution ‚Äúfeels right‚Äù beyond strict correctness.
- linter: a static analysis tool that flags style and pattern issues; the paper primarily uses Ruff (aggregates 800+ Python rules).
- AST (Abstract Syntax Tree): a structural representation of code used to implement some verifiers.

## 3. Technical Approach
The paper‚Äôs methodology has three parts: building a verifiable instruction taxonomy (VeriCode), augmenting existing code benchmarks with these instructions (Vibe Checker testbed), and evaluating models in both single- and multi-turn settings.

1) VeriCode taxonomy (Section 2)
- Goal: Create a compact, practical set of non-functional code instructions that can each be automatically checked with a deterministic pass/fail verifier.
- Source and filtering pipeline:
  - Start with >800 industry rules aggregated by the Ruff Python linter, then add whole-response instructions (e.g., documentation outside code blocks) that static linting alone cannot assess (Section 2.2, ‚ÄúCandidate Pool Sourcing‚Äù).
  - Filter for scope/relevance (prefer broadly applicable rules), remove overlaps, and ensure instructions apply widely across tasks (Section 2.2).
  - Difficulty filter: evaluate candidate instructions on a hard dataset (BigCodeBench-Hard) with a strong model (Gemini 2.5 Flash). If an instruction is too easy (success >90% without hurting pass@1), drop it; borderline cases go to manual review (Section 2.2).
  - Manual expert review finalizes clarity and real-world relevance (Section 2.2).
- Resulting taxonomy:
  - 30 instructions across five categories (Section 2.3; Table 1 gives examples):
    - Coding Style & Conventions (9)
    - Logic & Code Patterns (9)
    - Documentation & Commenting (6)
    - Error Handling & Exception Management (4)
    - Library & API Constraints (2)
  - 27 of 30 checks use Ruff rules; the rest use AST or regex (Appendix B.1).
  - Each instruction includes: category, description, prompts for both single-turn generation and multi-turn editing, parameters (e.g., `line_length`, `max_branches`, `convention`), and the verifier code (Section 2.3; Appendix B.2 shows full versions for 5 instructions).
  - Example verifiable mappings (Table 1 and Appendix B.2):
    - ‚ÄúLimit lines to ‚â§ {line_length}‚Äù ‚Üí PEP8 `E501` rule.
    - ‚ÄúAt most {max_branches} branches per function‚Äù ‚Üí `PLR0912` (cyclomatic complexity proxy).
    - ‚ÄúUse {Google/NumPy/PEP257} docstring format‚Äù ‚Üí `D` ruleset with configured convention.
    - ‚ÄúReplace OSError aliases with OSError‚Äù ‚Üí `UP024`.
    - ‚ÄúUse pathlib over os/os.path/glob/open‚Äù ‚Üí `PTH` rules; notes caution about test environments that mock `open()` (Appendix Figure 11, Notes).
  - Parameters make instructions extensible; the 30 core instructions can become hundreds of verifiable variants (Section 2.3).

2) Benchmark augmentation: Vibe Checker testbed (Section 3.1)
- Base datasets:
  - BigCodeBench (real-world programming; 1,140 instances).
  - LiveCodeBench v1‚Äìv6 (algorithmic/contest problems; 1,055 instances).
- For each instance, add 5 instructions selected from VeriCode via a two-stage LLM pipeline (Section 3.1):
  - Instruction selection:
    - Randomly permute all 30 instructions.
    - An LLM-based selector scans once and keeps items that are (a) relevant to the problem and (b) non-conflicting with previously kept items (Section 3.1, ‚ÄúInstruction Selection‚Äù).
    - The accepted list (in that permuted order) becomes the constraints for that task.
  - Parameter selection:
    - Another LLM proposes values for any instruction parameters (guided by allowed types/ranges and the problem context), then a rule-based validator drops unsupported keys and reverts invalid values to defaults (Section 3.1, ‚ÄúParameter Selection and Validation‚Äù).
  - Selector choice:
    - Both Gemini 2.5 Pro and Claude 4 Opus perform similarly; Claude 4 Opus has a lower invalid-parameter rate (0.96% vs. 2.47%), so it is used (Section 3.1).
- The augmented suites:
  - BigVibeBench: BigCodeBench + instructions (more Error Management and Library constraints).
  - LiveVibeBench: LiveCodeBench + instructions (more Logic instructions), reflecting each dataset‚Äôs nature (Appendix Figure 12).

3) Evaluation protocol and metrics (Section 3.2; Figure 2)
- Interaction modes:
  - Single-Turn Generation: the base prompt plus all selected instructions are given at once; the model returns a single implementation.
  - Multi-Turn Editing: the model first responds to the base prompt only; instructions are then revealed one by one across turns; the model edits the code each round; the final code is evaluated.
- Metrics:
  - Functional correctness is measured with the original unit tests, reported as pass@1. They define functional regression under k added instructions:
    - FR_k = (S0 ‚àí Sk) / S0, where S0 is pass@1 without instructions and Sk is pass@1 with k instructions (Section 3.2).
  - Instruction Following (IF) is measured two ways for a task with k instructions (Section 3.2):
    - Instruction-level: the average fraction of instructions passed (per-instruction binary verifiers).
    - Task-level: success only if all k instruction verifiers pass simultaneously (harder).
- Implementation details to ensure comparability:
  - 31 LLMs across 10 families; API access via Vertex AI and OpenRouter; temperatures follow benchmark defaults (0.0 for BigVibeBench, 0.2 for LiveVibeBench). Thinking mode is enabled wherever available; Claude‚Äôs thinking mode requires temperature 1.0 (Section 4.1).
  - Context length capped at 32,768 tokens (Section 4.1).
  - The final corpus involves >10K instruction-level evaluations (Section 4.1).

Analogy for clarity:
- Think of a driving test that currently only checks whether you reach the destination (unit tests). This work adds checks like ‚Äúdid you obey speed limits,‚Äù ‚Äúdid you use turn signals,‚Äù and ‚Äúdid you follow a requested route type (highways vs. local roads).‚Äù Each check has a radar gun or camera (deterministic verifier). The new score combines destination success with driving etiquette‚Äîand that combined score matches what human evaluators prefer.

## 4. Key Insights and Innovations
- A verifiable, extensible taxonomy for non-functional code instructions (VeriCode)
  - Novelty: Unlike prior style/readability efforts judged by humans or LLMs, each instruction maps to a deterministic checker (mostly Ruff rules) returning pass/fail (Section 2; Table 1; Appendix B.1‚ÄìB.2).
  - Significance: Enables scalable, objective evaluation and can serve as a reward signal for training (Section 1).

- A testbed that simultaneously measures functionality and instruction following
  - Novelty: Augments established benchmarks with explicit, relevant constraints chosen per-instance by an LLM selector, and evaluates both axes in single-turn and multi-turn settings (Section 3; Figure 2).
  - Significance: Captures real interaction patterns and the trade-off between correctness and adherence to constraints‚Äîcentral to ‚Äúvibe coding.‚Äù

- Empirical discovery: non-functional instructions cause measurable functional regression
  - Different from prior expectations that style constraints should not affect correctness; they often do, especially with multiple constraints or in multi-turn edits (Section 4.2; Table 2; Figure 3a).

- Human preference aligns best with a composite of functionality and instruction following
  - Measured against LMArena‚Äôs coding Elo ratings, mixtures (Œ±¬∑IF + (1‚àíŒ±)¬∑Func) correlate better than either metric alone, with sizable weight on IF, particularly for real-world coding tasks (Section 4.5; Figure 5).

These are more than incremental tweaks: the taxonomy plus deterministic verifiers and the composite evaluation protocol form new infrastructure for measuring‚Äîand eventually training‚Äîmodels toward human-aligned ‚Äúvibes.‚Äù

## 5. Experimental Analysis
- Evaluation methodology (Section 4.1; Section 3)
  - Datasets: BigVibeBench (1,140 BigCodeBench tasks + 5 instructions each) and LiveVibeBench (1,055 LiveCodeBench tasks + 5 instructions each).
  - Metrics: pass@1 and FR_k for functionality; instruction-level IF and task-level IF (all constraints must pass) for instruction following (Section 3.2).
  - Models: 31 LLMs from Gemini, Claude, OpenAI (including `o4 mini`), DeepSeek, Qwen, Grok, Gemma, Mistral, MiniMax, Kimi (Section 4.1; Appendix Table 4).
  - Prompting: System prompts ensure full, runnable code (or functions for algorithmic tasks) and require satisfying all instructions (Appendix Figures 13‚Äì14).
  - Error handling: On LiveVibeBench, higher provider/length failures led to excluding models with >10% error rate; analysis uses 24 LLMs (Appendix D.1).

- Main quantitative results
  - Functional regression grows with more instructions and is worse in multi-turn editing.
    - Averaged across models, BigVibeBench regression rises from 2.48% (1 instruction) to 5.76% (5 instructions) in single-turn, and from 3.18% to 9.31% in multi-turn (Figure 3a).
    - Table 2 shows concrete model-level regressions. Examples under 5 instructions:
      - On BigVibeBench, `o4 mini` multi-turn FR_5 = 8.05%; `GPT 5` multi-turn FR_5 = 5.46%; `Claude 4 Opus` multi-turn FR_5 = 3.78%.
      - On LiveVibeBench, regressions are larger: `o4 mini` multi-turn FR_5 = 15.92%; `Kimi K2` multi-turn FR_5 = 12.79%; even strong models like `GPT 5` show FR_5 = 9.02% (Table 2).
    - Quote:
      > ‚ÄúAlthough the added instructions do not target functionality, pass@1 decreases across all models.‚Äù (Section 4.2; Table 2; Figure 3a)
  - Multi-instruction instruction-following is hard; task-level success decays sharply.
    - Task-level IF (must satisfy all k instructions) drops quickly with k (Table 3; Figure 3b). Under 5 instructions:
      - BigVibeBench single-turn: best model `Claude 4 Opus` reaches 46.75%; others like `GPT 5` at 34.39%, `o4 mini` at 41.32% (Table 3).
      - LiveVibeBench single-turn: `GPT 5` reaches 40.95%; `Claude 4 Opus` reaches 35.17%; several models fall below 30% (Table 3).
    - Quote:
      > ‚ÄúWith three or more instructions, most advanced models fall below 50 across both benchmarks.‚Äù (Section 4.3; Table 3)
  - Single-turn vs. multi-turn trade-off:
    - Single-turn better preserves functionality (Figure 3a; Section 4.2).
    - Multi-turn achieves higher instruction following (Figure 3b): on LiveVibeBench, the advantage reaches ~8% at the task level (Section 4.3).
  - Positional effects: lost-in-the-middle even with short prompts (Section 4.4; Figure 4).
    - BigVibeBench shows a U-shape: mid-position instructions are followed less reliably than the first or last.
    - Single-turn shows primacy (best at position 1), while multi-turn shows recency (best at last position).
    - LiveVibeBench shows consistent primacy (single-turn) and recency (multi-turn) biases even if the U-shape is less pronounced.
    - Quote:
      > ‚ÄúSingle-turn generation shows a primacy bias‚Ä¶ while multi-turn editing displays a clear recency bias.‚Äù (Section 4.4; Figure 4)
  - Human preference correlation (Section 4.5; Figure 5)
    - Using LMArena coding Elo, the best Pearson correlation on BigVibeBench occurs with Œ±=0.4 (40% IF, 60% functionality), and the best Spearman with Œ±=0.7 (70% IF, 30% functionality) (Figure 5a).
    - For LiveVibeBench, Pearson again peaks at Œ±=0.4; Spearman peaks at Œ±=0.6 (Figure 5b).
    - Quote:
      > ‚ÄúIn all cases, the mixture outperforms either isolated metric by a clear margin.‚Äù (Section 4.5; Figure 5)
    - Interpretation: IF is a primary differentiator for real-world programming tasks (BigVibeBench), while functionality weighs more for algorithmic tasks (LiveVibeBench), but the optimum always mixes both.
- Robustness and additional details
  - The instruction-selector distributions are similar across two different LLMs (Gemini 2.5 Pro vs. Claude 4 Opus), and Opus has fewer invalid parameters (Section 3.1).
  - Appendix E.2 shows correlations remain consistent when toggling LMArena ‚ÄúStyle Control‚Äù on/off and across correlation types (Pearson/Spearman/Kendall), with peaks always at mixed weights (Appendix Figure 15).

- Do experiments support claims?
  - Yes. The testbed directly measures both axes and reveals consistent trade-offs across 31 models. The correlation with human preference (Figure 5) empirically grounds the central thesis that IF is essential alongside functionality.

## 6. Limitations and Trade-offs
- Scope limited to Python
  - The taxonomy and verifiers target Python (Section 2.3). While the framework is language-agnostic in principle, it relies heavily on Ruff and Python-specific rules. Extending to other languages requires comparable linter coverage and re-implementation.

- Verifier coverage and fidelity
  - Deterministic checks can be both a strength (scale, objectivity) and a blunt instrument. For example, enforcing `pathlib` might break tests in environments that mock `open()` (Appendix Figure 11, Notes). Some stylistic preferences are nuanced and may not map cleanly to a binary checker.

- Instruction selection via LLM introduces bias/variance
  - Relevance and non-conflict are decided by an LLM selector (Section 3.1). While two selectors yielded similar category distributions, per-instance choices can still vary; the paper does not ablate different selection strategies or human-curated selections.

- Interaction protocol choices
  - Exactly five instructions are injected per instance (for main analyses). Different counts or more complex constraint interactions are not extensively ablated. Position effects are measured, but mitigation strategies (e.g., formatting, chunking) are not tested.

- Evaluation settings and comparability
  - Some models use ‚Äúthinking mode,‚Äù and Claude requires temperature 1.0 in that mode (Section 4.1), which slightly complicates strict apples-to-apples comparisons.
  - On LiveVibeBench, models with >10% provider/length errors are excluded, potentially biasing the evaluated subset toward more reliable APIs (Appendix D.1).

- Functional metric is pass@1
  - The focus on pass@1 (as in the underlying benchmarks) leaves open how findings change for pass@k or self-reflection/retry strategies.

- Correlation with human preference is aggregate
  - LMArena Elo reflects crowd preferences across varied tasks and styles; while the mixture metric correlates best (Figure 5), causality is not established, and task-specific weightings may differ.

## 7. Implications and Future Directions
- How this work changes the field
  - It reframes code evaluation from ‚Äúpure correctness‚Äù to ‚Äúcorrectness + verifiable instruction adherence,‚Äù aligning benchmarks with real developer preferences. The infrastructure (VeriCode + Vibe Checker) provides a practical path to train and evaluate models that ‚Äúpass the vibe check.‚Äù
  - It supplies scalable, deterministic rewards for non-functional code qualities‚Äîuseful for post-training (SFT/RL) beyond pass@k (Section 1; Section 2.3).

- What follow-up research it enables
  - Training with IF rewards: Incorporate VeriCode verifiers in RL or direct preference optimization alongside unit tests; study how mixed rewards shift model behavior over turns.
  - Cross-language generalization: Port the taxonomy to JavaScript/TypeScript (ESLint), Java/Kotlin (Checkstyle/Detekt), C/C++ (clang-tidy), etc.; compare language-specific biases in IF vs. functionality trade-offs.
  - Selector and parameter curriculum: Replace/augment LLM selection with human-in-the-loop or rule-based policies; explore curricula that gradually increase instruction difficulty or conflict.
  - Position-bias mitigation: Test prompt structuring, chunking, or retrieval to reduce ‚Äúlost-in-the-middle‚Äù in single-turn and recency overreliance in multi-turn (Section 4.4; Figure 4).
  - Richer non-functional dimensions: Add verifiable checks for performance budgets, memory usage, security patterns, or minimal-edit constraints (diff size), expanding VeriCode beyond style/logic/docs.
  - Beyond pass@1: Evaluate pass@k, self-correction loops, and agentic tool use under multi-instruction settings to see if functionality loss can be recovered without sacrificing IF.

- Practical applications
  - IDEs and code assistants can re-rank candidates by a composite of unit tests and verifiable IF checks, better matching human ‚Äúvibe‚Äù choices.
  - Continuous integration can enforce project-specific VeriCode policies automatically on AI-proposed patches, preventing style and architectural drift.
  - RL training pipelines can add IF verifiers as reward components to produce models that maintain correctness while adhering to team conventions.

In short, Vibe Checker operationalizes ‚Äúwhat humans actually check‚Äù in AI-generated code. The central empirical message is twofold: non-functional constraints materially affect functionality, and a mixture of functional correctness and verifiable instruction following best predicts human preference (Figure 5). This opens the door to training and benchmarking practices that reward both substance and style‚Äîwhat developers already value in the everyday ‚Äúvibe check.‚Äù

# LongCodeZip: Compress Long Context for Code Language Models

**ArXiv:** [2510.00446](https://arxiv.org/abs/2510.00446)

## üéØ Pitch

LongCodeZip introduces the first training-free, model-agnostic compression framework tailored for code LLMs handling extremely long contexts. By combining conditional perplexity-based mutual information scoring for function selection with fine-grained, knapsack-optimized block pruning, it aggressively reduces code context size‚Äîup to 5.6√ó compression‚Äîwithout degrading, and sometimes even improving, performance across tasks like code completion, summarization, and repository-level QA. This breakthrough enables efficient and cost-effective scaling of code intelligence systems to real-world, large-scale software, overcoming bottlenecks of input length, latency, and information overload that previously limited practical use of code LLMs.

---

## 1. Executive Summary
LongCodeZip is a training-free, model-agnostic framework that compresses very long code contexts so code-focused large language models (LLMs) can reason efficiently without losing task-critical information. It does this via a two-stage process‚Äîfirst selecting the most relevant functions using a mutual-information-like score based on conditional perplexity, then pruning within those functions using perplexity-driven block chunking with budget-aware, knapsack-based selection‚Äîachieving up to 5.6√ó compression with no loss (and sometimes gains) in performance across code completion, summarization, and repository-level question answering (Abstract; Figs. 1‚Äì2; Sec. V; Tables II‚ÄìIV).

## 2. Context and Motivation
- Problem addressed
  - LLMs for code must often consider tens of thousands of tokens (whole modules, multiple files, or entire repos). Long inputs inflate latency and cost due to attention‚Äôs quadratic complexity and API pricing, and models struggle to locate truly relevant content in the noise (Sec. I: ‚ÄúThree major challenges‚Ä¶‚Äù).
  - Even with 128k-token windows, real projects can exceed limits, forcing truncation and degrading outputs (Sec. I).
  - Code has structure and long-range dependencies (variables, classes, functions across files). When context windows are saturated or noisy, models produce code that doesn‚Äôt compile or violates constraints (Sec. I).

- Why it matters
  - Real-world tasks‚Äîrepository-level QA (RepoQA), long-context code completion, and module summarization‚Äîare bottlenecked by context processing costs and by models ‚Äúgetting lost‚Äù in lengthy prompts (Sec. I; Datasets in Sec. IV-B).

- Prior approaches and their gaps
  - Retrieval-augmented generation (RAG): retrieves relevant chunks via embedding similarity (e.g., cosine similarity with UniXcoder or CodeBERT). Works when lexical overlap is high but misses implicit dependencies (e.g., configuration classes used in training loops) because similarity is surface-level (Sec. II; Fig. 1 right panel).
  - General text compression (LLMLingua, LongLLMLingua, LLMLingua-2): effective for natural language, but can break code syntax/structure or ignore code-specific dependencies (Sec. I; Related Work Sec. VIII-B).
  - Code-specific simplifiers (DietCode, SlimCode): largely heuristic, often function-level only, and not designed for long multi-function contexts; limited cross-language/task generalization (Sec. I; Related Work Sec. VIII-B).

- Positioning
  - LongCodeZip is a plug-and-play, code-aware compression framework tailored to long contexts. It combines:
    - Instruction-aware function selection using conditional perplexity as an approximate mutual information signal.
    - Intra-function pruning with perplexity-based block detection and knapsack selection under an adaptive, importance-weighted token budget (Sec. III; Fig. 2).
  - It targets long-context code tasks broadly (completion, summarization, QA), not just retrieval or a single language (Sec. IV‚ÄìV).

## 3. Technical Approach
LongCodeZip compresses a long code context `c` with respect to a task instruction `q` under a token budget `B`, producing a compressed context `c'` that preserves relevance for the downstream model (Sec. III-A).

Key definitions (Sec. III-A; Eqs. 1‚Äì3):
- `Perplexity (PPL)`: a standard measure of how ‚Äúsurprised‚Äù a language model is by a sequence. Lower PPL means higher predicted likelihood. Conditional perplexity `PPL(q | c)` measures how well the model predicts tokens of `q` given context `c`.
- `Approximated Mutual Information (AMI)`: defined as `AMI(c, q) = PPL(q) ‚àí PPL(q | c)` (Eq. 1). If providing `c` lowers the perplexity of `q`, then `c` is helpful (higher AMI). This serves as an instruction-aware relevance score.

Overall pipeline (Fig. 2; Sec. III-B):
1) Coarse-grained compression: select the most relevant functions/classes.
2) Fine-grained compression: within those retained functions, detect semantic blocks and select only the most useful ones, respecting a token budget.

Step-by-step

A) Coarse-grained: relevant function selection (Sec. III-C)
- Function-level chunking: Split source along function/class boundaries. Rationale: functions are modular, syntactically valid units that keep semantics intact when moved/retained (Sec. III-C).
- Instruction-aware ranking with AMI: For each candidate function `c·µ¢`, compute how much it reduces `PPL(q)` when included, i.e., `AMI(c·µ¢, q)`. Rank functions by AMI descending (Sec. III-C; Eq. 1).
  - Why AMI vs embedding similarity? AMI captures dependency value‚Äîhow much a chunk helps predict the desired output‚Äîeven if surface words differ (Sec. II; Fig. 1 contrasts).
- Budget-constrained selection: Greedily keep top-ranked functions under a coarse budget `B_coarse = B / R_fine`, where `R_fine` is the planned fine-grained retention ratio (Sec. III-C). Non-selected regions may be replaced with placeholders to preserve global structure while cutting tokens.

B) Fine-grained: intra-function pruning (Sec. III-D)
- Block-level chunking by perplexity:
  - Treat each line as the smallest unit; compute line-wise perplexity and group consecutive lines into blocks.
  - Mark a block boundary when a line‚Äôs perplexity shows a sharp local increase‚Äîexceeding its neighbors by at least `Œ±` standard deviations across the function‚Äôs lines (Sec. III-D, ‚ÄúBlock-Level Chunking‚Äù).
  - Intuition: within a coherent subroutine, uncertainty (perplexity) falls as context accumulates; spikes signal semantic shifts (Fig. 4 ‚ÄúPerplexity Distribution‚Äù).
- Adaptive budget allocation across functions (Algorithm 1; Eqs. 4‚Äì6):
  - Compute a baseline retention ratio for large functions: `R_base = (B ‚àí Œ£_{small} T_j) / Œ£_{large} T_k` (Eq. 4), so short functions (e.g., <5 lines) can be kept in full.
  - Normalize function-level AMI to `[0,1]`, then bias each large function‚Äôs retention via `R_biased,i = R_base ¬∑ (1 + Œ≤ ¬∑ (2¬∑AMI_norm,i ‚àí 1))` (Eq. 5), where `Œ≤` controls how much more budget to give important functions.
  - Clamp rates to `[0,1]` and rescale so total retained tokens match the available budget for large functions `B_large` (Eq. 6).
  - Design rationale: preserve more detail where relevance is higher; avoid over-allocating to trivial functions.
- Dynamic block selection via 0/1 knapsack (Algorithm 2):
  - Treat each block as an item with ‚Äúvalue‚Äù = (normalized) AMI and ‚Äúweight‚Äù = token count.
  - Solve the 0/1 knapsack to maximize total value under the per-function budget, optionally forcing preservation of specific blocks (`P`) if user-specified.
  - Why knapsack? It balances value-density vs size, ensuring the most helpful set of blocks survives within the token limit.

Implementation notes (Sec. IV-E)
- Hyperparameters vary by task:
  - Code completion: `B=2k`, `R_fine=0.8`, `Œ≤=0.5`.
  - Summarization: `B=5k`, `R_fine=0.3`, `Œ≤=0.5`.
  - RepoQA: `B=2k`, `R_fine=1.0` (to preserve function structure).
- The compression model typically mirrors the generation model but can be smaller (Sec. V-C; Table VIII shows cross-model robustness, including 0.5B compressors).
- Cost: modest GPU memory/time for compression; much larger savings in generation due to smaller prompts (Sec. V-D; Table IX).

Concrete intuition (Fig. 1 and Fig. 4)
- If the instruction is ‚Äúcomplete `train_model`,‚Äù the `Config` class with optimizer parameters may share few surface words but is crucial. AMI detects that including `Config` reduces uncertainty about the answer, so it ranks highly (Fig. 1, right).
- Within a long function, perplexity spikes mark shifts (e.g., new helper logic). Fine-grained pruning keeps blocks that best reduce uncertainty about the instruction (Fig. 4).

## 4. Key Insights and Innovations
- Instruction-aware relevance via conditional perplexity (AMI)
  - Novelty: Uses ‚Äúhow much this chunk reduces PPL of the instruction/target‚Äù rather than static embedding similarity (Sec. III-A/C; Eq. 1). This captures non-lexical dependencies (e.g., configuration classes) that RAG often misses (Fig. 1).
  - Significance: In ablations, replacing AMI with similarity drops ES by 7.89 points and EM by 7.2 at the same budget (Table VII, ‚Äúw/ Similarity-based Ranking‚Äù).

- Perplexity-based block detection inside functions
  - Novelty: Identifies semantic boundaries by local perplexity spikes instead of line breaks or AST-only rules (Sec. III-D ‚ÄúBlock-Level Chunking‚Äù; Fig. 4).
  - Significance: Outperforms simple line chunking by 1.57 ES points and 1.2 EM (Table VII, ‚Äúw/ Line Chunking‚Äù), improving information density without heavy parsing.

- Adaptive, AMI-weighted budget allocation across functions
  - Novelty: Allocates more tokens to more relevant functions using a controllable importance parameter `Œ≤` (Algorithm 1; Eqs. 4‚Äì6).
  - Significance: Removing adaptivity reduces ES by 2.34 and EM by 3.0 (Table VII, ‚Äúw/o Adaptive Budget Allocation‚Äù).

- Knapsack-based block selection for maximum relevance per token
  - Novelty: Formalizes intra-function pruning as a 0/1 knapsack, selecting the highest-value (AMI) blocks within budget (Algorithm 2).
  - Significance: Beats random intra-function selection by 2.48 ES and 3.40 EM (Table VII, ‚Äúw/ Random Line Selection‚Äù).

These are more than incremental tweaks: together they establish a code-aware, instruction-aware compression pipeline that consistently beats both general text compressors and code-specific heuristics across diverse tasks and models (Tables II‚ÄìIV).

## 5. Experimental Analysis
- Evaluation setup (Sec. IV)
  - Datasets (Table I):
    - Long Code Completion: 500 Python examples with contexts >5k tokens.
    - Long Module Summarization: 139 Python modules >2k tokens.
    - RepoQA: 600 repo-level QA examples across 6 languages (Python, Java, JS/TS, Rust, Go, C++).
  - Metrics:
    - Compression Ratio `= |C_original| / |C_compressed|` (Eq. 7).
    - Completion: Edit Similarity (ES) and Exact Match (EM).
    - Summarization: LLM-as-judge ‚ÄúCompScore‚Äù using GPT-4o-mini with order-robust prompting (Eq. 8; Sec. IV-D).
    - RepoQA: retrieved function accuracy by BLEU > 0.8 (Sec. IV-D).
  - Baselines (Sec. IV-C):
    - No Compression / No Context; Random Token/Line.
    - RAG: Sliding Window and Function Chunking with UniXcoder embeddings.
    - Text compressors: LLMLingua, LongLLMLingua, LLMLingua-2.
    - Code compressors: DietCode, SlimCode (Python reproduction via tree-sitter for fair comparison).
    - Advanced RAG for completion: A3-CodGen, cAST, RepoGenix, RLCoder (Table VI).
  - Models:
    - Open-source: DeepSeek-Coder-6.7B, Qwen2.5-Coder-7B, Seed-Coder-8B (instruct variants); also smaller compressors (0.5B/1.5B/3B; Table VIII).
    - Closed-source: GPT-4o, Claude-3.7-Sonnet (Table V).

- Main quantitative results
  - Long code completion (Table II):
    - With Qwen2.5-Coder-7B, LongCodeZip reaches ES 57.55 and EM 32.40 at 4.3√ó ratio, beating RAG(Function) ES 52.79 / EM 26.00 at 3.1√ó.
    - With Seed-Coder-8B, LongCodeZip attains ES 63.11 / EM 37.40 at 5.6√ó‚Äîvery close to No Compression ES 64.04 / EM 40.20 while using ~1/5 tokens.
    - With DeepSeek-Coder-6.7B, LongCodeZip exceeds the no-compression ES (60.58 vs 57.14) and EM (35.40 vs 34.40) at 5.3√ó compression.
    - Quote:
      > Table II (Seed-8B): LongCodeZip 63.11 ES / 37.40 EM at 5.6√ó vs RAG(Function) 60.52 / 35.00 at 3.7√ó.
  - Module summarization (Table III):
    - DeepSeek-Coder-6.7B: LongCodeZip achieves CompScore 28.01 at 2.5√ó, beating next best 22.95 (RAG Sliding).
    - Qwen2.5-Coder-7B: LongCodeZip 56.47 at 1.7√ó, slightly above No Compression 56.00.
    - Seed-Coder-8B: LongCodeZip 55.07 at 3.5√ó, outperforming RAG and code/text compressors.
    - Quote:
      > Table III (Qwen-7B): LongCodeZip 56.47 CompScore at 1.7√ó vs RAG(Sliding) 53.50 at 1.7√ó.
  - RepoQA (Table IV):
    - DeepSeek-Coder-6.7B: LongCodeZip average accuracy 75.3 at 5.3√ó, vs 59.3 for LongLLMLingua at 3.0√ó and 38.3 No Compression.
    - Qwen2.5-Coder-7B: 87.2 at 4.5√ó (close to 86.0 No Compression, and >71.3 LongLLMLingua).
    - Seed-Coder-8B: 80.7 at 5.3√ó, surpassing No Compression 69.0.
    - Quote:
      > Table IV (Qwen-7B): LongCodeZip 92.0/78.0/87.0/85.0/86.0/95.0 per-language, average 87.2 at 4.5√ó.
  - Closed-source models (Table V):
    - GPT-4o: Completion ES 64.72 at 4.3√ó (very close to 65.13 No Compression); RepoQA 88.9 at 5.1√ó (higher than 87.8 baseline).
    - Claude-3.7-Sonnet: Completion ES 66.27 at 4.3√ó (matches 66.24 baseline); RepoQA 88.9‚Äì90.7 at 5.1√ó (meets/exceeds baseline).
    - Quote:
      > Table V (GPT-4o RepoQA): LongCodeZip avg 88.9 at 5.1√ó vs 87.8 No Compression.
  - Against advanced RAG (Table VI):
    - Seed-8B: LongCodeZip 63.11/37.40 at 5.6√ó vs RepoGenix 60.28/34.70 at 3.5√ó.
    - Claude-3.7-Sonnet: LongCodeZip 66.27/40.20 at 4.3√ó vs RLCoder 62.76/37.90 at 4.0√ó.
  - Ablations (Table VII):
    - Largest hit comes from replacing AMI ranking with similarity: ‚àí7.89 ES / ‚àí7.20 EM.
    - Removing fine-grained stage: ‚àí1.45 ES / ‚àí1.20 EM.
    - Replacing perplexity chunking with line chunking: ‚àí1.57 ES / ‚àí1.20 EM.
  - Efficiency (Table IX; Fig. 3):
    - Generation time: reduced from 15.70s (No Compression) to 6.59s with LongCodeZip; compression adds only 2.58s.
    - GPU memory: small extra during compression (Base + 0.69 GB), and similar generation memory to other compressors.
    - Performance vs remaining context (Fig. 3): LongCodeZip dominates other methods across all compression levels; gains are especially large at severe compression (<10% remaining).

- Robustness and transferability
  - Cross-model compressor/generator combinations (Table VIII): Even a 0.5B compressor performs on par with larger ones (Avg ES 60.13 vs 60.58 with 7B), indicating strong generalization and practicality for resource-constrained settings.

- Do the experiments support the claims?
  - Coverage: three tasks, multiple datasets/languages, open/closed models, advanced baselines, ablations, efficiency, and a qualitative case (Fig. 4).
  - Results consistently show equal-or-better downstream performance at substantially higher compression ratios than baselines, validating both effectiveness and efficiency claims (Sec. V; Tables II‚ÄìVI, IX).

- Failure modes and threats
  - Failure cases: when context lacks relevant info or the instruction is ambiguous, selection can falter (Sec. VI-B ‚ÄúCase Study‚Äù).
  - Summarization metric uses LLM-as-judge; mitigated by order-robust prompting and a distinct referee model (Sec. VII ‚ÄúThreats to Validity‚Äù).

## 6. Limitations and Trade-offs
- Dependence on perplexity calibration
  - AMI and block segmentation hinge on the compression model‚Äôs perplexity estimates. If the model is poorly calibrated for a language or code style, relevance estimates may be noisy (Sec. III-A/D). Cross-model results mitigate but do not eliminate this risk (Table VIII).

- Hyperparameter sensitivity and task tuning
  - Thresholds like the boundary spike factor `Œ±`, importance `Œ≤`, and budgets (`B`, `R_fine`) are tuned per task on a held-out set (Sec. IV-E). Different domains may require re-tuning.

- Two-stage overhead vs savings
  - Although compression overhead is small (2.58s; Table IX), it adds latency before generation. For cheap or short-generation tasks, the amortization may be less favorable (Sec. V-D; Fig. 3 shows diminishing returns at milder compression).

- Structural assumptions
  - Function-level chunking presumes code follows typical structure (functions/classes). Non-standard scripts, heavily macro‚Äôd code, or generated code could reduce boundary quality. Perplexity-based block detection uses line granularity; languages with significant semantics across single lines (e.g., minified JS) may be harder (Sec. III-D).

- Scope of tasks and metrics
  - Benchmarks focus on completion, summarization, and RepoQA. Other tasks (e.g., compilation success under edits, project-level refactoring planning) were not evaluated. Summarization relies on LLM-as-judge (Sec. IV-D; VII).

- Missing static analysis signals
  - The method is purely neural without explicit program analysis (data/control flow, symbol resolution). While AMI captures implicit usefulness, certain dependency types might benefit from static cues (Related Work Sec. VIII-B).

## 7. Implications and Future Directions
- Impact on the field
  - Demonstrates that instruction-aware, perplexity-driven compression can safely shrink long code contexts by 4‚Äì6√ó while preserving or improving quality (Tables II‚ÄìIV). This reframes long-context handling: rather than only expanding windows or relying on RAG similarity, one can compress adaptively by ‚Äúwhat helps predict the answer.‚Äù

- Practical applications
  - IDE integration for repository-level completion: faster, cheaper suggestions under token budgets.
  - Documentation and code review: generate module-level summaries from large files with reduced compute (Table III).
  - Repository QA and navigation assistants: more accurate function retrieval and reasoning across languages (Table IV).

- Research directions
  - Hybrid static‚Äìneural compression: fuse AMI with program analysis (symbol tables, dependency graphs) to preserve non-local constraints.
  - Learned importance models: train small compressors to predict AMI or boundary spikes, reducing passes needed for perplexity.
  - Multi-file and cross-repo dependency handling: budget allocation across files and packages; hierarchical budgets across project graphs.
  - Interactive/constraint-aware compression: let users pin regions or specify policies (e.g., ‚Äúnever drop import blocks,‚Äù aligning with Algorithm 2‚Äôs preserved-set `P`).
  - End-to-end pipelines: pair RAG retrieval to pre-filter a repository, then apply LongCodeZip for final compression (noted as complementary in Sec. V).
  - Evaluation broadening: include compile/run correctness, human-in-the-loop developer studies, and more languages/formats (e.g., notebooks, configs).

> Overall: LongCodeZip contributes a principled, code-aware compression framework using conditional perplexity as an effective proxy for instruction relevance (Eqs. 1‚Äì3), coupled with adaptive budgeting and knapsack selection (Algorithms 1‚Äì2). Evidence across datasets, models, and baselines shows strong performance at aggressive compression ratios with tangible latency and cost reductions (Tables II‚ÄìV, IX; Fig. 3).

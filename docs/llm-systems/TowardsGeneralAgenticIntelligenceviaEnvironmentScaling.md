# Towards General Agentic Intelligence via Environment Scaling

**ArXiv:** [2509.13311](https://arxiv.org/abs/2509.13311)

## ğŸ¯ Pitch

This paper presents AgentScaler, a systematic pipeline that automatically builds large and diverse simulation environments for training language agents in robust function-calling and tool-use, overcoming the bottleneck of scarce, expensive real-world agentic data. By programmatically materializing API environments and employing a two-stage training strategy, AgentScaler enables small- and medium-scale models to achieve state-of-the-art performance on agent benchmarks, marking a major step toward practical, reliable, and scalable agentic intelligence for real-world applications.

---

## 1. Executive Summary
This work introduces AgentScaler, a full pipeline that builds large, verifiable tool-use environments automatically and then trains language agents in two stages (general then domain-specific) on the experiences collected in those environments. The key significance is that it replaces scarce, expensive, and hardâ€‘toâ€‘verify real-world tool data with scalable, programmatically verified simulations, yielding strong functionâ€‘calling performance with relatively small models across multiple agent benchmarks (Ï„â€‘bench, Ï„Â²â€‘Bench, ACEBench).

## 2. Context and Motivation
- Problem the paper tackles
  - Large Language Models (LLMs) need reliable â€œagenticâ€ capabilities: deciding when and how to call tools/APIs in multi-step tasks to affect the external world. The bottleneck is the scarcity of highâ€‘quality â€œagentic data,â€ i.e., full trajectories of tool calls with arguments, tool responses, and resulting state changes (Abstract; Section 1).
  - Building those trajectories in real environments is costly, fragile (API downtime), and hard to supervise. Synthetic approaches often lack naturalness or verifiability (Section 1).

- Why this matters
  - Real-world deployments (customer support, booking, operations, enterprise automation) rely on precise, robust function calling across heterogeneous APIs. Without consistent tool-use competence, models hallucinate or fail in multi-step workflows.

- Prior approaches and their limits
  - Reverse synthesis: generate user queries from observed tool calls (produces less realistic trajectories; Section 1).
  - Forward simulation with high-level intents and humanâ€“agent interplay: more natural but typically requires manual environment construction and is not scalable (Section 1).
  - LLM-simulated tool responses: cheap but can hallucinate and vary inconsistently (Section 6.1).
  - Offline execution environments for evaluation (e.g., Ï„â€‘bench): useful but usually manually built and not scaled for large-scale training (Section 6.1).

- Positioning
  - This work centers the environment itself as the lever for scaling agentic intelligence. It programmatically constructs heterogeneous, verifiable, fully simulated environments from ~30,000 APIs, then performs large-scale experience collection and two-stage fine-tuning (Sections 2â€“3). It claims state-of-the-art open-source performance under 1T parameters on multiple benchmarks (Table 1).

Definitions used selectively:
- Function calling: the model invokes external tools/APIs by emitting a structured call with `function_name` and `arguments`.
- Agentic data: trajectories of multi-turn interactions where the assistant performs tool calls that read or modify an environment.
- Environment (here): a domain-specific, readâ€“write database that tools operate on (Section 2, â€œDesign Principalâ€).
- pass^k: the accuracy when a model must answer the same question correctly in all k independent trials; lower as k increases indicates instability (Figure 4).

## 3. Technical Approach
The pipeline has two major phases: environment build & scaling (Section 2; Figure 1) and agent experience learning (Section 3; Figure 2).

A. Unifying view: tools as readâ€“write operators over a database
- Core abstraction (Section 2, â€œDesign Principalâ€):
  - Each tool is an operator of type `read` or `write` on an underlying database `D`. A tool call with arguments `Î±` is represented as `op(func)(Î±; D)`.
  - Tools within a domain share a common database schema `S_k`. The challenge becomes: partition tools into domains `{T_1, â€¦, T_M}` and assign each a schema `S_k` that captures shared readâ€“write patterns.

Why it matters
- This makes environment feedback executable and verifiable: you can check tool arguments and confirm the database state transitions caused by `write` operations.

B. Environment automatic build (Section 2.1; Figure 1, left/middle)
1) Scenario collection
   - Aggregate >30,000 APIs from ToolBench, APIâ€‘Gen, and internal repositories (Section 2.1 â€œScenario Collectionâ€).
   - Filter and rewrite API descriptions to add explicit inputâ€“output specs; also construct tool compositions from inputâ€“output relationships to form a large API pool `Î˜_F` (~30k tools).

2) Tool dependency graph modeling
   - Build a graph where nodes are tools and edges mean compositional compatibility (arguments/outputs align).
   - For two tools `i` and `j`, compute parameter embeddings `Ï•(P_func)` and add an edge if cosine similarity > threshold `Ï„`:
     - Equation (1): `E = { (i, j) | sim(Ï•(P_i), Ï•(P_j)) > Ï„, i â‰  j }` (Section 2.1).
   - Cluster this graph into domains with Louvain community detection (a standard graph clustering algorithm).
   - Within each domain, refine edges using an LLM to check pairwise dependencies more carefully (to fix vector-similarity errors). Outcome: `M` domains (over 1,000).

   Design choice rationale
   - Louvain gives scalable, unsupervised community detection.
   - Parameter-embedding similarity quickly proposes edges; a second LLM pass improves precision.

3) Function schema programmatic materialization
   - For each domain, use tool parameter definitions to synthesize a domain-specific database schema (the environment state).
   - Implement each tool as Python code that executes readâ€“write operations over that schema (Section 2.1 â€œFunction Schema Programmatic Materializationâ€).
   - Manual checks show the produced schemas and code align closely with Ï„â€‘benchâ€™s official implementations for overlapping domains (Section 2.1).

   Why this matters
   - It turns abstract tool specs into executable, testable code grounded on a concrete databaseâ€”enabling deterministic execution and verifiable state transitions.

C. Agentic task construction (Section 2.2; Figure 1, right)
- Initialize an environment state that is diverse across instances.
- Sample a logically coherent tool sequence from the domainâ€™s directed dependency graph:
  - Start at a random tool; do a directed walk until max steps or a node with no outgoing edges (Section 2.2).
- For each tool in sequence:
  - Generate the arguments.
  - Execute the tool against the database, tracking state changes.
- Synthesize the overall user intent that matches the tool sequence and environment trajectory (Figure 1, â€œTasksâ€).
- Two levels of verifiability (Section 2.2):
  - Database-level: final state matches expected â€œgoldâ€ state when `write` tools are used.
  - Tool-sequence level: exact match on tool names and arguments for read-only or mixed sequences.

D. Humanâ€“agent interplay for experience collection (Section 3.1; Figure 2)
- Instantiate a simulated user with the overall intent.
- Let a task agent interact through multi-turn conversation and tool calls until the (simulated) user deems the task done.
- Record trajectories of turns, tool calls, tool responses, and environment states (Figure 2).

Data filtering: a three-stage funnel (Section 3.1 â€œFilteringâ€)
1) Validity control
   - Remove malformed dialogues and highly repetitive reasoning (nâ€‘gram filtering).
2) Environment state alignment
   - Keep only trajectories whose final database state equals the gold stateâ€”verifies effective `write` operations.
3) Function calling exact match
   - For read-only (or read-heavy) trajectories, require exact match of both tool names and arguments to the gold planâ€”ensures high-fidelity supervision.

Note: They keep trajectories with tool-call errors if the final goal is still achieved, to improve robustness (Section 3.1).

E. Agentic experience learning (Section 3.2)
- Training objective masks out human instructions `h_i` and tool response tokens `Ï_t`, but conditions on them; the loss only supervises assistant tool-calls `Ï„_t` and assistant replies `y_t`.
  - Equation (2) (Section 3.2): a masked next-token loss over tokens in set `T` (assistant tool calls and natural language replies):
    - Intuition: learn when/how to call tools and how to wrap results in user-facing responses, without being penalized for tool outputs or user text.

- Two-stage experience learning (Section 3.2)
  1) Stage 1: General toolâ€‘use pretraining across many domains to acquire foundational skills (tool selection, argument formation, integrating tool outputs).
  2) Stage 2: Domain specialization on verticals (e.g., retail, airline, telecom) to adapt to domainâ€‘specific goals and constraints.

F. Models
- Train `AgentScaler` models at 4B, 8B, 30Bâ€‘A3B scales on Qwen3 backbones (Section 4.1 â€œBackbonesâ€).

## 4. Key Insights and Innovations
1) Automated, verifiable environment scaling
   - Whatâ€™s new: A principled way to convert tens of thousands of tool specs into executable, domainâ€‘clustered environments with shared database schemas and Python-implemented tools (Section 2.1; Figure 1).
   - Why it matters: Enables largeâ€‘scale, lowâ€‘latency, fully simulated interaction with precise verification at both state and toolâ€‘sequence levels (Section 2.2). This is a step change from adâ€‘hoc, manually assembled environments.

2) Graphâ€‘centric domain discovery and task sampling
   - Novelty: Build a tool dependency graph from parameter embeddings (Equation (1)); cluster with Louvain; then sample coherent tool sequences by directed walks (Sections 2.1â€“2.2).
   - Significance: Systematically broadens scenario diversity while maintaining internal logic, improving coverage of functionâ€‘calling patterns beyond curated datasets.

3) Threeâ€‘stage trajectory filtering anchored in ground truth state
   - Innovation: The funnelâ€”validity control â†’ environment state alignment â†’ exact matchâ€”balances realism (allowing some errors) with reliability (stateâ€‘based verification) (Section 3.1).
   - Impact: Produces highâ€‘fidelity supervision signals without human annotation, critical for scalable training.

4) Twoâ€‘stage agent training: general â†’ vertical
   - Distinctive design choice: First acquire generic toolâ€‘use competence, then specialize for domain realism (Section 3.2; Figure 3).
   - Evidence: Ablation shows both stages improve performance; Stage 2 further boosts multiâ€‘step â€œAgentâ€ tasks in ACEBenchâ€‘en (Figure 3).

5) Strong results with compact models
   - Insight: A 4B model trained with this pipeline competes with or surpasses many 30B baselines on toolâ€‘use benchmarks (Table 1), suggesting that highâ€‘quality, verifiable experience can substitute for sheer parameter count.

Fundamental vs. incremental:
- Fundamental: the environment scaling and verifiable simulation framework (Sections 2â€“3).
- Incremental but effective: masking strategy in the loss (Equation (2)); Louvain clustering; exactâ€‘match filtering. These choices are not entirely new individually but combine into a compelling endâ€‘toâ€‘end system.

## 5. Experimental Analysis
Evaluation setup (Section 4.1):
- Benchmarks and metrics
  - Ï„â€‘bench (retail, airline): report pass^1 (Figure/Table references throughout the paper).
  - Ï„Â²â€‘Bench (retail, airline, telecom): report pass^1 and pass^k (Figure 4).
  - ACEBenchâ€‘en: report Accuracy on Normal, Special, Agent, and Overall categories (Table 1).
- Baselines
  - Closed source: Geminiâ€‘2.5â€‘pro, Claudeâ€‘Sonnetâ€‘4, GPTâ€‘o3, GPTâ€‘o4â€‘mini, GPTâ€‘5â€‘think.
  - Open source: GPTâ€‘OSSâ€‘120Bâ€‘A5B, DeepSeekâ€‘V3.1â€‘671Bâ€‘A37B, Kimiâ€‘K2â€‘1Tâ€‘A32B, Qwen3â€‘Thinkingâ€‘235Bâ€‘A22B, Seedâ€‘OSSâ€‘36B, Qwenâ€‘Coderâ€‘30Bâ€‘A3B, xLAMâ€‘2 variants, and Qwen3 baselines (Table 1).
- AgentScaler models
  - 4B, 8B, 30Bâ€‘A3B trained on Qwen3 series (Section 4.1 â€œBackbonesâ€).

Main quantitative findings (Table 1)
- Ï„â€‘bench (pass^1)
  - `AgentScalerâ€‘30Bâ€‘A3B`: Retail 70.4; Airline 54.0.
  - `AgentScalerâ€‘4B`: Retail 64.3; Airline 54.0 (notable for a small model).
- Ï„Â²â€‘Bench (pass^1)
  - `AgentScalerâ€‘30Bâ€‘A3B`: Retail 70.2; Airline 60.0; Telecom 55.3.
- ACEBenchâ€‘en (Accuracy)
  - `AgentScalerâ€‘30Bâ€‘A3B`: Normal 76.7; Special 82.7; Agent 60.0; Overall 75.7.
  - `AgentScalerâ€‘4B`: Normal 70.3; Special 76.7; Agent 30.8; Overall 65.9.
- Baseline comparisons from Table 1
  - Against strong open-source Kimiâ€‘K2â€‘1Tâ€‘A32B (â‰ˆ1T parameters), `AgentScalerâ€‘30Bâ€‘A3B` is comparable:
    - Ï„â€‘bench Retail 70.4 vs 73.9; Airline 54.0 vs 51.2.
    - Ï„Â²â€‘Bench Retail 70.2 vs 70.6; Airline 60.0 vs 56.5; Telecom 55.3 vs 65.8 (Kimi stronger on Telecom).
    - ACEBenchâ€‘en Overall 75.7 vs 77.4 (close).
  - Against base Qwen3 models of similar size:
    - `AgentScalerâ€‘4B` (Overall 65.9) greatly improves over Qwen3â€‘Thinkingâ€‘4B (49.5) on ACEBenchâ€‘en.
    - `AgentScalerâ€‘30Bâ€‘A3B` (Overall 75.7) vs Qwen3â€‘Thinkingâ€‘30Bâ€‘A3B (67.2) shows a large gain.

Summary claim supported by Table 1:
> Across Ï„â€‘bench, Ï„Â²â€‘Bench, and ACEBenchâ€‘en, AgentScaler models at each comparable scale (4B, 8B, 30Bâ€‘A3B) outperform their base Qwen3 counterparts and are competitive with or better than most openâ€‘source models below 1T parameters.

Ablation on twoâ€‘stage training (Figure 3)
- Stage 1 and Stage 2 both improve ACEBenchâ€‘en on Normal, Agent, and Overall subsets.
- Multiâ€‘step agent tasks (the â€œAgentâ€ subset) benefit especially from Stage 2â€™s domain specialization.

Stability via pass^k (Figure 4)
- On Ï„Â²â€‘Bench, `AgentScalerâ€‘30Bâ€‘A3B` has higher weighted overall scores than Qwen3â€‘Thinkingâ€‘30Bâ€‘A3B across all k.
  - Example from â€œWeighted Overallâ€ plot: pass^1 â‰ˆ 62.5 vs 45.3; pass^4 â‰ˆ 30.6 vs 27.7.
- Scores decrease as k increases (both models), highlighting remaining instability in LLM tool use:
> â€œa clear downward trend in scores is observed as k increasesâ€ (Figure 4 commentary, Section 5).

Outâ€‘ofâ€‘distribution robustness: ACEBenchâ€‘zh (Table 2)
- `AgentScalerâ€‘30Bâ€‘A3B` achieves Overall 81.5 (+7.3 over Qwen3â€‘Thinkingâ€‘30Bâ€‘A3B).
- Large gains for smaller models:
  - `AgentScalerâ€‘4B` Overall 65.6 (+21.7); â€œAgentâ€ 38.4 (+31.7).
- Mixed results on the Special subset (e.g., âˆ’3.4 for 30B), suggesting domain nuances.

Longâ€‘horizon toolâ€‘calling difficulty (Figure 5)
- Accuracy declines as the number of tool calls increases in Ï„â€‘bench (both retail and airline).
- The trend lines show a negative correlation between toolâ€‘call count and task accuracy, indicating remaining challenges in extended tool chains.

Do the experiments support the claims?
- The breadth (three benchmarks; English and Chinese), explicit SOTA comparisons for subâ€‘1T models (Table 1), ablations (Figure 3), stability analysis (Figure 4), and longâ€‘horizon diagnosis (Figure 5) provide a convincing empirical case that the pipeline improves functionâ€‘calling competence and stability, especially for compact models.

## 6. Limitations and Trade-offs
Assumptions and scope
- Simulated environments are adequate proxies for real APIs
  - Tools are grounded in a database abstraction. Many real APIs have side effects and nondeterminism not captured in a readâ€“write DB (e.g., network latency, asynchronous jobs, quotas). This can limit transfer to deployment settings.
- Determinism and verifiability
  - The validation relies on deterministic state changes and exact matching (Section 3.1). This may bias training toward a single â€œgoldâ€ sequence, even when multiple valid tool plans exist.

Scenarios not fully addressed
- Long-horizon planning remains weak (Figure 5).
- Complex crossâ€‘domain tasks that require switching among multiple domains/tools with nontrivial dependencies may exceed the directed-walk samplerâ€™s coverage.

Computational and data considerations
- Building the tool graph and refining edges with an LLM across >30k tools and >1k domains can be computeâ€‘intensive (Section 2.1). The paper does not report the cost/time of environment construction.
- Simulated user quality: outcomes depend on how well the simulated user reflects realistic intents and feedback; details of user simulation are not deeply quantified (Figure 2 provides the conceptual flow).

Methodological tradeâ€‘offs
- Filtering keeps some trajectories with intermediate errors to improve robustness (Section 3.1). While sensible, it can introduce noisy supervision if the final success masks brittle behavior.
- The masked loss (Equation (2)) does not directly optimize toolâ€‘response interpretation quality; it relies on conditioning on tool outputs without supervising them, which may limit learning to detect tool-response anomalies.

Model scale and learning paradigms
- Current validated upper bound is 30B parameters (Limitations). Effects at 200B+ or trillionâ€‘scale are untested.
- No reinforcement learning (RL) yet, despite the environment being â€œRLâ€‘readyâ€ (Limitations).

## 7. Implications and Future Directions
How this work changes the landscape
- It reframes agent training around scalable, verifiable environment construction rather than around rare realâ€‘world logs or fragile LLMâ€‘simulated tool responses.
- It demonstrates that compact models (4Bâ€“30B) can acquire strong toolâ€‘use competence when trained on highâ€‘fidelity simulated experiences, potentially lowering the barrier for onâ€‘device or latencyâ€‘sensitive applications.

What it enables next
- RL on top of simulated environments
  - The pipelineâ€™s determinism and low latency make it ideal for policy-gradient style training, preference learning for tool plans, or curriculum learning over tool-graph difficulty (Limitations; Conclusion).
- Tackling longâ€‘horizon tool use
  - Incorporate planning modules, hierarchical policies, or search over tool graphs; augment the sampler with constraints to encourage diverse yet optimal plans (Figure 5 diagnosis).
- Multi-modality and realâ€‘world deployment
  - Extend schemas and tools to cover vision, speech, and sensor streams; progressively â€œswap inâ€ real MCP servers for subsets of tools to bridge simâ€‘toâ€‘real.

Practical applications
- Enterprise operations: customer support, order management, and scheduling (Ï„â€‘bench domains) with verifiable action traces.
- Agent evaluation and data generation platforms: a dropâ€‘in environment builder to synthesize diverse, testable tool-use tasks for training/evaluation at scale.
- Safety and reliability: stateâ€‘based verification and exactâ€‘match filters can be adapted as guardrails and auditing tools for production agents.

Representative supporting quotes and pointers
- On environment abstraction and verification:
  > â€œeach function â€¦ can be assigned an operator type â€¦ over the database D â€¦ enabling verifiability at both the environment level and the toolâ€‘argument response level.â€ (Section 2; Figure 1)
- On the graph-based domain scaling:
  > Equation (1) defines edges via parameterâ€‘similarity; domains come from Louvain community detection with an LLM refinement pass. (Section 2.1)
- On training supervision:
  > Equation (2) masks human text and tool responses, optimizing only assistant tool calls and replies while conditioning on the rest. (Section 3.2)
- On main results:
  > Table 1: `AgentScalerâ€‘30Bâ€‘A3B` achieves ACEBenchâ€‘en Overall 75.7 and Ï„Â²â€‘Bench Retail/Airline/Telecom 70.2/60.0/55.3, competitive with much larger open models.
- On stability and longâ€‘horizon challenges:
  > Figure 4 shows `pass^k` declines with k but AgentScaler is consistently above its Qwen3 base; Figure 5 shows accuracy decreases as toolâ€‘call counts increase.

In sum, AgentScaler delivers a principled, scalable route to train and evaluate toolâ€‘using agents by turning the â€œenvironmentâ€ into a firstâ€‘class, verifiable object. The combination of environment scaling, trajectory filtering anchored in state verification, and twoâ€‘stage training yields strong empirical gains, especially for smaller models, and opens the door to RL, longerâ€‘horizon planning, and simâ€‘toâ€‘real deployment pipelines.

# Efficient Memory Management for Large Language Model Serving with PagedAttention

**ArXiv:** [2309.06180](https://arxiv.org/abs/2309.06180)

## ğŸ¯ Pitch

This paper introduces PagedAttention, a new attention algorithm and memory management strategy for LLM serving, inspired by operating system virtual memory and paging. By breaking up large, contiguous KV cache allocations into small, flexible 'pages' that can be dynamically allocated, shared, and copy-on-written, the vLLM system nearly eliminates memory waste and dramatically increases the number of requests that can be batchedâ€”enabling 2â€“4Ã— higher throughput compared to prior systems at the same latency. This breakthrough directly tackles the growing memory bottleneck in LLM serving, making deploying and scaling large models vastly more cost-effective and capable.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces PagedAttention and the vLLM serving engine, a rethinking of how large language models (LLMs) store and access their attention â€œkey-valueâ€ (KV) cache during generation. By replacing large, contiguous KV tensors with small, fixed-size â€œpagesâ€ that can be allocated, shared, and copied on demandâ€”much like an operating systemâ€™s virtual memoryâ€”vLLM reduces memory waste to near zero and enables 2â€“4Ã— higher serving throughput at similar latency compared to state-of-the-art systems (Figures 1, 2, 12; Â§6).

## 2. Context and Motivation
- Problem addressed
  - Serving LLMs efficiently is limited by GPU memory devoted to the `KV cache`â€”the stored â€œkeyâ€ and â€œvalueâ€ vectors that let the model attend to previously seen tokens during generation (Â§2.2). This cache is large, grows token-by-token, and its final size per request is unknown beforehand.
  - Existing systems allocate the KV cache as big, contiguous tensors per request and â€œreserveâ€ space up to a maximum length, which causes fragmentation and prevents sharing (Â§3.1, Figure 3).

- Why this matters
  - Real systems are memory-bound: for a 13B-parameter model on an NVIDIA A100 40GB, ~65% of memory holds weights while nearly 30% is for KV cache (Figure 1, left). Inefficient KV management limits how many requests can be batched together, directly reducing throughput and raising cost per request (Â§1).
  - GPU compute capacity is growing faster than memory capacity (e.g., A100â†’H100 FLOPS >2Ã—, but max memory still 80GB), so memory bottlenecks will worsen (Â§3).

- Where prior approaches fall short
  - FasterTransformer provides high-performance kernels but no fine-grained batching or KV memory virtualization; batch size is capped by static pre-reservation (Â§6.1).
  - Orca enables iteration-level scheduling (add/remove requests each decoding step) but still stores each requestâ€™s KV cache in contiguous chunks and must over-reserve space, causing internal and external fragmentation (Â§3.1). In experiments, effective KV usage for such systems can be as low as 20â€“38% (Figure 2).

- Positioning
  - The paper reframes KV memory as a virtual, paged address space: map many small logical â€œKV blocksâ€ to physical blocks that need not be contiguous or pre-reserved (Â§4.1â€“4.3). This eliminates most fragmentation, enables sharing across sequences (parallel sampling, beam search), and supports preemption with swap/recompute strategies (Â§4.4â€“4.5). It complements prior scheduling work by making more requests fit in memory per step (Â§9).

## 3. Technical Approach
The system has two parts: the PagedAttention kernel (how attention reads/writes non-contiguous KV) and the vLLM memory/scheduling runtime (how KV blocks are allocated, shared, and moved).

- Background needed (brief)
  - KV cache: during the prompt (â€œprefillâ€) phase, the model computes key/value vectors for all prompt tokens; during the generation phase it appends one token at a time, reusing the cached KV for attention (Â§2.2). The cost/latency of generation is dominated by reading these stored KV vectors.

- Core idea: KV as paged memory
  - Replace one large, contiguous KV tensor per sequence with many fixed-size â€œKV blocksâ€ of size `B` tokens (default B=16; Â§7.2). Each block stores keys/values for a small run of positions (across all attention heads/layers; a practical design stores per-head/layer blocks; Â§4.1 footnote).
  - Maintain a `block table` per sequence that maps `logical` block indices (0,1,2,â€¦) to `physical` block IDs and tracks how many token slots in the last block are filled (Â§4.2, Figure 6).
  - Allocate a fresh physical block only when the last logical block becomes full; only the last block is partially filled. This limits per-sequence unused space to at most one block (Â§4.3).

- PagedAttention kernel (how queries read non-contiguous KV)
  - Instead of multiplying a query with one large, contiguous matrix of keys/values, the kernel iterates over logical blocks and fetches their physical locations via the block table (Figure 5).
  - Formulation (Eq. 4 in Â§4.1): attention on token i is computed in block chunks: compute scores against keys in each block, normalize over all blocks up to âŒˆi/BâŒ‰, then form the output by weighting and summing the value blocks. In essence, matrix-vector attention is decomposed into several block multiplications that are stitched together.
  - GPU optimizations (Â§5.1): fused reshape+write for KV block output, fused block-read with attention, and a fused â€œblock copyâ€ kernel (for copy-on-write) to minimize kernel launch and small-copy overheads. Coalesced access is maintained by assigning one GPU warp per block.

- KV cache manager and scheduler (Â§4.2â€“Â§4.3; Figure 4)
  - A centralized scheduler coordinates distributed GPU workers (Megatron-LM style tensor-parallel execution; Â§4.6).
  - A `block engine` on each worker holds a large pre-allocated slab of GPU memory sliced into fixed-size physical blocks; the scheduler manages logicalâ€“physical mappings via block tables that are broadcast each iteration (Â§4.6).
  - Iteration flow (Figure 6): during prefill, allocate only the blocks needed to hold the prompt tokens; during each generation step, either append into the last partially filled block or allocate exactly one new physical block. New blocks are assigned only when needed and only for the sequences actually active in that step.

- Memory sharingâ€”two major patterns (Â§4.4)
  - Parallel sampling (â€œN best-of samples from the same promptâ€):
    - Sequences share the promptâ€™s blocks; a per-block `reference count` tracks how many sequences point to the same physical block (Figure 8).
    - If a shared block must be modified (e.g., writing the next token into the last partially filled prompt block), vLLM performs `copy-on-write` (COW) at block granularity: allocate a new block, copy old contents, decrement the old blockâ€™s refcount, then write. This is identical to OS page COW but at token-block granularity.
  - Beam search:
    - Candidates form a tree that frequently share long prefixes. vLLM maintains sharing at block granularity across the beam; when a candidate drops from the top-k, its logical blocks are freed and any physical blocks whose refcount hits zero are released (Figure 9).
    - This avoids the â€œcopy the whole prefixâ€ behavior of conventional systems; only when writing inside a shared (non-final) block does vLLM need to copy a single block (COW).

- Shared prefix caching: popular â€œsystem promptsâ€ (e.g., instructions and examples) can be materialized once and reused across requests, much like shared libraries across processes; only user-specific tail prompts are computed and stored anew (Â§4.4, Figure 10).

- Scheduling and preemption (Â§4.5)
  - Policy: first-come-first-serve (FCFS) to avoid starvation.
  - Eviction granularity: `all-or-nothing` per â€œsequence groupâ€ (e.g., the N beams in one beam search request are gang-scheduled). Since all blocks for a sequence are needed together, partial eviction offers no benefit (Â§4.5).
  - Recovery options when GPU blocks are exhausted:
    - `Swapping`: evicted KV blocks are copied to CPU RAM; vLLM includes a CPU block allocator. Swap space is bounded by KV memory size on GPU (Â§4.5).
    - `Recomputation`: when resumed, regenerate the KV cache by treating generated tokens so far as an extended prompt; this is faster than the original because it uses the batched, parallel â€œprefillâ€ computation (Â§4.5).
  - Which to use? Microbenchmarks show recomputation cost is stable across block sizes, while swapping is inefficient for small blocks due to many tiny PCIe transfers; for medium block sizes (16â€“64) both are comparable (Figure 19; Â§7.3).

- Distributed execution (Â§4.6)
  - Tensor-parallel SPMD execution with all-reduce; the same block tables are broadcast to all workers each step. Each worker stores KV only for its subset of heads. No per-step synchronization is needed for memory management beyond the initial broadcast (Figure 4; Â§4.6).

- Implementation notes (Â§5)
  - 8.5K lines Python + 2K lines C++/CUDA; supports GPT/OPT/LLaMA; uses NCCL for communication and a FastAPI frontend with an OpenAI-compatible API.
  - Despite added indirection, the attention kernel overhead is modest: 20â€“26% slower per-kernel than FasterTransformer, yet overall throughput is much higher due to larger batch sizes enabled by paging (Figure 18a; Â§7.1).

## 4. Key Insights and Innovations
- KV cache virtualization for LLM serving (fundamental)
  - Whatâ€™s new: Treat KV memory like OS virtual memoryâ€”paged, non-contiguous, mapped by a block table; attention runs on â€œpagesâ€ (KV blocks) instead of monolithic tensors (Â§4.1â€“4.3).
  - Why it matters: It eliminates both internal and external fragmentation (Figure 3), reducing waste to roughly one block per sequence and raising effective KV usage from ~20â€“38% in prior designs to ~96% (Figure 2). This directly increases feasible batch size and throughput (Figures 12â€“13).

- Block-granular sharing with copy-on-write (fundamental)
  - Whatâ€™s new: Seamless sharing of long common prefixes across samples/beams and across requests (for shared system prompts), with automatic COW only when a shared block must be modified (Â§4.4; Figures 8â€“10).
  - Why it matters: Sharing reduces KV memory footprints by 6â€“10% for parallel sampling and 38â€“55% for beam search on Alpaca; even larger on ShareGPT (16â€“30% and 44â€“66%) (Figure 15; Â§6.3). It also removes expensive bulk prefix copies that older systems perform in beam search (Â§4.4).

- Preemption with swapping or recomputation (practical innovation)
  - Whatâ€™s new: When memory is tight, evict whole sequence groups and either swap blocks to CPU RAM or recompute later (Â§4.5).
  - Why it matters: This prevents deadlock under unknown and varying output lengths. The paper shows regimes where recomputation is preferable (small blocks) and where swapping is competitive (larger blocks) (Figure 19; Â§7.3).

- Kernel fusion for paged attention (incremental but necessary)
  - Whatâ€™s new: Fused kernels for block reshaping/writing, block-reading-with-attention, and bulk block copying (Â§5.1).
  - Why it matters: It offsets the cost of indirection and small-copy overhead, making the paged design viable on GPUs.

## 5. Experimental Analysis
- Setup (Â§6.1; Table 1; Figure 11)
  - Models: OPT-13B (1Ã—A100-40GB), OPT-66B (4Ã—A100), OPT-175B (8Ã—A100-80GB); LLaMA-13B for a translation test. Table 1 details parameter sizes and available KV memory (e.g., 13B: 26GB params, 12GB KV).
  - Workloads: synthetic traces built from real conversations/instructions:
    - ShareGPT (long prompts, long outputs; mean input 161 tokens, output 338; Figure 11a).
    - Alpaca (shorter; mean input 19 tokens, output 58; Figure 11b).
  - Arrivals: Poisson with varying request rates. Metric: `normalized latency` (mean latency per request divided by its output length, s/token). This captures throughput saturation (curves â€œspikeâ€ when the systemâ€™s capacity is exceeded; Figures 12, 14, 16, 17).
  - Baselines:
    - FasterTransformer with a dynamic batching scheduler (max batch constrained by memory).
    - Orca variants: `Oracle` (knows the true output lengthâ€”upper bound), `Pow2` (reserves up to next power-of-2), `Max` (reserves up to model max length). All use contiguous KV and buddy allocation (Â§6.1).

- Main results
  - Effective KV memory utilization
    - Quote (Figure 2): vLLM reaches â€œ96.3%â€ KV usage vs â€œ38.2%â€ (Orca-Oracle), â€œ26.8%â€ (Orca-Pow2), and â€œ20.4%â€ (Orca-Max). The remaining portions are reservation/internal/external fragmentation.
  - Basic sampling throughput (Figure 12; Â§6.2; Figure 13)
    - On ShareGPT, vLLM sustains â€œ1.7Ã—â€“2.7Ã—â€ higher request rates than Orca-Oracle and â€œ2.7Ã—â€“8Ã—â€ than Orca-Max at similar latency. It sustains up to â€œ22Ã—â€ higher rates than FasterTransformer, whose batch size is severely limited by KV reservation.
    - Example at OPT-13B: vLLM batches â€œ30.42â€ concurrent requests vs â€œ13.62â€ (Orca-Oracle) and â€œ7.00â€ (Orca-Max) (Figure 13a).
    - On Alpaca (short sequences), the advantage vs Orca is smaller for OPT-175B because the setup has abundant KV memory, making the workload compute-bound rather than memory-bound (Â§6.2; Figure 12f; Figure 13b).
  - Parallel sampling and beam search (Figure 14; Â§6.3)
    - As parallel size or beam width grows, vLLMâ€™s advantage increases because more sharing becomes possible. On OPT-13B/Alpaca, improvement over Orca-Oracle grows from â€œ1.3Ã—â€ (basic sampling) to â€œ2.3Ã—â€ (beam width 6) (Â§6.3).
    - Memory savings from sharing: â€œ6.1â€“9.8%â€ for parallel sampling and â€œ37.6â€“55.2%â€ for beam search on Alpaca; on ShareGPT, â€œ16.2â€“30.5%â€ and â€œ44.3â€“66.3%â€ (Figure 15).
  - Shared prefix caching (Figure 16; Â§6.4)
    - Englishâ†’German translation with LLaMA-13B:
      - With a 1-shot prefix (80 tokens), vLLM delivers â€œ1.67Ã—â€ higher throughput vs Orca-Oracle (Figure 16a).
      - With a 5-shot prefix (341 tokens), the gain increases to â€œ3.58Ã—â€ (Figure 16b).
  - Chatbot setting (Figure 17; Â§6.5)
    - With long conversation history (truncated to last 1024 tokens), all Orca variants must reserve 1024 tokens for outputs due to buddy allocation; vLLM still pages. vLLM sustains roughly â€œ2Ã—â€ higher request rates than all Orca variants at similar latency.
  - Ablations and microbenchmarks (Â§7)
    - Kernel overhead: PagedAttentionâ€™s attention kernel is â€œ20â€“26%â€ slower than FasterTransformerâ€™s per call (Figure 18a), yet end-to-end is faster due to larger batches.
    - Block size trade-off: best performance at block sizes â€œ16â€“128â€ on ShareGPT and â€œ16â€“32â€ on Alpaca; too large increases internal fragmentation, too small underutilizes GPU (Figure 18b). Default is 16 (Â§7.2).
    - Swap vs recompute: swapping is inefficient at small blocks due to many small PCIe copies; recomputation cost is flat across block sizes and is never more than â€œ20%â€ slower than swapping when swapping is favorable (Figure 19; Â§7.3).

- Overall assessment
  - The evaluation mixes challenging (ShareGPT) and easier (Alpaca) traces, three model sizes including multi-GPU settings, and diverse decoding methods. The consistent throughput gains, the sharp increase in batched requests (Figure 13), and the measured KV usage (Figure 2) collectively support the central claim: paging KV enables substantially higher throughput by fitting more concurrent sequences in memory.
  - The ablations convincingly address concerns about kernel overhead and design choices (block size, recovery method).

## 6. Limitations and Trade-offs
- Indirection overhead and kernel complexity
  - Attention kernels must follow block tables and handle non-contiguous memory, adding branching and address computations. This costs â€œ20â€“26%â€ extra kernel time (Figure 18a). vLLM compensates with larger batches, but per-op latency is higher.
- Block size tuning
  - Too small: poorer GPU utilization and more metadata; too large: more internal fragmentation and fewer sharing opportunities (Figure 18b). A fixed block size (default 16) may be suboptimal for very short or very long prompts.
- Preemption granularity
  - All-or-nothing eviction of sequence groups simplifies design but may be coarse; evicting an entire group to free memory can cause head-of-line blocking for those requests (Â§4.5).
- Swapping constraints
  - Swap performance depends on CPUâ€“GPU bandwidth and suffers at small block sizes (Figure 19a). Systems without ample CPU RAM or with limited interconnects may prefer recomputation, which increases compute load (Â§7.3).
- When the workload is compute-bound
  - If KV memory is abundant (e.g., OPT-175B on many GPUs + short Alpaca sequences), benefits shrink since throughput is limited by arithmetic rather than memory (Figure 12f; Â§6.2).
- Scope
  - The work targets inference-time serving for autoregressive Transformers. It does not cover training, non-autoregressive architectures, or model-specific tricks like KV quantization/compression; nor does it introduce QoS-aware scheduling beyond FCFS.

## 7. Implications and Future Directions
- How this changes the landscape
  - Treating the KV cache as a virtual, shareable, copy-on-write address space reframes LLM serving as a memory-systems problem. This unlocks larger effective batch sizes, enables mixing decoding strategies in one batch (Â§4.4), and reduces the per-request cost of serving long-context applications (Figure 17).

- Practical applications
  - Hosted LLM APIs and chat systems can cut cost and increase capacity without changing models; multi-sample code assistants and beam-search-heavy tasks (e.g., translation) benefit disproportionately due to sharing (Figures 14â€“16).
  - Shared-prefix caching can accelerate enterprise deployments where prompts share instruction templates (Â§6.4).

- Follow-up research directions
  - Adaptive paging policies: dynamic block sizes, smarter eviction than all-or-nothing, or cost-aware swap vs recompute decisions based on live bandwidth/compute measurements (Â§7.3).
  - KV compression and quantization within pages: combine paging with lossy or lossless KV compression to further increase capacity.
  - Hardware support: NIC/GPU/DRAM features for page-table-assisted gathers, block-level DMA, or KV-aware caches.
  - Multi-tenant schedulers: integrate paging with SLA-aware or priority scheduling instead of FCFS, exploring fairness/latency trade-offs.
  - Extending to other sequence models and long-context techniques (e.g., retrieval-augmented generation): page KV alongside external memory indices.
  - Distributed paging across nodes: remote KV â€œpagingâ€ over high-speed interconnects to pool memory across machines.

In short, this work contributes a robust system designâ€”paging the KV cache with OS-style mechanismsâ€”that materially improves the throughput of LLM serving, especially for long contexts and complex decoding, while introducing a clean abstraction layer that future techniques (compression, smarter scheduling, hardware help) can build on.

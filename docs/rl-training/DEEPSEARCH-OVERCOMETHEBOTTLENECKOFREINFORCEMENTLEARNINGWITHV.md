# DEEPSEARCH: OVERCOME THE BOTTLENECK OF REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS VIA MONTE CARLO TREE SEARCH

**ArXiv:** [2509.25454](https://arxiv.org/abs/2509.25454)

## üéØ Pitch

DeepSearch fundamentally reimagines reinforcement learning with verifiable rewards (RLVR) by embedding Monte Carlo Tree Search (MCTS) directly into the training loop‚Äîtransforming search from a mere inference tool into a driver of systematic exploration and granular learning. By enabling models to efficiently discover, credit, and learn from diverse reasoning paths during training (not just at test time), DeepSearch breaks through performance plateaus, delivering new state-of-the-art accuracy on challenging mathematical reasoning benchmarks while using dramatically less computational resources. This approach paves the way for more robust, sample-efficient, and scalable reasoning in language models, with far-reaching implications for math, coding, and other domains where reliable automated reasoning is paramount.

---

## 1. Executive Summary
DeepSearch integrates Monte Carlo Tree Search (MCTS) into the training loop of reinforcement learning with verifiable rewards (RLVR), rather than using search only at inference time. By restructuring training around systematic exploration and fine-grained credit assignment, it breaks performance plateaus: on six math-reasoning benchmarks, a 1.5B model reaches 62.95% average Pass@1 accuracy with far less compute than prolonged RL training (Table 1, Table 2).

## 2. Context and Motivation
- Problem/gap:
  - RLVR trains language models with automatically checkable rewards (e.g., whether a final math answer matches the ground truth). However, standard RLVR uses limited direct rollouts during training, leading to sparse exploration and missing key reasoning paths. The paper documents plateaus after thousands of steps where extra training compute yields diminishing gains (¬ß1; Table 2).
  - Meanwhile, structured search (e.g., tree search) is often applied only at inference, not during training, creating a mismatch: models are asked to explore at test time without having learned from exploration during training (¬ß1).
- Why it matters:
  - Practically, improved reasoning is valuable in math, coding, scientific problem solving, and any domain with verifiable outcomes. Theoretically, it addresses how to turn test-time search into training-time learning signals, improving sample efficiency and credit assignment.
- Prior approaches and shortfalls:
  - Prolonged RL training (e.g., DAPO-based) improves performance but shows clear diminishing returns even with thousands of extra steps (Table 2).
  - Inference-only search (Tree-of-Thoughts, beam/MCTS variants) boosts accuracy at test time, but training still relies on sparse direct rollouts, leaving exploration signals untapped (¬ß1; Appendix A).
- Positioning:
  - DeepSearch moves search into training, coupling systematic exploration with verifiable rewards and new credit-assignment and data-curriculum mechanisms (global frontier selection, entropy-guided negative selection, adaptive replay buffer; ¬ß2‚Äì¬ß3, Fig. 1, Algorithm 1).

## 3. Technical Approach
DeepSearch is a training framework that builds a search tree for each problem during training, uses verifiable rewards to score terminal nodes, and backpropagates informative signals to intermediate reasoning steps. It combines a global view of the search frontier with targeted selection of positive or ‚Äúconfidently wrong‚Äù traces to train the policy.

Key components (refer to Fig. 1 and Algorithm 1):

1) Search tree construction and notation (¬ß2)
- Setup:
  - Problem `x` is the root; each child node is an intermediate reasoning step `s` generated by the policy `œÄŒ∏`.
  - A complete path x ‚Üí s1 ‚Üí ‚Ä¶ ‚Üí send forms a trajectory `t`. Terminal nodes `send` are either solved (correct) or ended by reaching max depth `dT`.
  - A verifier `V(send) ‚àà {0,1}` labels terminal nodes as correct (1) or incorrect/incomplete (0) (Eq. 1; ¬ß2.1).
- Why a tree: Multiple branches allow systematic exploration of alternative reasoning paths that a single rollout usually misses.

2) Expansion with entropy-based guidance (¬ß2.1)
- Expansion:
  - From the current observation `o` (prefix steps), the policy `œÄŒ∏(¬∑|o)` proposes `n` next-step candidates. Expansion continues until terminal nodes for the iteration are reached (S_end(k)).
- If no correct terminal nodes found at this iteration:
  - Pick the most ‚Äúconfidently wrong‚Äù terminal node `s*neg` with the lowest average trajectory entropy (Eq. 2‚Äì3). Here, ‚Äúentropy‚Äù measures uncertainty in the token distribution; low entropy means the model is confident.
- Intuition:
  - When no success occurs, train on the most confidently wrong trajectory to correct high-confidence mistakes‚Äîthe errors the model is most likely to repeat.

3) Heuristic score backup (assigning credit to steps) (¬ß2.2)
- Terminal rewards:
  - `q(send) = +1` for correct; `-1` for incorrect or incomplete (Eq. 6).
- Temporal decay:
  - When backpropagating the terminal reward to earlier steps along the selected trajectory, a decay factor `Œ≥(i, l) = max(i/l, Œ≥min)` emphasizes steps nearer to the end (Eq. 5; `Œ≥min=0.1`).
- Constrained update:
  - Intermediate node q-values `q(si)` are updated so that nodes on a correct path remain nonnegative, while nodes leading to incorrect outcomes can be negative (Eq. 4, 7).
- Why this matters:
  - It provides fine-grained credit assignment across steps, not just supervising the final answer. Steps on successful paths get consistent positive credit; steps on failing paths get penalized.

4) Hybrid selection: local UCT + global frontier (¬ß2.3)
- Local sibling selection (UCT):
  - Among candidate children of the same parent, use UCT to choose which to add: `UCT(s) = Q(s) + Œª sqrt(ln Nparent(s)/N(s))` (Eq. 8). This balances exploitation (high average reward) and exploration (less-visited nodes).
- Global frontier selection:
  - After score backup, choose the next node to expand from all frontier nodes of the entire tree, not just following a root-to-leaf traversal (Eq. 9‚Äì10).
  - Frontier score `F(s)` combines:
    - Quality potential: `tanh(Qparent(s))` to prefer parents with high value.
    - Uncertainty bonus: entropy `H(œÄŒ∏(s|o))` (coefficient can reward high or low uncertainty).
    - Depth bonus: `D(d(s)) = sqrt(d(s)/dT)` to encourage deeper exploration without being too aggressive (Eq. 10).
- Why this design:
  - Traditional MCTS repeatedly traverses from root, which can be myopic and wasteful. The global frontier view allocates expansions where they are most promising across the entire tree, improving coverage and efficiency.

5) Adaptive training with progressive filtering and replay (¬ß3)
- Hard-example curriculum:
  - Define `Pass1@K(x, œÄ)` as the chance that at least one of K samples solves `x`. Build an initial ‚Äúhard‚Äù subset `D_hard^(0)` with problems where `Pass1@K` is below a threshold `Œ¥(0)` (Eq. 11). After each training iteration, re-filter to keep only still-hard problems (Eq. 12; typical `Œ¥(i)=25%`).
- Replay buffer with cached solutions:
  - Store correct trajectories found by MCTS for problems that still fail the threshold (Eq. 13).
  - At the next iteration, if a problem `x` has a cached solution, do not rerun MCTS; train on the cached solution plus a few fresh direct rollouts (`Œ≤¬∑B` samples). Otherwise, run full MCTS (Eq. 14‚Äì15).
- Why this matters:
  - Focuses compute on unsolved cases; prevents forgetting solved ones; avoids redundant search.

6) Tree-GRPO training objective (¬ß3.3)
- Stabilizing q-values:
  - Soft-clip intermediate q-values with `tanh` to bound them while keeping gradients (Eq. 16). Terminal q-values remain ¬±1 (Eq. 6).
- Policy optimization (PPO-style):
  - Optimize a clipped objective on token-level log-prob ratios `œÅj,k(Œ∏)` with ‚Äúclip-higher‚Äù bounds (Eq. 17). No KL regularization is used (they ‚Äúlet KL diverge‚Äù), and overlong responses (>4096 tokens) are penalized.
- Advantages:
  - Node-level advantages use mean-only normalization relative to the average terminal reward `¬µ_t`, `√Ç_j,k = q(sj) - ¬µ_t` (Eq. 18). This preserves relative scales but stabilizes updates.
- Why this matters:
  - Converts the tree‚Äôs step-level q-values into a stable training signal; mean-only normalization avoids issues seen with variance scaling in GRPO.

7) Implementation details (Appendix B)
- Search parameters: max depth `dT=64`, expansion width `n=8`, 256 tokens per node; UCT exploration coefficient Œª=2.0; depth bonus coefficient `Œª3=0.01`; default frontier weights `(Œª1=0.4, Œª3=0.01)`; optional uncertainty bonus `Œª2` (¬ßB.2).
- Training: initialized from `Nemotron-Research-Reasoning-Qwen-1.5B v2`; trained on `DeepMath-103K` with a math-specific prompt; dynamic batching; AdamW LR 1e-6; Clip-Higher bounds 0.2/0.28; no KL penalty (¬ßB.1‚ÄìB.3).
- Sampling: 8 rollouts per prompt during training; evaluation with temperature 0.6, top-p 0.95 (¬ßB.4).

## 4. Key Insights and Innovations
- Training-time MCTS, not inference-only:
  - Fundamental shift: embed search into RLVR training, so the model learns from exploration itself (¬ß1‚Äì¬ß2; Fig. 1; Algorithm 1). This improves coverage and step-level credit assignment beyond direct-rollout RL.
- Global frontier selection (Eq. 9‚Äì10):
  - Innovation over standard root-to-leaf MCTS: compare and prioritize all frontier nodes across the tree using a composite score (parent quality, entropy, depth). Table 3 shows it reduces iterations and improves rewards versus vanilla UCT while keeping similar depth and entropy.
- Entropy-guided ‚Äúconfident negative‚Äù selection (Eq. 2‚Äì3):
  - When no correct solution appears in an iteration, choose the lowest-entropy incorrect trajectory for supervision. This focuses learning on the model‚Äôs highest-confidence mistakes‚Äîthose it is most likely to repeat (¬ß2.1).
- Adaptive replay with cached solutions (Eq. 11‚Äì15):
  - Efficiently reuses discovered solutions and concentrates MCTS on unsolved problems (¬ß3.1‚Äì3.2). This both preserves knowledge and saves compute.
- Tree-GRPO with soft-clipped q-values and mean-only normalization (Eq. 16‚Äì18):
  - A stable way to translate tree credits into token-level policy gradients. Table 4 shows stepwise gains from outcome-only to node-specific advantages and from standard to mean-only normalization.

Collectively, these are more than incremental tweaks: they reframe exploration as a first-class training signal, with algorithmic designs to make it efficient and stable.

## 5. Experimental Analysis
- Evaluation setup (¬ß4; Appendix B):
  - Training data: `DeepMath-103K` (challenging, decontaminated) (¬ß4.1; He et al., 2025c).
  - Base model: `Nemotron-Research-Reasoning-Qwen-1.5B v2`.
  - Benchmarks: AIME 2024, AIME 2025, AMC 2023, MATH500, Minerva, Olympiad (six datasets spanning contest-style and textbook quantitative reasoning).
  - Metric: Pass@1 accuracy with 32 samples per question (Table 1). Evaluations executed on a 128√óH100 cluster.
  - Baselines: strong 1.5B models across base, RL, and search paradigms (e.g., DeepScaleR, Nemotron v1/v2, Qwen2.5-Math variants; Table 1).
- Main results (Table 1):
  - DeepSearch-1.5B average Pass@1: 62.95%, the new state-of-the-art among 1.5B models.
  - Gains over prior SOTA Nemotron v2 (61.70%): +1.25 points average. Notable per-benchmark gains include AIME24 (53.65% vs 51.77%) and AMC (90.39% vs 88.83%).
  - Quote:
    > DeepSearch-1.5B: 53.65 (AIME24), 35.42 (AIME25), 90.39 (AMC23), 92.53 (MATH), 40.00 (Minerva), 65.72 (Olympiad), Avg 62.95 (Table 1).
- Efficiency analysis (Table 2; Fig. 2):
  - Prolonged DAPO training plateaus: +1,875 additional steps raise accuracy from 61.70% to only 62.02%, costing 1,883.2 GPU hours.
  - DeepSearch uses only +50 training steps and 330 GPU hours to reach 62.95%‚Äî5.7√ó fewer GPU hours than the prolonged training to 1,883.2 hours.
  - Quote:
    > ‚ÄúDeepSearch-1.5B Tree-GRPO +50 steps‚Ä¶ 330 GPU hours ‚Ä¶ 62.95‚Äù vs ‚ÄúExtended Training DAPO +1875‚Ä¶ 1883.2 GPU hours ‚Ä¶ 62.02‚Äù (Table 2).
  - Fig. 2 shows faster improvement over time compared to DAPO after 3K RLVR steps.
- Search strategy ablation (Table 3):
  - Global frontier selection (Œª1=0.4) vs vanilla UCT:
    - Fewer iterations: 187.7 vs 209.6 (‚àí10.4%).
    - Better trajectory rewards: ‚àí0.65 vs ‚àí0.82 (higher is better here, since ‚àí0.65 is closer to 0).
    - Similar search depth and entropy.
  - Depth bonus:
    - Linear `d(s)` is fastest (time per tree 480.9s) but hurts reward (‚àí0.76).
    - `sqrt(d(s)/dT)` balances speed and quality; adopted default: Œª1=0.4, Œª3=0.01.
  - Uncertainty bonus (Œª2=0.4) increases entropy/diversity but adds iteration variance.
- Algorithm evolution ablation (Table 4):
  - Starting from Nemotron v2 (61.70 Avg), each component adds value:
    - Moving from vanilla DeepSearch to enhanced q-update and fine-grained token scores improves Avg to 61.85.
    - Switching to mean-only normalization: 62.32.
    - Adding frontier selection: 62.95 (largest single jump).
- Do experiments support the claims?
  - Yes: They show accuracy gains vs strong baselines, clear compute savings vs prolonged RL, and ablations isolating the impact of search strategy, advantage shaping, and normalization.
  - Caveat: All evaluations are in the math domain with verifiable end answers. Generalization to non-verifiable tasks is not demonstrated (acknowledged in Limitations/Future Work).

## 6. Limitations and Trade-offs
- Reliance on verifiable rewards:
  - The method depends on a reliable verifier `V` (Eq. 1, 6). Math tasks are ideal since answers are checkable; many real-world reasoning tasks lack such verifiers (¬ßLimitations and Future Work).
- Domain scope:
  - All experiments are on mathematical reasoning. Porting to subjective or open-ended tasks would require approximate or rubric-based verifiers (acknowledged in Ethics/Limitations).
- Compute vs. efficiency:
  - While more efficient than prolonged RL (Table 2), MCTS during training is still computationally heavy, with numerous hyperparameters (depth up to 64, width 8, entropy estimates, frontier weights) (¬ßB.2). Training/evals use large GPU clusters.
- Hyperparameter sensitivity:
  - Global frontier selection uses tunable weights `(Œª1, Œª2, Œª3)` and different depth functions (Table 3). Performance/efficiency trade-offs depend on these choices.
- Negative selection bias:
  - Preferring ‚Äúconfident negatives‚Äù (Eq. 2‚Äì3) focuses on fixing high-confidence errors, which is useful, but may pay less attention to low-confidence but frequent mistakes.
- Potential overfitting to cached solutions:
  - The replay buffer injects cached correct trajectories (Eq. 13‚Äì15). This accelerates learning but could bias toward known solutions if not balanced with fresh exploration (the method mitigates this by adding direct rollouts `Œ≤¬∑B`).
- Length and formatting fragility:
  - They explicitly filter out garbled/infinite outputs and penalize overlong responses to avoid training collapse (¬ß3.2; ¬ß3.3). These heuristics are necessary but task-specific.

## 7. Implications and Future Directions
- Landscape shift:
  - DeepSearch reframes RLVR training from ‚Äúmore steps‚Äù to ‚Äúbetter exploration.‚Äù It turns test-time search into training-time supervision, showing that algorithmic exploration can outperform brute-force compute scaling (Table 2, Fig. 2).
- Practical applications:
  - Any domain with verifiable outcomes is a candidate: math, program synthesis, theorem proving, data cleaning with exact checks, and other ‚Äúcompile-or-check‚Äù workflows.
- Research directions:
  - Verifier design beyond math:
    - Approximate verifiers (learned or rubric-based), hybrid human-in-the-loop evaluation for subjective steps, and decomposable verifiers for complex tasks (¬ßLimitations and Future Work).
  - Unifying training and inference search:
    - Co-design of training-time search and inference-time budgeting, possibly learning the frontier scoring function end-to-end.
  - Transfer and robustness:
    - Study whether search-augmented training yields stronger transfer across domains or more robust reasoning under distribution shift.
  - Reward shaping and credit assignment:
    - Alternative decay functions `Œ≥(i,l)`, different entropy criteria, or learned backup strategies; explore the impact of uncertainty direction (positive vs negative entropy bonus in Eq. 10).
  - Scaling laws for exploration:
    - Formalize how tree width/depth, verifier reliability, and replay dynamics affect sample efficiency and generalization.

In short, DeepSearch demonstrates that embedding principled search into RLVR training can materially improve both effectiveness and efficiency. The framework‚Äôs components‚Äîglobal frontier selection, entropy-guided negatives, adaptive replay, and Tree-GRPO‚Äîwork together to convert exploration into stable, fine-grained supervision (Fig. 1; Eq. 2‚Äì18; Algorithm 1), establishing a practical path to scaling reasoning skills through smarter exploration rather than just longer training.

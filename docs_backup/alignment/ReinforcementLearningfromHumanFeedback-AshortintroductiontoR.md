# Reinforcement Learning from Human Feedback: A short introduction to RLHF and post-training focused on language models.

**ArXiv:** [2504.12501](https://arxiv.org/abs/2504.12501)

## ðŸŽ¯ Pitch

This monograph provides the definitive, reproducible guide to post-training large language models (LLMs) using Reinforcement Learning from Human Feedback (RLHF), unifying core concepts, practical recipes, and modern advancements like direct alignment algorithms and reasoning-focused RL. By disentangling and systematizing the full RLHF pipelineâ€”from data collection and reward modeling to optimization and evaluationâ€”it empowers researchers and practitioners to reliably transform pretrained LLMs into capable, user-aligned assistants, making cutting-edge alignment techniques accessible and actionable for maximum impact.

---

## 1. Executive Summary (2â€“3 sentences)
This monograph-systematizes â€œpostâ€‘trainingâ€ for large language models (LLMs)â€”with Reinforcement Learning from Human Feedback (RLHF) at its coreâ€”into a complete, reproducible pipeline. It unifies definitions, mathematics, practical recipes, and modern variants (e.g., direct alignment algorithms and RL with verifiable rewards for reasoning) and shows how to make, regularize, and optimize preference signals, with stepâ€‘byâ€‘step derivations (e.g., of DPO) and codeâ€‘level guidance (Chs. 4â€“14).

## 2. Context and Motivation
- Gap addressed
  - There has been no canonical, endâ€‘toâ€‘end reference for how to collect preference data, train reward models, regularize, and optimize LLMs with RLHF; practices were scattered across papers, blog posts, and closedâ€‘lab lore (Intro; Chs. 4â€“13).  
  - The field conflates RLHF with other postâ€‘training methods (instruction tuning, rejection sampling, DPO) and newer reasoningâ€‘style RL (â€œRLVRâ€); this work disentangles scope and relationships (Intro; Chs. 12 and 14).

- Why it matters
  - RLHF transformed base LLMs into useful assistants, often by reshaping responseâ€‘level style and behavior beyond tokenâ€‘level imitation (Sec. 1.1; contrast examples for â€œThe president of the united states in 2006â€¦â€) and has become essential for capability and UX (Chs. 18â€“19).  
  - Postâ€‘training can elicit large performance gains without changing pretrainingâ€”e.g., a reported evaluation average jump â€œfrom 35 to 48â€ during a postâ€‘training iteration (Sec. 1.2, citing [11]).

- Prior approaches and their limits
  - Early RLHF (2019â€“2022) used PPO with learned reward models for specific tasks: summarization, WebQA, general dialogue (Ch. 2.2). These worked but left unclear best practices for data, KL control, and overâ€‘optimization, and were expensive to replicate.  
  - Instruction tuning alone improved formatting and some â€œchatâ€ benchmarks but did not generalize as robustly as preferenceâ€‘based methods (Sec. 1.1).  
  - Direct preference optimization (DPO) simplified pipelines but raised new issues like preference displacement and offlineâ€‘data ceilings (Chs. 12.2 and 12.4; Fig. 16).

- Positioning
  - The work is a practical, rigorous â€œplaybookâ€: clear problem formulation (Ch. 4), full derivations (e.g., DPO in Sec. 12.1.2 with Eqs. 65â€“86), regularization tools (Ch. 8), detailed recipes (e.g., InstructGPT, TÃ¼lu 3, DeepSeek R1 in Secs. 4.2.1â€“4.2.3), and evaluation pitfalls (Ch. 16), plus modern reasoning training (Ch. 14).

## 3. Technical Approach
The book organizes the RLHF/postâ€‘training stack as a sequence of wellâ€‘specified stages, with explicit formulations and implementations.

- Problem formulation and regularization (Ch. 4)
  - RLHF adapts the RL objective to singleâ€‘turn language generation: maximize expected reward over responses with a KL penalty to stay close to a reference policy (`Ï€_ref`) (Sec. 4.1.2):  
    J(Ï€) = E[rÎ¸(s,a)] âˆ’ Î² D_KL(Ï€_RL(Â·|s) || Ï€_ref(Â·|s)) (Eq. 8).  
    â€¢ Why a KL term? It anchors the finetuned policy near a strong starting model and prevents rewardâ€‘hacking/overâ€‘optimization (Ch. 8).

- Stage A â€” Instruction finetuning (IFT) (Ch. 9)
  - Purpose: teach format and task schemas (chat templates, roles, system/user/assistant) so the model can accept prompts and respond consistently (Sec. 9.1; template example).  
  - Mechanics: standard nextâ€‘token loss; mask prompts so loss applies only on assistant completions; multiâ€‘turn training masks earlier assistant turns too (Secs. 9.2 and 6.3.3).

- Stage B â€” Preference data (Ch. 6)
  - How preferences are gathered:
    â€¢ Pairwise/ranking interfaces (Figs. 7â€“9); thumbsâ€‘up/down in products (Fig. 10).  
    â€¢ Scales: 5â€‘ or 8â€‘point Likert variants (Sec. 6.3.2).  
    â€¢ Multiâ€‘turn and structured settings (Secs. 6.3.3â€“6.3.4), including verifiable constraints and correctness (e.g., math or â€œIFEvalâ€â€‘style prompts).  
    â€¢ LLMâ€‘asâ€‘aâ€‘judge (RLAIF) is a scalable alternative when human data is costly; prompt template given in Sec. 7.8.
  - Practicalities: vendor cycles, instructions, staged batches (Fig. 12); known biases (length, formatting, sycophancy; Sec. 6.2).

- Stage C â€” Reward modeling (Ch. 7)
  - Core model: a scalar scorer trained with a Bradleyâ€“Terry pairwise likelihood (Eqs. 12â€“13): for prompt `x` and chosen/rejected completions `y_w, y_l`, minimize âˆ’log Ïƒ(rÎ¸(x,y_w) âˆ’ rÎ¸(x,y_l)).  
  - Architecture: LM encoder + small classification head outputting one logit per sequence (Sec. 7.2). Train for 1 epoch to avoid overfit (Sec. 7.3).  
  - Variants: margin losses (Llama 2; Eq. 14), promptâ€‘balanced batching (Eq. 15), Kâ€‘wise/Plackettâ€“Luce losses (Sec. 7.4.3).  
  - Alternatives: Outcome reward models (perâ€‘token probability of correctness; Eq. 17) and Process reward models (scores per reasoning step with special separators; Sec. 7.6).

- Stage D â€” Optimization options (Chs. 10â€“12)
  - Rejection Sampling (RS, Ch. 10): generate `N` candidates per prompt, score with RM, select top candidates (perâ€‘prompt `argmax` or topâ€‘K overall; Sec. 10.1.2), then fineâ€‘tune with SFT on those â€œacceptedâ€ outputs (Fig. 13). This is the simplest preferenceâ€‘finetuning baseline.
  - Policyâ€‘gradient RL (Ch. 11):
    â€¢ REINFORCE and baselines: update âˆ‡Î¸ log Ï€(a|s) times advantage; â€œleaveâ€‘oneâ€‘outâ€ baselines use other samples in the same prompt group (Eqs. 41â€“45).  
    â€¢ PPO: perâ€‘token clipped objective with ratio R(Î¸)=Ï€Î¸/Ï€_old to limit step size (Eqs. 46â€“48; perâ€‘token form Eq. 47).  
    â€¢ GRPO: PPOâ€‘like but no learned value function; advantages computed groupâ€‘wise across `G` responses to the same prompt (Eq. 55) with normalized advantage (Eq. 57). The book shows algebraic equivalence (up to a scale) to RLOO when removing stdâ€‘norm (Eq. 60).  
    â€¢ Implementation: how to aggregate losses per token vs per sequence (Sec. 11.2.2), KL application either as reward penalty or explicit loss (Sec. 11.2.5), and asynchronous rollouts vs learning (Fig. 14).
  - Direct Alignment Algorithms (DAAs, Ch. 12):
    â€¢ DPO minimizes a logistic loss on the logâ€‘probability differences of chosen vs rejected completions, normalized by a reference model (Eq. 65).  
    â€¢ The book derives the optimal RLHF solution Ï€*(y|x) âˆ Ï€_ref(y|x) exp(r/Î²) (Eq. 80) and shows how replacing `r` with preference likelihoods yields DPOâ€™s implicit reward (Secs. 12.1.2.1â€“12.1.2.2).  
    â€¢ DPO gradient (Eq. 67) reveals the mechanism: increase chosen logâ€‘probability, decrease rejected, with weights larger when the current ordering is wrong.  
    â€¢ Practical notes: fixed KL via Î² (static), caching reference logâ€‘probs for memory (Sec. 12.3); caveat of preference displacement (Fig. 16).

- Stage E â€” Reasoning training with verifiable rewards (RLVR, Ch. 14)
  - Replace learned RM with an automatic checker (`r=1` if correct, else 0) and run policyâ€‘gradient on repeated attempts per question (Fig. 17).  
  - Used in modern â€œreasoning modelsâ€ (e.g., DeepSeek R1 steps in Sec. 4.2.3): coldâ€‘start SFT on reasoning traces; largeâ€‘scale RLVR; rejection sampling polish; mixed RL with preference signals.

- Canonical recipes (Sec. 4.2)
  - InstructGPT: SFT (~10k), RM (~100k pairs), PPO (~100k prompts) (Fig. 4).  
  - TÃ¼lu 3: SFT (~1M, mostly synthetic), onâ€‘policy preference tuning (~1M pairs), small RLVR for skills (Fig. 6).  
  - DeepSeek R1: coldâ€‘start reasoning traces (100k+), long RLVR, RS polish, mixed RL polish (Sec. 4.2.3).

- Regularization and control (Ch. 8)
  - KL on generated tokens versus reference model (`Ï€_ref`) (Eq. 19) with efficient approximation E_{xâˆ¼Ï€} [log Ï€ âˆ’ log Ï€_ref] (Eq. 21).  
  - Optionally add a pretraining gradient term to prevent regressions (Eq. 23); reward margins (Eq. 26).

## 4. Key Insights and Innovations
- RLHF optimizes at the response level, not token level (Sec. 1.1)
  - Insight: SFT teaches specific tokens in formats; RLHF says â€œwhat a better whole answer looks like.â€ This contrast explains broader generalization across domains (Sec. 1.1) and motivates contrastive losses in RMs (Eqs. 12â€“13).  
  - Significance: clarifies why RLHF changes style and behaviorâ€”the userâ€‘visible â€œassistant personaâ€â€”and why it needs careful regularization (Ch. 8).

- A unified, derivationâ€‘first treatment of DPO as RLHF in closed form (Ch. 12)
  - Whatâ€™s new here for a practitioner: complete derivation from the RLHF objective with KL (Eq. 80) to DPOâ€™s logistic loss (Eq. 65) and gradient (Eq. 67), exposing the implicit reward and the role of Î² as a fixed KL control.  
  - Significance: gives users a principled lens for when DPO suffices (offline data, simpler infrastructure) and where its ceilings arise (Sec. 12.4, offline/onâ€‘policy gap; Fig. 16 displacement).

- Practical, implementationâ€‘level guidance rarely consolidated elsewhere
  - Perâ€‘token vs perâ€‘sequence loss normalization effects (Sec. 11.2.2; worked example).  
  - RLOOâ€“GRPO connection (Eq. 60) demystifies many â€œnewâ€ algorithms as simple advantage estimators.  
  - Asynchronous rollouts vs learning loops, sequenceâ€‘level packing, and offâ€‘policy buffers for long reasoning traces (Sec. 11.2.3; Fig. 14).

- Connecting preferenceâ€‘tuning to modern reasoning training (Ch. 14)
  - Conceptual bridge: RLVR uses the same infrastructure as RLHF but swaps â€œsoft,â€ learned rewards with â€œhard,â€ verifiable checkersâ€”explaining why RL has resurfaced at scale in 2024â€“2025 (Sec. 14.1).  
  - Practical blueprint: staged recipes (Sec. 4.2.3) and common stabilizers (curricula, KL removal in some regimes, format and languageâ€‘consistency rewards; Sec. 14.2.3).

- A clear taxonomy of reward signals and heads (Sec. 7.7)
  - Distinguishes `Reward Models (sequence logit)`, `Outcome RMs (perâ€‘token correctness)`, `Process RMs (perâ€‘step)`, and `Value functions`, with training losses and heads summarized (Sec. 7.7).  
  - Significance: prevents mismatched heads/losses that degrade learning, a common pitfall.

## 5. Experimental Analysis
This work is a methods â€œfield guide,â€ not a single new benchmark paper, but it grounds practice in concrete evaluation design and known empirical signatures.

- Evaluation methodology (Ch. 16)
  - Three eras: (i) early chat (MTâ€‘Bench, AlpacaEval; Sec. 16), (ii) multiâ€‘skill (knowledge: MMLU, PopQA; reasoning: BIGâ€‘BENCH Hard; math: MATH, GSM8K; code: HumanEval; safety suites), (iii) reasoning/tools (GPQAâ€‘Diamond, SWEâ€‘Bench+, LiveCodeBench) with chainâ€‘ofâ€‘thought prompts and verifiers (Secs. 16.1 and 16.2).  
  - The book emphasizes formatting sensitivity (fewâ€‘shot vs zeroâ€‘shot vs CoT; Sec. 16.1), LLMâ€‘asâ€‘aâ€‘judge care, inferenceâ€‘time scaling control, and contamination/decontamination (Sec. 16.3).

- Quantitative references reported inâ€‘text
  - Postâ€‘training can yield large gains without changing pretraining, illustrated by an evaluation average improving â€œfrom 35 to 48â€ across a product iteration (Sec. 1.2, Fig./note referencing [11]).  
  - Overâ€‘optimization signature: reward goes up while downstream fails to improve; a train/test RM split shows divergence after ~150k RL samples (Fig. 20; Sec. 17.2).  
  - KL control: the entire framework treats Î² or target KL as the â€œbudgetâ€ to spend (Secs. 4.1.2 and 8.1).

- Baselines and setups
  - Recipes show SFTâ†’RMâ†’PPO (InstructGPT; Fig. 4), RS as a strong nonâ€‘RL baseline (Ch. 10), and DPO (Ch. 12) as an offline alternative.  
  - Reasoning evaluation shifts necessitate RLVR training and toolâ€‘assisted checking (Ch. 14; Fig. 17).

- Ablations / robustness
  - Reward model variants (margins, Kâ€‘wise) and balancing multiple comparisons per prompt (Secs. 7.4.1â€“7.4.3, 7.4.2) target overfit and data imbalance.  
  - Loss aggregation choices (Sec. 11.2.2) and KL placement (Sec. 11.2.5) materially change stability.  
  - DPOâ€™s preference displacement (Fig. 16) highlights a failure mode in offline preference fitting and motivates online/mixture training (Sec. 12.2).

- Do the experiments support the claims?
  - The document compiles wellâ€‘known empirical patterns (e.g., RLHF overâ€‘optimization curves, effect of Î², offline vs onâ€‘policy gap) and connects them to the exact equations and recipes. Where it cites concrete numbers (e.g., the 35â†’48 example in Sec. 1.2, train/test RM divergence in Fig. 20), they illustrate the bookâ€™s broader lessons rather than claim novel SOTA.

## 6. Limitations and Trade-offs
- Soft rewards and overâ€‘optimization
  - Learned RMs are proxies; they can be overâ€‘fit or gamed, making KL budgeting and early stopping crucial (Ch. 8; Fig. 19; Fig. 20).  
  - Biases in preference data (length, formatting, sycophancy) can propagate to models (Sec. 6.2).

- Data and cost constraints
  - Highâ€‘quality human preferences remain expensive and operationally complex (Sec. 6.3.5; Fig. 12). RLAIF scales cheaper but can import judgeâ€‘model biases (Sec. 7.8; Ch. 13).

- Algorithmic ceilings and assumptions
  - DPO and other DAAs may underperform on tasks that benefit from onâ€‘policy exploration; Î² fixes KL implicitly and may cap attainable behaviors when offline data is narrow (Sec. 12.4).  
  - PPO/GRPO stability depends on implementation details (value head init, reward whitening, asynchronous rollouts; Sec. 11.2) and may be computeâ€‘intensive for long CoT.

- Scope not covered by this framework
  - Multiâ€‘turn state (beyond formatting) and toolâ€‘use credit assignment are only partially addressed; most formulations are singleâ€‘turn bandits (Sec. 4.1.1).  
  - The book surfaces but does not resolve philosophical limits: interpersonal comparison of preferences, Arrowâ€‘style aggregation problems, timeâ€‘varying preferences (Ch. 5.1.2).

- Evaluation ambiguities
  - Public leaderboards vs private evaluation stacks differ in prompts, formats, and independence from training data; contamination is hard to fully verify (Sec. 16.2â€“16.3).

## 7. Implications and Future Directions
- Field impact
  - The book reframes postâ€‘training as an â€œelicitationâ€ layer (Sec. 1.2): SFT learns formats; preferenceâ€‘tuning shapes behavior; RLVR pushes verifiable capabilities. This clarifies how to combine methods rather than debate which single method â€œwins.â€  
  - It normalizes RL again as a central tool for LLMsâ€”first for preference alignment, now for reasoningâ€”by sharing infrastructure and recipes (Chs. 11 and 14).

- Research directions
  - Better reward modeling: aspectâ€‘conditioned or multiâ€‘objective RMs; robust, deâ€‘biased judges; PRMs for intermediate steps (Secs. 7.4, 7.6â€“7.9).  
  - Online/async pipelines: offâ€‘policy policyâ€‘gradient variants, distributed RLHF/RLVR with long traces (Sec. 11.2.3).  
  - Closing the DPO gap: hybrid online DAAs, displacementâ€‘aware objectives, and calibration (Sec. 12.2 and 12.4; Fig. 16).  
  - Preference science: pluralistic alignment and personalization that respect aggregation impossibility results (Ch. 5).

- Practical applications
  - Productionized assistants: character training and â€œmodel specsâ€ to steer UX traits (Chs. 18â€“19).  
  - Domain specialists: coding/math tutors via RLVR; constraintâ€‘following systems via structured preference data (Sec. 6.3.4).  
  - Data engines: synthetic generation and distillation for economical scaling, with safeguards against formatting brittleness and contamination (Ch. 15; Sec. 16.3).

> Bottom line: this work gives practitioners a principled mapâ€”from data collection interfaces (Figs. 7â€“11), to RM equations (Eqs. 12â€“13), to KL control (Eq. 19), to PPO/GRPO/DPO mechanics (Eqs. 46, 55, 65) and reasoning RL (Fig. 17)â€”and it flags the exact places where things break (overâ€‘optimization, displacement, formatting sensitivity), so that teams can build and iterate credible postâ€‘training pipelines.

# To Code, or Not To Code? Exploring Impact of Code in Preâ€‘training

**ArXiv:** [2408.10914](https://arxiv.org/abs/2408.10914)
**Authors:** Unknown (paper metadata not directly shown in summary)
**Institutions:** 

## ğŸ¯ Pitch

This study uncovers that incorporating code into the pre-training of language models significantly boosts general reasoning, knowledge, and generation capabilities, provided it is added in optimal proportions and at the right stages. By offering a systematic framework and evidence-backed guidelines, it reshapes pre-training strategies, ensuring models are more robust and efficient, and sets new standards for leveraging code in non-code tasks.

---

## 1. Executive Summary
This paper runs a large, controlled set of training ablations to answer a simple question: does adding source code to the preâ€‘training mix help language models on nonâ€‘code tasks, and how should it be added? Across 64 preâ€‘training runs (470M and 2.8B parameters), the study shows that including code is a key ingredient for better general reasoning, knowledge, openâ€‘ended generation, and (unsurprisingly) codingâ€”provided it is used in the right proportions, quality, and training stage.

## 2. Context and Motivation
- Problem addressed
  - Many top LLMs include code in preâ€‘training (e.g., Llama 3 increases code share relative to Llama 2), but there has been no systematic, largeâ€‘scale analysis isolating the effect of code on nonâ€‘code tasks. The study asks: how much, what kind, and when should code be included to improve general (nonâ€‘code) performance? See Section 1 and Figure 1.

- Why it matters
  - Preâ€‘training data composition is one of the strongest levers for model quality. Getting the code mix wrong can harm world knowledge while getting it right yields outsized improvements in reasoning and generative quality. The paper provides concrete recipes that can save compute and guide data curation.

- Prior approaches and gaps
  - Prior work hints that code can help (e.g., smallâ€‘scale or limitedâ€‘data settings, benefits to math/entity tracking), but none provide comprehensive, controlled ablations across:
    - Model initialization strategies (from scratch vs. continued from a code model),
    - Proportion of code,
    - Types/quality of code (including synthetic and codeâ€‘adjacent),
    - Training stage (early vs. cooldown),
    - Model scale (470M vs. 2.8B).
  - The present work fills this gap with a methodical, multiâ€‘axis exploration (Sections 1, 2; Figure 1).

- Positioning relative to existing work
  - Rather than proposing a new model, this paper contributes a systematic experimental framework and evidenceâ€‘backed guidelines for data mixture design (Sections 2â€“3). It is complementary to model/algorithm advances by clarifying how code should be used in preâ€‘training mixtures.

## 3. Technical Approach
This is an empirical study with tightly controlled training and evaluation. Below is the experimental design, terminology, and pipeline.

- Definitions (only those that are paperâ€‘specific or uncommon)
  - `token budget`: the total number of tokens a model consumes during training (e.g., 200B tokens).
  - `continued (continual) preâ€‘training`: training that starts from an already preâ€‘trained checkpoint and continues on a possibly different data mixture.
  - `cooldown`: a brief, final training phase where highâ€‘quality datasets are upâ€‘weighted and the learning rate is linearly annealed, often to improve instruction following and overall quality (Section 2.1; Section 2.3 describes LR schedule details).
  - `LLMâ€‘asâ€‘aâ€‘judge` and `winâ€‘rate`: an LLM compares two model outputs per prompt and selects a preferred answer; winâ€‘rate is the percentage of pairwise wins (Section 2.2).
  - `pass@1` (code): success rate when taking a single generated completion for a coding task (HumanEval/MBPP).
  - `markupâ€‘style languages`: data like Markdown, HTML, CSS (Appendix A.3).
  - `codeâ€‘adjacent data`: developer artifacts such as GitHub commits/issues, Jupyter notebooks, and StackExchange threads (Section 2.1).
  - `synthetic code`: code generated by a system and verified for correctness; here, a proprietary, formally verified Python dataset (3.2B tokens; Section 2.1 and Section 3.4).

- Models and training (Section 2.3)
  - Architectures: 470M and 2.8B parameter decoderâ€‘only Transformers with parallel attention, SwiGLU, no dense biases, 256k BPE vocab, context length 8,192.
  - Optimization: AdamW, batch size 512, cosine LR (warmup 1,325 steps) for preâ€‘training; linear anneal to 1eâ€‘6 for cooldown.
  - Hardware: TPU v5e; 64 preâ€‘training runs in total. Each 200Bâ€‘token run: 4,736 TPUâ€‘chipâ€‘hours (470M) or 13,824 (2.8B). Cooldown (40B tokens): 1,024 TPUâ€‘chipâ€‘hours (470M).

- Data (Section 2.1; Appendix A)
  - Text: SlimPajama (503B tokens after removing GitHub and StackExchange to ensure no code contamination).
  - Code: The Stack (permissive GitHub code) filtered to top 25 programming languages (Appendix A.2); markup subset (e.g., Markdown, HTML/CSS) treated separately (Appendix A.3).
  - Codeâ€‘adjacent: GitHub commits/issues, Jupyter notebooks, StackExchange (21.4B tokens).
  - Synthetic code: 3.2B formally verified Python tokens.
  - Cooldown mix: highâ€‘quality text, math, code, and instructâ€‘style data (details summarized in Section 2.1).

- Experimental levers (Figure 1; Sections 3.1â€“3.5)
  - Initialization:
    - `textâ€‘only` (from scratch on text, 400B tokens): baseline.
    - `balancedâ€‘only` (50% text / 50% code from scratch, 400B tokens).
    - `balanced â†’ text`: initialize from a balanced model then continue on mostly text (200B init + 200B continue; during continue, 10% code is retained to avoid distribution shift; Section 3.1, footnote 5).
    - `code â†’ text`: initialize from a codeâ€‘only model (200B code) then continue on mostly text with 10% code (200B).
  - Code proportion: 0%, 25%, 50%, 75%, 90%, 100% code for 200B tokens (Section 3.3).
  - Code properties: replace portions of web code with markup, codeâ€‘adjacent, or synthetic code (Section 3.4).
  - Cooldown: compare no cooldown vs. cooldown with code (20% of cooldown tokens) vs. cooldown without code (Section 3.5; Figure 6 and Figure 7).
  - Scale: replicate key settings at 2.8B to test whether trends hold (Section 3.2; Figure 3).

- Evaluation (Section 2.2; Table 1)
  - Natural language reasoning (11 benchmarks): BoolQ, PIQA, SciQ, SocialIQA, QuAC, SuperGLUEâ€‘CB/COPA, StoryCloze, HellaSwag, Winogrande, ARCâ€‘Easy. Metric: accuracy.
  - World knowledge: NaturalQuestionsâ€‘Open, TriviaQA. Metric: exact match accuracy.
  - Code: HumanEvalâ€‘Python, MBPP. Metric: pass@1.
  - Openâ€‘ended generation: Dollyâ€‘200 (English), LLMâ€‘asâ€‘aâ€‘judge with Cohere Commandâ€‘R+ (Appendix B provides the judge prompt).

## 4. Key Insights and Innovations
- A principled, multiâ€‘axis ablation of code in preâ€‘training at scale
  - Whatâ€™s new: The study varies when code is used (initialization vs. cooldown), how much is used (0â€“100%), what kind is used (web, markup, codeâ€‘adjacent, synthetic), and at two model sizesâ€”while holding token budgets fixed. This breadth and control are unusual and provide actionable guidance (Figure 1; Sections 3.1â€“3.5).

- Code helps nonâ€‘code tasks, but the timing and proportion matter
  - Novelty: The work pinpoints that initializing with code and then continuing on text (with a small code fraction) yields the strongest general reasoning and generative results (Section 3.1; Figure 2). It also identifies an optimal code fraction (â‰ˆ25%) for maximizing average nonâ€‘code performance in fromâ€‘scratch runs (Section 3.3; Figure 4).

- Highâ€‘quality synthetic code delivers outsized gains from a small share
  - Result: Replacing only 10% of web code with verified synthetic code boosts both NL reasoning (+9% relative) and code (+44.9%) in codeâ€‘only preâ€‘training (Section 3.4; Figure 5a), and those gains transfer when continued on text (+2% NL, +35% code; Figure 5b). This highlights â€œcode qualityâ€ as a lever, not just â€œmore code.â€

- Code in cooldown improves both discriminative and generative performance
  - Finding: Adding code into the short, highâ€‘quality cooldown phase increases NL reasoning (+3.6%), world knowledge (+10.1%), and code (+20%) relative to the model before cooldown (Figure 6), and further raises winâ€‘rates in openâ€‘ended generation (+4.1% over a cooldown without code; Figure 7).

## 5. Experimental Analysis
- Evaluation setup (Section 2.2; Table 1)
  - Diverse suite: 11 NL reasoning tasks, 2 world knowledge tasks, 2 code tasks, plus openâ€‘ended generation judged by an LLM. All runs are zeroâ€‘shot. At 470M, they exclude tasks where performance is near random to ensure fair comparisons (Section 2.2).

- Main quantitative results
  - Initialization matters for NL tasks (Section 3.1; Figure 2; Table 2)
    - `balanced â†’ text` and `code â†’ text` outperform `textâ€‘only` on NL reasoning by 8.2% and 8.8% relative, respectively.
    - For world knowledge, `balanced â†’ text` is best, with a 4.1% relative gain over `textâ€‘only`, while `code â†’ text` ties the text baseline.
    - Tradeâ€‘off: `balancedâ€‘only` (50% code throughout) yields the best code pass@1 (9.0 vs. 4.8 for `balanced â†’ text` and 4.1 for `code â†’ text`), but underperforms on nonâ€‘code tasks (Figure 2; Table 2).
    - Openâ€‘ended generation: both models initialized with code show higher winâ€‘rates than `textâ€‘only`. In Section 3.6 they further report:
      > â€œ`balanced â†’ text` achieves higher winâ€‘rates against `textâ€‘only` with 37.7% vs 34.7%; `balancedâ€‘only` loses against `textâ€‘only` with 32.7% vs 35.7%.â€ (Section 3.6; also see Figure 8 and Appendix C.1).

  - Scale to 2.8B preserves trends and amplifies knowledge and code (Section 3.2; Figure 3)
    - Moving from 470M to 2.8B increases average results by about 30% across NL reasoning and knowledge for `balanced`, `balanced â†’ text`, and `code â†’ text`. World knowledge and code nearly triple (e.g., `balanced â†’ text`: +2.7Ã— in world knowledge, +2.5Ã— in code over 470M).

  - Optimal code fraction from scratch is ~25% for nonâ€‘code tasks (Section 3.3; Figure 4)
    - NL reasoning: peak at 25% code (+3.4% over 0% code); performance drops sharply at 100% code (âˆ’18.3% vs. 0% code).
    - World knowledge: steadily degrades as code share rises; 100% code collapses world knowledge (âˆ’86% vs. textâ€‘only).
    - Code tasks: improve almost linearly with more code; 100% code yields â‰ˆ2.6Ã— the pass@1 of 25% code.
    - Takeaway: there is a nonâ€‘linear tradeâ€‘offâ€”too much code hurts world knowledge and eventually NL reasoning, while code tasks prefer more code.

  - Code quality and composition (Section 3.4; Figure 5)
    - Replacing portions of web code with markup or codeâ€‘adjacent data slightly helps NL reasoning but hurts code performance (e.g., `code + markup`: +2.8% NL, âˆ’15.7% code; `code + adjacent`: +6.3% NL, âˆ’9.4% code).
    - Synthetic code (10% of code tokens) is most impactful:
      > â€œ`code + synth` improves NL reasoning by 9% and code by 44.9% relative to the webâ€‘codeâ€‘only baseline.â€ (Figure 5a)
    - When transferred to continued preâ€‘training:
      > â€œ`balanced + synth â†’ text` improves NL reasoning by 2% and code by 35% over `balanced â†’ text`.â€ (Figure 5b)

  - Cooldown with code vs. without code (Section 3.5; Figures 6â€“7)
    - Relative to the preâ€‘cooldown model, adding code in cooldown yields:
      > â€œ+3.6% NL reasoning, +10.1% world knowledge, +20% code.â€ (Figure 6)
    - Generative quality improves with any cooldown; with code it is best:
      > â€œWinâ€‘rate 52.3% (cooldown with code) vs. 48.2% (cooldown without code) over the noâ€‘cooldown model, a +4.1% advantage.â€ (Figure 7)

  - Overall summary and recipe (Section 3.6; Table 2)
    - Compared to `textâ€‘only`, the best preâ€‘cooldown generalist variant (`balanced â†’ text`) shows:
      > â€œ+8.2% NL reasoning, +4.2% world knowledge, +6.6% winâ€‘rate, and â‰ˆ12Ã— code performance.â€ (Section 3.6; Table 2)
    - Cooldown with code further improves the same model by:
      > â€œ+3.6% NL reasoning, +10.1% world knowledge, +20% code relative to before cooldown,â€ and yields the highest openâ€‘ended winâ€‘rates (Figure 7; Table 2).

- Do experiments support the claims?
  - Strengths
    - Tight controls: fixed token budgets, same architectures/optimizers, explicit ablations of one factor at a time.
    - Breadth: multiple model sizes (470M, 2.8B), multiple code sources/types, several training stages, and rich evaluation (Table 1).
    - Replicable enough to verify trends (data sources and setups are detailed, with filtering criteria in Appendix A.1).
  - Caveats
    - The most impactful synthetic code dataset is proprietary (Section 2.1), limiting independent replication of those exact gains.
    - LLMâ€‘asâ€‘aâ€‘judge uses a single judge model (Cohere Commandâ€‘R+); while common, judge choice can bias outcomes (Section 2.2; Appendix B).
    - Scales are modest (up to 2.8B); trends are likely but not guaranteed to hold at very large scales.

- Ablations, edge cases, robustness
  - Ablations are extensive: proportions (Figure 4), data types/quality (Figure 5), initialization routes (Figure 2), cooldown compositions (Figure 6â€“7), and size (Figure 3).
  - Noted tradeâ€‘offs: high code fractions damage world knowledge (Figure 4); maximizing code performance (`balancedâ€‘only`) sacrifices NL tasks (Figure 2; Table 2). Cooldown without code does not help NL reasoning and code (Figure 6).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Findings are derived from decoderâ€‘only Transformers at 470M and 2.8B parameters with specific optimizer and LR schedules (Section 2.3). Different architectures or larger models may shift optimal points.
  - The evaluation focuses on Englishâ€‘centric tasks; multilingual or domainâ€‘specific outcomes are not studied.

- Data constraints
  - The most impactful code sourceâ€”formally verified synthetic Pythonâ€”is proprietary (Section 2.1), so others cannot precisely reproduce those gains without a similar dataset.
  - â€œTextâ€‘onlyâ€ is SlimPajama with GitHub/StackExchange removed (Section 2.1). This is careful, but it also means the text corpus may be less diverse than typical web mixtures that include QA forums or code snippets, possibly accentuating codeâ€™s added value.

- Computation and budget
  - Each condition uses substantial compute (e.g., 13,824 TPUâ€‘chipâ€‘hours per 2.8B/200Bâ€‘token run; Section 2.3), which limits the number of scales explored and makes fineâ€‘grained hyperparameter sweeps infeasible.

- Metric and benchmark limits
  - World knowledge evaluation uses exactâ€‘match metrics (Table 1), which can be sensitive to phrasing. Openâ€‘ended generation relies on one LLM judge (Section 2.2), whichâ€”while practicalâ€”may carry modelâ€‘specific biases.

- Tradeâ€‘offs surfaced by the study
  - Code fraction: increasing code linearly increases code performance but harms world knowledge beyond ~25% (Figure 4).
  - Initialization vs. throughout: training fully balanced throughout helps code but hurts NL tasks; initializing with code and continuing with mostly text shifts the balance back toward general NL performance (Section 3.1; Table 2).
  - Cooldown: including code in cooldown helps across the board, but cooldown composition must be curated (Figure 6â€“7).

## 7. Implications and Future Directions
- How this changes the landscape
  - Code should be treated as a firstâ€‘class component in preâ€‘training mixes for generalâ€‘purpose LLMsâ€”not only for coding. The study provides concrete, evidenceâ€‘based recipes for when and how to add code (Sections 3.1, 3.3, 3.5, 3.6).

- Practical training recipes (derived directly from Section 3.6, Table 2, and Figures 2, 4, 6â€“7)
  - For best overall general performance (reasoning, knowledge, generation):
    - Include code during initial preâ€‘training, but not too much: around 25â€“50% code early works, with the best overall configuration seen when initializing with a balanced model and then continuing mostly on text with a small fraction of code (`balanced â†’ text`; Figure 2 and Section 3.1).
    - Add code to cooldown: upâ€‘weighting highâ€‘quality text, math, and code in a short, linearlyâ€‘annealed cooldown improves all tasks and yields the strongest openâ€‘ended generation (Figures 6â€“7).
    - Prefer highâ€‘quality synthetic code if available: even 10% of code replaced with verified synthetic code produces large gains that transfer to continued preâ€‘training (Figure 5).
  - For maximizing code performance specifically:
    - Maintain a high proportion of code throughout (`balancedâ€‘only`) while being aware of the tradeâ€‘off in nonâ€‘code tasks (Figure 2; Table 2).

- Followâ€‘up research directions
  - Scaling studies: validate whether the identified optima (e.g., ~25% code from scratch; code in cooldown) hold at 7Bâ€“70B+ scales and larger token budgets (Section 6 notes compute limits).
  - Better synthetic code: open, highâ€‘quality, verified code datasets in multiple languages beyond Python; explore syntheticâ€‘generation strategies and curricula (Section 3.4).
  - Mixture scheduling: dynamic curricula that adjust code share over training time (beyond the twoâ€‘phase â€œinitialize then continueâ€ and â€œcooldownâ€ explored here).
  - Multilingual and domain transfer: measure whether code benefits generalize across languages and specialized domains (medicine, law).
  - Evaluation diversity: crossâ€‘validate LLMâ€‘asâ€‘aâ€‘judge outcomes with human studies and multiple judge models; expand knowledge benchmarks beyond exactâ€‘match QA.

- Downstream applications
  - Building generalist assistants: incorporate curated code (especially highâ€‘quality/synthetic) to enhance reasoning and explanation quality even for nonâ€‘technical users.
  - Enterprise LLM training: tune code proportions and cooldown composition to target use cases (e.g., documentation Q&A, dataâ€‘toâ€‘text reporting, or codeâ€‘aware chatbots).
  - Data curation platforms: prioritize code quality filters (Appendix A.1) and verified synthetic data generation pipelines; treat markup and codeâ€‘adjacent sources as auxiliary tools to shape NL behavior.

> Bottom line (Section 3.6): â€œCompared to textâ€‘only preâ€‘training, for the best â€˜balanced â†’ textâ€™ variant, adding code yields +8.2% NL reasoning, +4.2% world knowledge, +6.6% winâ€‘rate, and ~12Ã— code performance; adding code in cooldown adds another +3.6% NL, +10.1% knowledge, +20% code and achieves the highest openâ€‘ended winâ€‘rates.â€ These findings convert a common practitioner intuition into concrete, testable training recipes.

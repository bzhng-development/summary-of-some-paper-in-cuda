# Differential Transformer

**ArXiv:** [2410.05258](https://arxiv.org/abs/2410.05258)
**Authors:** Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei
**Institutions:** Microsoft Research, Tsinghua University

## üéØ Pitch

DIFF Transformer innovatively introduces a 'differential attention' layer that subtracts one softmax attention map from another to eliminate irrelevant context, amplifying essential tokens while significantly reducing parameters and training requirements. This approach drastically improves performance across long-context tasks, retrieval efficiency, and in-context learning scenarios, offering a robust, parameter-efficient alternative that addresses core attention noise issues in standard Transformers.

---

## 1. Executive Summary
DIFF Transformer introduces a ‚Äúdifferential attention‚Äù layer that forms attention scores by subtracting one softmax attention map from another, cancelling shared (irrelevant) context and amplifying relevant tokens. Across language modeling, long-context use, retrieval, in-context learning, hallucination mitigation, and quantization robustness, DIFF consistently outperforms comparable Transformers while using roughly two-thirds of the parameters or training tokens needed to match performance (Figure 3).

## 2. Context and Motivation
- Problem addressed
  - Standard Transformer attention often spreads probability mass over many irrelevant tokens in long or cluttered contexts, a phenomenon the paper calls ‚Äúattention noise.‚Äù This dilutes the signal from relevant spans so the model misses key information even when it is present. Figure 1 (left) visualizes this: the ‚Äúcorrect answer‚Äù receives a small fraction of the attention while irrelevant context receives substantial mass.
- Why it matters
  - Practical: Long-context uses (e.g., retrieval from large documents, long in-context exemplars, summarization, multi-document QA) hinge on focusing attention on the right places. Over-attending to noise leads to failures like ‚Äúlost in the middle‚Äù and contextual hallucinations.
  - Theoretical: It suggests a structural weakness of standard softmax attention‚Äînon-negligible mass on many distractors reduces the effective signal-to-noise ratio (SNR).
- Prior approaches and shortcomings
  - Many works scale context or add memory mechanisms; others craft prompting strategies. But the core attention mechanism remains unchanged, so over-allocation to irrelevant tokens persists.
  - The paper positions DIFF as an architectural, drop-in alternative to the attention operator that directly attacks attention noise by design (Section 2, Figure 1).
- Positioning
  - DIFF keeps the macro architecture of decoder-only Transformers (pre-RMSNorm, SwiGLU) but replaces the attention operator. It is analogous to noise-canceling headphones and differential amplifiers that remove shared (common-mode) noise by subtraction (Section 2.1, Figure 1).

## 3. Technical Approach
At a high level, DIFF attention computes two independent softmax attention maps and subtracts them, then uses the result to mix values. The intuition: if both maps respond to broadly present patterns (e.g., frequent or generic tokens), their difference cancels those shared responses, leaving sharper peaks on task-relevant spans.

- Differential attention operator (Section 2.1; Equation (1))
  - Inputs: a sequence `X ‚àà R^{N√ód_model}`.
  - Project into two sets of queries/keys and one set of values:
    - `[Q1; Q2] = X W_Q`, `[K1; K2] = X W_K`, `V = X W_V` with `Q1,Q2,K1,K2 ‚àà R^{N√ód}`, `V ‚àà R^{N√ó2d}`.
  - Compute two softmax attention maps:
    - `A1 = softmax(Q1 K1^T / sqrt(d))`
    - `A2 = softmax(Q2 K2^T / sqrt(d))`
  - Differential mix:
    - `Out = (A1 ‚àí Œª A2) V`
  - Interpretation:
    - `A1` tracks signal; `A2` tracks a competing view. Where both assign mass to ubiquitous or irrelevant tokens, subtraction cancels it; where only `A1` lights up on a relevant span, that span is amplified (Figure 1, middle).
  - Note on negative weights: Unlike standard attention, the difference can be negative at some positions. In practice, the subsequent per-head normalization and output projection stabilize learning (Equation (3) and Appendix G).

- Learnable `Œª` with stabilized dynamics (Equation (2))
  - `Œª` is reparameterized to balance the two maps and enable stable optimization:
    - `Œª = exp(Œª_q1 ¬∑ Œª_k1) ‚àí exp(Œª_q2 ¬∑ Œª_k2) + Œª_init`
  - `Œª_init` is set per layer to start subtraction at a controlled strength: `Œª_init = 0.8 ‚àí 0.6 √ó exp(‚àí0.3¬∑(l‚àí1))` (layer index `l`; Section 2.1). Ablations show robustness to other constants (Table 6, last two rows).

- Multi-head differential attention and head-wise normalization (Equation (3); Figure 2)
  - As in standard Transformers, DIFF uses multiple heads. Each head applies the differential operator with its own projections, while a single `Œª` is shared across heads in a layer.
  - Head-wise RMSNorm (‚ÄúGroupNorm‚Äù in Figure 2 notation) is applied to each head before concatenation to stabilize diverse head statistics in the sparser, more selective regime.
  - A fixed multiplier `(1 ‚àí Œª_init)` follows per-head norm to align gradient magnitudes with the Transformer baseline. Appendix G derives that gradient flow magnitudes become comparable to standard attention, enabling reuse of typical hyperparameters.

- Parameter/FLOP alignment and practicality
  - Because DIFF uses `V ‚àà R^{N√ó2d}`, the number of heads is halved to keep total parameters/FLOPs comparable. For example, in 3B experiments: 24 heads for Transformer vs. 12 for DIFF with the same head dimension (Section 3.1).
  - Training recipe retains common practices (pre-RMSNorm, SwiGLU) and can reuse FlashAttention kernels with minor adjustments (Appendix A). Two implementations are provided depending on whether the FlashAttention variant supports different `QK` and `V` dimensions.

- Efficiency
  - Throughput is within 5‚Äì12% of standard attention in measured settings (3B/13B models at 2K/4K lengths; Table 7). The overhead stems from computing two attention maps; the authors note the gap can shrink with FlashAttention-3 kernels (Table 7 discussion).

- Overall layer/stack (Section 2.2)
  - Each layer: `Y_l = MultiHead(LN(X_l)) + X_l`, then `X_{l+1} = SwiGLU(LN(Y_l)) + Y_l`. The only architectural change vs. a standard decoder-only Transformer is the attention operator.

- Why this approach over alternatives
  - Rather than adding external retrieval, reranking, or training heuristics, DIFF attacks the root cause‚Äîattention noise‚Äîinside the attention operator. Subtraction explicitly removes tokens that both views agree on (common-mode), boosting SNR on unique, task-relevant spikes (Figure 1).

## 4. Key Insights and Innovations
- Differential denoising inside attention (fundamental innovation)
  - Novel operator: attention scores as a difference of two softmax maps (Equation (1)). This creates naturally sparse, high-contrast attention patterns that better distinguish signal from noise (visualized in Figure 1; quantified in Table 3).
- Stabilized training while changing the operator (important engineering insight)
  - Reparameterized `Œª` (Equation (2)), per-head normalization, and a fixed `(1 ‚àí Œª_init)` scale align gradient magnitudes with standard attention (Appendix G), letting practitioners reuse hyperparameters. Ablations confirm stability and effectiveness (Table 6).
- Better use of long contexts and key retrieval (new capability)
  - The operator sharply focuses on answer spans in long documents, greatly improving multi-needle retrieval up to 64K tokens (Figure 5; Table 2) and lowering negative log-likelihood across positions in 64K sequences (Figure 4).
- Reduced activation outliers enabling lower-bit attention (practical systems impact)
  - DIFF produces far smaller extreme values in attention logits and hidden states (Table 5) and maintains accuracy at 6-bit and even 4-bit quantization of attention logits, where baseline Transformers degrade sharply (Figure 8). This opens the door to faster, low-precision attention kernels.

## 5. Experimental Analysis
- Evaluation methodology
  - Architectures and training
    - 3B models trained either on 350B or 1T tokens with 4K context (Section 3.1; Appendix B). For long context, the 3B models are extended to 64K context via continued training and RoPE scaling (Section 3.3).
    - Scaling studies span 830M to 13.1B parameters (Section 3.2).
  - Tasks and metrics
    - LM Eval Harness: ARC-C/E, BoolQ, HellaSwag, OBQA, PIQA, WinoGrande (Table 1 and Table 8).
    - Long-context modeling: cumulative average negative log-likelihood on 64K book data (Figure 4).
    - Key information retrieval: multi-needle ‚ÄúNeedle-in-a-Haystack‚Äù (N=1‚Äì8, R=1‚Äì2) at 4K and 64K contexts (Section 3.4; Table 2; Figure 5).
    - In-context learning: many-shot classification on TREC (6 classes), TREC-fine (50), Banking-77 (77), Clinic-150 (150) up to 64K tokens; robustness to order permutations (Section 3.5; Figures 6‚Äì7; Appendix F).
    - Contextual hallucination: summarization (XSum, CNN/DM, MultiNews) and QA (Qasper, HotpotQA, 2WikiMQA), judged by GPT-4o binary accuracy as a proxy for human annotation (Section 3.6; Table 4a‚Äìb).
    - Activation outliers and quantization robustness (Section 3.7; Table 5; Figure 8).
    - Ablations: head count, head dimension, head-wise norm, and `Œª` initialization (Section 3.8; Table 6).
    - Additional math reasoning (Appendix C): two-stage training (synthetic math + distillation from DeepSeek-R1) evaluated on eight math datasets (Figures 9‚Äì10).

- Main quantitative results
  - General language modeling
    - With 1T tokens, the 3B DIFF model outperforms comparably trained 3B Transformers on LM Eval Harness:
      > ‚ÄúDIFF-3B: Avg 60.6 vs OpenLLaMA-v2-3B 57.5 and StableLM-base-alpha-3B-v2 56.8; notable gains on ARC-E (72.9 vs 67.3) and WinoGrande (67.1 vs 62.1)‚Äù (Table 1).
    - With 350B tokens and identical recipes, DIFF beats Transformer in zero- and 5-shot averages (56.2 vs 55.4 zero-shot; 58.0 vs 56.4 5-shot; Table 8).
  - Scaling efficiency (Section 3.2; Figure 3)
    - Parameters: DIFF matches an 11B Transformer with a 6.8B DIFF (‚âà62.2% of params) and a 13.1B Transformer with a 7.8B DIFF (‚âà59.5%).
    - Data: For the 3B setting, DIFF trained on ~160B tokens matches a Transformer trained on ~251B tokens (‚âà63.7% of tokens).
    - Summary: ‚Äúabout 65% of model size or training tokens‚Äù to reach comparable loss (Figure 3 captions).
  - Long-context modeling (Section 3.3)
    - DIFF shows consistently lower cumulative average NLL across sequence positions up to 64K compared to Transformer (Figure 4), indicating better use of long context.
  - Key information retrieval (Section 3.4)
    - At 4K context:
      > ‚ÄúN=6, R=2: DIFF 0.85 accuracy vs Transformer 0.55‚Äù (Table 2).
    - At 8K‚Äì64K context (N=8, R=1):
      > ‚ÄúDIFF maintains high accuracy across depths and lengths; Transformer‚Äôs average accuracy drops steadily as length grows; at 25% depth in 64K, DIFF achieves a 76% accuracy improvement‚Äù (Figure 5).
    - Mechanistic evidence:
      > ‚ÄúDIFF assigns far higher attention to answer spans (0.27‚Äì0.40 vs 0.03‚Äì0.09) and far lower attention noise (0.01‚Äì0.02 vs 0.49‚Äì0.54) across depths‚Äù (Table 3).
  - In-context learning (Section 3.5)
    - Many-shot: Averaged gains of +5.2% to +21.6% across the four datasets once performance stabilizes (Figure 6).
    - Robustness to order:
      > ‚ÄúOn TREC, the margin between best and worst accuracy shrinks from 19.0 (Transformer) to 4.0 (DIFF) with random ordering; with class-alternating ordering it shrinks from 56.7 to 13.4‚Äù (Figure 7). Similar patterns across datasets (Appendix F).
  - Contextual hallucination (Section 3.6)
    - Summarization accuracy (meaning ‚Äúfree of hallucination‚Äù per GPT-4o judge):
      > ‚ÄúXSum 0.53 vs 0.44; CNN/DM 0.41 vs 0.32; MultiNews 0.61 vs 0.42‚Äù (Table 4a).
    - QA:
      > ‚ÄúQasper 0.39 vs 0.28; HotpotQA 0.46 vs 0.36; 2WikiMQA 0.36 vs 0.29‚Äù (Table 4b).
  - Activation outliers and quantization (Section 3.7)
    - Top activation magnitudes:
      > ‚ÄúAttention logits top-1: 38.8 (DIFF) vs 318.0 (Transformer); Hidden states top-1: 1688.2 vs 3608.6‚Äù (Table 5).
    - Quantization:
      > ‚ÄúOn HellaSwag, DIFF retains high accuracy down to 6-bit attention logits, and 4-bit DIFF ‚âà 6-bit Transformer while outperforming 4-bit Transformer by ~25%‚Äù (Figure 8).
  - Ablations (Section 3.8; 1.4B models; Table 6)
    - Head-wise normalization is necessary for DIFF; removing it degrades validation loss from 3.062 to 3.122 and fine-grained metrics likewise.
    - `Œª` initialization is robust: constant 0.8 or 0.5 yields similar validation loss to the exponential schedule.

- Do the experiments support the claims?
  - The mix of macro benchmarks (LM Eval), targeted diagnostics (multi-needle, long-context NLL), behavioral tests (ICL robustness), and systems metrics (outliers, quantization) coherently supports the thesis that differential attention improves signal-to-noise focus and yields tangible benefits.
  - The strongest evidence comes where attention noise is known to harm performance‚Äîmulti-needle retrieval (Table 2, Figure 5), long-context NLL (Figure 4), and order sensitivity in ICL (Figure 7). Mechanistic attention-score analysis (Table 3) ties the effects directly to the operator.
  - Limitations: Most core training is at 3B scale; throughput is modestly lower (Table 7). Some benchmark metrics rely on GPT-4o judgments (Table 4), which, while validated in prior work, are still automatic proxies.

## 6. Limitations and Trade-offs
- Computational overhead and kernels
  - Two attention maps per head add overhead; throughput is 5‚Äì12% lower in tested settings (Table 7). Specialized kernels (e.g., FlashAttention-3) and low-bit implementations may mitigate this but are not demonstrated here.
- Architectural complexity and implementation details
  - The value projection uses dimension `2d` per head; to keep parity with baselines, DIFF halves the number of heads. Some FlashAttention variants need customized handling because `QK` and `V` dimensions differ (Appendix A).
- Potentially negative attention weights
  - The difference `A1 ‚àí Œª A2` can be negative at some positions, which deviates from the probabilistic interpretation of attention. Stability relies on per-head normalization and the `(1 ‚àí Œª_init)` scaling (Section 2.1, Equation (3); Appendix G). While effective empirically, this departs from the usual ‚Äúconvex combination‚Äù view.
- Scope of evidence
  - Most primary results are at 3B scale; while scaling-law plots (Figure 3) suggest generality, evaluations at very large scales are not included.
  - Hallucination results depend on GPT-4o-based judgments (Section 3.6), which‚Äîthough shown to correlate with human ratings‚Äîare not human evaluations themselves.
- Sensitivity and robustness
  - DIFF is robust to `Œª` initialization choices (Table 6), but performance degrades without head-wise normalization. Practitioners must keep these stabilizers.

## 7. Implications and Future Directions
- Changing the default attention operator
  - DIFF provides a concrete, trainable, drop-in replacement that directly combats attention noise. If adopted broadly, it may become a new default for long-context LLMs where retrieval, ICL, and hallucination resistance matter.
- Systems impact: low-bit and sparse attention
  - The substantial reduction in activation outliers (Table 5) and robustness to 6‚Äì4 bit attention logits (Figure 8) suggest opportunities for faster, low-precision attention kernels and memory-efficient serving. The paper explicitly points to ‚Äúlow-bit FlashAttention‚Äù and KV-cache compression as promising next steps (Conclusion).
- Research avenues
  - Theory: Analyze when and why `A1` vs. `A2` specialize to ‚Äúsignal‚Äù vs. ‚Äúnoise,‚Äù and how subtraction shapes the spectrum and rank of attention matrices (the paper cites supporting spectral analysis in related work; Section 2.1).
  - Variants of differential attention:
    - More than two maps; head-specific `Œª`; adaptive or data-dependent `Œª` scheduling; structured constraints to control negative mass.
  - Integration:
    - Combine DIFF with retrieval-augmented generation, routing, or memory modules; explore multimodal DIFF attention for vision‚Äìlanguage or audio‚Äìtext tasks.
  - Scaling:
    - Validate at larger parameter counts and longer contexts; evaluate downstream tasks like coding, tool use, and agentic planning.
- Practical applications
  - Long-context summarization and QA (Section 3.6) where reduced contextual hallucination is critical.
  - Enterprise search and analytics with multi-needle retrieval (Section 3.4).
  - Large in-context learning scenarios (Figure 6) such as classification with thousands of exemplars, benefiting from DIFF‚Äôs order robustness (Figure 7).
  - Edge and latency-sensitive deployments leveraging low-bit attention (Section 3.7).

Overall, DIFF Transformer is a principled and practical redesign of attention focused on cancelling irrelevant context. The mechanism is simple to integrate (Equation (1)), well stabilized (Equation (2), Appendix G), and empirically strong across tasks that stress a model‚Äôs ability to find needles in very large haystacks.

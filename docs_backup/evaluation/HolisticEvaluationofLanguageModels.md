# Holistic Evaluation of Language Models

**ArXiv:** [2211.09110](https://arxiv.org/abs/2211.09110)

## ğŸ¯ Pitch

This paper introduces HELM, a comprehensive framework for evaluating language models across a uniquely broad taxonomy of 42 scenarios and 7 key metric familiesâ€”including accuracy, robustness, fairness, toxicity, and efficiencyâ€”enabling standardized, head-to-head comparison of 30 state-of-the-art models. By systematically surfacing both strengths and shortcomings across diverse use cases and metrics, HELM reveals critical trade-offs and sets a new standard for transparency and rigor in language model assessment, informing responsible development and deployment for real-world impact.

---

## 1. Executive Summary (2â€“3 sentences)
HELM (â€œHolistic Evaluation of Language Modelsâ€) introduces a taxonomy, benchmark, and tooling to evaluate language models comprehensively across 42 scenarios and 7 metric families, and reports a dense, standardized comparison of 30 prominent models. It shows how capabilities, risks, and efficiency vary by scenario and adaptation method, and surfaces consistent patterns (e.g., instruction-tuned models lead on accuracy and robustness; accuracy correlates strongly with robustness and fairness), enabling more transparent, headâ€‘toâ€‘head evaluations and clearer tradeâ€‘offs (see Â§1.1, Â§1.2; Fig. 3, Fig. 4, Fig. 24â€“26).

## 2. Context and Motivation
- Problem/gap addressed
  - Evaluation of language models (LMs) is fragmented and narrow:
    - Most benchmarks emphasize a single metric (accuracy) and a small set of tasks, leaving important desiderata (e.g., robustness, fairness, toxicity, efficiency) under-measured or siloed into separate bespoke datasets (Fig. 3; Â§1.1 â€œMulti-metric measurementâ€).
    - Headâ€‘toâ€‘head comparisons have been hard because models are adapted differently (e.g., fine-tuning vs. 0/5â€‘shot prompting), evaluated on disjoint scenario sets, and sometimes behind proprietary APIs (Â§1.1 â€œStandardizationâ€; Â§6).
  - Before HELM, prominent models often had no overlapping test sets; on average they were evaluated on only 17.9% of HELMâ€™s core scenarios even after aggregating across multiple papers (Fig. 4, top; Â§1.1).
- Why this matters
  - Real-world deployments hinge not only on accuracy but on reliability (calibration and robustness), equity (fairness and bias), safety (toxicity), and usability (efficiency); failing to evaluate these in the same contexts obscures tradeâ€‘offs critical for responsible use (Â§1, Â§4).
- Prior approaches and shortcomings
  - Task suites like GLUE/SuperGLUE, BIG-bench, or LM harnesses advanced breadth, but typically:
    - Center accuracy, rarely report multiple nonâ€‘accuracy metrics per use case (Fig. 3, left).
    - Lack standardization of adaptation (prompting vs. fineâ€‘tuning) that affects outcomes (Â§7, Â§8.2).
    - Provide limited coverage across domains, dialects, and targeted risks (e.g., memorization, disinformation) (Â§5; Â§10).
- This paperâ€™s positioning
  - HELM supplies: (i) a topâ€‘down taxonomy for scenarios (by task, domain, language) and metrics; (ii) a concrete benchmark implementation emphasizing coverage and multiâ€‘metric density (98 of 112 core scenarioâ€“metric pairs, Table 4); and (iii) standardized evaluation of 30 models under unified conditions, improving overlap to 96% across core scenarios (Fig. 4, bottom; Â§1.1, Â§3â€“4, Â§6â€“7).

## 3. Technical Approach
HELMâ€™s methodology has three pillarsâ€”taxonomy-driven coverage, multiâ€‘metric measurement, and standardized adaptationâ€”and a largeâ€‘scale execution across models.

- Abstraction and primitives (Â§2; Fig. 5â€“7)
  - `Scenario`: a use case defined as a list of instances, each with an `input` and reference `outputs` (with labels or properties). Scenarios are structured by task, domain, and language (Fig. 8).
  - `Adaptation`: a method to turn a general LM into a solver for a scenario (e.g., 5â€‘shot prompting; Â§7).
  - `Metric`: a quantitative function over model outputs (and probabilities) to assess performance.
- Scenario taxonomy and selection (Â§3; Fig. 8)
  - Taxonomy dimensions:
    - Task (e.g., question answering, information retrieval, summarization, sentiment analysis, toxicity detection, miscellaneous classification).
    - Domain decomposed as â€œwhatâ€ (genre), â€œwhoâ€ (speaker demographics), and â€œwhenâ€ (time) (Fig. 8).
    - Language (focus here is English and English varieties; Â§3.1â€“3.2; Fig. 10).
  - Core set: 16 userâ€‘facing scenarios across six task families; plus 26 targeted scenarios for language, knowledge, reasoning, memorization/copyright, disinformation, bias, toxicity (Â§3, Â§5; Table 4).
- Metric taxonomy and selection (Â§4)
  - Seven metric families span system desiderata; HELM instantiates those that can be measured with blackâ€‘box model access:
    - `Accuracy` (taskâ€‘specific, e.g., Exact Match, F1, ROUGEâ€‘2, RR@10, NDCG@10; Â§4.3; Appx C.1).
    - `Calibration` via ECE (expected calibration error) and `selective classification` (accuracy at a given confidence coverage) (Â§4.4; Fig. 17; Appx C.2).
      - ECE compares predicted confidence with empirical accuracy across probability bins.
    - `Robustness` as worstâ€‘case accuracy over semantic-preserving perturbations (invariance) and over human-crafted contrast sets (equivariance) (Â§4.5; Fig. 18; Appx D.1).
    - `Fairness` via counterfactual perturbations (dialect, gender, race term substitutions; Fig. 19; Appx D.2) and performance disparities when demographic metadata exists (Â§4.6).
    - `Bias` in generations measured as demographic representation skew and stereotypical association distance from a uniform reference across groups (Â§4.7; Fig. 20; Appx C.5 describes word lists and formulas).
    - `Toxicity` in generations via Perspective API, reporting rate of toxic completions (Â§4.8; Fig. 21).
    - `Efficiency` covering training energy/emissions and two inference metrics:
      - `Denoised runtime`: the providerâ€™s stack with queueing noise factored out.
      - `Idealized runtime`: unified optimized hardware/software (A100 + Megatron) for applesâ€‘toâ€‘apples LM comparison (Â§4.9; Fig. 22; Appx C.7).
  - Multiâ€‘metric density: 98/112 core scenarioâ€“metric pairs (87.5%) measured (Table 4).
- Standardized adaptation (Â§7; Fig. 23; Table 7)
  - Models are treated as blackâ€‘box textâ€‘toâ€‘text APIs (no training data or internals required). All models are adapted with fewâ€‘shot prompting (by default 5 inâ€‘context examples) using the same prompt templates and decoding settings where applicable:
    - Inâ€‘context examples are fixed across test instances (to reflect real fewâ€‘shot use) and sampled to cover label classes; experiments are repeated with 3 different example sets to estimate variance (Â§7; Â§8.2; Fig. 31).
    - Multipleâ€‘choice scenarios are adapted in three ways and compared (Â§8.2; Fig. 33):
      - `Joint`: present all choices and predict the label token.
      - `Separate`: score each choice with the prompt and pick the highest.
      - `Separateâ€‘calibrated`: normalize scores by choice priors.
    - Decoding: temperature 0 for short, deterministic tasks; higher temperature for generation; unified stopping conditions (Â§7; Appx J.3â€“J.4).
- Models evaluated (Â§6; Table 5)
  - 30 models spanning open, limitedâ€‘access APIs, and closed deployment (12 organizations). Where possible, HELM also estimates training energy/emissions and measures inference efficiency on common hardware (Appx C.7; Â§4.9).

## 4. Key Insights and Innovations
- A. A topâ€‘down taxonomy + dense multiâ€‘metric benchmark (fundamental innovation)
  - Whatâ€™s new: HELM frames evaluation as a matrix of scenarios Ã— metrics, chosen from explicit taxonomies (Fig. 2, Fig. 8; Â§3â€“4) rather than an adâ€‘hoc list of datasets. It then implements a dense subset: 16 core scenarios Ã— 7 metrics (98/112 measured), plus 26 targeted evaluations (Table 4; Â§5).
  - Why it matters: Measuring multiple desiderata in the same context exposes tradeâ€‘offs (Fig. 24â€“25) and prevents safety/equity metrics from being sidelined (Fig. 3).
- B. Standardized, headâ€‘toâ€‘head evaluation of 30 models (fundamental innovation)
  - Whatâ€™s new: All models are evaluated under unified adaptation (5â€‘shot prompting with identical templates), prompting variants are analyzed (Â§8.2), and scenario overlap is raised from 17.9% to 96.0% across core scenarios (Fig. 4).
  - Why it matters: Fair comparisons are possible; sensitivity to adaptation becomes explicit (e.g., multipleâ€‘choice methods drastically change accuracy, Fig. 33).
- C. New efficiency metrics enabling fairer runtime comparison (incremental but impactful)
  - Whatâ€™s new: `Denoised` and `idealized` inference runtime separate provider stack effects from modelâ€‘intrinsic speed (Fig. 22; Â§4.9.2), plus training energy/COâ‚‚ estimates for models with enough transparency (Appx C.7).
  - Why it matters: Users and researchers can evaluate capabilityâ€“efficiency tradeâ€‘offs (Fig. 24, bottom right).
- D. Targeted evaluations of risks and primitives (incremental breadth)
  - Whatâ€™s new: Dedicated suites for linguistic phenomena (BLiMP, ICE; Â§5.1), knowledge (WikiFact; Â§5.2), reasoning (Dyck, GSM8K, MATH, LSAT, bAbI, HumanEval/APPS; Â§5.3), memorization/copyright (books, Linux code; Â§5.4), disinformation with human evaluation (Â§5.5, Â§8.5), and bias/toxicity beyond core (Â§5.6â€“5.7).
  - Why it matters: The benchmark reveals capability/risk profiles that core tasks alone would miss (e.g., memorization risk correlates with model capability; Â§5.4).

## 5. Experimental Analysis
- Evaluation methodology
  - Scenarios and datasets: 16 core userâ€‘facing scenarios (e.g., NaturalQuestions, MS MARCO ranking, CNN/DM, XSUM, IMDB, CivilComments, RAFT) plus 26 targeted across language, knowledge, reasoning, copyright, disinformation, bias, toxicity (Table 4; Â§3, Â§5).
  - Metrics: Accuracy variants (EM/F1/ROUGE/RR/NDCG), Calibration (ECE; selective accuracy), Robustness (invariance and contrast sets), Fairness (counterfactuals; disparities), Bias (representation/associations), Toxicity (Perspective API), Efficiency (training and inference) (Â§4; Table 4).
  - Adaptation: Unified 5â€‘shot prompting; multipleâ€‘choice variants compared (Â§7; Â§8.2; Fig. 33).
- Main quantitative results and comparisons
  - Overall headâ€‘toâ€‘head performance (Fig. 26):
    - > â€œtextâ€‘davinciâ€‘002â€ wins the most headâ€‘toâ€‘head accuracy comparisons (>90% win rate), and also leads on robustness and fairness; TNLG v2 (530B) is second on accuracy and fairness; Anthropicâ€‘LM v4â€‘s3 (52B) is consistently topâ€‘3 on accuracy, robustness, and fairness (Â§1.2; Fig. 26).
  - Model accessibility and accuracy (Fig. 28):
    - > Limitedâ€‘access models (e.g., â€œtextâ€‘davinciâ€‘002â€) generally outperform open models across core scenarios; open models are sometimes competitive but lag on knowledgeâ€‘heavy QA (e.g., MMLU, closedâ€‘book NQ) and IR (Â§1.2 finding 2; Fig. 28).
  - Accuracyâ€“robustnessâ€“fairness correlation (Fig. 24â€“25):
    - > Across scenarios, accuracy correlates strongly with robustness and fairness; however, top models can still suffer larger drops on some tasks (e.g., NarrativeQA robustness drop: TNLG v2 530B from 72.6% to 38.9%; Â§1.2 finding 4).
  - Calibration is scenarioâ€‘dependent (Fig. 24â€“25):
    - > On HellaSwag, improving accuracy worsens calibration (higher ECE); on OpenBookQA, accuracy improvements align with better calibration (Â§1.2 finding 3; Â§8.1).
  - Sensitivity to adaptation (Fig. 33; Â§8.2):
    - > For HellaSwag, `separate` > `separateâ€‘calibrated` > `joint`; OPTâ€‘175B shifts from 79.1% EM (separate 0â€‘shot) to 30.2% EM (joint 5â€‘shot) (Fig. 33; Â§8.2). Anthropicâ€‘LM v4â€‘s3 (52B) reverses this pattern on some tasks (joint works best), underscoring that a single â€œstandardâ€ format can advantage some models over others.
  - Information retrieval (MS MARCO; Â§3.4; Â§8.3 IR):
    - > On â€œregularâ€, best models reach 39.8% RR@10 (boosted) and ~22.5% RR@10 (vanilla) vs BM25 19.0%; on â€œTRECâ€, best models reach 65.3% NDCG@10 (boosted) and 61.0% (vanilla) vs BM25 50.6% (Â§1.2 finding 9; Â§8.3 IR).
  - Summarization (CNN/DM, XSUM; Â§8.3):
    - > Metrics often fail to discriminate quality; TNLG v2 (530B) tops XSUM ROUGEâ€‘2 at 17.9 versus 15.6 for OPTâ€‘175B (Fig. 34; Â§1.2 finding 10).
  - Sentiment and misc. classification (Â§8.3):
    - > IMDB: many models >90% EM; best GLM (130B) at 95.5%; calibration variesâ€”BLOOM (176B) ECEâ‰ˆ0.35 (Â§1.2 finding 11).
    - > RAFT: GLM (130B) reaches 85.8% overall; performance varies widely across its 11 subâ€‘tasks (Â§1.2 finding 13).
  - Toxicity detection (CivilComments; Â§8.3):
    - > Most models near chance; best â€œtextâ€‘davinciâ€‘002â€ ~66.8% EM; large robustness/fairness drops (OPTâ€‘175B Black split from 51.3% to 8.8% under robustness perturbations; White split 50.8%â†’24.3%) (Â§1.2 finding 12).
  - Linguistic evaluations (Â§5.1; Â§8.4):
    - > Language modeling BPB: Pileâ€‘trained models (e.g., GPTâ€‘NeoX, OPT) are strongest on The Pile, TwitterAAE, and ICE; BLiMP scores are similar across models, with largest spread on irregular morphology where some top downstream models underperform (Fig. 36; Â§1.2 finding 14).
    - > Dialect disparities: On TwitterAAE, all models have higher BPB (worse) on AAE vs White English (e.g., OPTâ€‘175B: 2.114 vs 1.506 BPB; Â§1.2 finding 5).
  - Knowledge and reasoning (Â§5.2â€“5.3; Â§8.4):
    - > Knowledge: â€œtextâ€‘davinciâ€‘002â€ leads TruthfulQA by a wide margin (62.0% vs 36.2% for Anthropicâ€‘LM v4â€‘s3) and MMLU (57.0% vs 49.8%); TNLG v2 (530B) excels on closedâ€‘book NQ and WikiFact (Â§1.2 finding 15; Fig. 37).
    - > Reasoning: code models dominate; â€œcodeâ€‘davinciâ€‘002â€ achieves 52.1% on GSM8K (vs 35.0% for textâ€‘davinciâ€‘002, others â‰¤16%) and leads on synthetic reasoning (Fig. 38; Â§1.2 finding 16).
  - Memorization/copyright (Â§5.4; Â§8.4; Fig. 39):
    - > Verbatim regurgitation is rare but noticeable for popular books and correlates with accuracy (e.g., textâ€‘davinciâ€‘002, davinci, Anthropicâ€‘LM v4â€‘s3 show highest regurgitation; Â§1.2 finding 17).
  - Disinformation (human evaluation; Â§5.5; Â§8.5; Table 8):
    - > Models can generate stylistically plausible headlines and divisive messages; â€œtextâ€‘davinciâ€‘002â€ and Anthropicâ€‘LM v4â€‘s3 score highest on style/quality for reiteration; wedging results are mixed, with limited accurate audienceâ€‘targeting (Â§1.2 finding 18; Â§8.5).
  - Generative harms in core tasks (Fig. 24 bottom row; Â§8.3):
    - > Average bias and toxicity in core scenario generations are low and largely constant across models, but targeted prompts (RealToxicityPrompts) elicit substantially higher toxicity (Â§1.2 findings 6, 20).
- Ablations and robustness checks
  - Prompt sensitivity: number/choice of inâ€‘context examples and prompt formatting shift results nonâ€‘trivially (Fig. 31â€“32; Â§8.2).
  - Multipleâ€‘choice adaptation strongly affects accuracy and calibration and the ranking of models (Fig. 33; Â§8.2).
- Do results support claims?
  - Yes, quantitatively and at scale. The reported headâ€‘toâ€‘head charts (Fig. 26), crossâ€‘metric correlations (Fig. 24â€“25), and scenarioâ€‘specific result pages (linked throughout Â§8.1â€“8.4) provide converging evidence for the major claims (instructionâ€‘tuning advantage; accessibility gap; accuracyâ€“robustnessâ€“fairness coupling; calibration tradeâ€‘offs; codeâ€‘model advantage on reasoning).

## 6. Limitations and Trade-offs
- Assumptions and scope limits
  - Englishâ€‘centric evaluation with targeted but limited coverage of dialects/varieties; multilingual and multimodal tasks are largely out of scope (Â§3.1â€“3.2; Â§10).
  - Only blackâ€‘box access is assumed, which prevents metrics that require internals (e.g., interpretability based on activations) (Â§4.1â€“4.2).
  - Models are adapted via fewâ€‘shot prompting only; fineâ€‘tuning or instructionâ€‘optimization is not explored here, and prompting details substantially affect outcomes (Â§7; Â§8.2).
- Measurement constraints
  - Fairness and robustness rely on perturbations; while scalable, they approximate rather than perfectly instantiate social/linguistic variation (Appx D.1â€“D.2; Â§4.6 â€œDiscussionâ€).
  - Toxicity uses Perspective API; known limitations and cultural biases apply (Â§4.8).
  - Some metrics (e.g., training emissions) require modelâ€‘provider disclosures; estimates are approximate (Appx C.7).
- Dataset/model contamination and validity
  - Trainingâ€“test contamination cannot be ruled out for many models due to incomplete transparency; known evidence is cataloged (Appx G; Table 13). Validity of some standard datasets (e.g., summarization) is debated (Â§3.5; Â§11.2).
- Computational cost and scalability
  - Running HELM comprehensively is expensive (12.2B tokens; 17.4M queries; ~$38k API cost plus ~19.5k GPU hours; Â§1.2) and requires prioritization (Appx H).
- Tradeâ€‘offs observed
  - Accuracy vs calibration can conflict (Fig. 24â€“25); accuracy correlates with robustness/fairness, but top models still suffer large drops on certain tasks (Â§8.3 QA).
  - Efficiencyâ€“capability tradeâ€‘offs exist but are modelâ€‘family specific; no universal Pareto dominates across all scenarios (Fig. 24 bottomâ€‘right; Â§1.2 finding 7).

## 7. Implications and Future Directions
- How this work changes the landscape
  - HELM establishes a blueprint for holistic, standardized LM evaluation with explicit taxonomies, dense multiâ€‘metric measurement, and reproducible adaptation. It provides a shared â€œscore matrixâ€ rather than a single scalar score, making realâ€‘world tradeâ€‘offs visible and comparable (Fig. 3â€“4; Table 4).
- Followâ€‘up research it enables
  - Extension of taxonomies and coverage to:
    - Multilingual and multimodal evaluation; richer domain â€œwho/when/whyâ€ coverage (e.g., biomedical, finance, education, and nonâ€‘US demographic categories) (Â§10.1).
    - Deeper fairness/robustness measures, including humanâ€‘inâ€‘theâ€‘loop redâ€‘teaming, causal fairness analyses, and improved toxicity detectors (Â§10.2â€“10.3).
  - Exploration of adaptation axes (fineâ€‘tuning, parameterâ€‘efficient tuning, prompt optimization, retrievalâ€‘augmentation) under standardized multiâ€‘metric evaluation (Â§10.5).
  - Better efficiency metrics (endâ€‘toâ€‘end energy per request) and emissions reporting standards; principled capabilityâ€“efficiency Pareto analyses (Â§4.9; Â§10.2).
- Practical applications and downstream use
  - Model selection for deployment can now weigh accuracy against calibration, robustness, fairness, and efficiency within the same scenario contexts (Fig. 24â€“26).
  - Policy and governance: evidence for responsible disclosure (e.g., contamination, emissions), model access standards, and best practices for evaluation of APIs/closed models (Â§6; Appx G; Â§10.4).
  - Risk assessment: targeted evaluations (e.g., disinformation, memorization) provide templates for auditing domainâ€‘specific harms (Â§5.4â€“5.5; Â§8.4â€“8.5).

> â€œBy both evaluating these models on the same scenarios and by conducting the evaluation under standardized conditions â€¦ we facilitate direct headâ€‘toâ€‘head comparisonsâ€ (Fig. 4; Â§1.1 â€œStandardizationâ€).

> â€œOf the 112 possible (core scenario, metric) pairs, we measure 98 (87.5%)â€ (Table 4).

> â€œtextâ€‘davinciâ€‘002 achieves an accuracy of 62.0% [TruthfulQA] â€¦ the next best is 36.2%â€ (Fig. 37; Â§1.2 finding 15).

> â€œOPT (175B) is 79.1% on HellaSwag with separate 0â€‘shot but 30.2% with joint 5â€‘shotâ€ (Fig. 33; Â§8.2).

> â€œOn MS MARCO (TREC), best models reach 65.3% NDCG@10 (boosted) vs BM25 50.6%â€ (Â§8.3 IR).

> â€œAll models show higher BPB (worse modeling) on AAE vs. White English [TwitterAAE] (e.g., 2.114 vs 1.506 for OPTâ€‘175B)â€ (Â§8.4; Â§1.2 finding 5).

> â€œCivilComments: most models near chance; best ~66.8%, with large robustness/fairness drops on Black splitâ€ (Â§8.3).

---

If you plan to use HELMâ€™s results or extend the suite, prioritize (a) aligning scenarioâ€“metric pairs with your deployment context, and (b) stressâ€‘testing sensitivity to adaptation choices (prompting formats and multipleâ€‘choice strategies) to avoid misleading model rankings.

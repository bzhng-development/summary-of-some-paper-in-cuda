# A Survey on Diffusion Language Models

**ArXiv:** [2508.10875](https://arxiv.org/abs/2508.10875)

## ğŸ¯ Pitch

This comprehensive survey maps out the emerging landscape of Diffusion Language Models (DLMs), which generate text by refining entire sequences in parallel through iterative denoisingâ€”breaking through the sequential bottlenecks of standard autoregressive models. By offering an in-depth taxonomy, synthesizing state-of-the-art techniques, and benchmarking DLMsâ€™ efficiency and quality, the paper highlights DLMsâ€™ potential to revolutionize language generation with faster inference, richer context handling, and unified multimodal capabilities, setting the agenda for both research and real-world applications.

---

## 1. Executive Summary (2-3 sentences)
This survey systematizes the fastâ€‘growing field of Diffusion Language Models (DLMs)â€”models that generate text by iteratively denoising masked or noised sequences in parallel rather than predicting tokens oneâ€‘byâ€‘one. It builds a unified taxonomy (continuous, discrete, hybrid), formalizes training and inference mechanics with concrete objectives and algorithms, consolidates evidence that DLMs can match comparable autoregressive (AR) models while offering substantial speedups, and maps key challenges and research directions (Sections 2â€“8; Figs. 1â€“6; Tables 1â€“2).

## 2. Context and Motivation
- Problem addressed
  - Language generation is dominated by AR models that emit tokens sequentially. This creates a fundamental latency bottleneck and prevents easy use of bidirectional context during generation. DLMs aim to break this bottleneck by generating many tokens in parallel via iterative denoising (Section 1; Fig. 4).
- Why it matters
  - Practical: Lower latency and higher throughput are critical for interactive systems and largeâ€‘scale serving. DLMs promise parallel generation, improved controllability, and bidirectional conditioning that can benefit editing, infilling, and multimodal tasks (Section 1: â€œParallel Generationâ€¦ Bidirectional Contextâ€¦ Controllabilityâ€¦ Unified Modeling Across Modalitiesâ€; bullets on p. 3).
  - Scientific: Diffusion has transformed vision; this work consolidates how its principles translate to discrete language, clarifying where theory and engineering solutions already exist and where gaps remain (Sections 2â€“5).
- Prior approaches and shortcomings
  - AR LMs: Scalable and strong but inherently sequential (Eqs. 3â€“4; Section 2.1.2). Even multiâ€‘token prediction partially retains sequential dependencies (Section 2.1.2).
  - Masked Language Models (MLMs): Excellent for understanding but not designed for openâ€‘ended generation (Eq. 1; Section 2.1.1).
  - Early DLMs: Proofs of concept in continuous embeddings and discrete token spaces, but lagged AR on quality and lacked standardized training/inference tooling (Sections 2.2â€“2.3; Fig. 1 timeline).
- Positioning
  - The survey provides a complete stack viewâ€”formalisms (Eqs. 6â€“13), model families (Table 1), inference accelerators (Fig. 5), postâ€‘training methods (Table 2), multimodal integrations (Section 5), performance synthesis (Fig. 6), and challenges with concrete examples (Fig. 7)â€”to establish DLMs as a viable, distinct paradigm.

## 3. Technical Approach
This is a survey; the â€œapproachâ€ is a structured framework that specifies model paradigms, objectives, and inference procedures. It explains mechanics with equations and concrete decoding schedules, then aggregates postâ€‘training and acceleration methods.

- Paradigms and their mechanics (Section 2; Fig. 4)
  - Autoregressive baseline (context for comparison)
    - Factorizes sequence likelihood into leftâ€‘toâ€‘right conditionals (Eqs. 3â€“4).
    - Strength: quality and simplicity of sampling; Weakness: sequential latency.
  - Continuous DLMs (Section 2.2)
    - Idea: Diffuse in a continuous spaceâ€”either embeddings or logitsâ€”then round to tokens.
    - Forward (noising) process: sample a Markov chain q(x1:T|x0) with Gaussian steps; many methods use the closedâ€‘form reparameterization xt = Î±t x0 + bt Îµ (Eqs. 6â€“8).
    - Reverse (denoising) process: learn fÎ¸(xt, t) to predict a target (clean data, noise, or velocity) via a simple squared error objective (Eq. 9).
    - Mapping back to text: nearestâ€‘neighbor or a decoder head converts denoised embeddings/logits to discrete tokens.
    - Variants: SED performs diffusion on a fixed embedding space with selfâ€‘conditioning; TESS/TESSâ€‘2 diffuse over a kâ€‘logit simplex to avoid embedding collapse (Section 2.2).
  - Discrete DLMs (Section 2.3)
    - Idea: Diffuse directly in token space using categorical transitions (D3PM; absorbing [MASK] state).
    - Forward (noising) process: apply transition matrices Qt; with an absorbing mask, tokens either stay or become `MASK`. Marginal: q(xt|x0) = Cat(xt; x0 QÌ„t) (Section 2.3).
    - Reverse process: learn to reconstruct clean tokens from a partially masked sequence.
    - LLaDAâ€‘style masked diffusion objective: crossâ€‘entropy on masked positions only, weighted by 1/t to emphasize early steps (Eq. 10). Inference starts from an allâ€‘`MASK` sequence; iteratively unmask highâ€‘confidence positions and keep remasking lowâ€‘confidence ones (Section 2.3).
  - Hybrid ARâ€‘Diffusion (Section 2.4)
    - Blockwise semiâ€‘autoregression: Generate blocks autoregressively while denoising tokens inside each block in parallel (BD3â€‘LM). Objective conditions each block on prior blocks while learning to denoise withinâ€‘block states (Eq. 11; Fig. 4 â€œBlockâ€‘DLMâ€).
    - Architectural or decoding hybrids: diffusion as a drafter paired with AR validation (SpecDiff in Section 2.4; also see Section 4.1).
- Training and postâ€‘training (Section 3)
  - Preâ€‘training: Either from scratch (e.g., `LLaDAâ€‘8B`) or initializing from AR LMs (`DiffuLLaMA`, `Dream`) or image diffusion models (Dâ€‘DiT, Muddit) to reuse capabilities (Section 3.1; Table 1).
  - Supervised fineâ€‘tuning (SFT): For masked DLMs, leave prompts visible and selectively mask responses to train conditional generation; continuous DLMs corrupt only response segments (Section 3.1).
  - Reasoning/postâ€‘training with RL and preferences (Section 3.2)
    - Challenge: No tractable ARâ€‘style sequence likelihood; perâ€‘step denoising is nonâ€‘factorized.
    - Workarounds:
      - Scoreâ€‘entropy policy gradients (SEPO): a lowâ€‘variance RL objective over diffusion steps using importance sampling (Eq. 12).
      - Practical logâ€‘prob approximations (d1/diffuâ€‘GRPO): meanâ€‘field decomposition + oneâ€‘pass perâ€‘token probabilities on masked variants.
      - Masking strategies during RL to expose different denoising stages (UniGRPO).
      - Preference optimization adapted with variance reduction (VRPO in `LLaDA 1.5`).
    - Nonâ€‘RL reasoning: DoT reframes chainâ€‘ofâ€‘thought as parallel â€œthought refinementâ€ along the denoising trajectory.
- Inference and acceleration (Section 4; Fig. 5)
  - Parallel decoding: accept multiple tokens per step by confidence thresholds or drafts (Section 4.1).
  - Unmasking/remasking: selectively fix confident tokens and reâ€‘open uncertain ones; new samplers permit revising already decoded tokens (Section 4.2).
  - Guidance: classifierâ€‘free guidance (CFG) combines conditional and unconditional scores to steer outputs: sguided = suncond + Î»(scond âˆ’ suncond) (Eq. 13; Section 4.3).
  - Caching:
    - KVâ€‘cache for semiâ€‘autoregressive or delayed caching settings (BD3â€‘LM, Fastâ€‘dLLM DualCache, dKVâ€‘Cache; Section 4.4).
    - Feature caching of intermediate activations across diffusion steps (dLLMâ€‘Cache, FreeCache).
  - Step distillation: train a small â€œstudentâ€ to emulate multiâ€‘step denoising with few or even one step (Di4C, DLMâ€‘One; Section 4.4).
- Multimodal/unified designs (Section 5)
  - VLMs with vision encoders and projectors (LLaDAâ€‘V, LaViDa, Dimple).
  - Unified token spaces with discrete image tokens (MMaDA, UniDisc, Muddit).
  - Dualâ€‘branch continuous diffusion (Dâ€‘DiT) for text and images trained jointly (Section 5).

## 4. Key Insights and Innovations
- A unified, endâ€‘toâ€‘end taxonomy of DLMs with working equations and decoding procedures
  - Whatâ€™s new: The survey aligns continuous, discrete, and hybrid methods within one formal frame (Eqs. 6â€“13; Fig. 4), and ties them to concrete training/inference recipes (Figs. 5; Sections 3â€“4).
  - Why it matters: Readers can directly see how to instantiate, train, and deploy each class, and how to compose hybrids (blockwise or speculative).
- Practical inference playbook for DLMs
  - Novel synthesis: Parallel decoding strategies, adaptive unmasking/remasking, CFG, KV/feature caches, and step distillation are laid out as modular tools (Section 4; Fig. 5).
  - Significance: These make DLMs competitive in latency while preserving or improving quality (quantified speedups below).
- Postâ€‘training for reasoning in nonâ€‘AR settings
  - Distinct challenge identified: lack of tractable ARâ€‘style likelihood.
  - Concrete solutions compared (Table 2; Section 3.2): meanâ€‘field likelihood approximations (d1), scoreâ€‘entropy RL (SEPO), structured noising during RL (UniGRPO), mask coupling to reduce variance (coupledâ€‘GRPO), and preference optimization with variance reduction (VRPO).
- Multimodal diffusion as a firstâ€‘class alternative to AR VLMs
  - Contributions: Multiple recipes for visual grounding (projectors vs unified discrete tokens) and joint training objectives (Sections 5.1â€“5.5).
  - Importance: DLMsâ€™ bidirectional context and iterative refinement map naturally to infilling/editing across modalities.

## 5. Experimental Analysis
While this is a survey, it compiles quantitative evidence (Sections 4â€“6; Fig. 6) and concrete acceleration numbers. Highlights are grounded with specific citations:

- Evaluation scope (Section 6; Fig. 6)
  - Datasets/metrics include PIQA and HellaSwag (language understanding), HumanEval (code), GSM8K (math reasoning), plus multimodal GenEval, MME, MMMU, GQA.
  - Model scales: predominantly <1Bâ€“8B parameters for open DLMs; AR baselines of similar sizes are plotted for comparison.
- Main quantitative takeaways (Section 6; Fig. 6)
  - General language understanding: `LLaDA` and peers are â€œslightly below or on parâ€ with similarly sized AR models (e.g., LLaMA2, Qwen2.5) on PIQA/HellaSwag.
  - Math and science reasoning: DLMs (`LLaDA`, `Dream`) â€œconsistentlyâ€ outperform comparable AR models on GSM8K, GPQA, MATH.
  - Code: `DiffuCoder` is competitive among openâ€‘source models on HumanEval.
  - Multimodal: `MMaDA` and `LLaDAâ€‘V` often surpass ARâ€‘based VLMs on understanding/generation.
- Inference speedups (Section 4; with citations)
  - Parallel decoding:
    > â€œFastâ€‘dLLM â€¦ realizes up to 27.6Ã— speedâ€‘ups without compromising quality.â€ (Section 4.1)
    > â€œSlowFast â€¦ up to 34Ã— acceleration when combined with caching.â€ (Section 4.1)
    > â€œSpecDiff â€¦ up to 7.2Ã— speedâ€‘ups over vanilla AR generation.â€ (Section 4.1)
  - Caching:
    > â€œFastâ€‘dLLM â€¦ DualCache â€¦ up to 27Ã— endâ€‘toâ€‘end throughput gains â€¦ with <1% accuracy loss.â€ (Section 4.4)
    > â€œdKVâ€‘Cache â€¦ achieves 2â€“10Ã— speedâ€‘ups â€¦ with negligible quality drop.â€ (Section 4.4)
    > â€œdLLMâ€‘Cache â€¦ up to 9Ã— endâ€‘toâ€‘end speedâ€‘ups â€¦â€ and â€œFreeCache â€¦ pushing acceleration â€¦ to 34Ã—â€ (Section 4.4)
  - Step distillation:
    > â€œDLMâ€‘One â€¦ generates an entire sequence in a single forward pass, realising up to 500Ã— acceleration with nearâ€‘teacher quality.â€ (Section 4.4)
- Trend and scale observations
  - Research volume: Fig. 2 shows discrete DLM papers accelerating sharply since 2023 (Section 1).
  - Timeline: Fig. 1 and Table 1 document the move from small continuous DLMs to large discrete and multimodal variants (2024â€“2025).
- Robustness/failure cases and ablations
  - Section 8 and Fig. 7 diagnose a core failure mode: quality degrades when too many tokens are accepted per step (the â€œParallel Decoding Curseâ€). Fig. 7 gives concrete failure examples at small step counts for `LLaDA` and `MMaDA`.
- Overall assessment
  - The compiled results substantiate that (a) DLMs can be competitive at similar scale, (b) strong speedups are feasible with the right inference stack, and (c) reasoning and multimodal tasks particularly benefit from bidirectional, iterative refinement. Results are necessarily heterogeneous (drawn from many sources), so exact headâ€‘toâ€‘head comparability depends on training data, step counts, and decoding settings (Section 6).

## 6. Limitations and Trade-offs
- Parallelism vs. coherence (Section 8.1; Fig. 7)
  - Assumption/tradeâ€‘off: Accepting many tokens per step increases parallelism but can ignore interâ€‘token dependencies, yielding incoherent sequences (â€œParallel Decoding Curseâ€).
  - Evidence: Fig. 7 shows correct, fluent outputs only when 1â€“2 tokens are unmasked per step; fewer steps (more parallelism) produce incorrect or garbled text for both `LLaDA` and `MMaDA`.
- Tooling and infrastructure (Section 8.1)
  - DLMs lack mature openâ€‘source training/serving stacks comparable to HuggingFace/vLLM, complicating deployment and making performance uneven across implementations.
- Long sequences and dynamic length (Section 8.1)
  - Computational scaling: full bidirectional attention at every denoising step implies O(NÂ²) per step; steps often scale with N, giving O(NÂ³) total without KVâ€‘cache or blockwise designs.
  - Dynamic stops: even after an `EOS` is predicted, the whole sequence continues to be processed in subsequent steps.
- Scale and data (Section 8.1)
  - Open DLMs are mostly â‰¤8B parameters, often adapted from AR models or trained on smaller corpora than leading AR LLMs. Closed DLMs (Mercury, Gemini Diffusion) still trail the very largest AR models on many benchmarks.
- Postâ€‘training complexity (Section 3.2; Table 2)
  - RL and preference methods require approximations to sequence likelihood, specialized masking schedules, and varianceâ€‘reduction tricks. These add engineering complexity and hyperparameter sensitivity.

## 7. Implications and Future Directions
- Field impact
  - The survey consolidates DLMs as a legitimate alternative to AR generation with clear recipes for making them fast and controllable (Sections 2â€“4). It also demonstrates credible multimodal unification routes (Section 5).
- Research avenues (Section 8.2)
  - Training efficiency: Increase token utilization (e.g., complementary masking), better noise schedules, and hybrid architectures to approach AR training efficiency.
  - Lowâ€‘bit DLMs: Quantization/binarization for memory and latency reductions remain largely unexplored.
  - Compression: Pruning and knowledge distillation (beyond step distillation) tailored to diffusion schedules and masking patterns.
  - Longâ€‘context scaling: Reduce O(NÂ³) behavior via blockwise diffusion with KVâ€‘cache (Fig. 4 â€œBlockâ€‘DLMâ€), dynamic step allocation, or sparse attention; adapt AR extrapolation tricks (e.g., RoPEâ€‘NTK) to DLMs (Section 2.3 cites LongLLaDA).
  - RLHF for diffusion: Improve likelihood surrogates, credit assignment across steps, and reward modeling that aligns with parallel refinement (Section 3.2).
  - Unified multimodal reasoning: Expand discreteâ€‘token unification (VQâ€‘VAEs) and dualâ€‘branch continuous diffusion to robustly coâ€‘generate and edit across modalities (Section 5).
  - Agentic DLMs: Leverage parallel draftâ€‘andâ€‘verify (SpecDiff + AR validators), remasking for selfâ€‘correction, and guidance for structural constraints (Section 4.3).
- Practical applications (Section 7)
  - Code assistants that benefit from global planning and fast drafts (`DiffuCoder`, Mercury Coder).
  - Highâ€‘throughput creative writing/infilling, paraphrasing, and constrained style transfer via guidance and remasking.
  - Scientific design: molecular/protein design using discrete diffusion with reward optimization (DRAKES, DPLM/2, CFPâ€‘GEN).

---

Selected figure/table/equation references for quick lookup:
- Taxonomy and timeline: Fig. 1 (timeline), Fig. 3 (taxonomy), Fig. 2 (trend).
- Core mechanics: Eqs. 3â€“4 (AR), Eq. 1 (MLM), Eqs. 6â€“9 (continuous diffusion), discrete forward with absorbing mask (Section 2.3), Eq. 10 (`LLaDA` loss), Eq. 11 (BD3â€‘LM block objective), Eq. 12 (SEPO RL objective), Eq. 13 (CFG).
- Inference toolbox: Fig. 4 (training/inference overview), Fig. 5 (parallel decoding, remasking, guidance, caches, distillation).
- Models and postâ€‘training at a glance: Table 1 (models/size/objectives), Table 2 (postâ€‘training algorithms).
- Performance synthesis: Fig. 6 (eight benchmarks).
- Failure mode under high parallelism: Fig. 7 (qualitative outputs across step counts).

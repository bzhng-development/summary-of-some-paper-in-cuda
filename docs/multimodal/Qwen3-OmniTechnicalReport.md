# Qwen3-Omni Technical Report

**ArXiv:** [2509.17765](https://arxiv.org/abs/2509.17765)

## ğŸ¯ Pitch

Qwen3-Omni introduces the first unified, end-to-end multimodal model that achieves state-of-the-art performance across text, image, audio, and videoâ€”with no loss relative to single-modality modelsâ€”via its innovative Thinkerâ€“Talker Mixture-of-Experts architecture and a novel AuT audio encoder. By delivering real-time, low-latency speech interaction in up to 119 text and 19 spoken languages, supporting 40-minute audio understanding, and pioneering open-source state-of-the-art results in audio/audio-visual tasks, Qwen3-Omni paves the way for seamless, highly capable AI assistants and sets a new milestone for non-degrading multimodal integration. This breakthrough demonstrates that early, joint multimodal training can consolidate and even enhance core language and vision skills, challenging old trade-offs and enabling more practical, scalable, and interactive AI systems.

---

## 1. Executive Summary
Qwen3-Omni introduces a single end-to-end multimodal model that matches same-sized, singleâ€‘modality Qwen models on text and vision while achieving state-of-the-art open-source results on most audio and audioâ€‘visual benchmarks. Its Thinkerâ€“Talker Mixtureâ€‘ofâ€‘Experts architecture, a new `AuT` audio encoder, and a multiâ€‘codebook streaming speech stack enable real-time voice interaction with a theoretical first audio packet in 234 ms (Table 1) and up to 40â€‘minute audio understanding (Sections 2â€“5).

## 2. Context and Motivation
- Problem/gap
  - Multimodal LLMs often suffer â€œmodality tradeâ€‘offsâ€: improving one modality degrades others (Introduction). The field lacks a demonstration that a unified model can avoid degradation while also excelling at crossâ€‘modal reasoning and lowâ€‘latency speech interaction.
  - There is no widely available generalâ€‘purpose audio captioning model for research (Abstract; Section 4.3).

- Importance
  - Practical: Real-time assistants need accurate long-form ASR, low-latency speech synthesis, and strong text/vision reasoning in one system. Latency and concurrency directly affect product usability and cost (Section 2.5; Table 2).
  - Scientific: Shows that early, joint multimodal pretraining can maintain language/vision parity and even improve some visual benchmarks, challenging the assumption that multimodal training inevitably harms core language ability (Section 6; Table 16).

- Prior approaches and shortcomings
  - Pipeline systems (separate ASR â†’ LLM â†’ TTS) incur latency and error compounding.
  - Prior unified models (e.g., Qwen2.5â€‘Omni) integrated modalities but with more limited audio scale, higher speech synthesis latency, and shorter input limits (Introduction; Section 2.1).

- Positioning
  - Qwen3â€‘Omni extends the earlier Thinkerâ€“Talker design (Qwen2.5â€‘Omni) with MoE in both modules, a fromâ€‘scratch `AuT` encoder trained on 20M hours, multiâ€‘codebook speech generation with an autoregressive + ConvNet pathway, and a training program that mixes unimodal and crossâ€‘modal data early (Sections 2â€“4). It empirically validates â€œnonâ€‘degrading multimodalityâ€ at 30B scale (Section 6; Table 16).

## 3. Technical Approach
Stepâ€‘byâ€‘step overview of the system (Figure 2; Section 2):

1) Two-module architecture with Mixtureâ€‘ofâ€‘Experts (MoE)
- `Thinker` (MoE Transformer, 30Bâ€‘A3B; Table 1) handles text generation and multimodal reasoning. MoE means multiple specialized expert sub-networks with a learned router that activates a sparse subset per token, improving throughput for long contexts while controlling compute.
- `Talker` (MoE Transformer, 3Bâ€‘A0.3B; Table 1) generates streaming speech tokens conditioned on multimodal features and the conversation state. It no longer consumes `Thinker`â€™s text embeddings (Section 2.1). Instead, it conditions directly on audio/visual features and a controlled text feed, enabling external interventions (RAG, safety filters) before synthesis and independent style prompts for text vs. audio (Section 2.1).

2) Perception (Section 2.3)
- Text: Qwen tokenizer (151,643 BPE tokens).
- Audio: 16 kHz waveform â†’ 128â€‘channel mel spectrogram â†’ `AuT` encoder.
- Images/Video: Qwen3â€‘VL vision encoder initialized from SigLIP2â€‘So400M (~543M params).
- Temporal alignment: `TMâ€‘RoPE` (Timeâ€‘aligned Multimodal Rotary Position Embedding) assigns separate rotary angle budgets to time/height/width (24/20/20) and uses absolute 80 ms temporal IDs for audio and timeâ€‘aligned video frames sampled dynamically (Section 2.3). This enables coherent audiovisual fusion, streaming, and longâ€‘sequence extrapolation better than prior Mâ€‘RoPE allocations.

3) Audio Transformer (`AuT`) encoder (Section 2.2; Figure 3)
- Trained from scratch on 20M hours: ~80% zh/en pseudoâ€‘labeled ASR, 10% otherâ€‘language ASR, 10% audio understanding.
- Conv2D downsampling by 8Ã— before attention yields a 12.5 Hz token rate (one token â‰ˆ 80 ms), enabling efficient realâ€‘time caching. Flash attention uses dynamic window sizes covering 1â€“8 seconds to balance streaming and offline performance. The `AuT` encoder has ~0.6B parameters (Section 2.2).

4) Streaming speech generation path (Section 2.4 and 2.5; Figure 2)
- Discrete speech tokens: The system uses `RVQ` (residual vector quantization) codebooks (multiâ€‘codebook representation). Each codec frame contains multiple codebook layers that capture different levels of acoustic detail (timbre, prosody).
- Autoregressive top-layer + `MTP` for residuals: `Talker` predicts the zeroth codebook per frame; a lightweight `MTP` (multiâ€‘token prediction) module then predicts remaining residual codebooks for that frame in a fixed-step autoregressive way (Section 2.4â€“2.5). This concentrates expressivity in discrete codecs while keeping perâ€‘step compute small.
- `Code2Wav` renderer: A causal ConvNet replaces heavier diffusion or DiT vocoders. It can synthesize waveform incrementally from the predicted codebooks, enabling frameâ€‘byâ€‘frame streaming (Section 2.4).

5) Streaming and concurrency optimizations (Section 2.5; Tables 1â€“2)
- Chunked prefilling: Audio/vision encoders emit temporal chunks. `Thinker` prefills chunk t while `Talker` asynchronously prefills its own chunk with `Thinker`â€™s last outputs, then `Thinker` starts chunk t+1 (asynchronous pipeline).
- Leftâ€‘contextâ€‘only codec generation: As soon as `Talker` emits one token, `MTP` fills the frame and `Code2Wav` renders, eliminating the blockâ€‘context waiting used in earlier systems (Section 2.5).
- Lightweight modules + MoE: The small `MTP` transformer (~80M params) and ConvNet decoder (~200M) have low FLOPs and batch efficiently; MoE reduces KVâ€‘cache IO for long sequences, improving tokens/sec (Section 2.5; Table 2).

6) Training program (Sections 3â€“4)
- Pretraining in three stages (Section 3):
  - S1 Encoder alignment: Freeze LLM (initialized from Qwen3). Train audio/vision encoders and adapters against a fixed LLMâ€”but first train adapters, then encodersâ€”to avoid encoders compensating for a frozen LLM in a way that harms perception.
  - S2 General: Unfreeze all and train on ~2 trillion tokens mixed across modalities: text (0.57T), audio (0.77T), image (0.82T), video (0.05T), videoâ€‘audio (0.05T).
  - S3 Long context: Raise context to 32,768 tokens; increase long audio/video share.
- Postâ€‘training of `Thinker` in three steps (Section 4.1):
  - Lightweight SFT on ChatML conversations across text/vision/audio/mixed.
  - Strongâ€‘toâ€‘Weak Distillation (offâ€‘policy teacher outputs, then onâ€‘policy KL to teacher logits; teachers include Qwen3â€‘32B, Qwen3â€‘235Bâ€‘A22B).
  - GSPO reinforcement learning with ruleâ€‘based rewards for verifiable tasks (math/coding/IF) and modelâ€‘based judges (Qwen3, Qwen2.5â€‘VL) supplied with references to reduce reward hacking.
- Postâ€‘training of `Talker` in four steps (Section 4.2):
  - Stageâ€‘1: Large mixed multimodal speech data to establish monotonic mapping from context to speech tokens.
  - Stageâ€‘2: Continual pretraining with highâ€‘quality data to reduce hallucinations and strengthen longâ€‘context stability.
  - Stageâ€‘3: Multilingual DPO with preference pairs to improve robustness.
  - Stageâ€‘4: Speaker fineâ€‘tuning for voice control, naturalness, and expressiveness.
- Audio captioner (Section 4.3): Fineâ€‘tune `Qwen3â€‘Omniâ€‘30Bâ€‘A3B` into `â€¦â€‘Captioner` to generate detailed, lowâ€‘hallucination audio captions (Appendix 9.2).

Why these choices?
- Early multimodal data mixing during general pretraining (S2) is presented as the key to â€œnonâ€‘degradingâ€ multimodality (Section 6; Table 16).
- Decoupling `Thinker`â€™s text from `Talker` lets safety/tools intervene in text before speech, and enables separate system prompts for text and audio styles (Section 2.1).
- Multiâ€‘codebook AR + MTP + ConvNet replaces blockwise diffusion, enabling immediate streaming with low latency and high throughput (Sections 2.4â€“2.5; Table 1â€“2).
- `AuT` trained from scratch on 20M hours aims to surpass Whisper-like backbones and to support long-duration, multilingual, general-purpose audio features (Section 2.2).

## 4. Key Insights and Innovations
1) Demonstration of â€œnonâ€‘degradingâ€ multimodal training at 30B scale
- What is new: A controlled comparison shows the 30B multimodal base matches or slightly exceeds sameâ€‘size textâ€‘only and visionâ€‘only counterparts on their own modalities while adding audio capabilities (Section 6; Table 16).
- Why it matters: Prior LLMâ€‘centric multimodal models often trade off modalities. Here, early multimodal integration yields parity on language and gains on several vision/OCR benchmarks (e.g., MMMUval 59.33 vs 57.22; InfoVQA 83.31 vs 81.17).

2) Lowâ€‘latency, multiâ€‘codebook streaming speech stack
- What is new: A leftâ€‘contextâ€‘only scheme where `Talker` predicts one token, `MTP` fills remaining codebooks for the frame, and `Code2Wav` streams audio immediately (Section 2.5; Figure 2).
- Why it matters: Theoretical endâ€‘toâ€‘end firstâ€‘packet latency is 234 ms for audio (Table 1) and remains streaming with Realâ€‘Time Factor < 1 under varied concurrency (Table 2: RTF 0.47 â†’ 0.66).

3) `AuT` audio encoder trained on 20M hours with 12.5 Hz tokenization
- What is new: A fromâ€‘scratch attention encoderâ€‘decoder with Conv2D downsampling, dynamic attention windows (1â€“8 s), and realâ€‘time prefill caching (Section 2.2).
- Why it matters: Drives strong ASR/S2TT and audio reasoning results across 36 benchmarks; supports up to 40â€‘minute inputs (Abstract; Section 5.1.2; Table 6â€“8).

4) Decoupled Thinkerâ€“Talker with independent prompts and toolability
- What is new: Talker no longer consumes Thinkerâ€™s text embeddings but conditions on multimodal features and controlled text inputs; separate system prompts control textual vs. audio style independently (Section 2.1).
- Why it matters: Improves controllability, safety, and integration with toolchains (RAG, function calling) without sacrificing latency.

5) Release of an audio captioner
- What is new: `Qwen3â€‘Omniâ€‘30Bâ€‘A3Bâ€‘Captioner` for detailed, lowâ€‘hallucination audio descriptions to fill a gap in general-purpose audio captioning research (Abstract; Section 4.3; Appendix 9.2).
- Why it matters: Enables data generation and evaluation scaffolding for audioâ€‘centric multimodal research.

## 5. Experimental Analysis
Evaluation design
- Modalities and directions:
  - Xâ†’Text: text, audio, vision, audioâ€‘visual video to text (Section 5.1).
  - Xâ†’Speech: text or crossâ€‘lingual to speech (Section 5.2).
- Datasets/metrics:
  - Textâ†’Text: MMLUâ€‘Redux, GPQA, AIME25, ZebraLogic, MultiPLâ€‘E, IFEval, Creative Writing v3, WritingBench, BFCLâ€‘v3, MultiIF, PolyMath (Tables 4â€“5).
  - Audioâ†’Text: ASR (Librispeech, Wenetspeech, FLEURS, CommonVoice), S2TT (FLEURS), Music (RULâ€‘MuchoMusic, GTZAN, MTGâ€‘Jamendo, MagnaTagATune), VoiceBench, MMAU, MMSU (Tables 6â€“8).
  - Visionâ†’Text: MMStar, HallusionBench, MMâ€‘MTâ€‘Bench, MathVista, MathVision, MMMU/MMMUâ€‘Pro, AI2D, ChartQA, CountBench, Videoâ€‘MME, LVBench, MLVU (Tables 9â€“10).
  - AudioVisualâ†’Text: WorldSense, DailyOmni, VideoHolmes (Tables 11â€“12).
  - Xâ†’Speech: Seedâ€‘TTS test sets, MiniMax multilingual, CosyVoice crossâ€‘lingual (Tables 13â€“15).
- Latency/concurrency measured with vLLM, torch.compile, CUDA Graph for MTP/vocoder (Table 2).
- Nonâ€‘degradation study: controlled pretraining of textâ€‘only, visionâ€‘only, and Omni models with matched size, data, and schedules; only Omni adds audio and audiovisual during pretraining (Section 6; Table 16).

Headline results
- Audio leads
  - ASR/S2TT: On Librispeech testâ€‘clean/testâ€‘other WER, `Qwen3â€‘Omniâ€‘Instruct` reaches 1.22/2.48, narrowly beating GPTâ€‘4oâ€‘Transcribe (1.39/3.75) and Voxtralâ€‘Small (1.56/3.30) (Table 6).
  - Multilingual ASR/FLEURS avg (19 langs): 5.33 WER for `Omniâ€‘Instruct`, competitive with Geminiâ€‘2.5â€‘Pro (5.55) (Table 6).
  - VoiceBench â€œOverallâ€: `Omniâ€‘Thinking` 89.5, on par with Geminiâ€‘2.5â€‘Pro 89.6 and ahead of GPTâ€‘4oâ€‘Audio 86.8 (Table 7).
  - Audio reasoning: MMAU 77.5 (`Omniâ€‘Instruct`), surpassing Geminiâ€‘2.5â€‘Pro 77.4; MMSU edges Geminiâ€‘2.5â€‘Flash and GPTâ€‘4oâ€‘Audio (Table 7).
  - Music understanding: SOTA on RULâ€‘MuchoMusic (52.0), GTZAN Acc. 93.0, and strong microâ€‘F1 across MTG subsets compared with specialists and generalists (Table 8).

- Text and vision parity
  - Textâ†’Text (nonâ€‘thinking): `Omniâ€‘Instruct` compares favorably against larger systems on some tasks. Example: AIME25 65.0 (vs Qwen3â€‘235Bâ€‘A22B Nonâ€‘Thinking 24.7; vs GPTâ€‘4oâ€‘0327 26.7) and WritingBench 82.6 (Table 4). On other tasks (MMLUâ€‘Redux), it trails GPTâ€‘4oâ€‘0327 (86.6 vs 91.3).
  - Visionâ†’Text: `Omniâ€‘Instruct` matches or exceeds strong baselines on Math/STEM (e.g., MATHâ€‘Vision full 56.3, beating GPTâ€‘4o 30.4 and Geminiâ€‘2.0â€‘Flash 48.6; Table 9). General VQA is competitive; video understanding mixed (Table 9).

- Audioâ€‘visual reasoning
  - WorldSense: `Omniâ€‘Instruct` 54.0 vs previous openâ€‘source SOTA 47.1 and Geminiâ€‘2.5â€‘Flash 50.9 (Table 11).
  - DailyOmni and VideoHolmes (Thinking): 75.8/57.3, exceeding Geminiâ€‘2.5â€‘Flashâ€‘Thinking on VideoHolmes and prior openâ€‘source SOTA on both (Table 12).

- Xâ†’Speech generation
  - Zeroâ€‘shot TTS (Seedâ€‘TTS): `Omni` achieves 1.07 WER (zh) and 1.39 WER (en). It beats CosyVoice3 on English (1.39 vs 1.45) but not on Chinese (0.71 for CosyVoice3) (Table 13).
  - Multilingual cloning (MiniMax set): Lower is better for â€œContent Consistencyâ€; higher is better for â€œSpeaker Similarityâ€. `Omni` excels on Chinese (0.716 WER; SIM 0.772) and English (1.069; 0.773), with competitive performance in other languages (Table 14).
  - Crossâ€‘lingual cloning: `Omni` improves over CosyVoice3 in many anyâ†’en and anyâ†’ko directions (e.g., zhâ†’en 2.76 vs 2.98; enâ†’ko 4.96 vs 5.87; Table 15). For anyâ†’ja, results are comparable despite no kana normalization.

- Latency and throughput
  - Firstâ€‘packet latency: 234 ms (audio) at singleâ€‘request concurrency; 728 ms at 4Ã—; 1172 ms at 6Ã— (Table 2).
  - RTF stays < 1 (0.47 â†’ 0.66), meaning sustained realâ€‘time streaming (Table 2).
  - Token rates: `Thinker` 75â†’53 tok/s; `Talker` 140â†’110 tok/s as concurrency rises (Table 2).

- Nonâ€‘degradation evidence (Table 16)
  - Language parity: MMLU 81.69 (`Omniâ€‘Base`) vs 81.24 (textâ€‘only base).
  - Vision gains: MMMUval 59.33 vs 57.22 (visionâ€‘only base); InfoVQA 83.31 vs 81.17; AI2D and ChartQA slightly up.
  - Video mixed: LVBench improves (51.07 vs 48.61), MVBench slightly down.

Robustness and ablations
- Thinking vs Instruct for perception tasks: Appendix 9.1 shows `Thinking` underperforms `Instruct` for ASR/S2TT and music understanding, suggesting that explicit chainâ€‘ofâ€‘thought adds little to primarily perceptual tasks and may introduce hallucinations.
- Longâ€‘video limitations acknowledged (Table 10 discussion): positional extrapolation and context length constraints.

Overall assessment
- The breadth and depth of benchmarks, plus the controlled nonâ€‘degradation study, support the central claims for parity and audio excellence. Where results are mixed (e.g., long video), the paper discusses causes and future fixes (Section 5.1.3).

## 6. Limitations and Trade-offs
- Long video reasoning remains suboptimal
  - The `Thinking` model lags on Videoâ€‘MME/LVBench/MLVU compared with Geminiâ€‘2.5â€‘Flashâ€‘Thinking (Table 10), attributed to limited positional extrapolation and context length (Section 5.1.3).

- Language coverage and balance
  - Speech understanding covers 19 languages and synthesis 10 (Table 3). Text supports 119 languages, but audio modalities are narrower.

- Data and compute intensity
  - Pretraining uses ~2T multimodal tokens and a 20Mâ€‘hour audio corpus (Section 3). Such scale may be prohibitive for many labs; provenance/quality control details are not exhaustively enumerated here.

- Streaming design tradeâ€‘offs
  - The 12.5 Hz codec rate (80 ms per frame) lowers latency and compute but could limit the finestâ€‘grain prosodic control compared with higherâ€‘rate codecs; the paper argues fidelity is still superior due to multiâ€‘codebook capacity and the ConvNet renderer (Sections 2.4â€“2.5), but no direct ablation of token rate is shown.

- Reasoning vs perception interference
  - `Thinking` improves complex reasoning but can reduce performance on pure perception tasks (Appendix 9.1), underscoring a need for mode selection or routing between â€œthinkingâ€ and â€œnonâ€‘thinkingâ€ behaviors.

- Generality of nonâ€‘degradation claim
  - The controlled study is at one scale (30Bâ€‘A3B) with matched schedules (Table 16). The paper notes cost prevented a full sweep across sizes (Section 6), leaving open how broadly the finding generalizes.

- Evaluation biases
  - Some postâ€‘training uses modelâ€‘asâ€‘judge rewards (Section 4.1). Although references are provided to stabilize judgments, automatic evaluators can encode biases or style preferences.

## 7. Implications and Future Directions
- Field impact
  - Provides concrete evidence that early joint multimodal training can achieve language/vision parity while adding strong audio and audioâ€‘visual abilities (Section 6; Table 16). This challenges the community view that multimodal integration inevitably dilutes core language skills.
  - Establishes a practical recipe for realâ€‘time, lowâ€‘latency speech interaction in a unified model (Tables 1â€“2), likely to become the new baseline for voice assistants.

- Practical applications
  - Real-time multilingual assistants with speech understanding up to 40 minutes and interactive synthesis at subâ€‘second first packet (Abstract; Sections 2.5, 5.2).
  - Meeting transcription, voice chat, and live translation across 19/10 input/output languages (Table 3).
  - Audioâ€‘visual agents for video understanding, surveillance triage, or media QA, supported by gains on WorldSense and DailyOmni/VideoHolmes (Tables 11â€“12).
  - Research tooling: an open audio captioner to generate labels and enable benchmarking of audioâ€‘centric tasks (Section 4.3; Appendix 9.2). Models are released under Apacheâ€‘2.0 (Abstract).

- Research directions
  - Architecture: Improve longâ€‘video handling via stronger positional extrapolation and longer contexts; explore routing between thinking vs. perception modes to avoid hallucinations on ASR/Music (Section 5.1.3; Appendix 9.1).
  - Audio: Extend to multiâ€‘speaker ASR, diarization, and more languages; explore higherâ€‘rate or adaptive codecs if ultraâ€‘fine prosody matters.
  - Vision: Enhance video OCR and temporal grounding; integrate proactive audiovisual learning (Conclusion).
  - Agentic workflows: Tighter coupling with function calling and retrieval, leveraging the decoupled Thinkerâ€“Talker to enforce safety and control (Section 2.1; Conclusion).

> Key quantitative highlights to remember:
> - First packet latency: 234 ms (audio), 547 ms (video) at single concurrency (Table 1).
> - VoiceBench Overall: 89.5 (Omniâ€‘Thinking), nearly tied with Geminiâ€‘2.5â€‘Pro 89.6 (Table 7).
> - Librispeech WER: 1.22/2.48 (Omniâ€‘Instruct), better than GPTâ€‘4oâ€‘Transcribe 1.39/3.75 (Table 6).
> - WorldSense: 54.0 (Omniâ€‘Instruct) vs 50.9 (Geminiâ€‘2.5â€‘Flash) and 47.1 prior openâ€‘source SOTA (Table 11).
> - Nonâ€‘degradation: MMLU 81.69 (Omniâ€‘Base) vs 81.24 (Textâ€‘only); MMMUval 59.33 (Omniâ€‘Base) vs 57.22 (Visionâ€‘only) (Table 16).

In sum, Qwen3â€‘Omni is a compelling demonstration that a unified multimodal model can retain singleâ€‘modality strength while unlocking lowâ€‘latency, realâ€‘time speech and crossâ€‘modal reasoningâ€”backed by broad empirical evidence and a clear systems design that balances accuracy, latency, and concurrency.

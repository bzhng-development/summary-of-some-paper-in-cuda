# KIMI LINEAR: AN EXPRESSIVE, EFFICIENT ATTENTION ARCHITECTURE

**ArXiv:** [2510.26692](https://arxiv.org/abs/2510.26692)

## ðŸŽ¯ Pitch

Kimi Linear introduces Kimi Delta Attention (KDA), a novel hybrid linear attention mechanism that, for the first time, surpasses full attention on language modeling quality while being dramatically more efficient in both speed and memory usage. By combining a fine-grained gating mechanism with a delta-ruleâ€“based update, Kimi Linear enables drop-in replacement of quadratic attention for million-token contexts and RL scaling, delivering up to 6Ã— faster decoding and 75% lower key-value cache usageâ€”making efficient, long-context LLMs truly practical without sacrificing performance.

---

## 1. Executive Summary
Kimi Linear introduces a hybrid attention architecture that replaces most full attention layers with a new linear module, Kimi Delta Attention (`KDA`), and showsâ€”for the first time under matched training recipesâ€”that it can outperform full attention in accuracy while being substantially faster and more memoryâ€‘efficient across short context, long context (up to 1M tokens), and reinforcement learning (RL) settings. The core significance is that it provides a dropâ€‘in, productionâ€‘ready alternative to quadratic attention that scales to millionâ€‘token contexts with up to 6Ã— decoding throughput and up to 75% lower KVâ€‘cache usage, without sacrificing quality (Figure 1; Â§4â€“Â§5â€“Â§6).

## 2. Context and Motivation
- Problem/gap
  - Full attention has quadratic time in sequence length and requires a keyâ€“value (`KV`) cache that grows linearly with the context, making long contexts (e.g., â‰¥128k) and decodingâ€‘heavy workloads slow and memoryâ€‘hungry (Â§1). This bottleneck becomes acute for agentic LLMs and RL testâ€‘time scaling that process long trajectories and tool-use traces.
  - Linear attention offers linear complexity but historically underperforms on language modeling and retrieval, especially for long or copy-heavy sequences (Â§1â€“Â§2.2). This quality gap has limited its adoption as a general replacement for full attention.
- Why it matters
  - Real-world impact: More efficient inference enables millionâ€‘token contexts, larger batch sizes, faster interactive agents, and lower serving cost for long-horizon tasks (Â§1, Figure 1b, Figure 7).
  - Theoretical significance: The work advances linear attention by combining the delta learning rule (a corrective, â€œfast-weightâ€ update) with fine-grained memory control, narrowing or surpassing full attention performance while keeping RNNâ€‘like constantâ€‘state inference (Â§2.2â€“Â§3).
- Prior approaches and shortcomings
  - Gating/decay + delta rule improved linear attention: RetNet (fixed decay), Mamba2 (data-dependent scalar decay), DeltaNet (delta rule), Gated DeltaNet (`GDN`) (delta + scalar forget) (Â§2.2; Table 6).
  - However, finite-state memory still limits perfect retrieval; purely linear models struggle with exact copying and long-range recall (Â§1, Â§5.1).
  - Hybrid models (interleaving linear and full attention) existed but typically did not beat full attention across diverse, large-scale benchmarks, or lacked unified, efficient kernels and fair, matched training (Â§1, Â§7.2).
- Positioning
  - This work builds a stronger linear module (`KDA`) by making the forget gate channelâ€‘wise (per feature dimension) instead of headâ€‘wise, integrates a hardwareâ€‘efficient chunkwise algorithm, and combines it with periodic global NoPE attention in a simple 3:1 ratio. It then validates at scale with matched tokens and near-identical recipes, open-sourcing kernels and models (Â§3â€“Â§4â€“Â§5.4â€“Â§5.5, Figure 1).

## 3. Technical Approach
The system has two layers of design: a new linear attention operator (`KDA`) and a hybrid model architecture (â€œKimi Linearâ€) that interleaves `KDA` with full attention.

- Kimi Delta Attention (`KDA`): a fineâ€‘grained gated delta rule
  - Intuition
    - Think of a small matrix `S_t` as an associative memory that stores keyâ†’value mappings (â€œfast weightsâ€). At each step, the model both forgets some of the memory and corrects it to better map the current key `k_t` to value `v_t`. The output is produced by querying this memory with `q_t` (Â§2.2).
  - Core update (Eq. 1)
    - Update memory: `S_t = (I âˆ’ Î²_t k_t k_t^T) Â· Diag(Î±_t) Â· S_{tâˆ’1} + Î²_t k_t v_t^T`.
      - `Î²_t` is a learnable step size (Sigmoidâ€‘bounded to [0,1]).
      - `Diag(Î±_t)` is a perâ€‘channel (featureâ€‘wise) forget gate where each channel has its own decay in [0,1].
      - The rankâ€‘1 term `(I âˆ’ Î²_t k_t k_t^T)` is the delta rule: a corrective, Householderâ€‘like transformation that fixes the memory toward mapping `k_t â†’ v_t` (Â§2.2, Eq. 1).
    - Output: `o_t = S_t^T q_t` (Eq. 1).
  - Why the fineâ€‘grained (perâ€‘channel) gate matters
    - Prior `GDN` used a single forget scalar per head; `KDA` makes this gate channelâ€‘wise, giving each feature dimension its own time constant. This increases control over what is retained or forgotten, improving copying, recall, and stability (Â§3; Figure 4).
  - Relation to positional encoding
    - `KDA`â€™s decayed, corrective transitions can be written as a product of dataâ€‘dependent matrices between positions (Eq. 12). This acts like a learnable, multiplicative positional encoding, analogous to RoPE but without fixed rotations (Â§6.1, Eq. 11â€“12; Table 6). This motivates using NoPE in the full attention layers so `KDA` carries the positional bias (Â§4, â€œNoPE for MLAâ€; Â§5.2 â€œNoPE vs. RoPEâ€).

- Hardwareâ€‘efficient chunkwise algorithm for `KDA`
  - Goal: Parallelize over chunks of length `C` to fully exploit GPU matmuls while keeping the recurrent semantics (Â§3.1).
  - Key ingredients
    - Chunk re-indexing and partial unrolling (Eq. 2): Unroll `C` steps into a closed form that depends on the initial state of the chunk and a sum of transformed rankâ€‘1 updates.
    - WY representation (Eq. 3â€“5): Packs products of rankâ€‘1 updates into compact matrices without explicit inverses, reducing overhead and improving numeric stability (Â§3.1).
    - UT transform (Eq. 6â€“7): Replaces some scalar FLOPs with triangular solves and matmuls, lowering nonâ€‘matmul overhead to get better Tensor Core utilization (Â§3.1).
    - Chunkwise state/output formulas (Eq. 8â€“9): Provide a batched, GPUâ€‘friendly way to update the state and compute outputs with an interâ€‘block recurrent / intraâ€‘block parallel schedule, maximizing matmul throughput (Â§3.1).
  - Specialized DPLR variant for speed and stability
    - Background: A general Diagonalâ€‘Plusâ€‘Lowâ€‘Rank (`DPLR`) transition writes the state transform as `D âˆ’ a_t b_t^T`. It is expressive but incurs extra matmuls and can require secondary chunking for numeric stability (Â§3.2, Â§6.2).
    - `KDA` ties both low-rank vectors to the key (`a=b=k`). This choice:
      - Avoids divisions by cumulative decays that cause instability in intraâ€‘chunk ops.
      - Cuts the number of â€œsecondâ€‘levelâ€ chunk matmuls from four to two and removes three additional matmuls (Â§3.2; Listing 8a vs 8b).
      - Yields roughly 2Ã— operator speed vs a general DPLR kernel up to 64k tokens (Figure 2), and â€œ~100% operator efficiency improvementâ€ vs DPLR in analysis (Â§3.2, Â§6.2).

- Kimi Linear architecture (how `KDA` is used)
  - Layout
    - Stack blocks with a tokenâ€‘mixing layer followed by a MoE channelâ€‘mixing layer (Figure 3). Token mixing alternates 3 `KDA` layers with 1 full attention layer (`MLA`, Multiâ€‘Head Latent Attention), i.e., a 3:1 ratio (Â§4).
  - Positional encoding: NoPE in the MLA layers
    - No positional encoding in global attention; `KDA` carries all positional/recency bias. Benefits: simpler longâ€‘context training (no RoPE tuning), and MLA heads can turn into efficient MQA at inference (Â§4 â€œNoPE for MLAâ€, Â§5.2 â€œNoPE vs. RoPEâ€).
  - Perâ€‘layer parameterization (Section 4)
    - `q,k,v` are produced via short depthwise convolutions + Swish, with `q,k` L2â€‘normalized for eigenvalue stability.
    - `Î±_t` (forget gate) uses a lowâ€‘rank projection and monotone map to [0,1] per channel.
    - `Î²_t` uses a Sigmoid.
    - An additional lowâ€‘rank output gate (Sigmoid) after headâ€‘wise RMSNorm improves stability and avoids attention sink (Eq. 10; Table 1 ablations).
    - Head dimensions `d_k=d_v=128` in experiments.
  - Inference strategy and complexity
    - Prefill uses the chunked kernel; decoding uses the recurrent update (Eq. 2). The model maintains a fixed state per head (`d_k Ã— d_v`), independent of sequence length, unlike KV caches (Â§6.3). FLOPs per head scale as `O(T d_h^2 + T C d_h + T C^2)` for `KDA` vs `O(T^2 d_h)` for full attention (Eq. 13â€“14).

- Implementation notes
  - Kernels are open-sourced with vLLM integration (links in Abstract). The chunked `KDA` pseudoâ€‘code is given in Appendix C (Listing 1), showing the matmulâ€‘heavy path needed for high GPU utilization.

## 4. Key Insights and Innovations
- Fineâ€‘grained, channelâ€‘wise gated delta rule (fundamental)
  - Whatâ€™s new: `KDA` extends `GDN`â€™s perâ€‘head decays to perâ€‘channel decays (`Diag(Î±_t)`) and keeps the delta ruleâ€™s corrective update (Â§3; Eq. 1).
  - Why it matters: More precise memory control increases expressivity without growing state size, enabling better recall/copy behavior and faster convergence on synthetic tasks (Figure 4) and better longâ€‘context performance (Â§5.1â€“Â§5.5).
- Bespoke chunkwise algorithm with specialized DPLR tying (fundamental + systems)
  - Whatâ€™s new: A numerically stable, matmulâ€‘centric chunked algorithm that combines WY + UT transforms and constrains DPLR to `a=b=k` (Â§3.1â€“Â§3.2; Â§6.2).
  - Why it matters: Substantially fewer matmuls and no secondâ€‘level chunking for divisions, yielding ~2Ã— kernel speed vs general DPLR up to 64k tokens (Figure 2) and enabling throughput at millionâ€‘token scales (Figure 1b, Figure 7).
- Simple, effective hybrid recipe: 3 `KDA` : 1 full attention with NoPE (architectural)
  - Whatâ€™s new: A layerwise interleaving that is infrastructureâ€‘friendly, reduces KV cache up to 75%, and empirically gives the best perplexity among tested ratios (Table 1), while preserving global information flow (Â§4, Â§5.2).
  - Why it matters: This combination beats matched full attention baselines in quality across short and long contexts and under RL while substantially improving speed and memory (Figure 1aâ€“b; Table 3â€“5; Figure 6â€“7).
- KDA as learnable positional encoding (conceptual)
  - Whatâ€™s new: A unifying view showing the gated delta recurrence forms a dataâ€‘dependent multiplicative positional encoding (Eq. 12; Table 6).
  - Why it matters: Explains why pairing NoPE global attention with `KDA` yields robust longâ€‘context extrapolation and reduces RoPE sensitivity (Â§6.1; Â§5.2 â€œNoPE vs. RoPEâ€).

## 5. Experimental Analysis
- Evaluation setup (fairness and breadth)
  - Models and training (Â§5.4)
    - 48B total parameters with MoE (8 of 256 experts active; ~3B activated params). Identical layer counts and heads across baselines.
    - Three matched models: full attention `MLA`, hybrid `GDNâ€‘H` (Gated DeltaNet + MLA), and hybrid `Kimi Linear` (KDA + MLA). A RoPE variant, `Kimi Linear (RoPE)`, isolates positional design effects.
    - Pretraining budget: 1.4T tokens, context 4,096; same optimizer/schedule. SFT and RL use identical recipes across models.
  - Benchmarks (Â§5.4)
    - Short-context knowledge/reasoning: HellaSwag, ARCâ€‘C, Winogrande, MMLU, MMLUâ€‘Redux/Pro, GPQAâ€‘Diamond, BBH.
    - Math & Code: GSM8K, MATH, AIME 2025, HMMT 2025, PolyMathâ€‘en, LiveCodeBench v6, EvalPlus, CRUXEval.
    - Long-context: RULER (128k), MRCR, HELMETâ€‘ICL, LongBench v2, Frames, RepoQA, Long Code Arena.
    - Chinese: Câ€‘Eval, CMMLU.
    - All generation with temperature 1.0; some tasks evaluated via perplexity (listed in Â§5.4); GPQA averaged over 8 runs.

- Main quantitative results
  - Synthetic tasks (Figure 4)
    - On Palindrome, MQAR (multiâ€‘query associative recall), and Stack tracking, `KDA` consistently achieves the highest accuracy as sequence length grows (256â†’2048) and converges faster at 1024â€‘token training than `GDN`; `Mamba2` fails in this configuration (Â§5.1).
  - Ablations (Table 1; Â§5.2)
    - Hybrid ratio: 3:1 `KDA:MLA` gives best train/valid perplexities among {1:1, 3:1, 7:1, 15:1, 0:1 full attention}. Too many linear layers hurt validation; too few increase inference cost.
    - Output gate: Removing it or using Swish hurts performance; Sigmoid is best (aligns with avoiding attention sink).
    - Short convolution: Removing it increases perplexity; local convolutions still help even in hybrid models.
  - Scaling law (Figure 5; Â§5.3)
    - Across 5 sizes (computeâ€‘optimal training), the fitted lossâ€“compute curve shows ~1.16Ã— compute efficiency gain for Kimi Linear over full attention MLA at the same PFLOP/sâ€‘days.
  - Shortâ€‘context pretrain results (Table 3; Â§5.5.1)
    - Kimi Linear tops most benchmarks at 1.4T tokens. Examples:
      - > MMLU: 73.8 (Kimi Linear) vs 72.2 (MLA) vs 71.6 (GDNâ€‘H).
      - > MMLUâ€‘Pro: 51.0 vs 47.2 vs 47.9.
      - > CRUXEvalâ€‘O (CoT): 62.0 vs 61.5 vs 58.1.
      - Small exceptions: EvalPlus slightly favors GDNâ€‘H (63.1) over Kimi Linear (60.2).
  - Instructionâ€‘tuned results (Table 4)
    - Kimi Linear leads broadly after the same SFT:
      - > MMLUâ€‘Redux: 80.3 vs 79.2 (MLA) vs 78.7 (GDNâ€‘H).
      - > GPQAâ€‘Diamond Avg@8: 62.1 vs 57.1 vs 58.6.
      - > LiveCodeBench v6 Pass@1: 26.0 vs 25.1 vs 25.4.
      - Exceptions: EvalPlus (61.0) trails MLA/GDNâ€‘H (~62.5) and MATH500 is slightly lower than GDNâ€‘H (81.2 vs 83.0).
  - Longâ€‘context results at 128k (Table 5)
    - Kimi Linear has the best average (54.5). Notable wins:
      - > RULER: 84.3 (Kimi Linear) vs 81.3 (MLA) vs 80.5 (GDNâ€‘H).
      - > RepoQA: 68.5 vs 63.0 vs 63.0.
    - `Kimi Linear (RoPE)` underperforms Kimi Linear on long context despite similar shortâ€‘context scores, supporting the NoPE design (Â§5.2).
  - RL training on math (Figure 6)
    - Using identical RLVR settings and data, Kimi Linear shows faster and higher accuracy improvements than MLA on the training set and generalizes better on MATH500 and AIME 2025 test curves across training steps (Â§5.5.1).
  - Efficiency: Prefill and decoding (Figure 7; Figure 1b; Â§5.6; Â§6.3)
    - Batch size 1:
      - > Prefill latency at 1M tokens: ~2.9Ã— faster than full attention; matches GDNâ€‘H (Figure 7a).
      - > Decoding TPOT at 1M: ~1.8â€“2.2Ã— faster than MLA; similar to GDNâ€‘H (Figure 7b).
    - With larger batches made possible by the small, constant state (no large KV cache), decoding TPOT improves up to 6.3Ã— at 1M (1.84 ms vs 11.48 ms; Figure 1b).
  - Extended training (Appendix D)
    - With 5.7T tokens, the released `Kimiâ€‘Linearâ€‘Instruct` reaches RULER 94.8 at 1M context and large gains on code and math over the Moonlight baseline (Table 9). These comparisons involve different total parameterizations (48B vs 16B) and are provided as a capability demonstration.

- Do the experiments support the claims?
  - Yes, on three fronts:
    - Quality: Kimi Linear consistently matches or beats the full attention baseline on a broad set of shortâ€‘context and longâ€‘context tasks at the same training tokens (Table 3â€“5), and improves RL learning curves (Figure 6).
    - Efficiency: Measured kernel speedups vs DPLR (Figure 2), prefilling and decoding speedups vs MLA (Figure 7) and the largeâ€‘batch TPOT result (Figure 1b) substantiate the efficiency claims.
    - Design choices: Ablations on hybrid ratio, gates, and NoPE vs RoPE (Table 1; Table 5; Â§5.2) buttress the architectural decisions.

- Notable caveats
  - Some code evaluations (EvalPlus) are mixed; GDNâ€‘H edges out Kimi Linear in a few cases (Table 3â€“4).
  - LongBench v2 and Frames do not show clear gains (Table 5), suggesting taskâ€‘dependent tradeâ€‘offs.

## 6. Limitations and Trade-offs
- Finiteâ€‘state constraint and retrieval
  - `KDA` maintains a fixedâ€‘size state per head; exact retrieval/copying over extreme ranges remains challenging for purely linear layers (Â§1, Â§7.2). The interleaved full attention layers mitigate but do not eliminate this limitation.
- Design sensitivity
  - Performance depends on the hybrid ratio and gating choices; too many linear layers hurt generalization (Table 1). The NoPE/positional design is importantâ€”`Kimi Linear (RoPE)` weakens longâ€‘context results (Table 5).
- Kernel specialization and numerical issues
  - The speedups rely on specialized kernels (WY + UT, tied DPLR). Portability across hardware backends or extreme precision regimes may require additional engineering (Â§3.1â€“Â§3.2; Appendix C).
- Benchmark coverage
  - While broad, some domains still show mixed results (e.g., EvalPlus, some long-context suites), indicating room for robustness improvements (Table 4â€“5).
- Training scope
  - The strongest â€œbeats full attentionâ€ claims are at 1.4T token parity. The 5.7T token results are informative but differ in total parameterization when compared to Moonlight (Appendix D).

## 7. Implications and Future Directions
- How this changes the field
  - Demonstrates that a linearâ€‘dominant hybrid can surpass full attention under matched training, reshaping the default assumption that full attention is necessary for peak quality. It unlocks millionâ€‘token contexts with practical throughput and memory footprints (Figure 1; Â§5.6), directly benefiting longâ€‘horizon tool use, codebaseâ€‘level reasoning, and RL at inference time.
- Research directions
  - Combining `KDA` with sparse attention for finer retrieval while keeping a small state (Â§7.1 discussion).
  - State expansion or mixtureâ€‘ofâ€‘memories to further close copying/selectivity gaps while maintaining efficiency (Â§7.1).
  - Theory: deeper analysis of `KDA` as learnable positional encoding and its extrapolation behavior vs RoPE (Table 6; Â§6.1).
  - Autoâ€‘hybridization: learning the layer ratio or scheduling linear/global layers by depth or domain (Â§7.2).
- Practical applications
  - Production LLM serving with millionâ€‘token contexts (chat assistants, retrievalâ€‘augmented code understanding, repositoryâ€‘level Q&A).
  - Agentic systems and RL testâ€‘time scaling where decoding speed and memory footprint dominate cost.
  - Onâ€‘device or edge deployment where KV cache growth is prohibitive, but a constantâ€‘state linear module is viable.

> Bottom line: With `KDA`â€™s fineâ€‘grained forget + delta updates, a hardwareâ€‘aware chunkwise algorithm, and a simple 3:1 hybrid with NoPE global attention, Kimi Linear achieves a rare combinationâ€”better accuracy than full attention at matched training, much faster longâ€‘context decoding (up to 6Ã— with large batches), and drastically reduced memory useâ€”making it a credible, dropâ€‘in replacement for full attention in longâ€‘horizon LLMs (Figure 1; Table 3â€“5; Figure 7; Â§6.3).

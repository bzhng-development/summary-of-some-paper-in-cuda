# Replacing softmax with ReLU in Vision Transformers

**ArXiv:** [2309.08586](https://arxiv.org/abs/2309.08586)
**Authors:** Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, Simon Kornblith
**Institutions:** Google DeepMind

## ğŸ¯ Pitch

This paper revolutionizes Vision Transformers by replacing softmax attention with a simple point-wise ReLU divided by sequence length, dramatically simplifying computation while maintaining accuracy. By eliminating the sequence normalization step, it enhances parallelizability and optimization, paving the way for efficient and scalable ViT architectures without sacrificing performance.

---

## 1. Executive Summary
This paper shows that Vision Transformers (ViTs) can replace the attention softmax with a simple pointâ€‘wise ReLU activation if the attention scores are divided by the sequence length `L` (i.e., use `ReLU / L`). With this single change, ViTs trained on ImageNetâ€‘21k achieve scaling behavior and accuracy that approach or match standard softmax attention, while enabling easier parallelization because normalization across the sequence is no longer required.

## 2. Context and Motivation
- Problem addressed
  - Standard attention uses a softmax to convert similarity scores between tokens into a probability distribution across the sequence. This softmax requires both an expensive exponentiation and a sum across the sequence dimension, which is a synchronization bottleneck on modern accelerators.
  - Prior attempts to remove softmax by using pointâ€‘wise activations (e.g., plain ReLU) often degrade accuracy.

- Why it matters
  - Removing the sequenceâ€‘wise normalization step would allow more parallel computation over sequence elements with fewer crossâ€‘token â€œgatherâ€ operations, which can translate into better hardware utilization (Section â€œIntroductionâ€; Figure 1 caption).
  - If accuracy can be preserved, this would simplify attention and open new implementation strategies, especially for large ViTs.

- Prior approaches and their gaps
  - Pointâ€‘wise activations without normalization: Replacing softmax with ReLU or squaredâ€‘ReLU has been explored, but prior work typically did not divide by sequence length and thus lost accuracy (Section 2).
  - Alternatives that still normalize across the sequence: Some methods remove softmax but keep a sequenceâ€‘wise normalization so weights sum to one, preserving the bottleneck (Section 2).
  - Linear attention: Methods that remove nonlinearities entirely to achieve linear complexity help with very long sequences, but in this paperâ€™s setting they reduced accuracy (footnote 1 under Section 3 and the line â€œremoving the activation entirely reduced accuracyâ€ in Section 2).

- Positioning
  - The paper proposes a minimal change to standard attentionâ€”swap softmax for `ReLU / L`â€”that retains the usual O(L^2) attention behavior and accuracy while reducing synchronization costs and enabling new parallelization opportunities (Figure 1 caption; Section â€œReLUâ€‘attentionâ€).

## 3. Technical Approach
Stepâ€‘byâ€‘step view of how the modified attention works:

- Baseline attention (Equation 1)
  - For each query vector `q_i` and all key vectors `k_j`, compute scaled dot products `q_i^T k_j / sqrt(d)`, where `d` is the head dimension.
  - Apply a transformation `Ï†` across the `j` dimension to get attention weights `Î±_ij`.
  - Compute the output for position `i` as a weighted sum of value vectors: `o_i = Î£_j Î±_ij v_j`.

- Standard choice of `Ï†` and its cost
  - In standard Transformers, `Ï†` is softmax across the sequence positions `j`. Softmax requires:
    - Exponentiation on each score.
    - A sum across `j` to normalize to probability weights (they sum to 1).
  - The crossâ€‘sequence sum makes parallelization harder because it forces synchronization across tokens (Introduction; references [24, 7]).

- Proposed change: pointâ€‘wise activation with sequenceâ€‘length scaling
  - Replace `Ï†` with a â€œpointâ€‘wiseâ€ function that operates independently on each score and does not sum across `j`.
  - Concrete proposal: `Ï† = L^{-1} * ReLU` (Section â€œReLUâ€‘attentionâ€).
    - â€œPointâ€‘wiseâ€ means apply `ReLU` to each score independently.
    - Scale the result by `1/L`, where `L` is the sequence length (number of tokens in the input).
  - General family tested: `Ï† = L^{-Î±} * h`, where:
    - `Î±` is a nonâ€‘negative exponent.
    - `h` is chosen from `{relu, relu^2, gelu, softplus, identity, relu6, sigmoid}` (Section â€œScaled pointâ€‘wise attentionâ€).
    - Figure 2 sweeps `Î±` from 0.0 to 2.0 for multiple choices of `h`.

- Why divide by `L`? The scale argument (Section â€œSequence length scalingâ€)
  - With softmax, for any query `i`, the weights across positions sum to one; thus the average weight per position is `E_j[Î±_ij] = 1/L`.
  - If one naively drops softmax and uses plain `ReLU`, the average weight can become O(1) at initialization because the inputs to `ReLU` are O(1), which makes the output sum across positions scale like O(L). This changes the scale of the attention outputs `o_i` and can destabilize training unless other hyperparameters are retuned.
  - Multiplying by `L^{-1}` restores the expected O(1/L) scale of weights at initialization so the overall scale of `o_i` remains close to the softmax regime without hyperparameter changes. The paper notes this as an empirical justification with a brief analytical motivation (Section â€œSequence length scalingâ€).
  - Note: squaredâ€‘ReLU (`relu^2`) is an exception in that it does not preserve O(1) magnitude, hence the benefit of careful scaling (footnote â€œWith the exception of squared ReLU.â€ under Section 4).

- Practical ablations and design choices
  - `qkâ€‘layernorm`: A variant in which queries and keys are each normalized by LayerNorm before computing dot products (Section 4, â€œExperimental setupâ€). This helps with stability at very large scale in prior work; the paper evaluates its effect for the proposed attention (Figure 3).
  - Gated attention unit: Add a gating projection whose output multiplies the attention result elementâ€‘wise before the final output projection (as in [15]). The paper tests whether gating removes the need for sequenceâ€‘length scaling (Figure 4).

- Why this approach over alternatives
  - It eliminates the sequenceâ€‘wise normalization (no sum across `j`) while preserving the scale of outputs, which prior ReLUâ€‘only attempts lacked.
  - It avoids adding complex mechanisms (e.g., kernel tricks of linear attention) and stays very close to the standard attention computation, reducing the need for hyperparameter retuning (Sections 1 and 3).

- Implementation and training setup (Section 4)
  - Codebase: BigVision.
  - Datasets and schedules:
    - ImageNetâ€‘21k pretraining for 30 epochs.
    - ImageNetâ€‘1k training for 300 epochs.
    - Both runs have roughly 9e5 optimization steps.
  - Models: ViT variants S/32, S/16, S/8, plus larger B/32, B/16, L/16 in scaling plots (Figure 1).
  - Reporting:
    - For ImageNetâ€‘21kâ€‘trained models, ImageNetâ€‘1k accuracy is computed by taking the top predicted class among those that exist in 1k, without fineâ€‘tuning (Figure 1 caption).
    - Transfer: 10â€‘shot linear probes on eight datasets, averaged over three seeds (Figure 1 caption). The datasets are CUBâ€‘200, Caltechâ€‘101, Stanford Cars, CIFARâ€‘100, DTD, Colorectal Histology, Oxfordâ€‘IIIT Pets, and UC Merced (Section 4).

- Computational advantage
  - Because `ReLU / L` is applied independently per score and does not require normalizing over `j`, it â€œcan be parallelized over the sequence length dimension with fewer gather operations than traditional attentionâ€ (Figure 1 caption). This is a hardwareâ€‘level benefit even though asymptotic complexity remains O(L^2) for full attention.

## 4. Key Insights and Innovations
- Sequenceâ€‘lengthâ€‘aware scaling is the missing piece for softmaxâ€‘free attention in ViTs.
  - Novelty: Prior ReLUâ€‘based attention did not divide by `L`. The paperâ€™s `L^{-1}` factor (or more broadly `L^{-Î±}` with `Î±â‰ˆ1`) preserves the O(1/L) average weight scale that softmax implicitly enforces (Section â€œSequence length scalingâ€; Figure 2).
  - Significance: This avoids reâ€‘tuning hyperparameters and stabilizes training, producing accuracy close to softmax.

- Pointâ€‘wise activations can match softmax scaling in ViTs when properly scaled.
  - Novelty: Show that `ReLU / L` can â€œapproach or matchâ€ the computeâ€“accuracy scaling of softmax across model sizes from small to large (Figure 1).
  - Significance: Retains the empirical benefits of softmax while simplifying the operation (no exponent, no sequence sum).

- The best Î± is consistently near 1 across models and datasets.
  - Novelty: A systematic sweep over `Î± âˆˆ [0, 2]` and over several `h` confirms that `Î±â‰ˆ1` is optimal in practice (Figure 2).
  - Significance: Provides a simple ruleâ€‘ofâ€‘thumb for implementation: set `Î±=1` and choose a fast pointâ€‘wise `h` such as ReLU.

- Removing sequenceâ€‘wise normalization remains beneficial even with gating or without qkâ€‘layernorm.
  - Novelty: Ablations show (i) qkâ€‘layernorm is not critical at the tested scales (Figure 3), and (ii) adding a gated attention unit does not obviate the need for `L^{-Î±}` scaling; best results still occur near `Î±=1` (Figure 4).
  - Significance: The core ideaâ€”`L`â€‘scaled pointâ€‘wise attentionâ€”is robust to common architectural variations.

These are incremental in mechanism but fundamental in implication: they demonstrate that softmax is not uniquely necessary for effective ViT attention if one preserves the correct scaling.

## 5. Experimental Analysis
- Evaluation methodology
  - Datasets:
    - ImageNetâ€‘21k (pretraining; 30 epochs).
    - ImageNetâ€‘1k (300â€‘epoch training; and used for evaluation of 21kâ€‘trained models by restricting predictions to 1k classes; Section 4).
    - Eight transfer datasets for 10â€‘shot linear probing (listed in Section 4).
  - Models: ViT S/32, S/16, S/8 for most ablations; scaling plots also include B/32, B/16, and L/16 (Figure 1).
  - Metrics:
    - ImageNetâ€‘1k topâ€‘1 accuracy (yâ€‘axis in multiple figures).
    - Average 10â€‘shot linear probe accuracy across eight datasets (Figure 1 right).
  - Compute accounting: xâ€‘axis in Figure 1 reports TPU core hours.

- Main quantitative findings
  - Softmax vs `ReLU / L` scaling (Figure 1):
    - On both ImageNetâ€‘1k accuracy and average 10â€‘shot transfer accuracy, the curves for `ReLU / L` track the softmax curves closely across small to large ViTs. The two lines nearly overlap, indicating no significant loss in accuracy at a given compute budget.
    - Practical note: Because `ReLU / L` avoids sequenceâ€‘wise normalization, it can be parallelized over tokens with fewer gather operations (Figure 1 caption), offering potential runtime benefits not captured by the accuracy plots.
  - Effect of `Î±` and choice of `h` (Figure 2):
    - Across S/32, S/16, and S/8 models trained on ImageNetâ€‘21k and ImageNetâ€‘1k, the best accuracy typically occurs for `Î±â‰ˆ1`.
    - No single activation `h` dominates at `Î±â‰ˆ1` (ReLU, GELU, softplus, etc. perform similarly), so the paper uses ReLU for speed (Figure 2 caption).
  - Effect of `qkâ€‘layernorm` (Figure 3):
    - Using or removing qkâ€‘layernorm has only a small effect on accuracy for S/32, S/16, S/8 with `L^{-Î±}`â€‘scaled ReLU or squaredâ€‘ReLU attention.
    - This suggests the proposed scaling is not dependent on this normalization at the tested scales.
  - Effect of gating (Figure 4):
    - Adding a gated attention unit does not eliminate the need for `L^{-Î±}` scaling; best results still cluster near `Î±â‰ˆ1`.
    - Gating increases compute by roughly 9.3% for the S/8 model with ReLU (Section â€œEffect of adding a gateâ€), with no clear accuracy advantage relative to simply using `Î±â‰ˆ1` without the gate.

- Do the experiments support the claims?
  - The evidence is consistent and multiâ€‘faceted: scaling plots (Figure 1), Î±â€‘sweeps across datasets and models (Figure 2), and ablations on qkâ€‘layernorm and gating (Figures 3â€“4).
  - The work does not present explicit wallâ€‘clock benchmarks, but it argues for hardware advantages qualitatively and via reduced synchronization requirements.

- Notable details and conditions
  - Training follows unmodified BigVision defaults (Section 4). This helps establish that `ReLU / L` works without retuning.
  - For ImageNetâ€‘21k pretraining, ImageNetâ€‘1k accuracy is computed by picking the top class among the overlapping 1k classes, without fineâ€‘tuning (Figure 1 caption). This is a conservative evaluation protocol.

## 6. Limitations and Trade-offs
- Theoretical understanding is partial.
  - The paper provides an initializationâ€‘scale argument for why dividing by `L` helps, but it does not offer a full theory of optimization dynamics or generalization under the new attention (Section 5 â€œConclusionâ€: â€œwe are unsure why the factor L^{-1} improves performance or if this term could be learnedâ€).

- Applicability and scope
  - Experiments are focused on ViTs for image classification and 10â€‘shot linear transfer. There are no results for language modeling, detection/segmentation, or very long sequences.
  - The method preserves O(L^2) attention complexity; it is not a linearâ€‘time attention method. Its advantage is fewer crossâ€‘sequence synchronizations, not a change in asymptotic cost (footnote 1 and Figure 1 caption).

- Numerical and stability considerations
  - Weights no longer sum to one, and normalization is not enforced. While `L^{-1}` keeps the expected scale similar to softmax at initialization, the behavior later in training depends on activations and data. The paper reports good results but does not analyze worstâ€‘case saturation or gradient issues.
  - Squaredâ€‘ReLU can change magnitude more aggressively; proper scaling is even more important (footnote in Section 4).

- Engineering tradeâ€‘offs
  - The paper qualitatively argues for speedups from reduced gathers but does not provide wallâ€‘clock or throughput numbers; actual runtime gains may depend on implementation details and hardware.
  - Adding gating increases compute by 9.3% for S/8 with ReLU (Section â€œEffect of adding a gateâ€), with limited benefit, so simplicity may be preferable.

## 7. Implications and Future Directions
- How this changes the field
  - It challenges the assumption that softmax is essential for attention in ViTs. With a simple `L`â€‘aware scale, pointâ€‘wise activations can be competitive.
  - It encourages implementations and kernels that exploit perâ€‘token parallelism without crossâ€‘sequence normalization, potentially simplifying highâ€‘performance attention kernels.

- Followâ€‘up research enabled or suggested
  - Learnable scaling: Make `Î±` (or the `1/L` factor) learnable, per head or per layer, and study stability and performance (Section 5 â€œConclusionâ€).
  - Broader activations: Search for alternative `h` that may outperform ReLU while retaining speed, especially dynamic or dataâ€‘dependent activations.
  - Beyond vision: Test on language models, multimodal Transformers, and very long sequences to see whether the `L^{-Î±}` rule generalizes.
  - Theoretical analysis: Develop a deeper understanding of optimization and generalization when attention weights are not normalized to sum to one.
  - Systems work: Benchmark wallâ€‘clock speed and memory on modern kernels (e.g., FlashAttentionâ€‘style implementations) to quantify real runtime benefits.

- Practical applications
  - Largeâ€‘scale pretraining of ViTs where hardware synchronization costs dominate; the method can reduce sequenceâ€‘axis gathers and may improve throughput.
  - Edge or lowâ€‘power deployments where eliminating exponentials and sequence normalization simplifies inference kernels.

Quoted touchpoints for quick reference:
- Equation 1 (Section 3): defines attention weights via `Ï†` applied to scaled dot products.
- Section â€œReLUâ€‘attentionâ€ and â€œScaled pointâ€‘wise attentionâ€: define `Ï† = L^{-1} ReLU` and the general `L^{-Î±} h` family.
- Section â€œSequence length scalingâ€: motivation for dividing by `L` to preserve expected O(1/L) weight scale.
- Figure 1: `ReLU / L` matches softmax scaling on ImageNetâ€‘1k accuracy and average 10â€‘shot transfer vs TPU core hours; notes parallelization advantage.
- Figure 2: best accuracy generally at `Î± â‰ˆ 1` across models and datasets; no clear winner among activations at that point.
- Figure 3: small effect from removing `qkâ€‘layernorm` at these scales.
- Figure 4: gating does not remove the need for `L^{-Î±}`; gating increases compute by ~9.3% for S/8 with ReLU.

# Rephrasing the Web: A Recipe for Compute and Dataâ€‘Efficient Language Modeling

**ArXiv:** [2401.16380](https://arxiv.org/abs/2401.16380)
**Authors:** Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly
**Institutions:** (not specified in arXiv metadata)

## ğŸ¯ Pitch

WRAP (Web Rephrase Augmented Pre-training) innovatively enhances language model training by rephrasing noisy web text into clean styles, tripling learning speed and doubling data efficiency without sacrificing robustness. This approach democratizes high-performance language model development by significantly reducing costs and computational demands, revolutionizing how data quality can be engineered for better generalization.

---

## 1. Executive Summary
This paper introduces WRAP (Web Rephrase Augmented Preâ€‘training), a recipe that uses an instructionâ€‘tuned language model to rephrase noisy web text into cleaner â€œstylesâ€ (e.g., Wikipediaâ€‘like or questionâ€‘answer) and mixes these rephrases with the original data during preâ€‘training. Across 1.3Bâ€‘parameter models, WRAP delivers roughly 3Ã— faster learning and up to ~5Ã— data efficiency, cutting average perplexity on The Pile by about 50% and raising zeroâ€‘shot QA accuracy by ~2 points compared with training on web scrapes alone (Figures 1bâ€“1c, 2; Tables 1â€“2).

## 2. Context and Motivation
- Problem gap
  - Modern language models are trained on massive web scrapes that are â€œunstructured, noisy, and poorly phrased,â€ making learning compute- and dataâ€‘hungry (Abstract; Â§1). Chinchilla scaling laws imply data must grow with model size (Hoffmann et al., 2022), but highâ€‘quality web data is scarce and repeating data quickly has diminishing returns (Muennighoff et al., 2023; Xue et al., 2023; Â§1, Â§2).
- Why this matters
  - Practically, preâ€‘training costs, duration, and the scarcity of clean data limit who can train useful models. Theoretically, it is unclear whether better data compositionâ€”not just more dataâ€”can improve outâ€‘ofâ€‘distribution (OOD) generalization (Â§1).
- Prior approaches and shortcomings
  - Data filtering heuristics (e.g., Reddit links, Wikipediaâ€‘likeness) are partly proprietary and require expensive retraining to validate (Â§2).
  - Synthetic data generated â€œfrom scratchâ€ (e.g., textbookâ€‘quality) can help, but it is costly (often GPTâ€‘3.5â€‘sized generators), opaque, and risks knowledge bias from topic selection (Â§1â€“Â§2).
- Positioning of this work
  - WRAP retains the information diversity of the web while upgrading phrasing â€œstyle,â€ using a smaller, open rephraser. It aims to: (i) preâ€‘train efficiently with limited highâ€‘quality data, (ii) reduce compute, and (iii) study how training â€œstyleâ€ affects OOD performance (Â§1, Â§3).

## 3. Technical Approach
WRAP is an endâ€‘toâ€‘end data pipeline and training recipe that â€œrephrases the webâ€ and coâ€‘trains on real + synthetic data.

- Core idea
  - Instead of generating new content, use an instructionâ€‘tuned model to paraphrase existing web passages into specific styles, then train on a 1:1 mixture of original and rephrased text (Â§3.1).
  - The hypothesis: styleâ€‘optimized text is easier to learn from while preserving the webâ€™s knowledge diversity; mixing with raw web preserves robustness to noise (Â§3.1 â€œCombining Real and Synthetic Dataâ€).

- Data source and chunking
  - Start from C4 (a CommonCrawlâ€‘derived corpus; ~170B tokens). Each example to be rephrased has a maximum of ~300 tokens to avoid information loss during rephrasing (Â§3.1 â€œGenerating Synthetic Dataâ€).

- Rephrasing model and prompts
  - Default rephraser: `Mistralâ€‘7Bâ€‘Instruct` (frozen; Â§3.1). Prompts produce four styles (Â§3.1 â€œRephrasing Stylesâ€; Appendix G):
    - `Easy`: toddlerâ€‘friendly sentences.
    - `Medium`: highâ€‘quality encyclopedic English (â€œlike Wikipediaâ€).
    - `Hard`: terse, abstruse scholarly language.
    - `Q/A`: turn text into multiple Question/Answer pairs.
  - Outputs are lightly postâ€‘processed to strip boilerplate like â€œHereâ€™s a paraphraseâ€¦â€ (Appendix B).

- Training mixture
  - Preâ€‘training samples original C4 and rephrased text 1:1 to balance robustness to noise with the benefits of cleaner style (Â§3.1). The authors explicitly warn that training only on synthetic text harms performance on some realâ€‘world domains (Figure 3; Tables 3â€“4).

- Model architectures and training setup
  - Decoderâ€‘only Transformers trained with Megatronâ€‘LM (Â§3.2):
    - `128M` (12 layers, 12 heads, d_model=768),
    - `350M` (24 layers, 16 heads, d_model=1024),
    - `1.3B` (24 layers, 16 heads, d_model=2048).
  - Sequence length 1024; batch â‰ˆ1M tokens; cosine LR schedule; Adam Î²1=0.9, Î²2=0.999; weight decay 0.01; gradient clipping 1.0 (Â§3.2).
  - Typical budget: 300k steps â‰ˆ 300B seen tokens (Â§3.2).

- Why evaluate on The Pile (not C4)?
  - Objective mismatch: training on a mixture minimizes risk over `Dc4 âˆª Dsyn`, not `Dc4` alone. Equations (1)â€“(2) formalize this: training on C4 alone optimizes `Î¸_c4 = argmin E_{x~D_c4} L(Î¸;x)`; WRAP optimizes `Î¸_WRAP = argmin E_{x~D_c4 âˆª D_syn} L(Î¸;x)` (Â§4). Evaluating only on C4 would unfairly penalize WRAP.

- Metrics
  - Language modeling: tokenâ€‘level perplexity (lower is better). Appendix D defines macro tokenâ€‘level perplexity as `P = exp(min(20, L/T))`, where `L` is total loss over tokens and `T` is token count (Eq. 3).
  - Task performance: zeroâ€‘shot accuracy (and some fewâ€‘shot in Appendix F) via the LLM Evaluation Harness (Â§5.1; Footnote 1).

- Experimental factors and ablations
  - Rephrase style choice (QA vs Medium vs others); syntheticâ€‘only vs mixed; multiâ€‘style mixing ratios; rephraser model quality (T5â€‘base vs Qwenâ€‘1.8B vs Mistralâ€‘7B vs Vicunaâ€‘13B); synthetic vs classic text augmentations; semantic leakage checks (Â§6; Figures 3â€“7).

Analogy: WRAP is like reâ€‘editing a noisy textbook into multiple readable editions (encyclopedia prose, Q&A guide, etc.) while keeping the original rough notes in the study packet. The student (the LM) learns faster from the edited versions but still sees enough rough notes to handle messy realâ€‘world text.

## 4. Key Insights and Innovations
- Styleâ€‘only synthetic data can drive large gains without adding knowledge.
  - WRAP rephrases existing passagesâ€”no new factsâ€”yet yields substantial OOD benefits. Average perplexity on The Pile drops by ~50% vs C4â€‘only training, with some domains (ArXiv, HackerNews) showing nearly 3Ã— reductions (Figure 1c; Â§4 â€œData Complexityâ€ and Figure 2). This isolates â€œstyleâ€ as a primary lever for data/compute efficiency.

- Compute and data efficiency at small scale
  - Learning curves show ~3Ã— faster zeroâ€‘shot progress (Figure 1b: WRAP achieves a given average accuracy with about oneâ€‘third the tokens). With fewer tokens (e.g., 150B vs 300B), WRAP models outperform C4â€‘only models (Figure 1c). In zeroâ€‘shot QA, WRAP trained on â€œ85B real tokens + rephrasesâ€ outperforms models trained on 170B real tokens, and competes with models trained on 320Bâ€“1T tokens (Tables 1â€“2).

- A practical, lowerâ€‘cost way to use synthetic data
  - Rephrasing requires smaller, open models (e.g., Mistralâ€‘7B, Qwenâ€‘1.8B) instead of GPTâ€‘3.5â€‘sized generators (Â§3.1; Â§7.1). Figure 5 shows even Qwenâ€‘1.8B produces highâ€‘utility paraphrases, while lowâ€‘quality T5â€‘base rephrases hurt.

- Mixture with real data is essential
  - Syntheticâ€‘only training hurts performance on domains with special tokens and noisy structure (OWT2, HN, Philpapers, Gutenberg). Adding back real data fixes this (Figure 3; Tables 3â€“4). This is not mere â€œaugmentationâ€: classic augmentations (synonym replacement, random deletion) do not match WRAPâ€™s gains (Figure 6).

- Styleâ€“task alignment matters, but multiâ€‘style mixing helps only slightly
  - QAâ€‘style rephrases help QA tasks most (Tables 3, 6; Â§6.1 RQ2), while Wikipediaâ€‘like (â€œMediumâ€) helps encyclopedic domains (Figure 4, Figure 10). Combining styles yields small average perplexity gains (Figure 4) but not clear wins on zeroâ€‘shot QA (Tables 5â€“6), suggesting diminishing returns from naive mixing.

## 5. Experimental Analysis
- Evaluation methodology
  - Language modeling: Perplexity on 21 Pile subâ€‘domains (weighted average; Appendix A.2; Figure 2, Figures 3â€“7, 12â€“13).
  - Zeroâ€‘shot QA: 13 tasks across general understanding and specialized knowledge (ARCâ€‘E/C, BoolQ, WinoGrande, PIQA, HellaSwag, TruthfulQA, OBQA, LogiQAâ€‘2, SciQ, PubMedQA, MathQA, MMLU) via Evaluation Harness (Tables 1â€“2; Â§5.1).
  - Ablations: rephraser quality (Figure 5); synthetic vs augmentation (Figure 6); styleâ€‘specific effects (Figure 7; Appendix C for reading level, typeâ€‘token ratio, syntactic complexity); semantic similarity to check leakage (Figure 8; Appendix C).

- Main quantitative results
  - Perplexity (The Pile)
    - > â€œOn average â€¦ perplexity reduces by ~50%,â€ and on ArXiv/HackerNews â€œnearly 3Ã—â€ vs C4â€‘only at 300B tokens (Â§4; Figure 2; Figure 1c).
    - With half the tokens (150B), WRAP still beats C4â€‘300B on average (Figure 1c). Even 350M models with WRAP on 15% of C4 outperform 1.3B models trained on full C4 (Â§1 end; Figure 1c; Appendix E).
  - Zeroâ€‘shot, General Understanding (Table 1; 1.3B models)
    - `Synthetic (85B)` avg 49.4% and `Synthetic+C4 (85B)` avg 49.4% vs `Half C4 (85B)` 47.4%, `Full C4 (170B)` 47.3%, `RW 320B` 47.5%, `TinyLlama (1T tokens)` 47.4%.
    - Largest singleâ€‘task lift: TruthfulQA rises to 44.0% for `Synthetic (85B)` from ~33â€“39% for realâ€‘data baselines.
  - Zeroâ€‘shot, Specialized Knowledge (Table 2)
    - `Synthetic+C4 (85B)` avg 45.5% vs `Half C4 (85B)` 43.1%, `Full C4 (170B)` 43.5%, `RW 320B` 44.3%, `Pythia-Pile (300B)` 44.6%, `TinyLlama (1T)` 45.6%.
    - Insight: synthetic text helps learning speed but cannot add new factual knowledge; larger real datasets still matter when the evaluation probes knowledge breadth (Â§5.2).
  - Learning speed
    - > â€œ~3Ã— fasterâ€ on zeroâ€‘shot curves (Figure 1b). At early checkpoints (10B tokens), WRAP already beats C4 at 150B tokens in perplexity (Â§4 â€œLearning Speedâ€).
  - Ablations and robustness
    - Real data matters: syntheticâ€‘only degrades on noisy domains; adding C4 restores generalization (Figure 3; Tables 3â€“4).
    - Multiâ€‘style mixing: small average improvements on perplexity; no clear QA gains over QAâ€‘only style (Figure 4; Tables 5â€“6).
    - Rephraser quality: Qwenâ€‘1.8B and Mistralâ€‘7B synthetic datasets outperform Vicunaâ€‘13Bâ€™s (Figure 5), indicating â€œbigger is not always betterâ€ if prompts/outputs differ in quality; a fineâ€‘tuned T5â€‘base performs poorly.
    - Not just augmentation: synonym replacement and random deletion lag far behind WRAP (Figure 6).
    - Data leakage check: cosine similarity with SimCSE shows rephrases are semantically close to originalsâ€”more than random pairsâ€”but not identical; rephrases mostly change style, not content (Figure 8; Appendix C, Figure 9).

- Cost analysis and practicality (Â§7.1)
  - Generating 85B tokens with Mistralâ€‘7B via vLLM â‰ˆ 25k GPUâ€‘hours on A100; training a 1.3B model for 300B tokens â‰ˆ 6k GPUâ€‘hours at reported throughput (64Ã—A100, 0.5M tok/s).
  - While generation cost seems high, it is oneâ€‘time and massively parallelizable; smaller rephrasers (Qwenâ€‘1.8B) are ~3Ã— faster, and speculative decoding could add 3â€“5Ã— more speedups.
  - Claim: at 13Bâ€‘scale, 3â€“10Ã— training cost savings can amortize generation costs in one run (Â§7.1).

- Do the experiments support the claims?
  - The paper backs its central claims with multiple model sizes, tokensâ€‘seen budgets, ablations, and task suites. WRAPâ€™s strongest evidence is the consistent perplexity and zeroâ€‘shot QA gains with less training data and faster learning (Figures 1bâ€“1c, 2; Tables 1â€“2).
  - Where claims are conditional, the paper is explicit: e.g., synthetic text does not inject new knowledge (Table 2), and syntheticâ€‘only hurts noisyâ€‘domain perplexity (Figure 3).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Rephrasing preserves information; this assumes â‰¤300â€‘token chunks can be paraphrased without loss (Â§3.1). It also assumes stylistic improvements, not knowledge addition, are the main driver of generalization (Â§5.2).
- What is not addressed
  - Very large models and very long sequence lengths (context length is 1024) are not studied.
  - Automatic selection of best style mixture per domain/task is not solved; combining styles helps only modestly (Figure 4; Tables 5â€“6).
- Computational and data constraints
  - Synthetic generation is still costly at tens of thousands of GPU hours for tens of billions of tokens (Â§7.1), though parallelizable.
  - The method depends on access to a competent instructionâ€‘tuned rephraser; lowâ€‘quality rephrasers (e.g., fineâ€‘tuned T5â€‘base) reduce downstream performance (Figure 5).
- Robustness and bias concerns
  - Potential â€œstyle biasâ€: QAâ€‘heavy generations might overfit to QAâ€‘style benchmarks if overused. The paper mitigates this by mixing in raw data (Figure 3).
  - Data leakage: semantic similarity analysis (Figure 8; Appendix C) suggests style change more than content change, but this is a proxy; stronger leakage analyses (e.g., nâ€‘gram overlap, nearâ€‘duplicate search at scale) would improve confidence.
- Generality beyond English
  - WRAP is evaluated on English C4; the approach likely helps lowâ€‘resource languages (Â§7.1), but multilingual generalization remains to be validated.

## 7. Implications and Future Directions
- How this changes the landscape
  - WRAP provides a practical, reproducible path to substantial data/compute savings by editing the style of existing web text rather than generating new content. It reframes â€œdata qualityâ€ for LMs as â€œstyle match + noise robustness,â€ which can be engineered via rephrasing plus mixing.
- Followâ€‘up research enabled
  - Automatic curriculum/mixing: learn to schedule style proportions over training based on validation feedback (cf. DoReMiâ€‘style reweighting).
  - Style discovery: search for rephrase styles that best predict performance on target domains. Appendix C hints that reading level and syntactic complexity relate to domain improvements (Figures 10â€“11).
  - Smaller/faster rephrasers: quantify the minimal rephraser quality needed (Figure 5) and exploit speculative decoding or distillation to reduce generation cost (Â§7.1).
  - Domainâ€‘specific WRAP: apply to coding/math/legal corpora with taskâ€‘aligned styles (e.g., docstringâ€‘style, stepâ€‘byâ€‘step proofs).
  - Lowâ€‘resource languages: combine scarce raw text with rephrases in multiple styles to approach webâ€‘scale generalization (Â§7.1).
  - Leakage and diversity audits: scale up semantic/lexical diversity measurements (Appendix C) and study longâ€‘term effects of training on successive generations to avoid â€œmodel collapse.â€
- Practical applications
  - Training compact, capable assistants for enterprises with limited budgets.
  - Rapid bootstrapping of domain assistants (customer support QA, biomedical summarization) by styleâ€‘matching preâ€‘training data to downstream use.
  - As a complement to instruction tuning/RLHF, WRAP bakes conversational and encyclopedic style into the base model, reducing downstream alignment burden (Â§8).

> Bottom line: By treating â€œstyleâ€ as a firstâ€‘class property of preâ€‘training data, WRAP shows that paraphrasing web text into evaluationâ€‘aligned stylesâ€”and mixing it with raw webâ€”can yield large efficiency gains without sacrificing robustness (Figures 1â€“7; Tables 1â€“2, 3â€“6), while keeping the synthetic data pipeline affordable and transparent (Â§7.1, Â§8).

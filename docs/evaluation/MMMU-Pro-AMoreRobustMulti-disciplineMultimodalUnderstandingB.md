# MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark

**ArXiv:** [2409.02813](https://arxiv.org/abs/2409.02813)

## ğŸ¯ Pitch

MMMU-Pro presents a new standard for evaluating multimodal AI by rigorously filtering out questions solvable by text alone, expanding multiple-choice options, and embedding questions within images to enforce true vision-language reasoning. This benchmark reveals that prominent models perform far worse when deprived of text-only shortcuts, exposing significant weaknesses in genuine multimodal understanding and shifting the challenge from superficial perception to deep reasoning. By highlighting these critical gaps, MMMU-Pro drives progress toward more robust, human-like AI systems ready for complex, real-world tasks.

---

## 1. Executive Summary (2â€“3 sentences)
MMMUâ€‘Pro is a redesigned evaluation suite for multimodal AI that removes textâ€‘only shortcuts, expands multipleâ€‘choice options, and forces models to read questions embedded inside images (â€œvisionâ€‘only inputâ€). It matters because many high scores on prior benchmarks (e.g., MMMU) can be achieved without real visual reasoning; MMMUâ€‘Pro exposes this gap, showing large accuracy drops of 16â€“27 percentage points across stateâ€‘ofâ€‘theâ€‘art systems (Table 1), and shifting the main failure mode from perception to reasoning (Figure 7).

## 2. Context and Motivation
- Specific gap addressed:
  - The original MMMU benchmark evaluates collegeâ€‘level multimodal reasoning, but many questions can be answered by large language models (LLMs) using text alone (Section 2.1, Figure 2). Two root causes are identified:
    - â€œTextâ€‘only dependencyâ€: the image is redundant for some items.
    - â€œShortcut exploitationâ€: models infer the answer via statistical cues in the options without integrating the image.
- Why this is important:
  - Overestimating multimodal understanding risks brittle realâ€‘world deploymentsâ€”e.g., misreading forms, lab results, or diagrams when the model must both â€œseeâ€ and reason (Introduction).
  - Human cognition smoothly integrates text and graphics; current models often donâ€™t. The paper aims to test that core skill (Figures 1 and 4).
- Prior approaches and shortcomings:
  - Existing benchmarks (e.g., VQA, OKâ€‘VQA, MMBench, MMâ€‘Vet) cover perception and some reasoning but are less rigorous on expertâ€‘level multimodal integration or can be gamed by superficial patterns (Related Work).
  - Original MMMU raised the bar with collegeâ€‘level, multiâ€‘discipline questions, but still contained text-onlyâ€‘solvable items and small option sets (4 choices), which inflate scores (Section 2.1, Figure 3).
- Positioning:
  - MMMUâ€‘Pro builds on MMMU with three interventions to suppress shortcuts and stress true visionâ€‘language reasoning: (1) textâ€‘only LLM filtering, (2) option augmentation, and (3) a visionâ€‘only input setting (Section 2.2; Figure 1).

## 3. Technical Approach
MMMUâ€‘Pro is constructed in three sequential stages designed to strip out shortcuts and increase the need for integrated reasoning.

1) Filter out textâ€‘onlyâ€‘solvable questions (Section 2.2; Figure 1, left)
- How it works:
  - Four strong textâ€‘only LLMsâ€”`Llamaâ€‘3â€‘70Bâ€‘Instruct`, `Qwen2â€‘72Bâ€‘Instruct`, `Yiâ€‘1.5â€‘34Bâ€‘Chat`, `Mixtralâ€‘8Ã—22Bâ€‘Instruct`â€”answer every MMMU question without images, 10 times each.
  - A question is flagged as â€œanswerableâ€ if any model gets it right in >5/10 trials. If at least 3 of 4 models answer it correctly by this criterion, the question is removed.
  - From the remainder, 1,800 questions are sampled (60 per subject across 30 subjects), then further refined to 1,730 after human review (details below).
- Why this design:
  - Repeated trials reduce stochastic â€œluck.â€ Requiring 3/4 models to succeed makes the filter conservative.
  - The process explicitly identifies items solvable from text patterns alone (Figure 2 shows concrete examples where a textâ€‘only LLM answers correctly by using general knowledge and option cues).

2) Augment candidate options up to 10 (Section 2.2; middle of Figure 1)
- How it works:
  - Human experts, aided by `GPTâ€‘4o` for generation and `Claude 3.5` for filtering, expand 4 options to as many as 10.
  - Two rounds of human validation ensure new distractors are plausible, nonâ€‘ambiguous, and require reasoning tied to the image. During validation, questions that still lack coherent imageâ€“text linkage are removed (70 items filtered, yielding 1,730).
- Why this design:
  - With more options, guessing is harder and â€œoptionâ€‘patternâ€ shortcuts are less effective. Figure 3 shows accuracy of strong textâ€‘only LLMs drops approximately by half after filtering and option augmentation.

3) Create a â€œvisionâ€‘only inputâ€ version (Section 2.2; Figure 4)
- How it works:
  - Every retained question is turned into a screenshot/photo where the question text and the choices are baked into the image itself (humans manually capture screens with varied fonts, backgrounds, and conditions).
  - Final benchmark: 3,460 items, half as standard (text + image files) and half as visionâ€‘only (just a single image containing everything).
- Why this design:
  - Models must perform OCRâ€‘like text reading and visual reasoning together, as humans naturally do when reading diagrams, screenshots, or posters. This simulates realistic inputs users actually send to AI systems.

Evaluation protocol (Section 3.1)
- Three settings per model:
  1) Standard, 4 options (for comparability with MMMU),
  2) Standard, 10 options (the robust variant),
  3) Visionâ€‘only input.
- The benchmark also studies:
  - `CoT` (Chainâ€‘ofâ€‘Thought) prompting: ask models to reason stepâ€‘byâ€‘step.
  - An `OCR prompt`: first extract the text from the image, then solve (Appendix A).
- Human performance is approximated using original MMMU expert annotations and a conservative adjustment for questions lacking written solutions (Appendix B; Table 4 and Table 5).

## 4. Key Insights and Innovations
1) A principled, dataâ€‘driven filter for textâ€‘only solvability
- Whatâ€™s new:
  - Rather than manual judgments, textâ€‘only solvability is empirically measured via repeated trials on multiple strong LLMs with a clear acceptance criterion (Section 2.2).
- Why it matters:
  - It operationalizes â€œimageâ€‘dependenceâ€ and removes items that inflate multimodal scores (Figure 3 shows sizable drops in textâ€‘only LLM accuracy after filtering).

2) Option augmentation to 10 choices
- Whatâ€™s new:
  - Systematic expansion and humanâ€‘validated distractors that are specifically designed to resist guessing and force reasoning (Section 2.2; Appendix C).
- Why it matters:
  - It reduces optionâ€‘based shortcuts. In Table 1, all models lose 10â€“20+ points when moving from MMMU (Val) to MMMUâ€‘Proâ€™s 10â€‘option standard; for `GPTâ€‘4o`, âˆ’15.1 points (69.1 â†’ 54.0).

3) Visionâ€‘only input setting
- Whatâ€™s new:
  - Questions and options are embedded inside images (Figure 4). Models receive only the image, so they must simultaneously read, perceive, and reason.
- Why it matters:
  - This setting exposes weaknesses in integrating text and graphics. Table 1 shows further dropsâ€”e.g., `LLaVAâ€‘OneVisionâ€‘72B` loses âˆ’32.8 points versus MMMU (Val) in Vision, far beyond the âˆ’18.8 in 10â€‘option standard.

4) Rigorous analysis of OCR vs reasoning and CoT effects
- OCR prompt: Table 2 reports negligible gains from explicitly prompting â€œfirst extract text, then solveâ€ (e.g., `GPTâ€‘4o` 49.4 â†’ 49.7 in Vision). Figure 6 shows that high OCR accuracy does not guarantee strong reasoning.
- CoT prompting: Figure 5 shows CoT improves many models in both settings (e.g., `Claude 3.5 Sonnet` 42.7% â†’ 55.0% in Standard). Table 6 and Figure 9 reveal CoT helps most in structured, calculationâ€‘heavy disciplines (e.g., Tech & Engineering: +14.49 points for `GPTâ€‘4o` in Vision), but can hurt in subjective domains (e.g., Art & Design for `LLaVAâ€‘OneVisionâ€‘72B`: âˆ’17.12).

Collectively, these are more than incremental tweaks: they reshape evaluation to target true multimodal integration and reasoning, not just pattern matching.

## 5. Experimental Analysis
- Datasets, metrics, and setup (Section 3.1):
  - Dataset: 3,460 questions across 6 disciplines, 30 subjects, 183 subfields.
  - Settings: Standardâ€‘4 options (for reference), Standardâ€‘10 options, and Visionâ€‘only input.
  - Models: Proprietary (e.g., `GPTâ€‘4o`, `Claude 3.5 Sonnet`, `Gemini 1.5 Pro`) and openâ€‘source (e.g., `Qwen2â€‘VL`, `InternVL2`, `LLaVA`, `VILA`, `MiniCPMâ€‘V2.6`, `Phiâ€‘3.5â€‘Vision`).
  - Scoring: Accuracy; MMMUâ€‘Pro headline score is the average of the 10â€‘option standard and Vision scores (used elsewhere in the paper; Table 1 reports perâ€‘setting scores sideâ€‘byâ€‘side for clarity).

- Main quantitative results (Table 1):
  - Performance drops when increasing options from 4 to 10:
    > â€œ`GPTâ€‘4o (0513)`: 69.1 (MMMU Val) â†’ 54.0 (Standardâ€‘10), Î”1 = âˆ’15.1; `Claude 3.5 Sonnet`: 68.3 â†’ 55.0, Î”1 = âˆ’13.3; `Gemini 1.5 Pro (0801)`: 65.8 â†’ 49.4, Î”1 = âˆ’16.4.â€
  - Additional drops in the Visionâ€‘only setting:
    > â€œ`GPTâ€‘4o (0513)`: 49.7 in Vision, Î”2 = âˆ’19.4 vs MMMU Val; `LLaVAâ€‘OneVisionâ€‘72B`: 24.0 in Vision, Î”2 = âˆ’32.8.â€
  - Humans (approximated) remain far ahead:
    > â€œEstimated Human Expert (High): 85.4â€“88.6 across settings (Table 1 and Appendix B Table 4).â€
  - Baselines bound the task difficulty:
    > â€œRandom choice â‰ˆ 12â€“13% on MMMUâ€‘Proâ€™s 10â€‘option and Vision settings; Frequent choice baseline â‰ˆ 12% as well (Table 1).â€

- Do the experiments support the core claims?
  - Yes, across all families of models, the two MMMUâ€‘Pro interventions (10 options, visionâ€‘only) consistently depress performance relative to MMMU (Val), indicating MMMUâ€‘Pro effectively reduces shortcuts (Section 3.2, Table 1).
  - Figure 3 further corroborates the construction pipeline by showing textâ€‘only LLM accuracy steadily declines after filtering and option augmentation.

- OCR vs multimodal reasoning (Section 3.4):
  - OCR accuracy is high for top models (e.g., `GPTâ€‘4o` 92.3%, `Gemini 1.5 Pro` 89.7%), yet Vision accuracy remains much lower (Table 2).
  - Explicit OCR prompting barely changes Vision accuracy (e.g., `GPTâ€‘4o` 49.4 â†’ 49.7).
  - Figure 6: models with comparable OCR can diverge widely in Vision reasoning (e.g., `LLaVAâ€‘OneVisionâ€‘72B` has OCR â‰ˆ `InternVL2â€‘Llama3â€‘76B`/`GPTâ€‘4oâ€‘mini` but is much worse on Vision accuracy), signaling reasoningâ€”not text extractionâ€”is the bottleneck.

- CoT prompting (Section 3.3):
  - Figure 5: CoT generally helps in both standard and vision settings; exceptions exist (e.g., `VILAâ€‘1.5â€‘40B` drops).
  - Table 6 and Figure 9: CoT gains are largest in structured domains (Tech & Engineering: +14.49 for `GPTâ€‘4o` in Vision; Science: +8.22). In Art & Design, CoT has minimal or negative impact.

- Qualitative failure analysis (Sections 3.5 and 3.6):
  - Reasoning is the dominant error in Vision (46% of annotated `GPTâ€‘4o` errors), followed by perceptual (27%) and knowledge errors (25%) (Figure 7). OCR errors are effectively 0%.
  - Increased cognitive load in Vision (needing to read and see) shortens analytical reasoning: GPTâ€‘4o outputs shift toward shorter â€œanalyticalâ€ content and more â€œdescriptiveâ€ tokens (Figure 8).
  - Common error modes documented with examples (Figures 10â€“43): picking â€œcloseâ€ options when there are 10 distractors; misbalancing visual vs textual cues (e.g., WWI/WWII posters â†’ mispredicting League of Nations vs United Nations, Figure 33).

- Ablations on vision encoders (Section 4; Table 3):
  > â€œOn a fixed MLLM (Cambrianâ€‘1 with Llama 3.1 8B), `DINOv2 ViTâ€‘Gâ€‘14` yields 17.4% on MMMUâ€‘Pro Vision vs `SigLIP ViTâ€‘SO400Mâ€‘14` at 16.7%, despite SigLIP doing slightly better on MMMU (Val).â€
  - Suggests that selfâ€‘supervised visual features may transfer better to textâ€‘rich vision settings.

Overall, the experiments are broad (many models), controlled (three settings, consistent prompts), and include robustness checks (OCR, CoT, encoder ablation), giving credence to the central thesis: MMMUâ€‘Pro is a tougher, more faithful test of multimodal understanding.

## 6. Limitations and Trade-offs
- Assumptions and scope:
  - Multipleâ€‘choice format remains: even with 10 options, it may still allow residual heuristics or nearâ€‘miss reasoning (Appendix J examples show â€œclosest answerâ€ confusions).
  - The benchmark focuses on collegeâ€‘level academic tasks; it may not capture all realâ€‘world multimodal tasks (Limitations section).
- Visionâ€‘only realism vs control:
  - Photos and screenshots add realism (fonts, lighting, backgrounds) but introduce variability that can conflate perception noise with reasoning difficulty (Section 2.2; Figure 4).
- Human performance estimation:
  - Human scores are approximated from the original MMMU experts rather than reâ€‘collected on MMMUâ€‘Pro (Appendix B). Although the method is conservative and justified (Equation 2; Table 5), it is still an extrapolation.
- Compute and evaluation cost:
  - Creating two versions (standard and vision) doubles evaluation effort; option augmentation and multiâ€‘round human reviews increase curation cost (Section 2.2; Appendix C).
- Remaining shortcuts:
  - Even after filtering, some items may contain subtle statistical cues (Limitations); multiâ€‘discipline breadth makes total removal difficult.

## 7. Implications and Future Directions
- How this changes the landscape:
  - MMMUâ€‘Pro reframes â€œmultimodal understandingâ€ to require simultaneous reading and seeing, not just answering questions where the text is given separately. Result: headline accuracies shrink (Table 1), and error profiles shift toward reasoning (Figure 7), giving a more realistic picture of model capability.
- What research it enables or suggests (Section 4):
  - Model scaling and backbone choice:
    - Larger language backbones consistently help (Table 1; e.g., `GPTâ€‘4o` > `GPTâ€‘4oâ€‘mini`), but Vision remains a bottleneck.
  - Vision encoder research:
    - Selfâ€‘supervised encoders (`DINOv2`) may offer better features for textâ€‘rich, reasoningâ€‘dependent scenes than languageâ€‘aligned encoders (`SigLIP`) in Visionâ€‘only inputs (Table 3).
  - Better crossâ€‘modal integration:
    - Architectures that deeply fuse visual and textual tokens (e.g., improved crossâ€‘modal attention and feature fusion) are needed; OCR alone is insufficient (Table 2, Figure 6).
  - Reasoning data and algorithms:
    - Targeted Chainâ€‘ofâ€‘Thought data generation for structured domains can deliver large gains (Figure 5; Table 6). Methods that allocate inference compute effectively (e.g., inferenceâ€‘time algorithms) are promising avenues (Section 4).
  - Data generation tools:
    - The paper develops a tool that converts textâ€‘image pairs into screenshotâ€‘style visionâ€‘only items, enabling scalable creation of â€œtextâ€‘rich, reasoningâ€ training/eval data (Section 4).
- Practical applications:
  - More reliable assistants for reading scientific plots, medical images with embedded text, lab reports, math/engineering diagrams, and GUI screenshots (Figures 4 and 10â€“43).
  - Safer deployment: benchmarks like MMMUâ€‘Pro act as a stressâ€‘test before models are used in critical domains (Introduction; Sections 3.5â€“3.6).

---

Selected citations to figures and tables for quick reference:
- Construction pipeline and motivation: Figure 1, Figure 4, Section 2.2.
- Textâ€‘only LLM solvability: Figure 2; filtering + option augmentation effects: Figure 3.
- Main results across models/settings: Table 1; perâ€‘discipline CoT effects: Table 6, Figure 9; CoT overall: Figure 5.
- OCR analysis: Table 2; OCR vs Vision accuracy correlation: Figure 6.
- Error taxonomy and response length: Figure 7 and Figure 8.
- Vision encoder ablation: Table 3.
- Human performance approximation: Appendix B (Table 4, Table 5).

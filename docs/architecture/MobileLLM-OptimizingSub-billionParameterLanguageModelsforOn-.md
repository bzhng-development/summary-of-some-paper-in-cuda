# MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

**ArXiv:** [2402.14905](https://arxiv.org/abs/2402.14905)

## ğŸ¯ Pitch

MobileLLM challenges the prevailing belief that only model size and data scale matter by demonstrating that careful architectural choicesâ€”such as deeper and thinner networks, strategic weight sharing, and grouped-query attentionâ€”enable sub-billion-parameter language models to achieve state-of-the-art accuracy while remaining fast and energy-efficient for mobile devices. By specifically designing and benchmarking models to fit within the tight memory and computational limits of smartphones, MobileLLM enables high-quality on-device language understanding, reducing reliance on the cloud for lower latency, improved privacy, and greatly improved accessibility.

---

## 1. Executive Summary
MobileLLM designs and trains subâ€‘billionâ€‘parameter language models that run efficiently on mobile devices by prioritizing architecture choices and weight reuse over sheer parameter count. It delivers new 125M and 350M modelsâ€”plus a layerâ€‘sharing variant (`MobileLLMâ€‘LS`)â€”that set stateâ€‘ofâ€‘theâ€‘art accuracy among peers while keeping memory and latency compatible with phone hardware.

## 2. Context and Motivation
- Problem addressed
  - How to build highâ€‘quality language models that can run locally on phones without relying on the cloud.
  - On-device memory is tight: DRAM is typically 6â€“12 GB and a single app should stay well below that budget (Figure 2). LLM inference is also energyâ€‘intensive; a 7Bâ€‘parameter model can consume ~0.7 J/token, making sustained mobile use impractical (Section 1).
- Why this matters
  - Practical: Onâ€‘device inference reduces latency, bandwidth, cloud costs, and privacy risks. Section 1 argues that if people use LLMs for ~5% of daily time, purely cloudâ€‘based serving at GPTâ€‘4â€‘scale would need ~10^8 H100 GPUsâ€”economically and environmentally daunting (Appendix I).
  - UX: Measured decoding speed on phones is far higher for smaller models; a 125M model can reach ~50 tokens/s, whereas a 7B model runs at ~3â€“6 tokens/s in an iPhone app (Section 1, footnote 5).
- Where prior work falls short
  - Prior subâ€‘billion models (e.g., OPTâ€‘125M/350M, GPTâ€‘Neoâ€‘125M, Pythiaâ€‘160M/410M, RWKVâ€‘169M/430M, BLOOMâ€‘560M) were not architected specifically under strict onâ€‘device memory/latency constraints or did not prioritize architectural choices for tiny models (Tables 3â€“4).
  - A common belief from scaling laws is that architecture matters little once parameter count and data are fixed (Section 2.2.2). This paper contests that belief at the subâ€‘billion scale.
- Positioning
  - MobileLLM focuses on architectural and weightâ€‘reuse choices tailored to small models: deepâ€‘andâ€‘thin networks, input/output embedding sharing, groupedâ€‘query attention, and a new blockâ€‘wise layerâ€‘sharing scheme. The result is a family of models that outperform prior subâ€‘billion baselines on reasoning, QA/RC, chat, and an APIâ€‘calling task (Tables 3â€“6).

## 3. Technical Approach
The work proceeds in two stages: build a strong baseline under tight parameter budgets, then add a new layerâ€‘sharing method that preserves model size but improves accuracy and onâ€‘device execution locality.

1) Build a compact yet strong baseline (â€œMobileLLM,â€ Section 2.2)
- Deepâ€‘andâ€‘thin Transformer
  - Depth vs width search: for a fixed parameter budget, more layers with smaller hidden sizes perform better than fewer, wider layers on subâ€‘billion models (Figure 4; Tables 11â€“12). Example: around 125M parameters, 30 or 42 layers outperform 12 layers across zeroâ€‘shot reasoning, QA (TriviaQA), and reading comprehension (RACE).
  - Intuition: extra depth stacks more nonâ€‘linear transformations, improving representation power for abstract reasoning, while keeping perâ€‘layer dimensions small to fit memory/compute budgets.
- `SwiGLU` feedâ€‘forward networks (FFN) instead of ReLU FFN
  - `SwiGLU` is a gated activation that improves gradient flow and expressivity in FFNs. Switching to `SwiGLU` yields a +1.3 average accuracy point at 125M and 350M (Table 10, â€œ+ SwiGLU in FFNâ€).
- Inputâ€“output `embedding sharing` (Section 2.2.3)
  - Definition: reuse the same embedding matrix to map tokens to vectors (input) and vectors to logits over the vocabulary (output). In a 32kâ€‘vocab with 512â€‘dim embeddings, sharing saves ~16M parametersâ€”> >10% at 125M scale (Table 1).
  - How it helps: saved parameters can be reallocated to add layers (â€œâ†‘ depthâ€), recouping or improving accuracy at similar total size (Table 1: 30â€‘layer 125M model with embâ€‘share + 2 extra layers reaches a higher average than the nonâ€‘shared 135M model).
- `Groupedâ€‘Query Attention (GQA)` (Section 2.2.4)
  - Definition: use fewer key/value heads than query heads and reuse (repeat) each KV head across multiple Q heads. If Q heads = 16 and KV heads = 4, each KV head serves 4 Q heads.
  - Why: reduces KV parameter redundancy and KVâ€‘cache memory at inference. Ablations show best performance near 16 Q heads, with KV heads reduced to 4 causing negligible loss (125M) or ~0.2pt drop (350M) while saving size (~10%) (Figure 5; Table 13).
- Baseline architecture choices
  - 125M: 30 layers, 576 embedding dim, 9 heads, 3 KV heads (Table 9 in Appendix A).
  - 350M: 32 layers, 960 dim, 15 heads, 5 KV heads.
- Training setup (Sections 2.1 and 3.1)
  - Hardware: 32Ã— A100 GPUs, batch size 32/GPU.
  - Data/steps: exploration on 0.25T tokens for 120k iters; final models trained 480k iters on ~1T tokens.
  - Optimizer/schedule: Adam, weight decay 0.1, initial LR 2eâ€‘3 with cosine decay.

2) Add `immediate blockâ€‘wise layer sharing` (â€œMobileLLMâ€‘LS,â€ Section 2.3)
- Definition: make adjacent Transformer blocks share weights and compute the same block twice in sequence. No new parameters; the block is reused immediately while its weights are still hot in cache (Figure 6b).
- Why this is different from other sharing layouts (Figure 6)
  - â€œRepeatâ€‘allâ€‘overâ€ and â€œreverseâ€ sharing slightly improve accuracy but do not respect mobile cache realities; a shared block may be evicted before reuse, forcing extra DRAM traffic (Table 2; Figure 2).
  - Immediate blockâ€‘wise sharing maximizes data locality: SRAM/L3 caches on phones are ~8â€“32MB (Figure 2, device table), typically fitting one blockâ€™s weights. Reusing the block immediately avoids new weight fetches from DRAM.
- Empirical effect
  - Accuracy: modest improvements over the nonâ€‘shared baseline (Table 2).
  - Latency on iPhone 13 (ExecuTorch + MPS): almost no overhead vs. nonâ€‘shared baseline of the same parameter size, while naive doubling of layers without sharing is much slower (Table 7).

3) Implementation notes
- Quantization compatibility
  - Postâ€‘training quantization (PTQ) to 8â€‘bit weights and activations (`W8A8`) yields <0.5 average accuracy drop and works with layer sharing (Figure 7; Table 15).
- Distillation
  - Knowledge distillation from LLaMAâ€‘v2â€‘7B via crossâ€‘entropy on logits (Equation 1) slowed training 2.6â€“3.2Ã— and gave no accuracy gain; the authors therefore train with standard label supervision (Table 16).

## 4. Key Insights and Innovations
- Depth beats width for tiny LLMs (fundamental insight)
  - Finding: at fixed size, more layers with smaller dimensions consistently outperform shallower, wider models on eight zeroâ€‘shot commonsense tasks (Figure 4aâ€“b), TriviaQA (Figure 4câ€“d), and RACE (Figure 4eâ€“f). Detailed sweeps are in Tables 11â€“12.
  - Significance: challenges the common reading of scaling laws that architecture matters little once size/data are fixed (Section 2.2.2). For subâ€‘billion models, design choices are decisive.
- Weight reuse across the stack (architectural economy)
  - Embedding sharing (Table 1) and GQA (Figure 5; Table 13) reclaim parameters and KVâ€‘cache budget without sacrificing accuracy; savings are reinvested into more layers. This reframes â€œwhere to spend parametersâ€ for small models.
- Immediate blockâ€‘wise layer sharing (systemsâ€‘aware innovation)
  - Novelty: a sharing pattern matched to mobile memory hierarchiesâ€”reuse a block immediately to exploit cache locality (Figure 6b).
  - Evidence: accuracy improves over nonâ€‘shared baselines (Table 2) and latency overhead on iPhone 13 is minimal compared to a naÃ¯ve 2Ã—â€‘deeper model (Table 7: +2.6% execute time vs +86% without sharing).
- Demonstrated onâ€‘device viability and capability (applied contribution)
  - Models not only score higher on standard reasoning/QA benchmarks but also show strong chat performance and nearâ€‘7Bâ€‘level exactâ€‘match for API intent/structure prediction (Tables 5â€“6), indicating fitness for common phone assistants.

## 5. Experimental Analysis
- Evaluation methodology
  - Zeroâ€‘shot commonsense: ARCâ€‘easy/challenge, BoolQ, PIQA, SIQA, HellaSwag, OBQA, WinoGrande (Section 3.2; Table 3).
  - QA and reading comprehension: TriviaQA (1/5/64â€‘shot F1) and RACE (middle/high accuracy) (Table 4).
  - Chat: AlpacaEval (GPTâ€‘4 judge) and MTâ€‘Bench (GPTâ€‘4 judge; multiâ€‘turn) with identical fineâ€‘tuning pipelines across baselines (Section 3.3.1; Table 5).
  - API calling: synthetic dataset, 5k train/2.5k test, ~8 turns per dialog; measures `EMintent` and `EMstructure` for the JSON API spec plus ROUGEâ€‘1/L for agent replies (Section 3.3.2; Table 6).
  - Onâ€‘device latency: iPhone 13 via ExecuTorch + Metal Performance Shaders (MPS) (Section 3.6; Table 7).
  - Ablations: FFN activation, depth/width, embedding sharing, heads/KVâ€‘heads, sharing layout, repetition counts (Tables 1â€“2, 10â€“14; Figures 4â€“5).
- Main quantitative results
  - Zeroâ€‘shot commonsense (Table 3)
    - 125M:
      - `MobileLLMâ€‘125M`: average 46.3 vs prior 125â€“170M models: 42.5â€“43.6.
      - `MobileLLMâ€‘LSâ€‘125M`: average 47.0.  
        > From Table 3: â€œMobileLLMâ€‘LSâ€‘125M â€¦ Avg. 47.0,â€ beating OPTâ€‘125M (42.6), GPTâ€‘Neoâ€‘125M (42.9), Pythiaâ€‘160M (42.5), and RWKVâ€‘169M (43.6).
    - 350M:
      - `MobileLLMâ€‘350M`: average 51.3; `MobileLLMâ€‘LSâ€‘350M`: 52.1.  
        > Table 3 shows `MobileLLMâ€‘LSâ€‘350M` outperforms Pythiaâ€‘410M (46.6), RWKVâ€‘430M (47.0), BLOOMâ€‘560M (44.2), and OPTâ€‘350M (43.9) by 4â€“8 points.
  - QA and reading comprehension (Table 4)
    - `MobileLLMâ€‘125M`: TriviaQA 1/5/64â€‘shot F1 = 13.9/14.3/12.5; RACE middle/high = 39.7/28.9.
    - `MobileLLMâ€‘350M`: 22.0/23.9/24.2 on TriviaQA; RACE middle/high = 45.6/33.8.  
      > Table 4: the 350M model is â‰ˆ10 F1 points higher than other 350â€“590M models on TriviaQA and clearly better on RACE.
  - Chat (Table 5)
    - `MobileLLMâ€‘LSâ€‘350M`: MTâ€‘Bench 3.16 and AlpacaEval 48.20% win vs textâ€‘davinciâ€‘001 baseline (GPTâ€‘3).  
      > Table 5: within <1B models, MobileLLM variants substantially exceed OPTâ€‘350M (1.37/6.80) and BLOOMâ€‘560M (1.73/10.29).
  - API calling (Table 6)
    - `MobileLLMâ€‘350M`: `EMintent` 65.3 and `EMstructure` 48.8 vs LLaMAâ€‘v2 7B at 62.8 and 50.9.  
      > Table 6: intent classification is slightly higher than 7B; structure exactâ€‘match is close; ROUGE is lower (46.8/44.6 vs 56.5/54.3), which matters less for API correctness.
  - Quantization (Figure 7; Table 15)
    - `W8A8` PTQ yields â‰¤0.5 average drop across 125M/350M and with/without layer sharing.  
      > Table 15: e.g., `MobileLLMâ€‘350M` BF16 = 49.9 avg; W8A8 = 49.9.
  - Onâ€‘device latency (Table 7)
    - For 125M on iPhone 13: load/init/execute times are 39.2/1361.7/15.6 ms for baseline vs 43.6/1388.2/16.0 ms for `LS` (â‰ˆ2â€“3% overhead).  
      > Table 7 also contrasts a naÃ¯ve 60â€‘layer (nonâ€‘shared) model: 68.6/3347.7/29.0 msâ€”much slower to load/init and execute.
- Ablations and what they teach
  - FFN activation: `SwiGLU` consistently improves accuracy over vanilla FFN (Table 10).
  - Depth/width: clear monotonic gains up to ~30 layers for both sizes; very shallow models (<10 layers) are weak on reasoning and comprehension (Tables 11â€“12).
  - Embedding sharing: saves ~11.8% parameters at 125M with negligible average drop, recovered by adding 2 layers (Table 1).
  - GQA: 16 Q heads with 4 KV heads balances accuracy and memory; validated at both scales (Figure 5; Table 13).
  - Layerâ€‘sharing layout: â€œrepeatâ€‘allâ€‘overâ€ may edge out â€œimmediateâ€ in average accuracy, but immediate sharing is chosen for cache locality on phones (Table 2 and Section 2.3).
  - Repetition times: doubling layers with sharing helps (+0.4â€“0.6 avg) but further Ã—3 or Ã—4 gives diminishing or inconsistent returns (Table 14).
- Do experiments support the claims?
  - The breadth of tasks (commonsense, QA/RC, chat, API) and the extensive ablations strongly support the central claims: depth matters, weight reuse works, and blockâ€‘wise sharing achieves the desired accuracy/latency balance.
  - One caveat: chat metrics use GPTâ€‘4 as judge (Table 5), which can introduce evaluation bias; nevertheless, comparisons were run under identical settings.

## 6. Limitations and Trade-offs
- Training cost and data
  - Final models are trained for 480k iterations on ~1T tokens (Section 3.1). While inference is light, pretraining is still computeâ€‘heavy; the data composition is not detailed, so domain bias and data quality control are opaque.
- Language and task coverage
  - Evaluations focus on English benchmarks. Longâ€‘context tasks, toolâ€‘use beyond the provided API dataset, and multilingual settings are not studied.
- Layer sharing tradeâ€‘offs
  - Sharing increases compute (the block is applied twice), though execution remains fast because memory traffic is the bottleneck on phones (Table 7). On architectures where compute is the bottleneck, sharing could be less favorable.
  - Sharing may constrain representational diversity across adjacent layers. The paperâ€™s ablations show gains at Ã—2 repetition (Table 14) but not monotonic improvements beyond that.
- Embedding sharing constraints
  - Tying input and output embeddings can reduce flexibility in specialized finetuning where decoupling could help. The paper offsets this by increasing depth, but specific finetuning tradeâ€‘offs arenâ€™t explored.
- GQA head reduction
  - Fewer KV heads reduce cache size, but potential impacts on tasks requiring very fineâ€‘grained multiâ€‘head representations or extremely long context attention are not evaluated.
- Chat evaluation
  - GPTâ€‘4â€‘judged benchmarks are helpful but imperfect proxies for real user satisfaction and safety. Human evaluation and safety audits are out of scope.

## 7. Implications and Future Directions
- How this changes the landscape
  - For subâ€‘billion LLMs, architecture matters decisively. The recipeâ€”deepâ€‘andâ€‘thin, embedding sharing, GQA, and immediate blockâ€‘wise sharingâ€”redefines best practice for models meant to live on phones.
  - The results close the gap between tiny models and multiâ€‘billion models on practical tasks like API calling:  
    > Table 6 shows `MobileLLMâ€‘350M` matching or surpassing 7B on intent prediction and approaching it on structure.
- Practical applications
  - Onâ€‘device assistants for messaging, reminders, and app control; privacyâ€‘sensitive chat; offline query routing; lowâ€‘latency UI helpers. The latency study (Table 7) and quantization results (Table 15) indicate deployability on current phones.
- Followâ€‘up research
  - Hardwareâ€‘aware architecture search at the subâ€‘billion scale to generalize the depthâ€‘first principle across devices and NPUs.
  - Dynamic or conditional layer sharing: choose when to reuse vs specialize blocks based on input difficulty to balance accuracy and speed.
  - Longâ€‘context and memoryâ€‘augmented variants under the same parameter budgets; evaluate with retrieval and toolâ€‘use over larger API/task suites.
  - Safety/robustness and multilingual training pipelines at these sizes.
  - Further integration with sparsity/pruning and mixedâ€‘precision techniques; the paper already demonstrates compatibility with `W8A8` PTQ (Figure 7; Table 15).

Overall, MobileLLM demonstrates that thoughtful architectural design and weight reuse can outperform simply scaling parameters for small models. With concrete, systemâ€‘aware choices and thorough ablations, it delivers a practical path to highâ€‘quality, onâ€‘device LLMs.

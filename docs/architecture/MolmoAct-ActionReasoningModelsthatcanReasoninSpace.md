# MolmoAct: Action Reasoning Models that can Reason in Space

**ArXiv:** [2508.07917](https://arxiv.org/abs/2508.07917)

## ğŸ¯ Pitch

MolmoAct introduces a new paradigm for robotic foundation models by explicitly factoring perception, planning, and control into a three-stage action reasoning pipeline: first predicting depth-aware perception tokens, then sketching a visual trajectory trace, and finally outputting low-level actions. This structure enables robots to perform explainable and steerable behaviors, dramatically improving generalization, interpretability, and real-world robustness over prior end-to-end or language-only reasoning approaches. As an open, state-of-the-art Action Reasoning Model, MolmoAct sets a new foundation for building trustworthy, adaptive, and human-guidable robotic agents.

---

## 1. Executive Summary
MolmoAct introduces an â€œAction Reasoning Modelâ€ (ARM) that reasons about robot actions through an explicit, three-stage chain: predict 3D-aware depth tokens, sketch a 2D trajectory trace in the image, and then output lowâ€‘level action commands. This structure yields explainable, steerable behavior and strong performance: zeroâ€‘shot 70.5% on SimplerEnv Google Robot visual matching (Table 1), 86.6% average on LIBERO after postâ€‘training (Table 2), and large gains in realâ€‘world task progression and outâ€‘ofâ€‘distribution generalization (Figures 5â€“6).

## 2. Context and Motivation
- Problem addressed
  - Most robot policies map images and language directly to control. These â€œend-to-endâ€ policies often lack adaptability, generalization, and interpretability. The missing piece is explicit, spatial reasoning between perception and actuation (Introduction; Figure 1).
- Why this matters
  - Real robots operate in 3D, cluttered, dynamic spaces. Without understanding depth and planned motion, policies are brittle (e.g., to lighting, object layout) and hard to steer or debug. Providing interpretable intermediate representations improves trust and enables human guidance.
- Prior approaches and their limits
  - Vision-Language-Action (VLA) models (e.g., RTâ€‘1, RTâ€‘2, OpenVLA, GR00T N1.5, Ï€0/Ï€0â€‘FAST) improve generalization via large datasets and web pretraining, but remain opaque and sometimes brittle, with limited insight into why a given action was chosen (Introduction; Â§6.1).
  - Language-based â€œchain-of-thoughtâ€ (CoT) for robotics decomposes tasks verbally (e.g., ECoT, CoTâ€‘VLA, ThinkAct; Â§6.3), but textual steps do not capture precise 3D geometry or sub-centimeter motion constraints required for manipulation.
- Positioning of this work
  - MolmoAct moves from language-only reasoning to â€œreasoning in spaceâ€: it adds two spatial, non-linguistic intermediate representationsâ€”depth perception tokens and a drawable 2D trajectory traceâ€”before predicting actions (Figure 1; Â§2.3â€“2.4). The model, code, and data are released openly.

## 3. Technical Approach
MolmoAct converts a standard vision-language backbone into an Action Reasoning Model (ARM) with three autoregressive stages. Below is the pipeline and the rationale behind each design.

- Backbone and variants (Â§2.1; Appendix A)
  - Start from Molmo, a vision-language model with a ViT image encoder, connector, and an LLM. Two 7B variants:
    - `MolmoAct-7B-D`: SigLIP2 ViT + Qwen2.5â€‘7B.
    - `MolmoAct-7B-O`: OpenAI CLIP ViT + OLMo2â€‘7B (most open).
  - Multi-image inputs (e.g., wrist + third-person views) are supported by concatenating image token streams with index markers (Appendix A.5).

- Stage 1: Depth Perception Tokens (Â§2.3 â€œDepth Perception Tokensâ€; Eq. 1â€“2)
  - Goal: internalize 3D geometry despite only having RGB input during inference.
  - Mechanism:
    - Train a specialist depth estimator (DepthAnything v2) and compress depth maps using a VQâ€‘VAE (â€œvector-quantized variational autoencoderâ€; trained on 10M depth maps for 20 epochs; Â§3.1).
    - The VQâ€‘VAE codebook has N=128 discrete codes. Each image is represented as a fixed-length string of M=100 depth tokens: `d = (<DEPTH_START>, DEPTH_z1, â€¦, DEPTH_zM, <DEPTH_END>)` (Eq. 2).
    - During MolmoAct training, the model learns to predict this depth token string from RGB (â€œspecialist-to-generalist distillationâ€), teaching it a depth-aware representation it can later condition on.

- Stage 2: Visual Reasoning Trace (Â§2.3 â€œVisual Reasoning Traceâ€; Eq. 3)
  - Goal: encode an explicit plan as a short, human-editable 2D path for the end-effector.
  - Representation:
    - A polyline `Ï„ = (p1,â€¦,pL)`, 1â‰¤Lâ‰¤5, points in image coordinates normalized to 0â€“255 (Eq. 3). `p1` is the current gripper location; later points come from evenly subsampling the future of the demo.
  - How labels are generated (Â§3.1):
    - Use a VLM (â€œMolmoâ€) trained on 2D pointing to localize the robot gripper in each frame via the prompt â€œpoint to the robot gripperâ€ (or left/right for bimanual). This yields pixel coordinates for every timestep, which are linked into traces; for each step, select up to 5 points from now to the episode end.

- Stage 3: Action Tokens and factorization (Â§2.2; Â§2.3; Eq. 4)
  - Action space discretization (Â§2.2):
    - For each action dimension (e.g., 6â€‘DoF end-effector deltas and gripper), normalize by dataset quantiles and discretize into 256 bins.
    - Instead of mapping bins to arbitrary â€œrareâ€ text tokens (which ignore ordinal structure), map them monotonically to 256 byteâ€‘level BPE symbols taken from the end of the Qwen2 tokenizer so adjacent bins have adjacent symbols. This similarityâ€‘preserving initialization makes learning smoother and faster.
  - Autoregressive factorization (Â§2.3; Eq. 4):
    - Generate depth tokens `d` first, then trace tokens `Ï„`, then action tokens `a`:
      - `p(d, Ï„, a | I, T) = âˆ p(di | I, T, d<i) Ã— âˆ p(Ï„j | I, T, d, Ï„<j) Ã— âˆ p(ak | I, T, d, Ï„, a<k)`
    - This conditions actions on explicit depth and planned trajectory, grounding control in spatial reasoning.

- Test-time steerability via user sketches (Â§2.4; Eq. 5; Figure 1, right; Figure 9)
  - Users can draw a trace on the camera image, `I+ = I âŠ• Ï„`.
  - The model conditions on this overlaid image to produce actions:
    - `p(a | I+, T) = âˆ p(ak | I+, T, a<k)` (Eq. 5).
  - The trace is unambiguous, precise, and editable, often more reliable than language-only corrections.

- Training data and stages (Â§3â€“Â§4; Figure 2; Figure 3)
  - Data curation for â€œaction reasoning dataâ€ (Â§3.1):
    - For any robot dataset with RGB, language, and actions, generate depth token strings and visual traces per frame using the specialist pipeline above.
  - Auxiliary robot data (Â§3.1):
    - Depth-only prediction; trace-only prediction; and trajectoryâ€‘conditioned actions (overlay trace on image and predict the next action)â€”the last is crucial for steerability.
  - Preâ€‘training (Â§4.1):
    - Mixture of OXE subset (RTâ€‘1, BridgeData V2, BCâ€‘Z) converted to action reasoning, plus auxiliary robot data and ~2M multimodal web samples (e.g., VQA, PixMo, LVIS pointing; Â§3.3). Total 26.3M samples; sample rates in Figure 3.
    - 100k steps on 256 H100s (9,728 GPU hours). Despite being much smaller than some competitors, achieves strong zeroâ€‘shot generalization (Table 1).
  - Midâ€‘training (Â§4.2; Â§3.2):
    - MolmoAct Dataset: 10,689 realâ€‘world trajectories on 93 tasks; two cameras + wrist view; average 112 steps; mix of home and tabletop environments (Figure 4; Appendix E).
    - Converted into 1M action reasoning and 1M trajectoryâ€‘conditioned samples; trained 50k steps on 128 H100s.
  - Postâ€‘training / adaptation (Â§4.3):
    - For new tasks or embodiments, collect 30â€“50 demos and LoRAâ€‘tune only lowâ€‘rank adapters (rank 32, alpha 16). Use action chunking (N=8) to predict short openâ€‘loop segments, then reâ€‘plan.

- Implementation details supporting robustness (Appendix Aâ€“B)
  - Highâ€‘res tiling + attention pooling to keep fine details (A.2â€“A.3).
  - Multiâ€‘image tokenization (A.5).
  - Stable distributed training, token reallocation for depth tokens (B.1).
  - Compute and cluster details (B.2).

## 4. Key Insights and Innovations
- Spatial chain-of-thought for control (Â§2.3; Figure 1)
  - Novelty: replaces textâ€‘only CoT with two spatial, tokenized representationsâ€”depth and 2D traceâ€”before predicting actions.
  - Why it matters: explicitly grounds actions in 3D perception and a visible plan. Each stage is decodable (depth map, overlayed trace, executed action), improving explainability and enabling human correction.

- Similarity-preserving action tokenization (Â§2.2)
  - Novelty: maps 256 discretization bins to adjacent byteâ€‘level BPE symbols so neighboring bins start with similar embeddings.
  - Impact: better inductive bias for ordinal structure; faster training. The preâ€‘training budget is 9,728 GPU hours, over 5Ã— less than a reported 50,000 GPU hours for GR00T N1.5 (Â§2.2).

- Steerability via visual traces (Â§2.4; Figure 9)
  - Novelty: a general, precise, testâ€‘time control interfaceâ€”draw the desired endâ€‘effector path in the image, and the model follows it.
  - Impact: resolves ambiguity of language corrections, enables interactive refinement during execution, and yields higher success than language-only steering (Figure 9, left).

- Open midâ€‘training dataset and full release (Â§3.2; Abstract; Conclusion)
  - The MolmoAct Dataset: >10k high-quality robot trajectories across 93 tasks; used to bridge preâ€‘training to realâ€‘world settings.
  - Impact: midâ€‘training on this dataset adds ~5.5% average realâ€‘world performance (Figure 6b) and provides a blueprint for building ARMs.

## 5. Experimental Analysis
- Evaluation setup and baselines
  - Simulation zeroâ€‘shot and fineâ€‘tuning on SimplerEnv Google Robot suite (visual matching and variant aggregation) with many competitive baselines (Table 1; Â§5.1, Â§5.3).
  - LIBERO postâ€‘training on four suites (Spatial, Object, Goal, Longâ€‘horizon), comparing against stateâ€‘ofâ€‘theâ€‘art VLA policies (Table 2; Â§5.2).
  - Realâ€‘world fineâ€‘tuning on singleâ€‘arm and bimanual Franka tasks (Figure 5; Appendix D.3).
  - Outâ€‘ofâ€‘distribution (OOD) generalization tests in SimplerEnv variant aggregation and realâ€‘world multiâ€‘task settings: language variation, spatial variation, distractors, novel objects (Figure 6a; Â§5.3; Appendix D.4).
  - Human evaluations for openâ€‘ended instruction following and for lineâ€‘trace generation (Figures 7â€“8; Â§5.5).
  - Steerability study: ambiguous instruction corrected by language or a userâ€‘drawn trace (Figure 9; Â§5.6; Appendix D.7).

- Main quantitative results
  - SimplerEnv (Google Robot; Table 1):
    - > â€œMolmoAct (zero-shot) 70.5% visual matchingâ€ and â€œ71.6% after RTâ€‘1 fineâ€‘tuning,â€ outperforming closed or strong open baselines like Ï€0/Ï€0â€‘FAST, GR00T N1.5, and Magma in relevant regimes.
    - Variant aggregation (OOD) after fineâ€‘tuning: > â€œ72.1%,â€ exceeding RTâ€‘2â€‘X by 7.8%; performance drop from visual matching to variant aggregation is <1%, indicating robustness (Â§5.3).
  - LIBERO (Table 2):
    - > â€œMolmoActâ€‘7Bâ€‘D average 86.6%,â€ the best among compared autoregressive policies; especially strong in Longâ€‘horizon: > â€œ77.2%,â€ a +6.3% gain over ThinkAct (Â§5.2).
  - Realâ€‘world fineâ€‘tuning (Figure 5; detailed perâ€‘trial tables in Appendix D.3):
    - Singleâ€‘arm tasks: average taskâ€‘progression improvement of about +10% over Ï€0â€‘FAST (e.g., Wipe Table average 1.00 vs. 0.817; Table 19).
    - Bimanual tasks: +22.7% over Ï€0â€‘FAST on average (e.g., Set Table, Fold Towel; Tables 15â€“17).
  - OOD generalization (Figure 6a; Table 1):
    - Simulation: top performance on variant aggregation (72.1%).
    - Real world: average +23.3% taskâ€‘progress improvement over Ï€0â€‘FAST across language, spatial, distractor, and novelâ€‘object perturbations.
  - Effect of midâ€‘training (Figure 6b; Â§5.4):
    - Midâ€‘training on MolmoAct Dataset provides ~â€œ+5.5%â€ average improvement across Close Lid, Rotate Pot, Pour Tea; even without midâ€‘training, MolmoAct outperforms Ï€0â€‘FAST and OpenVLA by 14.8% and 10.9%.
  - Human preference studies
    - Openâ€‘ended instruction following (Figure 8): MolmoAct receives the highest Elo rating; pairwise wins in 58% vs SpatialVLA and 81% vs OpenVLA.
    - Trace generation on Internet images (Figure 7): MolmoAct attains the top Elo, above GPTâ€‘4o, Geminiâ€‘2.5â€‘Flash, and a specialized trace VLM (â€œHAMSTERâ€).
  - Steerability (Figure 9; Â§5.6):
    - With ambiguous â€œpick up the bowlâ€ scenarios, visualâ€‘trace steering with MolmoAct achieves > â€œ0.75 success,â€ beating MolmoAct with openâ€‘ended language corrections (0.42) and Ï€0â€‘FAST with language corrections (0.29 difference vs MolmoAct).

- Do the experiments support the claims?
  - Yes. The structured chain (depthâ†’traceâ†’actions) is validated across simulation and real robots. Robustness is evidenced by minimal drop from visual matching to variant aggregation (<1%), strong OOD improvements (Figure 6a), and human evaluations favoring MolmoActâ€™s instruction following and trace quality (Figures 7â€“8).
  - Ablations and diagnostics:
    - Impact of midâ€‘training dataset (Figure 6b).
    - Realâ€‘world perâ€‘trial tables (Appendix D) show consistency and failure cases (e.g., occasional partial progression on Set Table).
  - Conditions and tradeâ€‘offs:
    - Visualâ€‘trace steering excels when users can specify precise paths; language-only steering remains weaker when instructions are ambiguous (Figure 9).

## 6. Limitations and Trade-offs
- Assumptions and data dependencies (Appendix G; Â§3.1)
  - Depth tokens rely on a specialist depth estimator and VQâ€‘VAE codebook learned from specific data (tabletopâ€‘heavy). Transfer to markedly different camera setups or scenes might degrade without reâ€‘distillation.
  - Visual traces come from gripper pointing via a VLM; accuracy depends on clear visibility. Occlusions or unusual embodiments may hurt trace quality (Limitations: â€œCamera Occlusion of Endâ€‘effectorâ€).
- Steerability is 2D (Â§G)
  - User sketches are in image space; without explicit 3D lifting, the controller may follow the correct inâ€‘plane path but drift along depth (outâ€‘ofâ€‘plane). The paper suggests conditioning on predicted depth tokens to â€œliftâ€ traces into 3D as future work.
- Control frequency and latency (Â§G)
  - Predicting depth/trace/action tokens adds latency; real robots may need higher control rates. The paper notes a mismatch between model inference and data collection frequency, suggesting model optimization or edge deployment as future directions.
- Resolution of depth tokens (Â§G)
  - Depth is compressed into 100 tokens per image; very fine manipulation might need more tokens or higherâ€‘resolution depth encodings.
- Training and compute
  - Although efficient relative to some baselines, the full pipeline still uses substantial GPU hours and curated data generation (e.g., the 26.3M-sample preâ€‘training mixture; Â§4.1). Highâ€‘quality postâ€‘training demos (30â€“50 per task) are still required for new tasks (Â§4.3).

## 7. Implications and Future Directions
- How this changes the landscape
  - Establishes â€œreasoning in spaceâ€ as a practical, scalable alternative to textâ€‘only CoT for robotics. By making intermediate perceptions and plans explicit and decodable, it improves interpretability, controllability, and generalization (Figure 1; Eq. 4â€“5).
  - Demonstrates that explicit spatial reasoning can reduce brittleness to visual shifts (Table 1) and improve longâ€‘horizon performance (Table 2).
- Next research steps enabled or suggested
  - Lift 2D traces into 3D using the predicted depth tokens, enabling richer testâ€‘time steering and safer manipulation in clutter (Appendix G).
  - Learn higherâ€‘fidelity or adaptive depth tokenizations (beyond fixed 100 tokens) for precision tasks.
  - Close the controlâ€‘rate gap with efficient, smaller ARMs or specialized accelerators; explore lowâ€‘latency onâ€‘robot inference.
  - Extend to mobile manipulation and multiâ€‘robot coordination by composing traces across views and agents; integrate proprioception when useful.
  - Formalize safety constraints and user intent alignment using editable traces and depth tokens as verifiable interfaces.
- Practical applications
  - Home assistance (dishwashing, wiping, table setting; Figures 4â€“5), warehouses (pickâ€‘place with OOD robustness), and teleoperation with humanâ€‘inâ€‘theâ€‘loop corrections by sketch.
  - Education and debugging: instructors can visualize the modelâ€™s plan and correct it without rewriting code or crafting fragile prompts.

> Bottom line: Sections 2.2â€“2.4 and Figure 1 define a clear threeâ€‘stage spatial reasoning pipeline; Sections 3â€“4 detail how to label, train, and adapt it; Tables 1â€“2 and Figures 5â€“9 show that this structure yields stateâ€‘ofâ€‘theâ€‘art results, strong robustness, and a practical, precise steering interface. The open release (models, code, and datasets) provides a reproducible blueprint for building ARMs that transform perception into purposeful action.

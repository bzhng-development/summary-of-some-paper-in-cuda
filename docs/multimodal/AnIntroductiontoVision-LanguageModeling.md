# An Introduction to Vision-Language Modeling

**ArXiv:** [2405.17247](https://arxiv.org/abs/2405.17247)

## ğŸ¯ Pitch

This paper delivers a unified, practice-driven introduction to Vision-Language Models (VLMs), mapping out the core training paradigmsâ€”contrastive, masking, generative, and pretrained-backboneâ€”and providing hands-on guidance for efficient data curation, model alignment, and responsible evaluation. By critically synthesizing what works (and what can go wrong), it empowers practitioners and newcomers to build and assess more reliable, real-world VLMs, addressing persistent challenges like spatial reasoning, prompt adherence, and hallucination that currently limit the technologyâ€™s impact.

---

## 1. Executive Summary (2â€“3 sentences)
This paper is a practiceâ€‘oriented introduction to Visionâ€‘Language Models (VLMs). It offers a unifying taxonomy of VLM training paradigms (contrastive, masking, generative, and pretrainedâ€‘backbone approaches; Figure 1), concrete training recipes (data curation, efficiency, alignment, grounding), and a critical guide to evaluation (benchmarks, bias, hallucination, memorization), with an extension to videos. It matters because VLMs still fail on core abilities (spatial relations, counting, attribute binding, and hallucination) and the field lacks a consolidated â€œhowâ€‘toâ€ for building and responsibly evaluating reliable systems.

## 2. Context and Motivation
- Problem/gap addressed
  - Many VLMs now exist, but reliable visionâ€‘toâ€‘language mapping remains unsolved: models struggle with spatial relationships, counting, attributes, prompt adherence, and hallucinations (Section 1: â€œmodels struggle to understand spatial relationships or countâ€¦ [and] can also hallucinateâ€). The literature is fragmented across objectives, architectures, datasets, and evaluation practices.
  - There is no single, accessible guide that connects learning objectives to practical training/evaluation choices while warning about common pitfalls.

- Why this is important
  - Realâ€‘world impact: VLMs power assistive tools, multimodal assistants, and generators; unreliability can mislead users or cause harm (Sections 1, 4.5).
  - Theoretical significance: Understanding how contrastive, masking, and generative objectives differâ€”and how they relate informationâ€‘theoreticallyâ€”helps clarify what models actually learn (Sections 2.2â€“2.4, 2.3.3).

- Prior approaches and their shortcomings
  - Early transformer VLMs (e.g., `VisualBERT`, `ViLBERT`) used masked modeling + sentence-image matching (Section 2.1), but scale and openâ€‘world generalization were limited.
  - Contrastive `CLIP` achieved strong zeroâ€‘shot classification via InfoNCE on web pairs (Section 2.2.1), but depends on huge batches and massive data; negatives and batch composition matter (Sections 2.2, Eq. 2).
  - Masking methods (`FLAVA`, `MaskVLM`) jointly reconstruct text/image tokens but can add decoders/overheads (Section 2.3).
  - Generative VLMs (`CoCa`, `CM3Leon`, `Chameleon`) unlock text/image generation and broader reasoning, but are computeâ€‘intensive and hard to optimize (Sections 2.4.1â€“2.4.2).
  - LLMâ€‘backbone VLMs (`Frozen`, `MiniGPT` family) are computeâ€‘efficient but inherit LLM biases and hallucinations, and rely on mapping between pretrained spaces (Section 2.5).

- How this paper positions itself
  - Not a comprehensive survey but an â€œonâ€‘rampâ€: a structured taxonomy (Figure 1), a practical training guide (Sections 3.1â€“3.7; Figure 2), an evaluation playbook (Section 4; Figure 3), and a synthesis of video extensions (Section 5), highlighting strengths, weaknesses, and tradeâ€‘offs of each choice.

## 3. Technical Approach
This is a methodological synthesis rather than a new algorithm. It organizes the space into four families (Figure 1), connects them to training recipes (Figure 2), and maps them to evaluation methods (Figure 3).

A) Four training paradigms (Section 2; Figure 1)
1) Contrastive (Section 2.2)
   - Mechanism: Learn a joint embedding where matched imageâ€“text pairs are close, mismatched are far. Formally grounded via energyâ€‘based models and Noiseâ€‘Contrastive Estimation (NCE; Eq. 1) and â€œInfoNCEâ€ (Eq. 2).
   - InfoNCE (Eq. 2) uses a softmax over similarities across the miniâ€‘batch; thus large batches and careful temperature tuning matter.
   - `CLIP` (Section 2.2.1) trains image and text encoders from scratch on 400M web pairs; achieves strong zeroâ€‘shot transfer but is batch/data hungry and sensitive to negative set quality.
   - Variants:
     - `SigLIP`: binary crossâ€‘entropy with sigmoid instead of softmax improves smallâ€‘batch training (Section 2.2.1).
     - `Llip`: conditions image encoding on the target caption via crossâ€‘attention to account for caption multiplicity (Section 2.2.1).

2) Masking (Section 2.3)
   - Mechanism: Predict masked tokens/patches from the other modality. Related to denoising autoencoders and Masked Language/Image Modeling.
   - `FLAVA` (Section 2.3.1): three transformers (image encoder `ViT`, text encoder, multimodal encoder with crossâ€‘attention and `[CLS]` tokens), trained with a mix of uniâ€‘ and multiâ€‘modal masked losses plus contrastive; pretrained on 70M imageâ€“text pairs.
   - `MaskVLM` (Section 2.3.2): applies masking directly in pixel space and text space; each modalityâ€™s reconstruction is conditioned on information from the other modality.
   - Informationâ€‘theoretic unification (Section 2.3.3):
     - Presents masking/contrastive as rateâ€“distortion tradeâ€‘offs (Eq. 3â€“4). The â€œrateâ€ term (entropy bottleneck) removes superfluous info; the â€œdistortionâ€ term preserves predictive information via reconstruction. Contrastive can be viewed as compression without reconstruction: it retains only what is necessary to discriminate equivalence classes.

3) Generative (Section 2.4)
   - Mechanism: Model the joint generative process of text and/or images; enables captioning, textâ€‘toâ€‘image, editing, and stronger compositional reasoning.
   - `CoCa` adds a textâ€‘decoder generative loss atop a contrastive imageâ€“text encoder (Section 2.4.1) to enable VQAâ€‘like tasks without explicit fusion layers.
   - Mixedâ€‘modal autoregressive (`CM3Leon`, `Chameleon`; Section 2.4.2):
     - All inputs/outputs are discrete tokens; images are tokenized into 1024 codes (vocab 8192), text vocab ~56k, with a `<break>` token to switch modalities.
     - Twoâ€‘stage training: retrievalâ€‘augmented pretraining (prepend retrieved multimodal docs, then predict next tokens) and supervised instruction tuning across tasks.
     - `Chameleon` is an earlyâ€‘fusion, fully tokenâ€‘based mixedâ€‘modal model trained endâ€‘toâ€‘end; stability requires architectural tweaks (e.g., queryâ€‘key normalization).
   - Generative models as discriminative classifiers (Section 2.4.3):
     - Classify via Bayes: pick class `c` maximizing `pÎ¸(x|c) p(c)` (Eq. 5). For autoregressive models, compute `log pÎ¸(x|c)` by summing perâ€‘token logâ€‘likelihoods (Eq. 6) after image tokenization via `VQâ€‘VAE`/`VQâ€‘GAN`. For diffusion, approximate a likelihood bound via denoising error (Eq. 7).
     - Tradeâ€‘off: better OOD and compositional robustness but expensive inference (hundreds/thousands of network evaluations per image for diffusion).

4) Pretrained backbones (Section 2.5)
   - Mechanism: Keep a powerful text LLM and often a vision encoder frozen; learn a lightweight mapping from visual features to LLM token space.
   - `Frozen` maps visual features to text embeddings of a 7B LLM trained on C4; the LLM is kept frozen and conditioned on interleaved imageâ€“text embeddings (Section 2.5.1).
   - `MiniGPTâ€‘4` aligns BLIPâ€‘2â€™s `Qâ€‘Former` outputs to `Vicuna` with a linear projector; first train ~5M pairs (20k steps, batch 256) on CC/SBU/LAION; then 400 steps of instruction tuning. Compute: â€œfour A100 GPUs for around ten hoursâ€ (Section 2.5.2).
   - Extensions: `MiniGPTâ€‘5` (generates interleaved imageâ€“text via â€œgenerative vokensâ€ into Stable Diffusion 2.1), `MiniGPTâ€‘v2` (unified multiâ€‘task interface via task identifiers), `Qwenâ€‘VL`, and `BLIPâ€‘2`â€™s `Qâ€‘Former` mapping (Section 2.5.3).

B) Training recipes (Section 3; Figure 2)
1) Data curation (Section 3.1)
   - DataComp benchmark (pools from 1.28M to 12.8B pairs; 38 tasks) standardizes model/hparams to compare dataset filtering; â€œdata pruning is a crucial stepâ€ (Section 3.1).
   - Strategies:
     - Heuristics: text complexity, language filtering, image resolution/aspect ratio; multimodal textâ€‘spotting to avoid OCR leakage (Section 3.1).
     - Modelâ€‘based ranking: `CLIPScore` filters by cosine similarity; `Tâ€‘MARS` masks text regions before scoring; `Sieve` uses captioners to reduce CLIPScore false positives/negatives (Section 3.1).
     - Diversity/balancing: `DataComp` sampling from curated sets; `MetaCLIP` uses ~500k WordNet/Wikipedia queries and caps samples per concept (â‰¤20k/query) to broaden coverage (Section 3.1).
     - Concept frequency matters: zeroâ€‘shot performance largely tracks how often downstream concepts appear in pretraining (Section 3.1: Udandarao et al. 2024).
   - Synthetic data: replace weak altâ€‘text with BLIP/LLaVA captions; or fully synthetic pairs via LLMâ€‘generated captions + textâ€‘toâ€‘image (Section 3.1.1).
   - Augmentation: add a selfâ€‘supervised image loss (SLIP); use multiâ€‘strength augmented pairs in contrastive training (`CLIPâ€‘rocket`) with separate projectors for weak/strong views (Section 3.1.2).

2) Interleaved data (Section 3.1.3)
   - Natural: `OBELICS` preserves HTML document structure, dedupes, filters, and retains coâ€‘occurring textâ€“image DOM nodes.
   - Synthetic: `MMC4` retrofits images onto text corpora by CLIP similarity for scalable multimodalization.

3) Data quality assessment (Section 3.1.4)
   - Text quality (e.g., `QuRating`), image aesthetics (`VILA`, LAIONâ€‘aesthetics), and imageâ€‘text alignment (CLIPâ€‘family) exist, but thereâ€™s no holistic metric for interleaved multimodal quality.

4) Human annotation (Section 3.1.5)
   - Rich but costly; best for evaluation/fineâ€‘tuning. Example: `DCI` densely describes SAâ€‘1B images (>1,000 words per image over 7,805 images; Section 4.1.7).

5) Software, compute, and efficiency (Sections 3.2â€“3.2.4)
   - OpenCLIP and HuggingFace transformers for baselines (Section 3.2.1).
   - Compute reality check: â€œtraining a contrastive model like CLIP â€¦ should not require more than 64 GPUs â€¦ ~10K USDâ€ if you have highâ€‘quality data and masking when using larger models (Section 3.2.2).
   - Speedups: `torch.compile`, `xformers`, but data loading is often the bottleneck. Use uncompressed storage and `FFCV` to accelerate IO (Section 3.2.3). Randomly mask image tokens to speed large models without losing accuracy (Section 3.2.3).

6) Choosing a model (Section 3.3)
   - Use `CLIP` for retrieval/strong embeddings and as a base to prototype grounding; beware large batch/data needs.
   - Masking to remove batchâ€‘dependence and for multiâ€‘modal reconstruction; may add decoder overhead.
   - Generative models for interpretability (decode what the model â€œknowsâ€) and joint distributions; heavier compute.
   - LLMâ€‘backbone mapping for low compute; inherit LLMâ€™s hallucinations/biases.

7) Improving grounding (Section 3.4)
   - Boxâ€‘supervision (e.g., `Xâ€‘VLM`) adds box regression/IoU losses across COCO/VisualGenome et al. (Section 3.4.1).
   - Pseudoâ€‘grounding at scale: parse nouns with spaCy; detect boxes with `GLIP`; compose grounded captions (Kosmosâ€‘2 pipeline; Section 3.4.1).
   - Negative captioning: contrast with carefully constructed negatives to teach relations/attributes/order (e.g., ARO benchmark; Section 3.4.2).

8) Improving alignment (Section 3.5)
   - Instruction tuning for multimodal chat (e.g., `LLaVA` with 150k synthetic visual instructions; Section 3.5.1).
   - RLHF to reduce hallucinations (`LLaVAâ€‘RLHF` uses factâ€‘augmented reward models; Section 3.5.1).
   - LLaVA evolution: `v1.5` (600k pairs, strong VQA), `NeXT v1.6` (higherâ€‘res features; better data mix; 34B backbone) approaches SOTA openâ€‘source performance (Section 3.5.1).

9) Textâ€‘rich image understanding (Section 3.6)
   - Instruction tuning on textâ€‘rich images (`LLaVAR`) boosts OCRâ€‘VQA by up to 20% (Section 3.6).
   - Highâ€‘resolution patching (`Monkey` up to 1344Ã—896) with visual resampler/LoRA for fine details (Section 3.6).
   - Decoupled onâ€‘device STR â†’ cloud LLM (`Lumos`) reduces latency and increases accuracy for complex text (Section 3.6).

10) Parameterâ€‘efficient fineâ€‘tuning (Section 3.7)
    - `LoRA/QLoRA/VeRA/DoRA`, prompt tuning (`CoOp`, `VPT`), adapters (`CLIPâ€‘Adapter`, `VLâ€‘adapter`, `LLaMAâ€‘Adapter v2`), and mappingâ€‘only (`LiMBeR`, `MAPL`).

C) Evaluation (Section 4; Figure 3)
- Captioning: serverâ€‘based BLEU/ROUGE have limits; `CLIPScore` is referenceâ€‘free but depends on CLIP quality (Sections 4.1.1â€“4.1.2).
- Textâ€‘toâ€‘image consistency: programmatic VQA (`TIFA`, `DSG`, `VPEval`), or direct VQAScore (Section 4.1.2).
- VQA: openâ€‘ended evaluation with LLMâ€‘judges addresses exactâ€‘match brittleness; selective prediction evaluates abstention ability (Section 4.1.3).
- Textâ€‘centric VQA: OCR, DocVQA, KIE, and HMER datasets (Section 4.1.4).
- Zeroâ€‘shot classification: prompt engineering and LLMâ€‘generated descriptions matter; â€œzeroâ€‘shotâ€ often correlates with concept frequency in pretraining (Section 4.1.5).
- Compositional reasoning: `Winoground`, `ARO`, `SUGARCREPE` with cautions about hackable negatives and an â€œequalâ€‘probability bugâ€ in binary setups (Section 4.1.6).
- Dense captioning: `DCI` cropâ€‘caption matching evaluates fineâ€‘grained grounding (Section 4.1.7).
- Synthetic eval: `PUG` controls scenes to probe spatial relations; many VLMs near chance on left/right/location (Section 4.1.8).
- Bias/disparity: classification tests on people attributes; embeddingâ€‘space association tests; beware language priors contaminating benchmarks (Sections 4.2â€“4.2.3).
- Hallucinations and memorization: `CHAIR`, `POPE`, GPTâ€‘4â€‘based evaluators; dejaâ€‘vu memorization via kâ€‘NN on training captions, mitigated by text randomization (Sections 4.3â€“4.4).
- Red teaming: define/measure risks (privacy, toxicity, bias) via adversarial prompts; mitigation with RLHF (Section 4.5).

D) Video extension (Section 5)
- Earlyâ€‘fusion BERTâ€‘style (`VideoBERT`, `MERLOT`) vs generative (`VideoOFA`) vs LLMâ€‘aligned (`Videoâ€‘LLaMA`, `Videoâ€‘LLaVA`, `MiniGPT4â€‘Video`).
- Practical tradeâ€‘offs: temporal compute, compressed video loaders, precomputed features, temporal pooling/masking (Section 5).
- Evaluation opportunities: longâ€‘form reasoning (`EgoSchema`), action localization (`MSVDâ€‘QA`, `MSRVTTâ€‘QA`), physics understanding via synthetic probesâ€”current VLMs often do not exceed random, while humans >80% (Section 5.4).
- Challenges: temporal supervision scarcity, noun bias, compute redundancy; masking helps (Section 5.5).

## 4. Key Insights and Innovations
- A unifying lens on VLM objectives (fundamental)
  - The informationâ€‘theoretic rateâ€“distortion view (Section 2.3.3; Eq. 3â€“4) shows masking and contrastive learning are two sides of the same coin: both control how much information is retained and what â€œdistortionâ€ is acceptable (reconstruction vs discrimination). This clarifies why mixing objectives can help.

- Generative models as strong discriminative classifiers (nonâ€‘obvious capability)
  - Section 2.4.3 formalizes how to turn textâ€‘toâ€‘image models into robust classifiers via Eq. 5â€“7. This explains improved OOD/compositional robustness but also the inference cost, guiding practitioners on when to pay that cost.

- Data over compute: data pruning, diversity, and concept coverage (practical innovation)
  - The DataComp framework (Section 3.1) and conceptâ€‘frequency findings (Udandarao 2024) shift emphasis from scaling laws to the content of data. The paper operationalizes this via concrete filters (heuristics, CLIPScore/Tâ€‘MARS/Sieve), balanced sampling (`MetaCLIP`), and synthetic augmentation.

- Concrete, underâ€‘discussed pitfalls and fixes in evaluation (practical but highâ€‘impact)
  - Language priors can make â€œmultimodalâ€ benchmarks solvable by text alone (Section 4.2.3).
  - A subtle â€œequalâ€‘probability bugâ€ in binary caption selection can inflate accuracy if argmax defaults to the first option (Section 4.1.6). The paper recommends adding epsilon noise or tracking ties.

- Endâ€‘toâ€‘end training efficiency tips (practitionerâ€‘centric)
  - Emphasis on data loading (uncompressed shards, `FFCV`) and token masking to speed large models (Section 3.2.3). The compute budget sanity check (â‰¤64 GPUs, ~10k USD under good data/masking) is unusually concrete (Section 3.2.2).

## 5. Experimental Analysis
Note: This is not a primary empirical paper; it synthesizes results from many sources and highlights evaluation designs.

- Evaluation methodology it recommends or dissects (Section 4; Figure 3)
  - Captioning: avoid overâ€‘reliance on BLEU/ROUGE; consider `CLIPScore`, questionâ€‘based metrics (TIFA/DSG/VQAScore).
  - VQA: use LLMâ€‘judge scoring for openâ€‘ended answers; evaluate selective prediction (coverage vs risk).
  - Compositionality: use `Winoground` (two images/two captions) and `ARO` (hard negatives for relation/attribute/order).
  - Dense grounding: `DCI` cropâ€‘caption matching.
  - Synthetic diagnostics: `PUG` for spatial relations.
  - Bias, hallucination, memorization: use CHAIR/POPE/GPTâ€‘4â€‘based evals and dejaâ€‘vu kâ€‘NN tests.

- Representative quantitative results cited
  - CLIP zeroâ€‘shot: â€œa ResNetâ€‘101 CLIP matched â€¦ 76.2% zeroâ€‘shot classification accuracyâ€ on ImageNet and surpassed supervised baselines on robustness (Section 2.2.1).
  - LLaVAâ€‘RLHF alignment: 
    > â€œOn LLaVAâ€‘Bench, LLaVAâ€‘RLHF achieves 94% performance level of GPTâ€‘4â€¦ On MMHALâ€‘BENCHâ€¦ outperforms baselines by 60%.â€ (Section 3.5.1)
  - LLaVAR OCR gains:
    > â€œâ€¦ up to a 20% accuracy improvement on textâ€‘based VQA datasets â€¦â€ (Section 3.6).
  - MiniGPTâ€‘4 training cost:
    > â€œonly four A100 GPUs for around ten hoursâ€¦ 20k stepsâ€¦ ~5M imageâ€‘text pairsâ€ (Section 2.5.2).
  - Generative diffusion classification cost:
    > â€œâ€¦ requires hundreds or thousands of network evaluations per test image.â€ (Section 2.4.3)
  - Physics reasoning on synthetic videos:
    > â€œmodels such as VideoLLaMA or PandaGPT do not exceed random performance, whereas humans achieve more than 80% accuracy.â€ (Section 5.4)

- Do the experiments support the guidance?
  - The cited results (e.g., CLIPâ€™s zeroâ€‘shot, LLaVAâ€‘RLHFâ€™s gains, OCRâ€‘VQA improvements) illustrate the strengths and tradeâ€‘offs in each family. The evaluation sectionâ€™s careful deconstruction (e.g., language priors; binaryâ€‘tie bug) justifies shifting toward stronger diagnostics (e.g., `PUG`, `DCI`, selective prediction).

- Ablations/robustness/failure cases discussed
  - Robustness gains from generative classifiers (OOD, compositional; Section 2.4.3).
  - Failure on spatial relations (nearâ€‘chance on `PUG`; Section 4.1.8).
  - Language priors solving supposedly multimodal tests (Section 4.2.3).
  - Memorization vs. generalization distinguished by referenceâ€‘model kâ€‘NN (Section 4.4).

- Conditional tradeâ€‘offs
  - Contrastive vs masking: batch sensitivity vs decoder overhead; smallâ€‘batch friendliness (Sections 2.2â€“2.3).
  - Generative: better reasoning/interpretability vs inference cost (Section 2.4.3).
  - LLMâ€‘backbone: low training cost vs inherited hallucinations/bias (Section 3.3.4).
  - Interleaved data: natural (`OBELICS`) preserves context vs synthetic (`MMC4`) scales easily but may miss nuance (Section 3.1.3).

## 6. Limitations and Trade-offs
- Scope limitations of the paper
  - It is intentionally not exhaustive (Section 1); it curates representative methods and recipes rather than providing new algorithms or comprehensive benchmarks.

- Assumptions and constraints in approaches discussed
  - Contrastive learning assumes good negatives and large batches (Eq. 2; Section 2.2).
  - Masking assumes reconstruction objectives plus crossâ€‘modal conditioning are sufficient, but they can add computational bottlenecks and may not directly optimize for downstream tasks (Section 2.3).
  - Generative models assume reliable tokenizers and tractable likelihoods; diffusion classification is expensive (Eq. 7; Section 2.4.3).
  - LLMâ€‘backbone mapping assumes a fixed LLM interface is â€œgood enoughâ€ for vision grounding; it inherits LLM biases/hallucinations (Section 3.3.4).

- Unaddressed scenarios/edge cases
  - Many VLMs still fail at precise spatial reasoning, counting, and negation (Sections 1, 4.1.8).
  - Hallucination reduction is incomplete; current evaluations are evolving and can be gamed (Sections 4.3, 4.2.3).
  - For videos, longâ€‘temporal reasoning and motion semantics remain challenging (Section 5.5).

- Computational/data constraints
  - Training from scratch at CLIPâ€‘scale remains expensive without careful data curation and IO optimization (Sections 3.2.2â€“3.2.3).
  - Temporal redundancy in videos multiplies storage/compute; many pipelines rely on compressed streams or precomputed features (Section 5).

- Open questions
  - What is the right blend of objectives to improve grounding without hallucination?
  - How to holistically measure multimodal data quality (Section 3.1.4)?
  - How to properly evaluate and mitigate memorization in crossâ€‘modal settings (Section 4.4)?
  - How to move beyond noun bias and capture interaction/motion semantics in video (Section 5.5)?

## 7. Implications and Future Directions
- How this work changes the fieldâ€™s landscape
  - Provides a shared vocabulary (Figure 1 taxonomy; rateâ€“distortion view) and an â€œengineering playbookâ€ (Figure 2 and Section 3) to design VLMs deliberately rather than by trial and error. It also sharpens evaluation practice (Figure 3; Section 4), warning against common traps.

- Followâ€‘up research enabled or suggested
  - Objective design: Mix contrastive/masking/generative in principled ways guided by the rateâ€“distortion perspective (Section 2.3.3).
  - Generativeâ€‘discriminative fusion: Practical hybrids that get generative classifiersâ€™ robustness without prohibitive inference cost (Section 2.4.3).
  - Dataâ€‘centric VLMs: Automated, holistic multimodal data quality measures; conceptâ€‘coverage planning; interleaved data construction at scale (Sections 3.1.3â€“3.1.4).
  - Grounding and alignment: Largeâ€‘scale, highâ€‘quality pseudoâ€‘grounding pipelines (GLIPâ€‘style) plus instructionâ€‘tuning/RLHF with factual constraints (Sections 3.4â€“3.5).
  - Reliable evaluation: Benchmarks resilient to language priors, with selective prediction, dense grounding, and synthetic diagnostics (Sections 4.1â€“4.2).
  - Video VLMs: Efficient temporal modeling, better supervision for actions/motion, physicsâ€‘aware reasoning (Section 5.5).

- Practical applications and use cases
  - Retrieval, captioning, visual dialog, OCRâ€‘VQA, and document understanding (Sections 4.1.1â€“4.1.4).
  - Multimodal assistants (LLaVAâ€‘style) aligned via instruction tuning/RLHF for safer deployment (Section 3.5).
  - Content moderation and fairness auditing using bias/embedding analyses and red teaming (Sections 4.2, 4.5).
  - Video search and understanding for education, robotics, and AR, with a roadmap to handle longâ€‘context reasoning and physics (Section 5.4).

> Bottom line: If you need to build or evaluate a VLM today, use Figure 1 to pick an objective family, Section 3 to design a dataâ€‘efficient training pipeline (with grounding/alignment addâ€‘ons), and Section 4 to choose robust evaluationsâ€”keeping in mind the specific pitfalls (language priors, binary tie bug) and the tradeâ€‘offs of each method.

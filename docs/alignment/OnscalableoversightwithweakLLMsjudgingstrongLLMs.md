# On scalable oversight with weak LLMs judging strong LLMs

**ArXiv:** [2407.04622](https://arxiv.org/abs/2407.04622)

## üéØ Pitch

This paper provides the first broad, systematic benchmark of scalable oversight protocols, using weaker language models to judge stronger ones across nine diverse tasks ranging from extractive QA to mathematics and multimodal reasoning. It shows that debate between two strong LLMs enables less capable judges to supervise more accurately than one-agent consultancy and reduces the risk of amplifying incorrect outputs‚Äîbut debate's advantage over simple question answering depends on the task and type of oversight gap. These results rigorously inform the path toward safe supervision of superhuman AI, highlighting both the promise and the limits of debate as a scalable alignment solution.

---

## 1. Executive Summary
This paper benchmarks scalable oversight protocols‚Äîways for a weaker judge to supervise stronger AI systems‚Äîby running large language models (LLMs) as both agents and judges across nine tasks (extractive QA, closed QA, and multimodal). It finds that debate between two agents generally helps weak judges more than one‚Äëagent consultancy and is less likely to amplify an agent‚Äôs wrong belief, though debate does not consistently outperform direct question answering baselines.

## 2. Context and Motivation
- Problem addressed
  - How can weaker evaluators (e.g., humans or weaker LLMs) reliably supervise stronger AI systems? This is the ‚Äúscalable oversight‚Äù problem. 
  - The paper evaluates two families of protocols that might make weak oversight work:
    - `Debate`: two AIs argue opposite sides; a judge decides.
    - `Consultancy`: one AI answers and responds to judge‚Äôs questions.
- Why it matters
  - Misaligned or simply overconfident models can exploit weak oversight signals, leading to unsafe or incorrect behavior (Introduction, citing Denison et al. 2024). If oversight scales, it can reduce risks such as deceptive behavior by stronger models.
- Prior approaches and gaps
  - Most prior LLM debate studies focus on a single ‚Äúextractive reading comprehension‚Äù dataset where debaters can quote a hidden article and the judge cannot read it (information asymmetry). This setup favors debate but does not reflect other capability gaps (math, logic, coding, multimodal) that matter for real oversight.
  - Previous results are mixed: some human‚Äëdebater studies saw little benefit; LLM‚Äëbased studies showed gains mostly on that one extractive dataset (Related Work; Section 1).
- Positioning of this work
  - Extends evaluation to nine datasets covering three gap types (Figure 1, Table 1; Appendix F):
    - `Extractive QA` with information asymmetry (QuALITY, BoolQ, GPQA‚ÄëExtractive).
    - `Closed QA` without extra sources (MMLU, GSM8KQA, PrOntoQA, TruthfulQA, GPQA).
    - `Multimodal` (MMMU).
  - Compares protocols in both ‚Äúassigned‚Äërole‚Äù and ‚Äúopen‚Äërole‚Äù versions:
    - `Assigned`: systems are forced to argue a given answer (correct or incorrect).
    - `Open`: a system chooses which answer to argue for, mimicking training-time behavior.
  - Studies multiple judge strengths (Gemma 7B, GPT‚Äë3.5, Gemini Pro 1.0, Gemini Pro 1.5) while typically using the strongest model (Gemini Pro 1.5) as the agent (Section 3.3, 4.1).

## 3. Technical Approach
Step‚Äëby‚Äëstep design of the evaluation framework.

- Core entities
  - `Judge`: the weaker LLM deciding the final answer.
  - `Debaters`/`Consultant`: stronger LLM(s) producing arguments. Debaters argue opposite sides; the consultant defends a single answer.
  - `Information asymmetry`: in extractive QA tasks, debaters/consultant see the article; the judge does not (Section 3.1).
- Protocols benchmarked (Section 3.2; Figure 1)
  - Direct QA baselines (no agent help):
    - `QA without article`: judge answers from the question only (works for all tasks).
    - `QA with article`: judge sees the full article (extractive tasks only).
  - Assigned‚Äërole oversight protocols:
    - `Consultancy`: consultant is randomly assigned either the correct or incorrect answer (50/50) and tries to convince a questioning judge for several rounds.
    - `Debate`: two debaters argue opposite answers for several rounds; the judge chooses.
  - Open‚Äërole oversight protocols (more ‚Äútraining‚Äëlike‚Äù):
    - `Open consultancy`: consultant chooses which answer to defend (by matching the consultant‚Äôs own direct‚ÄëQA choice; Section 3.2.3).
    - `Open debate`: one debater‚Äîthe `protagonist`‚Äîdefends the answer that same model would pick in direct QA; the other (`antagonist`) gets the opposite side.
- Interaction mechanics
  - Rounds: main results use 3 rounds (Section 4.1); ablations also test 1 round (Figure A.2).
  - Turn style: simultaneous vs sequential (Section 3.2.2); ablated in Figure A.6.
  - Best‚Äëof‚ÄëN: for debate, four sampled arguments per turn with a model‚Äëbased selector that picks ‚Äúthe most persuasive one‚Äù (Section 4.1 footnote; Figure A.3). Unlike prior work using log‚Äëprobabilities vs a dummy argument, selection is by model judgment among the actual candidates.
  - Chain‚Äëof‚Äëthought (CoT): agents produce hidden reasoning; only their final arguments are shown to the judge (Section 3.2).
- Passage verification tool for extractive tasks (Appendix G; Figure I.1‚ÄìI.2)
  - Debaters/consultants can include `<passage>...</passage>` excerpts from the hidden article.
  - The system verifies exact string matches:
    - If verified: tags become `<v_passage>...</v_passage>`.
    - If not: tags become `<u_passage>...</u_passage>`.
  - Judges are told to trust only verified passages (Judge prompts, Appendix H.4.1/H.3.1).
- Tasks and how they‚Äôre made binary (Appendix F)
  - `QuALITY`, `BoolQ`: directly binary.
  - `GPQA-Extractive`: article is an expert explanation of the correct answer; agents must extract from it.
  - `MMLU`: binary subset formed by picking the correct option and one random wrong option.
  - `GSM8KQA`: create two executable Python solutions; the correct/incorrect answers come from code that does/doesn‚Äôt match the official answer (Appendix F.2).
  - `PrOntoQA`: construct incorrect variants by negating logical statements in the chain-of-thought (Appendix F.2).
  - `TruthfulQA`, `GPQA`: binaryized multiple‚Äëchoice.
  - `MMMU`: multimodal image+text questions; only Gemini Pro 1.5 supports it (Appendix F.3; Figure 2).
- Models and capability gaps (Section 3.3)
  - Agents: primarily `Gemini Pro 1.5` (strongest) as debaters/consultant.
  - Judges: `Gemma 7B`, `GPT‚Äë3.5`, `Gemini Pro 1.0`, `Gemini Pro 1.5`. The judge is equal or weaker than the agents.
- Measurement and statistics
  - Metric: `judge accuracy` versus ground‚Äëtruth labels (Sections 3 and 4).
  - For open‚Äërole: also measure `win‚Äërate` (how often the judge selects the side the protagonist/consultant advocated; Figure 3 top).
  - Significance tests: paired permutation tests (10,000 resamples) for accuracy differences (Appendix A; Figure A.1). Error bars are 95% CIs (Appendix D).
- Persuasiveness via Elo (Section 4.3; Appendix E)
  - Create 1‚Äëturn simultaneous debates across five debater settings (Gemma 7B, GPT‚Äë3.5, Gemini Pro 1.0, Gemini Pro 1.5, Gemini Pro 1.5 with Best‚Äëof‚Äë4).
  - Cross‚Äëplay 512 pairings and judge with Gemini Pro 1.5 to estimate an Elo skill for each debater.
  - Expected win probability uses the standard Elo formula:
    > Appendix E: expected win-rate of i against j is `1 / (1 + 10^((E_j - E_i)/500))`.
  - Compute aggregate Elo, plus separate Elo when assigned the correct or incorrect side.

## 4. Key Insights and Innovations
- Broad, capability‚Äëdiverse evaluation of scalable oversight (Figure 1; Table 1; Sections 3‚Äì4)
  - Novelty: moves beyond one reading-comprehension dataset to nine tasks covering information asymmetry (extractive), intrinsic reasoning gaps (math, logic, knowledge in closed QA), and multimodal reasoning.
  - Significance: reveals where debate helps and where it does not, which prior single‚Äëtask studies could not establish.
- Open‚Äërole analysis that probes training‚Äëtime risks (Section 4.2; Figure 3)
  - Novelty: introduces `open debate` and evaluates `open consultancy` more comprehensively. In both, agents choose which side to argue for, revealing whether oversight amplifies an agent‚Äôs own mistaken belief.
  - Insight: when the agent is wrong, open debate helps the judge resist persuasion much more than open consultancy (Figure 3 bottom).
- Clear separation of information vs reasoning assistance (Sections 4.1 and A.1)
  - Insight: In extractive tasks, `QA with article`‚Äîgiving the judge the source directly‚Äîbeats debate/consultancy for all judge models (Figure 2). This shows that when the gap is ‚Äúmissing information,‚Äù direct access helps more than mediated arguments.
- Elo‚Äëbased persuasiveness linked to judge accuracy (Section 4.3; Figure 4)
  - Finding: higher debater Elo correlates with higher judge accuracy, especially on extractive tasks (Figure 4 bottom left). This supplies preliminary evidence that as agent skill scales, debate can deliver better oversight signals.

## 5. Experimental Analysis
- Evaluation setup and datasets
  - Nine tasks √ó 128 questions each ‚âà 5 million model calls (Section 1).
  - Main config: 3 rounds, simultaneous turns, Best‚Äëof‚Äë4 for debaters with a model‚Äëbased selector; judges are zero‚Äëshot (Section 4.1). Ablations vary turns, Best‚Äëof‚ÄëN, few‚Äëshot, CoT, turn order, and debate orderings (Figures A.2‚ÄìA.8; B.2‚ÄìB.8).
- Main results across protocols (Figure 2; Appendix A)
  - Debate vs consultancy (assigned‚Äërole):
    > Figure 2 and Figure A.1: ‚ÄúDebate consistently outperforms consultancy across all tasks.‚Äù  
    This holds for extractive, closed, and multimodal task types (except that multimodal‚Äôs difference is less pronounced; see A.1).
  - Debate vs direct QA:
    - Extractive QA with asymmetry: 
      > Section 4.1 and Figure 2: Debate > QA without article; `QA with article` is best for all judges.
    - Closed QA: mixed results (Section 4.1).
      - With very weak judge (Gemma 7B): QA without article > Debate.
      - `GPT‚Äë3.5` judge: Debate > QA without article with `p = 0.037` (Figure A.1).
      - `Gemini Pro 1.0` judge: No significant difference (`p = 0.1`).
      - `Gemini Pro 1.5` judge (same strength as debaters): Debate < QA without article.
    - Multimodal: Consultancy is significantly worse; Debate ‚âà QA without article (not statistically significant; `p = 0.11`, Section 4.1).
  - Takeaway: Debate helps more when the gap is missing information (extractive) than when the gap is pure reasoning or knowledge skill (closed, multimodal).
- Open‚Äërole findings (Section 4.2; Figure 3)
  - Frequency of choosing the correct side:
    > Figure 3 caption: Protagonist/consultant chooses correctly in 88% (extractive), 84% (closed), 71% (multimodal).
  - Win‚Äërate vs accuracy trade‚Äëoff:
    > Figure 3 top: Open consultancy yields higher or equal win‚Äërate than open debate, meaning judges agree with the consultant more often.
    - But higher win‚Äërate does not always mean higher judge accuracy.
  - Crucial robustness difference when the agent is wrong:
    > Figure 3 bottom: When the consultant picks the wrong answer, judges are often convinced in open consultancy (low accuracy, light blue bars).  
    > In open debate, when the protagonist is wrong, judge accuracy is much higher (light red bars).
  - Interpretation: Open debate reduces risk of amplifying agents‚Äô mistakes compared to open consultancy.
- Ablations and robustness checks
  - Number of rounds: 1 vs 3 has no consistent effect (Figure A.2; B.2).
  - Best‚Äëof‚ÄëN sampling for debaters: little to no consistent benefit (Figure A.3; B.3).
  - Few‚Äëshot prompting for judges: no consistent gains; can cause failures for the weakest judge due to long contexts (Figure A.4; B.4 caption).
  - Judge chain‚Äëof‚Äëthought: neutral or harmful (Figure A.5; B.5). The authors hypothesize that weak judges‚Äô own CoT may distract from stronger agents‚Äô arguments.
  - Turn style: simultaneous vs sequential‚Äîno significant difference (Figure A.6; B.6).
  - Positional bias and order: judges do exhibit positional bias (Figure A.8), but evaluating both orderings does not change mean accuracy; randomizing answer position suffices (Figure A.7; B.7‚ÄìB.8).
- Elo and scaling (Section 4.3; Figure 4; Appendix E)
  - Debater Elo separates models and shows a consistent advantage when assigned the correct side (Figure 4 top and middle).
  - Judge accuracy increases with debater Elo on extractive tasks (Figure 4 bottom left). On closed tasks, the correlation is weaker.
  - Additional plots with different judges and aggregation appear in Appendix E (Figures E.1‚ÄìE.2).
- Failure analysis (Appendix C)
  - In `PrOntoQA`, most judged mistakes (11/12 reviewed) come from the judge‚Äôs own logical errors, not from irresistible deception by the dishonest debater.
  - In `TruthfulQA`, errors often stem from ambiguity that dishonest debaters exploit or from missing external facts; sometimes the judge is inattentive to key aspects.
- Overall assessment
  - The experiments are extensive and carefully controlled with statistical tests (Figure A.1) and ablations. The conclusions are well supported:
    - Debate is a better oversight protocol than consultancy when consultants might be assigned the wrong side or choose it incorrectly.
    - Debate‚Äôs benefits over direct QA are conditional: clear for information asymmetry, mixed otherwise.

## 6. Limitations and Trade-offs
- Inference‚Äëonly evaluation (Section 5: Conclusion; Limitations)
  - No training via these protocols (no self‚Äëplay or RL). Debate is judged as a one‚Äëshot inference method, which may understate its potential when trained.
- Judge fine‚Äëtuning mismatch (Section 1 and Conclusion)
  - Judges are instruction‚Äëtuned and RLHF‚Äëtrained mostly on direct QA formats, not on evaluating debates. This likely favors the direct QA baselines.
- Agent alignment and helpfulness bias
  - Agent models are RLHF‚Äëtrained for honesty; dishonest roles may be artificially constrained (Limitations; also discussed relative to prior work).
- Open debate design
  - Only one debater is truly ‚Äúopen‚Äù (the protagonist); the other is auto‚Äëassigned (Section 3.2.3). Both choosing freely might yield different dynamics (future work suggested in Section 4.2).
- Human relevance
  - LLM judges stand in for humans; real human judges may behave differently, especially at superhuman capability gaps (Limitations).
- Task scope and tools
  - The extractive setting relies on exact‚Äëmatch passage verification, which may not reflect realistic evidence checking when humans supervise (Appendix F‚ÄìG; Limitations).
- Computational costs
  - Large‚Äëscale multi‚Äëagent interactions (‚âà5M generations) imply non‚Äëtrivial runtime and cost; Best‚Äëof‚ÄëN adds sample overhead without clear benefit (Figures A.3, E.1).

## 7. Implications and Future Directions
- What changes in the field
  - The work reframes debate as most promising when the oversight gap is primarily missing information and when avoiding error amplification is critical. It tempers expectations for debate vs direct QA on reasoning‚Äëonly tasks unless judges are trained to evaluate debates.
- Research enabled or suggested
  - Train debate and consultancy agents with the judge‚Äôs signal (self‚Äëplay / RL) and measure whether judge accuracy and debater Elo co‚Äëimprove (Conclusion; Future work).
  - Fine‚Äëtune judges specifically for debate/consultancy evaluation; test whether this closes the gap vs direct QA (Section 1 caveats; Conclusion).
  - Human‚Äëin‚Äëthe‚Äëloop studies to validate LLM‚Äëjudge findings (Conclusion).
  - Explore richer scalable oversight protocols:
    - Debate with `cross‚Äëexamination` (questioning the opponent‚Äôs claims with tools), iterative amplification, or market‚Äëbased schemes (Future work; Related Work).
    - Open debate where both debaters can choose the same side (Section 4.2 suggestion).
    - Introduce tools (code execution, retrieval, multimodal tools) asymmetrically to model real weak‚Äëstrong gaps (Future work).
  - Robustness under distribution shift and against misaligned agents (Future work).
- Practical applications
  - Oversight for AI assistants that have private tool access or proprietary data: debate can reveal and stress‚Äëtest claims when the human (or weak judge) cannot directly access the source (extractive tasks illustrate this).
  - Safer preference‚Äëlearning pipelines: open debate is less likely than open consultancy to amplify an agent‚Äôs mistaken beliefs (Figure 3 bottom), which is crucial when using overseer feedback to train future systems.

> Key quantitative highlights to remember:
> - Debate > Consultancy across all task types in assigned‚Äërole settings (Figure 2; Figure A.1).
> - In extractive QA, `QA with article` > Debate > `QA without article` (Figure 2).
> - In closed QA, Debate vs `QA without article` is mixed: depends on judge strength; only GPT‚Äë3.5 shows a significant win for Debate (`p=0.037`, Figure A.1).
> - Open‚Äërole: when the agent chooses incorrectly, judges are much less misled in open debate than in open consultancy (Figure 3 bottom).
> - Stronger debaters (higher Elo) modestly increase judge accuracy, especially for extractive tasks (Figure 4 bottom).

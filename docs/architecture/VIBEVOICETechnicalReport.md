# VIBEVOICE Technical Report

**ArXiv:** [2508.19205](https://arxiv.org/abs/2508.19205)

## ğŸ¯ Pitch

VIBEVOICE introduces a groundbreaking framework for long-form, multi-speaker text-to-speech synthesis by combining a large language model with a novel, ultra-compressed, continuous speech tokenizer and an autoregressive next-token diffusion decoder. This enables the system to generate up to 90 minutes of high-fidelity, natural-sounding conversational audio with up to four distinct voices, all within a single context. By preserving realistic conversational flow and dramatically improving computational efficiency, VIBEVOICE sets a new state-of-the-art for scalable, nuanced, and engaging audio generationâ€”surpassing both open and proprietary baselines in human evaluations of preference, realism, and richness.

---

## 1. Executive Summary
VIBEVOICE is a long-form, multiâ€‘speaker textâ€‘toâ€‘speech (TTS) system that combines a large language model with a tokenâ€‘level diffusion decoder and a new ultraâ€‘compressed, causal speech tokenizer. It synthesizes up to 90 minutes of conversational audio with up to four speakers in a single run and, on subjective human evaluations, outperforms strong open and closed baselines on preference, realism, and richness (Figure 1; Table 1).

## 2. Context and Motivation
- Problem addressed
  - Generating natural, long conversational audio (e.g., podcasts, multiâ€‘narrator audiobooks) is difficult. Stitching together short utterances misses turnâ€‘taking, conversational flow, and contextâ€‘aware prosody (Section 1).
  - Existing systems either are not openâ€‘sourced or struggle with stability and length for multiâ€‘speaker conversations [Goo24, PSJ+24, Nar25, Ses25, LWI+24, ZQW+25] (Section 1).

- Why it matters
  - Practical: highâ€‘quality podcastâ€‘like generation, scripted dialogues, and long narrative content creation.
  - Scientific: tests whether largeâ€‘context sequence modeling plus continuous (rather than discrete) acoustic targets can scale to hourâ€‘long synthesis without collapsing quality.

- Prior approaches and gaps
  - Singleâ€‘speaker, shortâ€‘utterance TTS has progressed rapidly (e.g., NaturalSpeechâ€‘2, Voicebox, Seedâ€‘TTS; Section 1), but these models do not target multiâ€‘speaker, hourâ€‘scale conversation.
  - Multiâ€‘speaker longâ€‘form work has appeared recently, but faces length/stability limits or is not open (Section 1).

- Positioning of this work
  - Introduces a unified, autoregressive â€œnextâ€‘token diffusionâ€ framework (Figure 2) paired with a new continuous, causal acoustic tokenizer operating at 7.5 Hz (3200Ã— compression from 24 kHz; Section 2.1). This drastically shortens the sequence the model must generate, enabling 90â€‘minute contexts while preserving quality (Introduction; Section 2).

## 3. Technical Approach
At a high level, VIBEVOICE turns a multiâ€‘speaker script and a set of short voice prompts into a single long waveform by:
1) encoding prompts and, during generation, alreadyâ€‘synthesized audio with two tokenizers,
2) running an LLM over an interleaved sequence of role tags, text, and speech features, and
3) using a diffusion head to predict continuous acoustic latents tokenâ€‘byâ€‘token, which a decoder converts to audio (Figure 2; Section 2.2).

Key components and how they work:

- Two complementary speech tokenizers (Section 2.1)
  - Acoustic tokenizer (Ïƒâ€‘VAE)
    - Goal: compress audio into continuous latent vectors that preserve timbre, prosody, and fidelity.
    - Ïƒâ€‘VAE idea: A VAE variant where the latent variance is not learned per input; instead, the model samples Ïƒ from a fixed distribution N(0, CÏƒ). This helps avoid â€œvariance collapseâ€ when such latents are later predicted autoregressively (Section 2.1).
    - Latent sampling: z = Î¼ + Ïƒ âŠ™ Îµ, with Îµ ~ N(0, 1) and Ïƒ ~ N(0, CÏƒ).
    - Architecture: a mirrorâ€‘symmetric encoderâ€“decoder with 7 hierarchical stages. Modified Transformer blocks use 1D depthâ€‘wise causal convolutions instead of selfâ€‘attention (for streaming). Six downsampling layers give an overall 3200Ã— reduction from 24 kHz to 7.5 tokens per second (Section 2.1).
    - Scale: each encoder/decoder is ~340M parameters. Training uses DACâ€‘style adversarial/discriminator losses for reconstruction quality [KSL+23] (Section 2.1).
  - Semantic tokenizer
    - Goal: encode content (what is being said) deterministically.
    - Architecture mirrors the acoustic encoder, but without the VAE. It is trained via an ASR proxy task: the encoderâ€™s output is decoded by a Transformer to predict transcripts; the decoder is discarded after preâ€‘training (Section 2.1).

- Input representation and sequence modeling (Section 2.2)
  - The model concatenates, as a single sequence, perâ€‘speaker voice prompts and text scripts, each prefixed with role identifiers:
    - X = [Speaker1: z1, â€¦, SpeakerN: zN] + [Speaker1: T1, â€¦, SpeakerN: TN]
    - `zk` are acoustic latent features (voice â€œfontâ€ from prompts). `Tk` are perâ€‘speaker text.
  - During generation, produced speech segments are encoded on the fly by both tokenizers to form a hybrid (acoustic + semantic) context for future steps (Section 2.2).
  - LLM backbone: Qwen2.5 at 1.5B and 7B parameters (Section 2.2).
  - Training strategy: tokenizers are frozen; only the LLM and a small diffusion head are trained. Input sequence length is increased via curriculum from 4,096 to 65,536 tokens to teach longâ€‘context handling (Section 2.2).
  - Important ratio: the 7.5 Hz acoustic tokens produce a speechâ€‘toâ€‘text token ratio of about 2:1 (two speech tokens per BPE text token), which keeps long sequences computationally tractable (Section 1).

- Tokenâ€‘level nextâ€‘token diffusion for acoustic prediction (Section 2.2)
  - What is â€œdiffusionâ€? A generative process that learns to reverse a gradual noising process; at inference, iterative denoising turns noise into a sample.
  - Here, the diffusion head (4 layers) predicts the acoustic VAE vector `za,i` for the next token i, conditioned on the LLM hidden state `hi` at that position (Figure 2; Section 2.2).
  - Training objective: predict the noise added to clean acoustic tokens (â€œnoiseâ€‘predictionâ€ training; [HJA20]).
  - Inference sampler: DPMâ€‘Solver++ for fast guided sampling in ~10 steps (Section 2.2).
  - Guidance: Classifierâ€‘Free Guidance (CFG) with scale 1.3 blends conditional (uses `hi`) and unconditional predictions for stronger alignment (Section 2.2).
  - Streaming: Because prediction happens tokenâ€‘byâ€‘token at 7.5 Hz, synthesis proceeds causally and scales to very long durations.

- Why these design choices?
  - Ultraâ€‘low token rate (7.5 Hz) drastically shortens the sequence the LLM must handle. This is essential for hourâ€‘long generation within a 64K token context (Introduction; Section 2.1).
  - Continuous latents + diffusion avoid discretization bottlenecks and can better capture prosody/timbre than discrete codebooks at such high compression rates (Section 2.1 and Table 3).
  - Two tokenizers separate content from acoustics, which the authors report helps longâ€‘form generation stability (Section 2.1).

## 4. Key Insights and Innovations
- Ultraâ€‘compressed, causal acoustic tokenizer at 7.5 Hz
  - Novelty: token rate of 7.5 frames/s (3200Ã— compression) while maintaining high perceptual quality (Section 2.1; Table 3).
  - Why it matters: longâ€‘form generation becomes feasible because the LLM sees far fewer acoustic tokens; the paper reports a speechâ€‘toâ€‘text token ratio near 2:1 (Section 1).
  - Evidence: Table 3 shows PESQ 3.068 and UTMOS 4.181 on LibriTTS testâ€‘clean at only 7.5 tokens/s, outperforming higherâ€‘rate systems in these metrics.

- Nextâ€‘token diffusion conditioned on LLM hidden states
  - Novelty: instead of predicting discrete acoustic tokens, the model predicts continuous VAE features with a small diffusion head per token (Section 2.2; Figure 2).
  - Why it matters: continuous targets avoid vectorâ€‘quantization errors and enable rich prosody at extreme compression, with efficient 10â€‘step DPMâ€‘Solver++ sampling (Section 2.2).

- Unified longâ€‘context, multiâ€‘speaker sequence modeling
  - Novelty: interleave perâ€‘speaker text and voiceâ€‘prompt features into one sequence the LLM processes (Section 2.2), rather than using separate pipelines.
  - Why it matters: simplifies architecture and allows contentâ€‘aware turnâ€‘taking and voice consistency across up to four speakers over 90 minutes (Introduction; Figure 2).

- Empirical scaling from 1.5B to 7B LLM
  - Insight: Larger LLM improves perceptual quality (richer timbre, more natural intonation) while keeping WER competitive (Section 1; Table 1).

## 5. Experimental Analysis
- Evaluation setup
  - Longâ€‘form podcast scenario (Section 3.1)
    - Data: 8 long conversational transcripts totaling ~1 hour; speech prompts ensure consistent timbre across models. For Geminiâ€‘2.5â€‘Proâ€‘Previewâ€‘TTS, default male/female voices are used because it does not accept speech prompts (Section 3.1).
    - Human evaluation: 24 annotators rate 6 models on Mean Opinion Score (MOS) for Realism, Richness, and Preference (Section 3.1).
    - Objective metrics: Word Error Rate (WER) using Whisperâ€‘largeâ€‘v3 and Nemo ASR; speaker similarity (SIM) via WavLMâ€‘large embeddings (Section 3.1).
  - Shortâ€‘utterance benchmarks (SEED; Section 3.2)
    - Data: ~1,000 English (CommonVoice) and ~2,000 Chinese samples (testâ€‘en/testâ€‘zh); metrics are WER (Whisperâ€‘largeâ€‘v3 for English, Paraformer for Chinese) and SIM (WavLMâ€‘large).
  - Tokenizer reconstruction (Section 3.3; Table 3)
    - Datasets: LibriTTS testâ€‘clean/testâ€‘other.
    - Metrics:
      - PESQ: signalâ€‘based perceptual quality score; higher is better.
      - STOI: objective intelligibility; higher is better.
      - UTMOS: neural MOS predictor; higher is better.

- Main quantitative results
  - Longâ€‘form conversation (Table 1)
    - Subjective MOS (higher is better):
      - VIBEVOICEâ€‘7B: Realism 3.71 Â± 0.98, Richness 3.81 Â± 0.87, Preference 3.75 Â± 0.94; Average 3.76 Â± 0.93.
      - VIBEVOICEâ€‘1.5B: Realism 3.59 Â± 0.95, Richness 3.59 Â± 1.01, Preference 3.44 Â± 0.92; Average 3.54 Â± 0.96.
      - Strong baselines: Geminiâ€‘2.5â€‘Proâ€‘Previewâ€‘TTS Average 3.66 Â± 1.16; Elevenlabs v3 alpha Average 3.40 Â± 1.09; others lower (Table 1).
      - Quote:
        > VIBEVOICEâ€‘7B achieves the highest subjective scores across all three dimensions among compared systems (Table 1).
    - Objective:
      - WER (lower is better): VIBEVOICEâ€‘1.5B 1.11 (Whisper) / 1.82 (Nemo); VIBEVOICEâ€‘7B 1.29 / 1.95.
      - SIM (higher is better): VIBEVOICEâ€‘7B 0.692; VIBEVOICEâ€‘1.5B 0.548. Many open baselines have higher WER and lower SIM (Table 1).
  - Shortâ€‘utterance (SEED; Table 2)
    - VIBEVOICEâ€‘1.5B (7.5 Hz): testâ€‘zh CER 1.16% (SIM 0.744); testâ€‘en WER 3.04% (SIM 0.689).
    - Some specialized shortâ€‘utterance systems achieve lower testâ€‘en WER (e.g., CosyVoiceâ€‘2 at 2.57%), but VIBEVOICE remains competitive despite being trained for longâ€‘form and operating at a much lower frame rate (Table 2).
  - Tokenizer reconstruction (Table 3)
    - At 7.5 tokens/s, the acoustic tokenizer attains PESQ 3.068 and UTMOS 4.181 on testâ€‘clean and PESQ 2.848 and UTMOS 3.724 on testâ€‘otherâ€”best among listed models on PESQ and UTMOS despite the lowest token rate. STOI is lower than some higherâ€‘rate codecs (e.g., WavTokenizer 0.914 vs 0.828), reflecting a tradeâ€‘off between extreme compression and intelligibility proxy scores.
    - Quote:
      > â€œOurs (Acoustic) 1 quantizer at 7.5 tokens/sâ€ achieves the top PESQ and UTMOS on both testâ€‘clean and testâ€‘other (Table 3).

- Do the experiments support the claims?
  - Longâ€‘form conversational superiority is supported: VIBEVOICEâ€‘7Bâ€™s MOS scores lead across all subjective dimensions in Table 1. Objective WER is also very low (â‰¤1.95), and speaker similarity is competitive/high (0.692).
  - Shortâ€‘utterance generalization is decent but not stateâ€‘ofâ€‘theâ€‘art on English WER; nevertheless, this is notable given the ultraâ€‘low frame rate and longâ€‘form focus (Table 2).
  - Tokenizer results substantiate the feasibility of 7.5 Hz compression without catastrophic loss of perceived quality (Table 3).

- Caveats and robustness
  - Subjective test set is compact (8 scripts totaling ~1 hour) and uses 24 annotators (Section 3.1). While thorough for long audio (~6 hours of listening per annotator), broader genres and languages are not included.
  - One baseline (Gemini) could not use voice prompts, potentially underrepresenting its speakerâ€‘matching ability (Section 3.1).
  - No ablations isolating the impact of the semantic tokenizer, CFG scale, sampler steps, or curriculum are reported. The 1.5B vs 7B comparison indicates scaling helps, but the specific contribution of each design choice remains unquantified.

## 6. Limitations and Trade-offs
- Stated limitations (Section 4)
  - Language coverage: English and Chinese only. Other languages may produce â€œunexpected audio outputs.â€
  - Nonâ€‘speech audio: background sounds, music, and sound effects are not modeled.
  - Overlapping speech: simultaneous multiâ€‘speaker overlap is not explicitly handled; turns are sequential.
  - Responsible use: potential misuse for impersonation and disinformation; the model is released for research only.

- Architectural and computational tradeâ€‘offs
  - While 7.5 Hz greatly reduces sequence length, STOI drops compared to some higherâ€‘rate tokenizers (Table 3). This implies a tradeâ€‘off: extreme compression favors scalability but can impact intelligibility proxies.
  - Inference still runs an iterative diffusion process per acoustic token; although sampling is only ~10 steps, very long outputs still accumulate latency.
  - The system relies on a sizable LLM (up to 7B) and a long context window (up to 65K tokens during training), implying significant training/inference memory requirements for long dialogues.

- Evaluation coverage
  - Longâ€‘form tests involve curated scripts and controlled voice prompts; spontaneous conversations, noisy environments, and codeâ€‘switching are not evaluated.
  - Objective intelligibility and alignment are measured via ASR WER, which can confound evaluation if ASR itself is biased by speaking style or prosody.

- Open questions
  - How much do the semantic tokenizer and hybrid conditioning improve longâ€‘form stability versus acousticâ€‘only conditioning?
  - What is the failure behavior at or beyond the 90â€‘minute mark (e.g., drift, voice leakage across speakers)?
  - How sensitive is quality to CFG scale, number of diffusion steps, and speakerâ€‘turn density?

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that extreme acoustic compression (7.5 Hz) plus tokenâ€‘level diffusion can carry longâ€‘form, multiâ€‘speaker generation without sacrificing perceptual quality (Figure 1; Table 3). This reframes longâ€‘form TTS as a tractable longâ€‘context sequence modeling problem rather than a stitching/concatenation problem.
  - Suggests continuous latents predicted via diffusion are a strong alternative to discrete acoustic tokens for largeâ€‘context speech LMs.

- Followâ€‘up research enabled or suggested
  - Overlap modeling: introduce mechanisms for controlled, simultaneous speech (e.g., multiâ€‘stream diffusion or maskâ€‘based generation).
  - Crossâ€‘lingual and codeâ€‘switching: extend tokenizers and training data to additional languages; test robustness in multilingual dialogues.
  - Ablations and interpretability: quantify the contributions of the semantic tokenizer, CFG, and sampler; analyze how the LLM manages speaker identity and turnâ€‘taking.
  - Efficiency: explore distillation or fewer diffusion steps; test nonâ€‘iterative decoders conditioned on LLM states while preserving longâ€‘form stability.
  - Safety and watermarking: develop builtâ€‘in safeguards (e.g., watermarking in acoustic latents) to mitigate misuse.

- Practical applications
  - Scripted podcast and audio drama production with consistent voices and controlled turnâ€‘taking.
  - Multiâ€‘narrator audiobooks and educational content.
  - Dialogue prototyping for games and virtual agents.
  - Given current limitations, deployments should avoid background sound requirements, overlapping speech, and unsupported languages, and include safety filters and human review (Section 4).

Quote highlights anchoring claims:
- â€œVIBEVOICE can synthesize longâ€‘form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakersâ€ (Introduction; Figure 2).
- â€œWeâ€¦ developed a causal speech tokenizer that achieves a 3200Ã— compression rate (i.e., 7.5 Hz frame rate)â€¦ [with] a speechâ€‘toâ€‘text token ratio of approximately 2:1â€ (Introduction; Section 2.1).
- â€œVIBEVOICEâ€‘7Bâ€¦ 3.71 Realism, 3.81 Richness, 3.75 Preferenceâ€¦ outperforming strong open/closedâ€‘source systemsâ€ (Table 1; Figure 1).
- â€œOurs (Acoustic) â€¦ 7.5 tokens/s â€¦ PESQ 3.068, UTMOS 4.181 (testâ€‘clean)â€ (Table 3).

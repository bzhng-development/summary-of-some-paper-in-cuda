# Scalable Diffusion Models with Transformers

**ArXiv:** [2212.09748](https://arxiv.org/abs/2212.09748)

## ğŸ¯ Pitch

This paper introduces Diffusion Transformers (DiTs), a pure transformer-based backbone for diffusion models that replaces the ubiquitous U-Net architecture and operates efficiently in the latent space of a VAE. By scaling model Gflops through depth, width, or increased token count, DiTs achieve state-of-the-art image synthesis performance on ImageNet benchmarks with substantially lower computational cost. This breakthrough demonstrates that diffusion models can fully leverage transformer architectures, setting the stage for unified, scalable generative modeling across domains and unlocking new avenues for efficiency and cross-domain research.

---

## 1. Executive Summary
This paper replaces the standard Uâ€‘Net backbone in diffusion models with a pure transformer, called the Diffusion Transformer (`DiT`), and shows that it scales predictably and efficiently when trained in the latent space of a VAE. By increasing the transformerâ€™s forward computation (Gflops) through depth/width or more tokens, `DiT` achieves stateâ€‘ofâ€‘theâ€‘art FID on classâ€‘conditional ImageNet at 256Ã—256 (FID 2.27) and 512Ã—512 (FID 3.04), while using far less compute than pixelâ€‘space Uâ€‘Nets (Tables 2â€“3).

## 2. Context and Motivation
- Problem/gap:
  - Diffusion models have advanced image synthesis, but almost all highâ€‘performing variants still use Uâ€‘Net backbones (Section 1). This limits crossâ€‘domain architectural unification and inherits convolutional inductive biases that may not be necessary.
  - Prior architectural analyses often rely on parameter counts, which poorly capture complexity for image models where resolution and sequence length dominate compute (Section 2 â€œArchitecture complexityâ€).
- Importance:
  - Transformers have shown superior scalability in language and vision (Section 2). If diffusion models could also use transformers effectively, they would benefit from shared training recipes, robustness, and clearer scaling laws across domains.
- Prior approaches and shortcomings:
  - Pixelâ€‘space DDPMs based on Uâ€‘Nets (e.g., ADM, Section 2) are computeâ€‘heavy at high resolution. Latent Diffusion Models (`LDMs`) reduce compute by running diffusion in a learned latent space, yet still rely on Uâ€‘Nets (Section 3.1 â€œLatent diffusion modelsâ€).
  - Conditioning in diffusion has leaned on crossâ€‘attention or adaptive normalization in Uâ€‘Nets; it is unclear which conditioning mechanisms best suit transformers (Figure 3, Section 3.2).
- Positioning:
  - This work introduces a transformerâ€‘only backbone (`DiT`) trained in VAE latent space, systematically explores conditioning mechanisms, and studies scaling through forward pass Gflops rather than only parameters. It provides a practical, computeâ€‘efficient alternative to Uâ€‘Nets with strong empirical scaling laws (Figures 6â€“9).

## 3. Technical Approach
Highâ€‘level pipeline
- Twoâ€‘stage latent diffusion setup (Section 3.1 â€œLatent diffusion modelsâ€):
  1) A pretrained VAE encodes images `x` (e.g., 256Ã—256Ã—3) into latents `z = E(x)` of size 32Ã—32Ã—4 (downsample factor 8).  
  2) The diffusion model operates on latents `z`; after sampling a new latent, the VAE decodes it back to an image `x = D(z)`.
- Diffusion training objective (Section 3.1 â€œDiffusion formulationâ€):
  - Forward noising: sample time `t`, draw noise `Îµ ~ N(0, I)`, and produce `x_t = sqrt(Î±Ì„_t) x_0 + sqrt(1 âˆ’ Î±Ì„_t) Îµ`.
  - Model learns the reverse process by predicting the noise `Îµ_Î¸(x_t, t, c)` (class label `c` is optional), trained with meanâ€‘squared error `||Îµ_Î¸ âˆ’ Îµ||^2` plus a KL term to learn diagonal covariance `Î£_Î¸` (ADM parameterization).
- Classifierâ€‘free guidance at sampling (Section 3.1 â€œClassifierâ€‘free guidanceâ€):
  - During training, randomly drop the condition `c` to learn an unconditional â€œnullâ€ embedding.
  - At sampling, compute a guided score `ÎµÌ‚_Î¸ = Îµ_Î¸(x_t, âˆ…) + s Â· (Îµ_Î¸(x_t, c) âˆ’ Îµ_Î¸(x_t, âˆ…))`, where `s > 1` controls strength.

`DiT` architecture (Sections 3.2 and Figure 3)
- Inputs as tokens (â€œPatchify,â€ Figure 4):
  - Patchify the noised latent `x_t` of shape `IÃ—IÃ—C` into a sequence of `T = (I/p)^2` tokens with hidden size `d`.  
  - `p` is patch size; smaller `p` â†’ more tokens â†’ higher Gflops; parameters barely change.
  - Add sineâ€‘cosine positional embeddings to the tokens.
- Transformer backbone (Table 1; model sizes S, B, L, XL):
  - Standard ViTâ€‘style stack: `N` blocks with multiâ€‘head selfâ€‘attention and MLP.
  - Conditioning enters via normalization layers (explained below).
- Conditioning mechanisms compared (Figure 3 right; Section 3.2 â€œDiT block designâ€):
  - Inâ€‘context tokens: append embeddings of time `t` and class `c` to the token sequence.
  - Crossâ€‘attention: process `t` and `c` as a separate 2â€‘token memory, add a crossâ€‘attention layer.
  - `adaLN` (adaptive LayerNorm): replace each blockâ€™s LayerNorm scale/shift (`Î³, Î²`) with outputs of an MLP driven by the sum of `t` and `c` embeddings.
  - `adaLNâ€‘Zero`: like `adaLN`, but also predict perâ€‘residual scaling factors `Î±` that are initialized to zero; this initializes each block as the identity, improving stability.
- Why `adaLNâ€‘Zero`?
  - It injects conditioning everywhere through normalization, adds negligible compute, and starts blocks as â€œdo nothing,â€ which stabilizes learning at scaleâ€”an idea inspired by residual network initialization (Section 3.2; Figure 5 shows its empirical dominance).
- Output head (â€œTransformer decoder,â€ Section 3.2):
  - Apply a final (adaptive) LayerNorm, then a linear layer maps each token to `pÃ—pÃ—2C` values (for both noise and diagonal covariance). Reshape back to the `IÃ—IÃ—C` grid.

Training and implementation (Section 4; Table 4; Appendix A)
- Dataset: ImageNet, classâ€‘conditional at 256Ã—256 and 512Ã—512.
- Optimizer/schedule: AdamW, constant learning rate 1eâ€‘4, batch 256, no weight decay, horizontal flip augmentation, EMA 0.9999, identical hyperparameters across all DiT variants.
- Diffusion schedule: Linear variance with `t_max = 1000`, same as ADM.
- VAE: pretrained Stable Diffusion VAE (downsample 8; 84M parameters, excluded from DiT parameter counts per Table 4 note).
- Hardware: JAX on TPUâ€‘v3 pods; `DiTâ€‘XL/2` trains at ~5.7 it/s on a v3â€‘256 pod (Section 4 â€œComputeâ€).

Stepâ€‘byâ€‘step generative process (sampling)
1) Sample latent noise `x_T ~ N(0, I)`.
2) For `t = T â€¦ 1`, run the transformer on the patchified `x_t` with `t` and (optionally) class `c` using `adaLNâ€‘Zero`.
3) Obtain predicted `Îµ_Î¸` and `Î£_Î¸`, optionally apply classifierâ€‘free guidance (`s`), and sample `x_{tâˆ’1}` from the learned Gaussian reverse process.
4) Decode final latent `z = x_0` to an image using the VAE decoder.

Why this approach over alternatives
- Replacing Uâ€‘Nets with a ViTâ€‘style backbone makes diffusion compatible with the transformer scaling toolkit, including tokenized inputs and compute that grows with sequence length instead of spatial convolutions (Sections 1 and 3.2).
- Conditioning via `adaLNâ€‘Zero` is more computeâ€‘efficient than crossâ€‘attention and empirically better than inâ€‘context tokens (Figure 5).
- Training in latent space keeps compute manageable while preserving image quality (Figure 2 right; Table 6).

## 4. Key Insights and Innovations
1) A pure transformer backbone for diffusion that scales cleanly in latent space
   - Whatâ€™s new: A ViTâ€‘style stack over latent patches replaces Uâ€‘Nets without specialized convolutional structure (Figure 3 left).
   - Why it matters: It demonstrates that the Uâ€‘Net inductive bias is not essential for highâ€‘quality diffusion; transformers can be competitive and easier to scale (Section 1; Figure 2 right).

2) `adaLNâ€‘Zero`: a simple, global conditioning method that initializes blocks as identity
   - Whatâ€™s new: Adaptive LayerNorm with an extra learnable residual scale `Î±` initialized to zero, so each transformer block starts as an identity mapping (Section 3.2).
   - Impact: It consistently yields the best FID during training while adding negligible compute, outperforming crossâ€‘attention and inâ€‘context conditioning (Figure 5).

3) Computeâ€‘centric scaling law: forward Gflops strongly predict quality, more than parameter count
   - Evidence: Across 12 models (S/B/L/XL Ã— patch sizes 8/4/2), FID improves monotonically with transformer Gflops; correlation âˆ’0.93 at 400K steps (Figure 8). Holding parameters roughly fixed and increasing tokens (smaller patch size) significantly improves FID (Figure 6 bottom).
   - Significance: It reframes architectural scaling for diffusion around forward compute, not just parameter counts.

4) Stateâ€‘ofâ€‘theâ€‘art ImageNet results with better compute efficiency than pixelâ€‘space Uâ€‘Nets
   - Results:
     - 256Ã—256: `DiTâ€‘XL/2â€‘G (cfg=1.50)` achieves FID 2.27, surpassing `LDMâ€‘4â€‘G` (FID 3.60) and StyleGANâ€‘XL (FID 2.30; Table 2).  
     - 512Ã—512: `DiTâ€‘XL/2â€‘G (cfg=1.50)` achieves FID 3.04, improving over ADMâ€™s best 3.85 (Table 3).
   - Compute comparison: At 512Ã—512, `DiTâ€‘XL/2` uses 524.6 Gflops vs ADM 1983 Gflops and ADMâ€‘U 2813 Gflops (Table 3; Section 5.1).

5) More sampling steps cannot compensate for insufficient model compute
   - Evidence: Even with 5Ã— higher sampling compute, a smaller model (e.g., `L/2` with 1000 steps) trails a larger model (`XL/2` with 128 steps) in FIDâ€‘10K (25.9 vs 23.7; Figure 10).
   - Takeaway: Invest in model compute (Gflops) over just increasing sampling iterations.

## 5. Experimental Analysis
Evaluation setup
- Datasets:
  - ImageNet, classâ€‘conditional, at 256Ã—256 and 512Ã—512 (Section 4).
- Metrics: FIDâ€‘50K (main), sFID, Inception Score (IS), Precision/Recall (Sections 4 and 5.1). FID computed with 250 DDPM steps via ADMâ€™s TensorFlow evaluator to ensure comparability (Section 4).
- Baselines:
  - Uâ€‘Net diffusion: ADM, ADMâ€‘U, ADMâ€‘G; latent Uâ€‘Net: LDMâ€‘4/8 (Tables 2â€“3).
  - GANs: BigGANâ€‘deep; StyleGANâ€‘XL (Tables 2â€“3).
- Model grid and compute:
  - 12 DiT variants: S/B/L/XL Ã— patch sizes 8/4/2 (Figure 6; Table 4).  
  - Gflops range from 0.36 to 118.64 at 256Ã—256; 524.6 at 512Ã—512 (Table 4).
  - Training hyperparameters kept constant across all variants (Section 4).

Main quantitative results
- Scaling trends:
  - â€œIncreasing transformer sizeâ€ (depth/width) at fixed patch size uniformly reduces FID across training (Figure 6 top).  
  - â€œDecreasing patch sizeâ€ (more tokens) at fixed model size also reduces FID (Figure 6 bottom).
  - Forward Gflops vs FID shows a strong negative correlation: âˆ’0.93 (Figure 8).
  - Larger models are more computeâ€‘efficient when plotting FID vs training compute (Figure 9).
- Stateâ€‘ofâ€‘theâ€‘art on ImageNet:
  - 256Ã—256 (Table 2):
    > `DiTâ€‘XL/2â€‘G (cfg=1.50)`: FID 2.27; IS 278.24; Precision 0.83; Recall 0.57.  
    > `LDMâ€‘4â€‘G (cfg=1.50)`: FID 3.60; IS 247.67; Precision 0.87; Recall 0.48.  
    `DiT` beats prior diffusion models and matches/approaches leading GANs on FID while offering higher recall than LDM variants at tested guidance scales.
  - 512Ã—512 (Table 3):
    > `DiTâ€‘XL/2â€‘G (cfg=1.50)`: FID 3.04; IS 240.82; Precision 0.84; Recall 0.54.  
    Improves upon ADM best FID 3.85 with far less compute (524.6 vs 1983â€“2813 Gflops; Table 3 and Section 5.1).
- Conditioning ablations (Figure 5):
  > `adaLNâ€‘Zero` consistently outperforms inâ€‘context tokens and crossâ€‘attention across training at similar or lower compute.
- Additional metrics and loss curves:
  - The computeâ€‘centric scaling trend extends to sFID, IS, Precision, and Recall (Figure 12).  
  - Larger models achieve lower training loss faster and settle at better optima (Figure 13).
- Sampling compute vs model compute (Figure 10):
  > Scaling sampling steps cannot close the FID gap to larger models; invest in model Gflops.
- VAE decoder ablation (Table 5):
  > Different pretrained decoders (original, ftâ€‘MSE, ftâ€‘EMA) lead to very similar results; final SOTA numbers use ftâ€‘EMA.

Qualitative results
- Visual samples improve with more Gflopsâ€”via larger backbones or more tokensâ€”holding everything else fixed (Figure 7).
- Highâ€‘quality, diverse classâ€‘conditional samples at 256Ã—256 and 512Ã—512 (Figure 1; Figures 11, 14â€“33 show uncurated grids across guidance scales).

Do the experiments support the claims?
- The paperâ€™s central claimsâ€”transformers can replace Uâ€‘Nets in diffusion, scale well via forward compute, and achieve SOTA with better compute efficiencyâ€”are supported by:
  - A systematic model/patch grid (12 variants) with consistent training settings (Figure 6; Table 4).
  - Clear compute analyses (Figures 8â€“10).
  - Strong benchmarks at two resolutions (Tables 2â€“3).
- Robustness and checks:
  - Conditioning ablations (Figure 5), multiâ€‘metric scaling (Figure 12), training loss trends (Figure 13), and VAE decoder swaps (Table 5) all point in the same direction.

## 6. Limitations and Trade-offs
- Reliance on latent space:
  - Results depend on a pretrained VAE; artifacts or biases from compression can cap ultimate fidelity and affect semantic alignment. Pixelâ€‘space DiT is not explored (Section 3.1, â€œcould be applied to pixel space without modificationâ€ but untested).
- Scope of conditioning:
  - Experiments are classâ€‘conditional. Textâ€‘conditional setups (which often use crossâ€‘attention) are only suggested as future work (Conclusion). The finding that `adaLNâ€‘Zero` beats crossâ€‘attention may not transfer unchanged to text prompts.
- Compute and hardware:
  - Although computeâ€‘efficient relative to pixelâ€‘space Uâ€‘Nets, top models are still expensive (e.g., `DiTâ€‘XL/2` at 524.6 Gflops for 512Ã—512; Table 4) and trained on TPU v3â€‘256 (Section 4). Memory footprint and wallâ€‘clock cost remain high.
- Fair accounting:
  - Reported DiT parameter and Flop counts exclude the VAE (84M params; Table 4 note). Endâ€‘toâ€‘end costs in practical deployments should include VAE encode/decode overhead.
- Limited resolutions and modalities:
  - Experiments cover 256Ã—256 and 512Ã—512 images on ImageNet. Generalization to higher resolutions, other datasets, video, audio, or 3D is not evaluated.
- Diversity vs fidelity tradeâ€‘offs:
  - As with classifierâ€‘free guidance in general, higher guidance scales increase fidelity but can reduce diversity (Appendix B; Figures 14â€“33).

## 7. Implications and Future Directions
- Field impact:
  - Establishes transformers as a viable, scalable backbone for diffusion, aligning image generation with the broader transformer ecosystem. The computeâ€‘centric scaling perspective (Gflops over parameters) provides a clearer design dial for future models (Figures 6â€“9, 12).
- Followâ€‘up research enabled:
  - Textâ€‘toâ€‘image DiT: Substitute labels with text embeddings; revisit conditioning (e.g., combine `adaLNâ€‘Zero` with crossâ€‘attention for language tokens). The Conclusion explicitly suggests `DiT` as a dropâ€‘in for DALLÂ·E 2 or Stable Diffusion.
  - Tokenization strategies: Explore dynamic patch sizes, learned tokenization, or hybrid early convolutions to reduce sequence length while retaining global modeling.
  - Pixelâ€‘space DiT: Test whether the compute scaling law holds without a VAE, possibly with improved training tricks or multiâ€‘grid tokenization.
  - Scaling laws: Extend the Gflopsâ€‘vsâ€‘quality analysis to more datasets, resolutions, and modalities; build predictive models of training compute vs performance (Figure 9 hints at compute efficiency crossâ€‘overs).
  - Efficient inference: Since more sampling steps cannot replace model compute (Figure 10), focus on architectural efficiency and distillation to speed sampling without quality loss.
- Practical applications:
  - Highâ€‘fidelity, classâ€‘conditional generation under tighter compute budgets than pixelâ€‘space Uâ€‘Nets (e.g., dataset augmentation, creative tools).
  - A unified architecture for multimodal generative modeling where transformers dominate, simplifying system integration and scaling strategies across tasks.

In short, `DiT` demonstrates that a pure transformer, equipped with a simple `adaLNâ€‘Zero` conditioning mechanism and trained in latent space, can deliver stateâ€‘ofâ€‘theâ€‘art diffusion performance with clear and favorable scaling behavior tied to forward compute. The work opens a practical path to transformerâ€‘native, computeâ€‘aware generative modeling across modalities.

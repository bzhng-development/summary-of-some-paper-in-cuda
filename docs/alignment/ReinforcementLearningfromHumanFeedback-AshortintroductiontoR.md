# Reinforcement Learning from Human Feedback: A short introduction to RLHF and post-training focused on language models.

**ArXiv:** [2504.12501](https://arxiv.org/abs/2504.12501)

## ğŸ¯ Pitch

This comprehensive book unifies the scattered landscape of Reinforcement Learning from Human Feedback (RLHF) and modern post-training for large language models (LLMs) into a single, practical blueprint. By meticulously guiding readers through data collection, reward modeling, diverse optimization algorithms (PPO, DPO, GRPO), regularization, evaluation, and advanced reasoning RL, it empowers practitioners to build, debug, and align powerful LLMs for real-world use. As RLHF and post-training have become essential for transforming pretrained language models into effective assistants, this hands-on reference fills a critical gap, enabling both newcomers and experts to master the art and science of LLM alignment.

---

## 1. Executive Summary (2â€“3 sentences)
This work is a book-length, practiceâ€‘oriented synthesis of Reinforcement Learning from Human Feedback (RLHF) and modern â€œpostâ€‘trainingâ€ for large language models (LLMs). It turns scattered research and lab knowâ€‘how into a single, endâ€‘toâ€‘end blueprintâ€”covering data collection, reward modeling, optimization (PPO/GRPO/REINFORCE), direct alignment (DPO), regularization, rejection sampling, reasoningâ€‘focused RL with verifiable rewards (RLVR), evaluation, and failure modesâ€”so practitioners can actually build, debug, and improve aligned LLMs.

## 2. Context and Motivation
- Problem addressed
  - There has been no canonical, handsâ€‘on reference that explains how the pieces of RLHF and postâ€‘training fit together operationally. The literature spans preferences, reward models, PPO variants, DPO, data vendors, AIâ€‘asâ€‘aâ€‘judge, and now reasoningâ€‘centric RLâ€”each with moving best practices. The bookâ€™s stated purpose is to â€œgive a gentle introduction to the core methodsâ€ and â€œdetail key decisions and basic implementation examplesâ€ across the entire pipeline (Abstract; Chapter 1).
- Why it matters
  - RLHF/postâ€‘training is now the default path from base LLMs to useful assistants. The book argues these stagesâ€”Instruction/Supervised Finetuning (SFT), Preference Finetuning (PreFT), and Reinforcement Finetuning (RFT)â€”are what turn raw models into real products (Section 1; Figure 1 and Figure 4). Without them, models tend to be verbose autocompleters rather than grounded assistants (Section 1.1â€™s contrast between preâ€‘trained Llamaâ€‘3.1â€‘405B continuation vs. postâ€‘trained TÃ¼lu 3 style answer).
- Prior approaches and gaps
  - Early RLHF recipes (InstructGPT, Sparrow, WebGPT) established the 3â€‘stage pipeline: SFT â†’ Reward Model (RM) â†’ RL optimization (Section 4.2.1; Figure 4). But later practice diversified (e.g., DPO, CAI/RLAIF, GRPO, RLVR), and concrete, endâ€‘toâ€‘end â€œhowâ€‘toâ€ guidance fell behind. The book fills that gap with math, code snippets, and implementation choices (e.g., Eq. 8, Eq. 11â€“13, Eq. 46â€“47, Figure 13, Figure 14).
- Positioning relative to existing work
  - Rather than proposing one new algorithm, this is a systematization of the fieldâ€™s â€œstable coreâ€ with modern recipes:
    - Canonical threeâ€‘step RLHF (Figure 4).
    - A contemporary, multiâ€‘round postâ€‘training recipe (TÃ¼lu 3, Figure 6) and a reasoning RL recipe (DeepSeek R1, Section 4.2.3).
    - A unification of rewardâ€‘learning variants (standard RMs vs. outcome/process RMs, Table 3) and of optimization choices (PPO/GRPO/REINFORCE, Chapter 11; DPO and derivation, Chapter 12).

## 3. Technical Approach
The book is organized as a stepâ€‘byâ€‘step pipeline. Below is the mechanismâ€‘level walkthrough with the specific equations/figures it builds on.

1) Problem formulation: RLHF as a banditâ€‘style RL objective with regularization
- Core objective (responseâ€‘level â€œbanditâ€ reward; no environment dynamics):
  - J(Ï€) = E[rÎ¸(s, a)] âˆ’ Î²Â·DKL(Ï€(Â·|s) || Ï€ref(Â·|s)) (Eq. 8; Figure 3).
  - Intuition: optimize a learned reward model `rÎ¸` over model completions, but constrain updates to stay close to a reference policy (`Ï€ref`, usually the SFT model) via a KL penalty with weight `Î²`. This combats overâ€‘optimization and style drift (Ch. 4.1.2; Ch. 8).
- Why this design: it captures what makes RLHF different from standard RLâ€”responseâ€‘level rewards and no transition dynamicsâ€”while making stability a firstâ€‘class concern via KL control (Figures 2â€“3; Sections 4.1â€“4.1.2).

2) Preference data collection (Chapter 6)
- Interfaces and labeling formats
  - Pairwise comparisons are the default (Figures 7â€“10). Labels often come on Likert scales (5â€‘ or 8â€‘point, Section 6.3.2), but are typically binarized for training.
  - Multiâ€‘turn data can be flattened into single prompts; losses are masked so only the final assistant turn contributes (Sections 6.3.3, 9.2).
- â€œStructuredâ€ preference data
  - In verifiable domains (math, strict formatting), construct synthetic positives/negatives by enforcing constraints or correctness checks (Section 6.3.4), e.g., â€œstart each sentence with â€˜gâ€™,â€ then score responses with/without the constraint.
- Sourcing reality
  - A candid account of working with vendors: access constraints, multiâ€‘week calibration cycles, contract pitfalls, and the need to iterate infrastructure alongside data delivery (Section 6.3.5; Figure 12).

3) Reward modeling (Chapter 7)
- Standard (Bradleyâ€“Terry) RM
  - Learn a scalar reward `rÎ¸(x,y)` so that chosen beats rejected with high probability:
    - Probability form (Eq. 10) and negative logâ€‘likelihood losses (Eq. 11â€“13).
  - Variants include margins (Eq. 14), perâ€‘prompt balancing (Eq. 15), and Kâ€‘wise Plackettâ€“Luce training (Eq. 16) used in Starling (Section 7.4.3).
- Outcome and Process RMs (Table 3)
  - Outcome RM (ORM): predict â€œcorrect/incorrectâ€ per token (Eq. 17), useful in verifiable domains (math/code).
  - Process RM (PRM): score reasoning stepâ€‘byâ€‘step at separators; trained only at step boundaries with labels like âˆ’1/0/1 (Section 7.6).
  - Why the split: standard RMs model human preference on whole answers; ORMs/PRMs exploit verifiability and intermediate supervision to guide reasoning.

4) Regularization (Chapter 8)
- KL penalty implementation
  - Use the expectation form DKL(P||Q) = E[log P âˆ’ log Q] for tractability (Eq. 21).
  - Implementation shows how to compute perâ€‘token logâ€‘ratios against a frozen reference (Section 8.1.2 code).
- Additional control levers
  - Add pretraining gradients / NLL to keep onâ€‘distribution (Eq. 23; Eq. 24â€“25 for DPO+NLL).
  - Practical note: most RMs themselves are trained with minimal regularization and only 1 epoch to avoid overfitting (Section 7.3).

5) Instruction finetuning (Chapter 9)
- Chat templates and masking
  - Messages with roles (`system`, `user`, `assistant`) are serialized with special tokens (Section 9.1 jinja template). Only assistant tokens are trained (Section 9.2).
  - Multiâ€‘turn examples can be packed; only the last assistant span contributes to the loss (Section 9.2).

6) Rejection sampling (RS) (Chapter 10)
- Mechanism
  - For each prompt, generate N candidates, score with RM, select top completions, and then do standard SFT on those winners (Figure 13; Sections 10.1.1â€“10.1.3).
- Selection strategies
  - â€œTop per promptâ€ vs. â€œglobal Topâ€‘Kâ€ over all promptâ€“completion pairs (Section 10.1.2, toy example and code).
- Why use it
  - A simple PreFT baseline used in WebGPT, Helpfulâ€‘Harmless, Llama 2 (Section 10), often strong when RL compute is limited.

7) Policyâ€‘gradient RL (Chapter 11)
- Fundamentals
  - Returns Gt (Eq. 29â€“31); policyâ€‘gradient form with advantage `A` (Eq. 34, Eq. 37).
- Algorithms
  - REINFORCE and RLOO: Monteâ€‘Carlo gradient with perâ€‘prompt leaveâ€‘oneâ€‘out baseline (Eq. 43â€“45); no value network needed (Sections 11.1.2â€“11.1.2.1).
  - PPO: clipped surrogate using perâ€‘token probability ratios (Eq. 46â€“47) to cap step sizes (Section 11.1.3) with value learning.
  - GRPO: PPOâ€‘like but avoids training a value network; computes perâ€‘prompt (group) advantage by standardizing rewards over many samples from the same prompt (Eq. 55â€“57), and typically adds KL inside the loss (Section 11.1.4).
- Implementation details that matter
  - Loss aggregation: sequenceâ€‘mean vs tokenâ€‘mean changes gradient magnitudes across short/long completions (Section 11.2.2, worked example).
  - Asynchronicity: run rollouts and learning on separate nodes; tolerate offâ€‘policy lag to keep GPUs busy (Figure 14; Section 11.2.3).
  - Oneâ€‘step simplification: if only 1 gradient step per batch, PPO/GRPO reduce to a simpler form without explicit clipping against â€œoldâ€ policy (Eq. 61; Section 11.2.4.1).

8) Direct alignment algorithms (Chapter 12)
- DPO from first principles
  - Start with the same regularized RL objective (Eq. 68â€“80) and Bradleyâ€“Terry preferences (Eq. 81) to derive an implicit reward `r*(x,y)=Î² log(Ï€*(y|x)/Ï€ref(y|x))` and the DPO loss (Eq. 65). Gradient form explicitly ups chosen and downs rejected with confidence weighting (Eq. 67).
- Practical contrasts
  - DPO uses offline preference pairs; Î² fixes the target KL implicitly. Itâ€™s simpler and cheaper than online RL, but typically underperforms SOTA online RL on difficult tasks (Section 12.4, citing Chapter 12 references).

9) Constitutional AI & AI feedback (Chapter 13)
- Two uses of a â€œconstitutionâ€ (Section 13.1):
  - Critique and revise SFT answers using principles.
  - Label pairwise preferences using a judge model conditioned on those principles (RLAIF).
- Broader â€œLLMâ€‘asâ€‘judgeâ€ tools and caveats (Section 7.8; 13.2): strong and cheap, but still weaker than dedicated RMs on RM benchmarks.

10) Reasoning training with verifiable rewards (RLVR) (Chapter 14)
- Mechanism
  - Replace learned RM with a verifier (unit tests, math checkers) and run RL that rewards correct outcomes (Figure 17).
- Modern recipe (Section 4.2.3)
  - DeepSeek R1 pipeline: coldâ€‘start reasoning samples â†’ largeâ€‘scale RLVR until convergence â†’ rejection sampling mix (reasoning + general) â†’ mixed RL (verifiable + preference RMs).
- Why it works now
  - Stable toolchains, stronger base models, and throughputâ€‘oriented infrastructure (Section 14.1.1â€“14.1.2). RLVR correlates with inferenceâ€‘time scaling: more thinking tokens generally improves accuracy when properly trained.

11) Evaluation and failure modes (Chapters 16â€“18)
- Prompt formats matter (fewâ€‘shot vs zeroâ€‘shot vs CoT); evaluation ecosystems and contamination (Chapter 16; Figure 18).
- Overâ€‘optimization: as the proxy reward improves, true utility can first rise then fall (Figure 19; Figure 20 shows train/test RM divergence around ~150k RL samples). Qualitative pathologies include length bias, sycophancy, and overâ€‘refusal (Chapter 17).

## 4. Key Insights and Innovations
- A single, principled objective for RLHF practice
  - The entire postâ€‘training stack can be viewed as maximizing a learned reward subject to a KL budget from a reference model (Eq. 8; Sections 4.1.2, 8). This framing clarifies why regularization is central and how DPO relates to RLHF via the same Lagrangian.
- A clean unification of rewardâ€‘learning choices
  - Table 3 contrasts standard RMs (wholeâ€‘response preference) with ORMs (perâ€‘token correctness) and PRMs (stepâ€‘level rewards). This makes explicit when to switch from human preferences to verifiability to guide learning, especially for reasoning.
- Practical training recipes you can follow
  - The text doesnâ€™t stop at diagrams; it specifies realistic scales and ordering. For example: InstructGPTâ€‘style counts (~10k SFT â†’ ~100k preferences â†’ ~100k RL prompts; Section 4.2.1; Figure 4), TÃ¼lu 3â€™s multiâ€‘millionâ€‘example data mix (Figure 6), and DeepSeek R1â€™s staged RLVR (Section 4.2.3). These ground abstract methods in executable plans.
- Implementation â€œgotchasâ€ that change outcomes
  - Loss aggregation (Section 11.2.2), KL approximation (Eq. 21), singleâ€‘step PPO simplification (Eq. 61), and async rollouts (Figure 14) are the details that decide whether training is stable and efficientâ€”rarely spelled out in papers.
- A candid look at the data pipeline
  - The section on vendor sourcing, batching, and contracts (Section 6.3.5) is unusual for research writing and invaluable for practitioners who must secure highâ€‘quality preferences on a clock and budget.

## 5. Experimental Analysis
This book is not a new empirical paper, but it does consolidate evaluation setups and quantitative anchors that matter for practitioners.

- Evaluation methodology (Chapter 16)
  - Datasets span chat preference benchmarks (MTâ€‘Bench, AlpacaEval), multiâ€‘skill suites (MMLU, BigBenchâ€‘Hard, DROP, MATH, GSM8K, HumanEval), and newer reasoning/tool tasks (GPQA Diamond, SWEâ€‘Bench+, LiveCodeBench). It emphasizes prompt format control (fewâ€‘shot vs CoT), contamination checks, and consistency in inference budgets.
- Concrete numbers and scales that shape training
  - InstructGPTâ€‘style recipe: ~10k SFT, ~100k preference pairs, ~100k RL prompts (Section 4.2.1; Figure 4).
  - TÃ¼lu 3 recipe: ~1M SFT, ~1M onâ€‘policy preference pairs, ~10k RLVR prompts (Figure 6).
  - Overâ€‘optimization curves: Figure 20 shows that gains on a trainâ€‘RM can diverge from a heldâ€‘out RM around ~150k RL training samples, illustrating the proxyâ€‘objective hazard.
  - Throughput techniques: asynchronous RL (Figure 14) and sequenceâ€‘level packing (Section 11.2.3) are presented as empirically necessary for longâ€‘trace reasoning runs.
- Support for claims
  - The bookâ€™s claims are primarily methodological (â€œhow toâ€) rather than new SOTA. Where performance assertions appear, they are tied to recipes (e.g., rejection sampling baselines used in Llama 2; Section 10) or wellâ€‘known public results (e.g., GRPO in DeepSeek; Section 11.1.4; RLVR figure and pipeline in Figure 17 and Section 4.2.3).
- Ablations and robustness
  - Instead of ablation tables, robustness themes are methodological:
    - Length bias and sycophancy as systemic effects (Chapter 17).
    - KL budget as a control knob (Chapter 8).
    - Tokenâ€‘ vs sequenceâ€‘level loss normalization materially changing gradient magnitudes (Section 11.2.2 example).
- Conditional results and tradeâ€‘offs
  - The text is explicit that DPO is simpler but typically underperforms online RL on the hardest tasks unless augmented with onâ€‘policy generation or relabeling (Section 12.4). It also highlights that LLMâ€‘asâ€‘judge is costâ€‘effective but not yet as reliable as strong RMs on RMâ€‘specific benchmarks (Section 7.8).

> â€œOverâ€‘optimization â€¦ is when optimizing the proxy objective causes the true objective to get better, then get worse.â€ (Chapter 17; Figure 19)

## 6. Limitations and Trade-offs
- Scope and evidence
  - This is a synthesis, not a single empirical study. It curates recipes and cites many external results; it does not present new, controlled headâ€‘toâ€‘head benchmarks for every choice it recommends.
- Assumptions the approach relies on
  - Availability of a strong SFT reference model (Eq. 8), sufficient preference data (Section 6), and a competent verifier for RLVR (Figure 17). Without these, KLâ€‘constrained optimization either stalls or overfits proxies.
- Scenarios not fully addressed
  - Nonâ€‘text or complex multiâ€‘modal RLHF is only briefly touched via rewardâ€‘bench extensions (Section 7.9) and MiMoâ€‘VL case notes (Section 14.2.3), without a full data/infra recipe.
  - Personalization and pluralistic value alignment are acknowledged as open problems (Chapter 5), with only early directions (e.g., aspectâ€‘conditioned RMs).
- Computational and data constraints
  - Preference data is costly in time and money (Section 6.3.5). Largeâ€‘scale RL and reasoning traces require highâ€‘throughput async infrastructure (Section 11.2.3). RM and PRM training need careful curation to avoid spurious correlations (e.g., length).
- Open questions
  - How to quantify and spend a â€œKL budgetâ€ optimally over multiâ€‘stage postâ€‘training (Chapter 8).
  - How to make LLMâ€‘asâ€‘judge robust enough to replace RMs widely (Section 7.8 notes a performance gap).
  - How to guard against preference displacement in DPO (Figure 16; Section 12.2) and to blend online/offline data safely (Section 12.4).

## 7. Implications and Future Directions
- How this work changes the landscape
  - It lowers the barrier to building aligned LLMs by consolidating equations, code patterns, and training recipes in one placeâ€”from Figureâ€‘level overviews (Figures 1, 4â€“6, 13â€“14, 17) to derivations (Eq. 65â€“86 for DPO) and edgeâ€‘case engineering (Section 11.2). That supports reproducibility and speeds iteration across labs and startups.
- Followâ€‘up research it enables
  - Principled KL budgeting across stages; hybrid pipelines that mix DPO with online relabeling and short RL bursts; better PRM tooling for chainâ€‘ofâ€‘thought supervision; robust judge models that close the gap with learned RMs; and asynchronous, offâ€‘policy policyâ€‘gradient methods specialized for long reasoning traces (Sections 11.2.3, 12.4, 14.2.3).
- Practical applications
  - Building domainâ€‘specific assistants that must follow instructions crisply (SFT + RS), align to nuanced style or safety policies (PreFT, CAI/RLAIF), and solve verifiable tasks (RLVR) such as math, coding, and toolâ€‘augmented workflowsâ€”while tracking evaluation rigor (Chapter 16) and avoiding known traps like overâ€‘refusal, sycophancy, and length bias (Chapter 17â€“18).

> â€œThe set of techniques is everything after the end of most of pretrainingâ€¦ instruction tuning, RLVR, preferenceâ€‘tuning, etc.â€ (Section 1.2)  
> â€œA good example is our change from the first version of OLMoE Instruct to the second â€” the postâ€‘training evaluation average from 35 to 48 without touching the majority of pretraining.â€ (Section 1.2)

In short, the book converts the fieldâ€™s collective experience into a coherent, technically precise manual. It explains not only what to do, but how and why each step works, where it breaks, and how modern variantsâ€”DPO, GRPO, RLVR, CAIâ€”fit into one conceptual frame anchored by the KLâ€‘regularized objective (Eq. 8).

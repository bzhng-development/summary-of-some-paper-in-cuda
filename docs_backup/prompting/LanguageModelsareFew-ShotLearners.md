# Language Models are Few-Shot Learners

**ArXiv:** [2005.14165](https://arxiv.org/abs/2005.14165)

## ğŸ¯ Pitch

This paper introduces GPT-3, a 175-billion-parameter language model that can perform a wide range of tasks by simply seeing a few examples or natural language instructionsâ€”without any parameter updates or task-specific training. By dramatically increasing model size and leveraging a prompt-based, in-context learning framework, GPT-3 matches or exceeds state-of-the-art results in many NLP benchmarks and even demonstrates surprising abilities like arithmetic and analogies, suggesting a major shift toward more general, flexible, and human-like language understanding systems.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces GPTâ€‘3, a 175â€‘billionâ€‘parameter autoregressive language model that performs a wide range of tasks by â€œinâ€‘context learningâ€ â€” using only naturalâ€‘language instructions and a few task demonstrations at inference time, without any gradient updates. By scaling model size and training data and standardizing a promptâ€‘based evaluation (zeroâ€‘, oneâ€‘, and fewâ€‘shot), GPTâ€‘3 reaches strong or stateâ€‘ofâ€‘theâ€‘art results on many benchmarks (e.g., LAMBADA, TriviaQA) and reveals new capabilities (onâ€‘theâ€‘fly arithmetic, analogies), suggesting a shift from taskâ€‘specific fineâ€‘tuning to taskâ€‘agnostic prompting.

## 2. Context and Motivation
- Problem addressed
  - Modern NLP pipelines typically require taskâ€‘specific labeled datasets and fineâ€‘tuning, which is costly, brittle out of distribution, and unlike how people learn from instructions or a handful of examples (Section 1).
  - Prior attempts at â€œmetaâ€‘learningâ€ with language models (LMs) â€” performing new tasks via instructions/demonstrations at inference time â€” had limited success at practical scales (e.g., 4% on Natural Questions in earlier work cited in Section 1).

- Why it matters
  - Practical impact: Reduces dependence on large labeled datasets for every task (Section 1).
  - Scientific significance: Tests the hypothesis that increasing model capacity and data enables general inâ€‘context learning, not just better language modeling (Figures 1.2 and 3.1).

- Prior approaches and their gaps
  - Fineâ€‘tuning large pretrained models yields strong benchmark scores but can overfit to narrow task distributions and exploit dataset artifacts (Section 1; references [HLW+20], [MPL19]).
  - Early inâ€‘context learning with smaller LMs showed promise but lagged far behind fineâ€‘tuned systems (Section 1).

- Positioning of this work
  - Train and analyze a family of eight GPTâ€‘3 models (125M â†’ 175B parameters; Table 2.1), and systematically evaluate zeroâ€‘shot (instruction only), oneâ€‘shot (one example), and fewâ€‘shot (dozens of examples) prompting across >25 tasks (Section 3; Figure 2.1).

## 3. Technical Approach
Stepâ€‘byâ€‘step, how GPTâ€‘3 is built and evaluated:

- Model family and architecture
  - Autoregressive transformer with alternating dense and locally banded sparse attention (akin to Sparse Transformer; Section 2.1).
  - Eight sizes from `125M` to `175B` parameters; context window `nctx = 2048`; feedforward width is 4Ã— `dmodel` (Table 2.1).
  - Training uses Adam, cosine LR decay, gradient clipping, and weight decay; mixed model parallelism to fit larger models (Section 2.3; Appendix B).

- Training data and preprocessing
  - A mixture emphasizing quality:
    - Filtered Common Crawl (~410B tokens postâ€‘filter), WebText2, Books1/2, Wikipedia (Table 2.2).
    - Quality filter: a logisticâ€‘regression classifier prefers web pages similar to curated corpora; fuzzy deâ€‘duplication reduces redundancy (Appendix A).
    - Sampling weights favor highâ€‘quality sources even if that means reâ€‘exposing them multiple times (Table 2.2 â€œEpochs elapsedâ€), accepting mild overfitting for quality (Section 2.2).
  - Total training budget ~300B tokens for each model (Table 2.1).

- What â€œinâ€‘context learningâ€ means here
  - Definition: The model adapts at inference time to the task described in its input context (prompt) â€” a naturalâ€‘language instruction optionally followed by K example pairs (â€œdemonstrationsâ€) â€” and then completes or answers the next instance (Figures 1.1, 2.1).
  - No gradient updates are performed at evaluation; the â€œlearningâ€ occurs in a single forward pass conditioned on the prompt.

- Evaluation protocol (Section 2.4)
  - Fewâ€‘shot: randomly sample K training examples (typically 10â€“100, bounded by the 2048â€‘token context) and append a new test instance to complete.
  - Oneâ€‘shot: same with K=1 plus an instruction.
  - Zeroâ€‘shot: instruction only.
  - Scoring conventions:
    - Multipleâ€‘choice: compare the (lengthâ€‘normalized) logâ€‘likelihood of each option (Section 2.4).
    - For some datasets (ARC, OpenBookQA, RACE), normalize choice likelihood by its unconditional prior to reduce length/priors bias (Section 2.4).
    - Freeâ€‘form generation: beam search (width 4, length penalty Î±=0.6) with Exact Match, F1, or BLEU as appropriate (Section 2.4).
    - Taskâ€‘specific framings matter; e.g., formatting LAMBADA as fillâ€‘inâ€‘theâ€‘blank enables oneâ€‘word completions (Section 3.1.2; Figure 3.2).

- Safety check: benchmark contamination analysis
  - Because pretraining data comes from the web, test sets may be present. A conservative nâ€‘gram overlap filter marks â€œdirtyâ€ examples; performance is reâ€‘computed on the â€œcleanâ€ subset (Section 4; Appendix C).
  - Most benchmarks show negligible change; a few (PIQA, Winograd, LAMBADA) are flagged (Figure 4.2).

Analogy to build intuition: Think of the prompt as a mini â€œinstruction manualâ€ plus a few worked examples that the model reads instantly before solving a new problem. Larger models â€œread and generalizeâ€ from these tiny manuals more effectively (Figure 1.2).

## 4. Key Insights and Innovations
- Scaling transforms prompting into a competitive alternative to fineâ€‘tuning
  - Insight: Validation loss continues to follow a powerâ€‘law with compute/size (Figure 3.1), and â€” crucially â€” downstream fewâ€‘shot performance improves faster than zeroâ€‘shot as size grows (Figure 1.3; Figure 3.8 on SuperGLUE).
  - Significance: The largest modelâ€™s fewâ€‘shot scores approach or surpass fineâ€‘tuned SOTA on several tasks without any gradient updates (e.g., LAMBADA and TriviaQA; Tables 3.2 and 3.3).

- Standardized, taskâ€‘agnostic prompting framework
  - Contribution: A uniform evaluation across zeroâ€‘/oneâ€‘/fewâ€‘shot settings with careful likelihood normalization and taskâ€‘specific prompt design (Section 2.4).
  - Why it matters: Shows that instruction wording, examples count `K`, and scoring choices materially affect results; provides replicable recipes for many tasks (Appendix G).

- Systematic contamination measurement
  - Method: Perâ€‘dataset conservative overlap detection (up to 13â€‘grams; with special handling for short synthetic tasks) and reâ€‘evaluation on clean subsets (Section 4; Appendix C).
  - Finding: Large potential overlaps do not necessarily inflate scores; when effects exist they are small (e.g., Winograd âˆ’2.6% absolute on the clean subset; Figure 4.2). PIQA and Winograd are explicitly annotated with asterisks (Tables 3.5, 3.6).

- Emergent testâ€‘time skills beyond memorization
  - New capability: Onâ€‘theâ€‘fly computation and pattern manipulation (e.g., 2â€“3 digit arithmetic, symbol insertion/anagrams) improves sharply with scale and number of demonstrations (Section 3.9; Figures 3.10, 3.11).
  - Evidence against rote memorization: Only 0.8% of 3â€‘digit addition test items were found verbatim in training data; common errors are procedural (e.g., missed carry), consistent with real computation (Section 3.9.1).

## 5. Experimental Analysis
- Evaluation setup
  - Datasets span language modeling/cloze, openâ€‘domain and closedâ€‘book QA, commonsense, reading comprehension, NLI, translation, and synthetic reasoning tasks (Sections 3.1â€“3.9).
  - Metrics: accuracy, F1, BLEU, perplexity; with detailed perâ€‘task scoring rules (Section 2.4; Appendix G).
  - Baselines: Prior fineâ€‘tuned SOTAs and strong pretrained models (e.g., T5â€‘11B, RoBERTa, ALUM); sometimes also human performance lines (Figures 3.2, 3.5, 3.6, 3.7).

- Headline quantitative results
  - Language modeling / completion
    - PTB (zeroâ€‘shot perplexity): `20.5` vs prior `35.8` (Table 3.1).
    - LAMBADA: fewâ€‘shot `86.4%` accuracy; zeroâ€‘shot `76.2%` (Table 3.2; Figure 3.2). Formatting as cloze is key to getting oneâ€‘word answers (Section 3.1.2).
    - HellaSwag: fewâ€‘shot `79.3%` vs SOTA `85.6%` (Table 3.2).
    - StoryCloze: fewâ€‘shot `87.7%` vs SOTA `91.8%` (Table 3.2).

  - Closedâ€‘book QA (no retrieval, no fineâ€‘tune)
    - TriviaQA (wiki split): zeroâ€‘shot `64.3%`, oneâ€‘shot `68.0%`, fewâ€‘shot `71.2%`, exceeding fineâ€‘tuned T5â€‘11B closedâ€‘book (`60.5%`) and matching a fineâ€‘tuned openâ€‘domain retrieverâ€‘generator in oneâ€‘shot (Table 3.3; Figure 3.3).
    - WebQuestions: fewâ€‘shot `41.5%` approaching fineâ€‘tuned T5â€‘11B+SSM `44.7%` (Table 3.3).
    - Natural Questions: fewâ€‘shot `29.9%` below fineâ€‘tuned T5â€‘11B+SSM `36.6%`, with large gains from zeroâ†’fewâ€‘shot suggesting distribution mismatch that prompts partially fix (Table 3.3).

  - Translation (unsupervised/fewâ€‘shot)
    - Fewâ€‘shot GPTâ€‘3 outperforms prior unsupervised NMT into English (e.g., Roâ†’En `39.5` BLEU; Deâ†’En `40.6` BLEU using multi-bleu) but lags when translating into other languages (e.g., Enâ†’Ro `21.0` BLEU) (Table 3.4; Figure 3.4).
    - Directional asymmetry aligns with GPTâ€‘3 being a stronger English LM and with byteâ€‘level BPE subword choices (Section 3.3).

  - Winogradâ€‘style coreference
    - Classic Winograd: ~`89%` across settings, with small contamination caveat (Table 3.5).
    - Adversarial Winogrande: fewâ€‘shot `77.7%`, close to fineâ€‘tuned RoBERTaâ€‘large (`79%`) but below overall SOTA `84.6%` (Table 3.5; Figure 3.5).

  - Commonsense QA
    - PIQA: fewâ€‘shot `82.8%` exceeding leaderboard baseline (but flagged for possible training overlap; Table 3.6; Figure 3.6).
    - ARCâ€‘Challenge: fewâ€‘shot `51.5%` (well below SOTA `78.5%`), but approaching fineâ€‘tuned RoBERTa baseline (Table 3.6).
    - OpenBookQA: fewâ€‘shot `65.4%`, below SOTA `87.2%` (Table 3.6).

  - Reading comprehension
    - CoQA: fewâ€‘shot `85.0` F1, within ~6 points of fineâ€‘tuned SOTA (`90.7`) (Table 3.7; Figure 3.7).
    - SQuADv2: fewâ€‘shot `69.8` F1; zeroâ†’fewâ€‘shot gain ~10 points (Table 3.7).
    - QuAC and RACE: relatively weak (`44.3` F1 and ~`47%`/`58%` accuracy respectively; Table 3.7).

  - SuperGLUE (test set)
    - Overall fewâ€‘shot score `71.8`, competitive with fineâ€‘tuned BERTâ€‘Large (`69.0`) but behind SOTA (`89.0`) (Table 3.8).
    - Strong tasks: COPA (`92.0` acc), ReCoRD (acc `90.2`, F1 `91.1`).
    - Weak task: WiC near random (`49.4%`), and middling RTE/CB (Table 3.8; developmentâ€‘set scaling in Figure 3.8).

  - NLI (ANLI)
    - Rounds 1â€“2 near chance for all model sizes; Round 3 fewâ€‘shot reaches `40.2%` on dev, roughly halfway from chance to SOTA (Figure 3.9; Appendix H).

  - Synthetic reasoning and qualitative probes
    - Arithmetic (fewâ€‘shot): 2â€‘digit add/sub = `100%`/`98.9%`; 3â€‘digit add/sub = `80.4%`/`94.2%`; 4â€‘digit ~`26%`; 5â€‘digit ~`10%`; 2â€‘digit multiplication `29.2%` (Table 3.9; Figure 3.10).
    - Scrambling: Symbolâ€‘insertion removal `67.2%`; cycling letters `37.9%`; reversing words remains near zero (Table 3.10; Figure 3.11).
    - SAT analogies: fewâ€‘shot `65.2%` vs historical human average `57%` (Figure 3.12).
    - News generation: humans identify GPTâ€‘3 articles at ~`52%` accuracy (chance is 50%), vs `86â€“88%` on a deliberately bad control model (Tables 3.11, 3.12; Figure 3.13).

- Do the experiments support the claims?
  - Yes, for the central claim that scaling improves inâ€‘context learning efficiency and utility:
    - Smooth scaling trends across tasks and K (Figures 1.2, 3.1, 3.3, 3.4, 3.5, 3.8).
    - Strong fewâ€‘shot performance on diverse tasks with no gradient updates.
  - Robustness/diagnostics:
    - Contamination analysis often shows negligible impact, with explicit caveats where effects might exist (Figure 4.2; Section 4).
    - Mixed results highlight boundaries: NLI (ANLI), WiC, some reading comprehension datasets remain challenging (Sections 3.6â€“3.8).

- Tradeâ€‘offs and conditions
  - Task format sensitivity: prompt phrasing and K strongly affect outcomes (LAMBADA cloze formatting; Section 3.1.2).
  - Directional biases in translation; reliance on Englishâ€‘centric training distribution (Section 3.3).
  - Comparisonâ€‘style tasks (WiC, ANLI) remain weak, possibly reflecting limitations of leftâ€‘toâ€‘right decoding for sentence pair comparison (Section 5).

## 6. Limitations and Trade-offs
- Modeling and objective assumptions (Section 5)
  - Pure autoregressive, unidirectional objective may hinder tasks requiring bidirectional context or explicit comparison (WiC, RTE, ANLI).
  - Selfâ€‘supervised nextâ€‘token prediction equally weights all tokens; lacks grounding and goalâ€‘directed objectives (Section 5).

- Data and distributional constraints
  - Training data is ~93% English; translation from English lags translation into English (Section 3.3).
  - Potential trainâ€“test contamination exists; while measured impacts are mostly small, a few benchmarks are affected (Section 4).

- Capability boundaries revealed by experiments
  - NLI and some reading comprehension remain far from SOTA (Tables 3.7, 3.8; Figure 3.9).
  - Symbolic operations scale with digits/complexity; performance drops on 4â€“5 digit arithmetic and multiplication (Table 3.9).

- Compute and efficiency
  - Training requires several thousand PFâ€‘days for 175B (Figure 2.2; Appendix D). Inference is expensive and latency is high without distillation.

- Social and fairness considerations (Section 6)
  - Bias: Gendered occupation associations skew male; raceâ€‘related sentiment disparities (Figure 6.1; Section 6.2.1â€“6.2.2; Table 6.1).
  - Misuse risks: Difficulty of human detection of generated news (Tables 3.11â€“3.12) raises concerns for misinformation.
  - Energy usage and environmental impact (Section 6.3).

## 7. Implications and Future Directions
- How this work changes the landscape
  - Establishes prompting â€” not fineâ€‘tuning â€” as a viable, sometimes superior way to adapt large LMs to new tasks (Figures 1.3, 3.8).
  - Normalizes the practice of zero/one/fewâ€‘shot evaluation with explicit prompts and scoring procedures (Section 2.4; Appendix G).
  - Validates scaling laws as a predictor not only of loss but also of emergent inâ€‘context capabilities (Figure 3.1).

- Followâ€‘up research enabled/suggested
  - Architectural/Objective advances:
    - Bidirectional or encoderâ€‘decoder models at GPTâ€‘3 scale to improve sentenceâ€‘pair and spanâ€‘selection tasks (Section 5).
    - Augment nextâ€‘token prediction with targeted objectives (entity/span prediction, reasoning) or RL from human feedback (Section 5; references [ZSW+19a]).
    - Retrievalâ€‘augmented prompting to combine parametric knowledge with external memory (Table 3.3 context; [LPP+20]).
  - Data and training:
    - More balanced multilingual corpora; subword vocabularies tuned for nonâ€‘English languages (Section 3.3).
    - Larger context windows and memory mechanisms to support longâ€‘document reasoning.
  - Efficiency and deployment:
    - Distillation and sparsity to reduce inference cost while keeping fewâ€‘shot behaviors (Section 5).
    - Better methods for automatic prompt construction and selection; learning to prompt.

- Practical applications and use cases
  - Lowâ€‘label or labelâ€‘free adaptation: QA, summarization, translation, grammar correction, and domainâ€‘specific text generation via prompts (Sections 3.1â€“3.7, 3.9.6).
  - Rapid prototyping: New tasks can be specified by plainâ€‘language instructions and a handful of examples (Appendix G).
  - Cautionary deployment: Monitoring for bias and misuse is necessary given nearâ€‘indistinguishable generated news and measured demographic skews (Section 6; Tables 3.11, 6.1).

> In short, the paper demonstrates that â€œscale + promptingâ€ is a powerful recipe: a single, taskâ€‘agnostic model trained once can adapt in seconds to many tasks through naturalâ€‘language instructions and a few examples, achieving competitive performance across a broad spectrum of benchmarks (Figures 1.2, 1.3; Section 3).

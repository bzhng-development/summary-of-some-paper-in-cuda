# RULER: Whatâ€™s the Real Context Size of Your Long-Context Language Models?

**ArXiv:** [2404.06654](https://arxiv.org/abs/2404.06654)

## ğŸ¯ Pitch

RULER introduces a flexible, synthetic benchmark that rigorously tests long-context language models (LLMs) beyond basic retrieval, encompassing multi-hop tracing, aggregation, and long-context question answering. By revealing how most models fail to sustain performance with increasing context length and complexity, RULER challenges current evaluation norms and provides a crucial tool to accurately measureâ€”and accelerateâ€”real progress in long-context AI.

---

## 1. Executive Summary
RULER is a configurable, synthetic benchmark for testing what longâ€‘context language models can actually do with very long inputs. It goes beyond simple â€œfind the needleâ€ retrieval to evaluate multiâ€‘hop tracing, aggregation, and longâ€‘context question answering, revealing that many models advertised with 32K+ context windows cannot maintain reliable performance as length and task complexity grow (Table 3).

## 2. Context and Motivation
- Problem addressed
  - The field lacks a comprehensive, reliable way to measure whether longâ€‘context language models (LLMs) truly use long inputs effectively. Most current evaluations rely on simple retrieval tests such as â€œneedleâ€‘inâ€‘aâ€‘haystackâ€ (NIAH)â€”finding a single item buried in long distractorsâ€”which captures only a narrow ability (Abstract; Sec. 1).
- Why it matters
  - Real use cases (e.g., legal analysis, scientific literature review, multiâ€‘document QA) require more than locating a single fact: models must trace references across a document, aggregate dispersed information, and answer questions precisely. Measuring only retrieval overestimates real capability and can mislead deployment decisions (Sec. 1; Table 1).
- Prior approaches and gaps
  - Realistic benchmarks (e.g., ZeroSCROLLS, Lâ€‘Eval, BAMBOO, LongBench; Table 1) use human or hybrid data but:
    - Often mix in â€œparametric knowledgeâ€ (knowledge stored in the modelâ€™s weights), which can hide whether the model actually uses the given context.
    - Have limited control over context length, task difficulty, and where key information appears.
  - Synthetic tests (NIAH, passkey/line/kv retrieval) offer control but mainly test simple retrieval (Table 1).
- How RULER positions itself
  - RULER is synthetic, so it controls sequence length and task difficulty while minimizing reliance on parametric knowledge. It extends beyond retrieval into:
    - Multiâ€‘hop tracing (variable tracking),
    - Aggregation (counting/common word detection under controlled distributions),
    - Longâ€‘context QA with distractors (Sec. 3, Table 2; Table 1).

## 3. Technical Approach
RULER is a suite of autoâ€‘generated tasks with tunable length and difficulty. Each example is a long input with a concise query and a precise target answer; models are evaluated by recallâ€‘based accuracy (Sec. 4: â€œappend the input with an answer prefix â€¦ check the presence of the target outputâ€).

Key design elements
- Synthetic construction to control difficulty and length
  - Inputs are generated so that:
    - The signalâ€‘toâ€‘noise ratio (how much relevant signal appears relative to distractors) and the number of target tokens are controllable proxies for task difficulty (Sec. 3).
    - Parametric knowledge is minimized by using synthetic words/numbers/UUIDs and generic texts (Table 2).
- Four task categories (Sec. 3; Table 2)
  1) Retrieval (Needleâ€‘inâ€‘aâ€‘Haystack family)
     - Definitions
       - â€œNeedleâ€: a keyâ€“value pair inserted somewhere in the long â€œhaystack.â€
       - Query: placed at the end, asks for the value(s) associated with specific key(s).
     - Variants test robustness and recall:
       - `Sâ€‘NIAH` (Single): one needle. Keys/values can be words, 7â€‘digit numbers, or 32â€‘digit `UUID`s; haystack can be repeated noise sentences or Paul Graham essays (Sec. 3.1; Table 2).
       - `MKâ€‘NIAH` (Multiâ€‘keys): many needles with different keys; only the queried key is relevant. Adding many distractor keys creates â€œhard distractors,â€ including an extreme setting where the entire haystack is filled with distractor needles (Sec. 3.1; Table 2).
       - `MVâ€‘NIAH` (Multiâ€‘values): multiple values share the same key; the model must return all valuesâ€”tests completeness of retrieval (recall) (Sec. 3.1; Table 2).
       - `MQâ€‘NIAH` (Multiâ€‘queries): multiple distinct keys to retrieve in one goâ€”tests recall when many items must be fetched (Sec. 3.1; Table 2).
  2) Multiâ€‘hop Tracing (`VT`: Variable Tracking)
     - Emulates following a chain of references, a minimal proxy for coreference resolution across long text.
     - Mechanism: initialize `X1 = V`; then insert statements like `X2 = X1; X3 = X2; â€¦` at positions throughout the long input. The query asks for â€œall variables assigned value V.â€ Difficulty increases with the number of hops (chain length) and the number of parallel chains (Sec. 3.2; Table 2).
  3) Aggregation (`CWE` and `FWE`)
     - `CWE` (Common Words Extraction): the input is a long list of synthetic words; a fixed set of â€œcommonâ€ words are injected more frequently. Task: output the topâ€‘K most common words. Difficulty scales by increasing uncommon words with length (Sec. 3.3; Table 2).
     - `FWE` (Frequent Words Extraction): frequencies follow a Zeta distribution (a heavyâ€‘tailed distribution closely related to Zipfâ€™s law; parameter `Î±` controls how steeply frequencies drop). Task: output the topâ€‘3 most frequent words. Lower `Î±` makes frequencies more similar, so counting is harder (Fig. 1; Sec. 3.3; Table 2).
  4) Longâ€‘context QA (`QA`)
     - Start from shortâ€‘context QA datasets (SQuAD for singleâ€‘hop; HotpotQA for multiâ€‘hop) and inject many distracting paragraphs from the same dataset; only a subset (â€œgoldâ€ paragraphs) contains the answer. Task: answer the question based solely on the provided documents (Sec. 3.4; Table 2).
- Experimental protocol (Sec. 4)
  - Models: 17 aligned longâ€‘context models (Geminiâ€‘1.5â€‘Pro, GPTâ€‘4â€‘1106, and 15 openâ€‘source), covering 7Bâ€“100B+ parameters and claimed windows 32Kâ€“1M (Appendix A).
  - Lengths and sampling: For each of 13 chosen task configurations, evaluate each model at 6 context lengths: 4K, 8K, 16K, 32K, 64K, 128K. For every (task, length), generate 500 examples (Sec. 4; Appendix B lists configurations).
  - Inference: vLLM with paged attention, bfloat16, greedy decoding on 8Ã—A100 GPUs; prompts follow each modelâ€™s chat template; an answer prefix prevents refusal or extra text (Sec. 4; Appendix D).
  - Metrics and scoring:
    - Primary metric: accuracy by exact presence of required answer tokens (â€œrecallâ€‘based accuracyâ€).
    - â€œEffective context lengthâ€: the largest length where a modelâ€™s average score across RULERâ€™s 13 tasks exceeds a fixed thresholdâ€”set to Llamaâ€‘2â€‘7Bâ€‘chat performance at 4K (85.6%, Table 3; justification in Sec. 4).
    - Weighted averages: two rankings aggregate performance across all lengths with linearly increasing weights (`wAvg (inc)`) or decreasing weights (`wAvg (dec)`), approximating usage skewed to long or short contexts (Sec. 4; Table 3).
  - Task selection: from an initial larger pool, tasks were clustered by correlation; redundant ones were removed, leaving 13 representative tasks spanning distinct behaviors (Appendix C; Fig. 5).

## 4. Key Insights and Innovations
1) A behaviorally rich, controllable longâ€‘context benchmark
   - Novelty: moves beyond singleâ€‘item retrieval to test multiâ€‘hop tracing and aggregationâ€”capabilities central to real longâ€‘document use, but largely missing from prior synthetic benchmarks (Sec. 3; Table 1).
   - Significance: reveals failure modes hidden by vanilla NIAH, where many models score perfectly (Tables 10â€“11) but then collapse on aggregation/QA at scale (Tables 15â€“16).
2) Systematic stress tests for retrieval robustness and recall
   - Novelty: the NIAH family explicitly varies (a) needle/haystack type; (b) number of distractor needles; (c) number of required outputs (MV/MQ). This isolates whether models can ignore hard distractors and return all relevant items (Sec. 3.1; Table 2).
   - Significance: shows substantial drops when facing distractors or multiple required items (Fig. 2), demonstrating retrievalâ€‘only testing overestimates effective longâ€‘context capabilities.
3) Aggregation tasks grounded in controlled frequency distributions
   - Novelty: `CWE` and `FWE` use uniform and Zetaâ€‘distributed sampling to force genuine counting/aggregation across long sequences (Sec. 3.3; Fig. 1). Tuning `Î±` adjusts hardness by reducing separability between topâ€‘frequency words.
   - Significance: exposes tendencies to rely on parametric priors (â€œtheâ€, â€œaâ€) or to copy prompts rather than compute counts (Sec. 5; Fig. 3 middleâ€‘right).
4) Two summary metrics that connect to deployment
   - â€œEffective context lengthâ€ and lengthâ€‘weighted averages provide interpretable summaries of practical capability at scale (Sec. 4; Table 3). This reframes â€œcontext windowâ€ claims (advertised token limits) into measured, taskâ€‘averaged effectiveness.
5) Empirical insights about scaling, training length, and architecture
   - Larger models are more robust at long context (Yiâ€‘34B > Yiâ€‘6B/9B; Fig. 4 middleâ€‘right).
   - Training on longer windows helps but can be inconsistent, and extrapolating beyond trained length causes abrupt drops (LWM series; Fig. 4 left, middleâ€‘left).
   - Nonâ€‘Transformer architectures tested (RWKVâ€‘v5, Mambaâ€‘2.8B) lag substantially behind a Transformer baseline on RULER (Fig. 4 right).

## 5. Experimental Analysis
- Evaluation setup (Sec. 4; Appendix Bâ€“D)
  - 17 aligned LLMs evaluated on 13 tasks at 6 lengths (4Kâ€“128K), 500 examples per (task, length).
  - Accuracy computed by matching the demanded outputs; a response prefix minimizes refusal/explanations.
- Main quantitative results (Table 3)
  - Overall ranking:
    - Weighted average across lengths:
      - Increasingâ€‘withâ€‘length weighting: 
        > â€œGeminiâ€‘1.5â€‘Pro: 95.5 (1st), GPTâ€‘4: 89.0 (2nd), GLMâ€‘4â€‘9B: 88.0 (3rd), Llamaâ€‘3.1â€‘70B: 85.5 (4th).â€
      - Decreasingâ€‘withâ€‘length weighting:
        > â€œGeminiâ€‘1.5â€‘Pro: 96.1 (1st), GPTâ€‘4: 94.1 (2nd), Llamaâ€‘3.1â€‘70B: 93.7 (3rd), Qwenâ€‘2â€‘72B: 92.3 (4th), Commandâ€‘Râ€‘plus: 92.1 (5th).â€
  - Effective context length (threshold = Llamaâ€‘2â€‘7Bâ€‘chat 4K score, 85.6%):
    - > â€œGeminiâ€‘1.5â€‘Pro: >128K (beyond tested maximum). GPTâ€‘4: 64K. Llamaâ€‘3.1â€‘70B: 64K. GLMâ€‘4â€‘9B: 64K. Qwenâ€‘2â€‘72B: 32K. Commandâ€‘Râ€‘plus: 32K. Yiâ€‘34Bâ€‘200K: 32K. Mixtralâ€‘8Ã—22B: 32K. Llamaâ€‘3.1â€‘8B: 32K. Others fall to 16K or belowâ€ (Table 3).
    - Many models fail to stay above threshold at their own claimed length; e.g., DBRX (claims 32K) drops to 8K effective length; several â€œ32Kâ€ models are <4K effective (Togetherâ€‘7Bâ€‘32K, LongChatâ€‘7B, LongAlpacaâ€‘13B; Table 3).
  - Retrieval tasks alone can be misleading:
    - Passkey retrieval and vanilla NIAH show nearâ€‘perfect scores for most models even up to 64Kâ€“128K (Tables 10â€“11). For instance:
      > â€œLlamaâ€‘3.1â€‘8B: 100% across all lengths in passkey retrieval (Avg 100.0)â€ (Table 10),
      yet its average over RULER drops to 77.0 at 128K (Table 3).
- Detailed behavior analyses (Sec. 5; Figs. 2â€“3)
  - Changing â€œneedleâ€ type reduces robustness:
    - Yiâ€‘34B performs well with wordâ€“number needles but degrades when keys/values are `UUID`s; sometimes returns incomplete 32â€‘digit UUIDs at >128K (Fig. 2, left).
  - Hard distractors lower precision:
    - In `MKâ€‘NIAH`, adding many distractor keys steadily reduces accuracy; with a â€œFULL haystackâ€ of distractors at 256K, Yi drops by âˆ¼40 points (Fig. 2, middleâ€‘left). Errors often come from retrieving a nearby (incorrect) valueâ€”coarse rather than precise matching.
  - Multiâ€‘item recall is fragile:
    - Increasing the number of required queries in `MQâ€‘NIAH` from 1 to 8 drops Yi by ~15 points at long lengths (Fig. 2, right). In `MVâ€‘NIAH`, Yi often duplicates some answers while missing others (Fig. 2, middleâ€‘right).
  - Multiâ€‘hop tracing degrades with scale and complexity:
    - In `VT`, more hops consistently reduce accuracy as length grows; with more parallel chains, degradation is pronounced beyond 128K (Fig. 3, left and middleâ€‘left). Common mistakes include returning empty strings or variables from other chains.
  - Aggregation reveals counting failures and prompt copying:
    - In `CWE`, Yi frequently copies the inâ€‘context example verbatim at long lengths (>80% of outputs at 128K), a behavior also seen in LWM and LongAlpaca but less in Mixtral (Sec. 5).
    - In `FWE`, lowering `Î±` (harder counting) markedly reduces accuracy for Yi (Fig. 3, middleâ€‘right). Some other models ignore the context and output highâ€‘frequency English stopwords (e.g., â€œtheâ€, â€œaâ€)â€”a sign of relying on parametric priors (Sec. 5).
  - Longâ€‘context QA approaches noâ€‘context behavior:
    - As distractors increase, Yiâ€™s QA accuracy trends toward its noâ€‘context baseline (Fig. 3, right), indicating hallucination and diminished use of provided context at large lengths.
- Ablations on scale, training length, and architecture (Sec. 6; Fig. 4)
  - Training length: Longer training windows usually help but not monotonically; e.g., `LWMâ€‘1M` can be worse than `LWMâ€‘512K` at 256K, possibly due to subâ€‘optimal adjustment to RoPE base frequency (Fig. 4, left/middleâ€‘left).
    - â€œRoPEâ€ (rotary positional embeddings) is a positional encoding; changing its base frequency is a common method to extend context, but requires careful training.
  - Model size: Bigger models degrade less and start higher; Yiâ€‘34B â‰« Yiâ€‘6B/9B (Fig. 4, middleâ€‘right).
  - Architecture: RWKVâ€‘v5â€‘7B and Mambaâ€‘2.8B fall sharply by 8K and lag far behind Llamaâ€‘2â€‘7B even at short lengths (Fig. 4, right).
- Do the experiments support the claims?
  - Yes. The breadth of tasks (13 configurations across 4 categories), multiple lengths (4Kâ€“128K), and model coverage (17 aligned LLMs) give a robust picture. The contrast between nearâ€‘perfect NIAH (Tables 10â€“11) and much lower RULER averages (Table 3) convincingly shows that vanilla retrieval is insufficient to gauge longâ€‘context competence.
  - Failure cases and robustness checks are explicit (Figs. 2â€“3), and ablations probe key design factors (Fig. 4). One subjective element is the threshold choice for â€œeffective lengthâ€ (85.6% = Llamaâ€‘2â€‘7Bâ€‘chat at 4K), which affects categorization but not the relative trends (Sec. 4; Table 3).

## 6. Limitations and Trade-offs
- Synthetic focus and proxy validity
  - RULER deliberately uses synthetic tasks to control length/difficulty and reduce parametric knowledge. While this isolates behavior, the paper acknowledges the need to verify correlation with realistic longâ€‘context tasks (Sec. 8: â€œLack of correlation with realistic longâ€‘context tasksâ€).
- Position control
  - RULER v1 does not vary the exact position (depth) of key information to test â€œlostâ€‘inâ€‘theâ€‘middleâ€ effects within the same length; adding position control is future work (Sec. 8).
- Shortâ€‘context coverage
  - Chosen tasks are tuned so most models perform reasonably at 4K; the benchmark focuses on degradation with length. Harder shortâ€‘length tasks exist but are not reported here (Sec. 8).
- Prompt and hyperparameter sensitivity
  - Only limited exploration of prompt robustness and certain hyperparameters (e.g., variable name length in `VT`, vocabulary size in `CWE/FWE`) was performed (Sec. 8).
- Threshold choice for â€œeffective context lengthâ€
  - The 85.6% threshold (Llamaâ€‘2â€‘7Bâ€‘chat at 4K) is a reasonable but ultimately arbitrary reference; different thresholds would shift the absolute effective lengths though not the qualitative trends (Sec. 4; Table 3).
- Compute and scope
  - Even with synthetic generation, testing 500 examples per taskÃ—lengthÃ—model across 13 tasks and 6 lengths is computeâ€‘intensive; broader coverage (e.g., 256K for all models) was only done in focused analyses (Yiâ€‘34B; Sec. 5; Figs. 2â€“3).

## 7. Implications and Future Directions
- Reframing contextâ€‘window claims
  - A modelâ€™s advertised context size is not the same as its effective context size. Practitioners should demand multiâ€‘behavior, multiâ€‘length evidence like RULERâ€™s: retrieval robustness, multiâ€‘hop tracing, aggregation, and QA under distractors (Table 3; Sec. 5).
- Benchmarking practice
  - Vanilla NIAH or passkey tests (Tables 10â€“11) are necessary but far from sufficient. Benchmarks should include:
    - Multiple needle types and hard distractors,
    - Multiâ€‘item recall (MV/MQ),
    - Aggregation/counting under controlled distributions,
    - QA with distractor paragraphs from the same domain.
- Model development guidance
  - Training implications suggested by ablations (Sec. 6; Fig. 4):
    - Scale helps: larger models better sustain performance at long lengths.
    - Training windows matter, but simply increasing context length isnâ€™t a silver bullet; proper adaptation of positional encoding (e.g., RoPE base frequency) is crucial.
    - Nonâ€‘Transformer alternatives need substantial improvements to compete on longâ€‘context tasks tested here.
  - Mitigating failure modes:
    - Resist prompt copying at long lengths (e.g., curriculum with varied demonstrations, antiâ€‘copy objectives).
    - Improve multiâ€‘item recall and distractor rejection (e.g., training with synthetic distractors, contrastive objectives).
    - Strengthen aggregation/counting (e.g., explicit counting heads, toolâ€‘augmented counting, or hybrid retrieval+aggregation pipelines).
    - Reduce reliance on parametric priors in aggregation/QA via training that enforces context use (e.g., contextâ€‘faithfulness objectives).
- Future research with RULER
  - Add positionâ€‘controlled placements to probe depth effects (â€œlost in the middleâ€).
  - Bridge to realistic longâ€‘document tasks (e.g., aligning RULER performance with real summarization or legal/biomedical QA).
  - Explore instructionâ€‘following and reasoning at long length (not heavily tested here).
  - Expand to multimodal longâ€‘context settings (images + text over long sequences).

Blockâ€‘quoted highlights for quick reference
- Table 3 (overall longâ€‘context performance, 13 tasks):
  > â€œGeminiâ€‘1.5â€‘Pro: wAvg(inc)=95.5 (1st), wAvg(dec)=96.1 (1st); GPTâ€‘4: 89.0 (2nd), 94.1 (2nd); Llamaâ€‘3.1â€‘70B: 85.5 (4th), 93.7 (3rd). Effective lengths: GPTâ€‘4=64K; Llamaâ€‘3.1â€‘70B=64K; Qwenâ€‘2â€‘72B=32K; GLMâ€‘4â€‘9B=64K.â€
- Retrieval is easy; RULER is not:
  > â€œAlmost all models have perfect score [in passkey retrieval and vanilla NIAH] at their claimed lengthâ€ (Tables 10â€“11), yet many â€œfail to maintain performance above the Llamaâ€‘2â€‘7B baseline at their claimed lengthâ€ on RULER (Table 3).
- Failure patterns at scale (Yiâ€‘34B; Figs. 2â€“3):
  > â€œIncreased distractors and multiple queries/values reduce accuracy substantially; models copy prompt examples at 128K in `CWE`; QA accuracy trends toward noâ€‘context baselines as distractors increase.â€

In sum, RULER offers a rigorous, behaviorâ€‘focused way to measure longâ€‘context competence. It shows that genuine longâ€‘context understanding is more than surviving a single â€œneedleâ€ testâ€”and that many models are not there yet, especially when asked to trace, aggregate, and answer precisely at 32Kâ€“128K tokens and beyond.

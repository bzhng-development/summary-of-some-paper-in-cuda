# INSTRUCTION TUNING WITH GPT-4

**ArXiv:** [2304.03277](https://arxiv.org/abs/2304.03277)

## üéØ Pitch

This paper pioneers the use of GPT-4 as a high-quality generator of both instruction-following data and feedback for fine-tuning open-source language models, specifically LLaMA. By leveraging GPT-4 to produce and evaluate 52K English and Chinese instruction‚Äìresponse pairs, the authors demonstrate significant boosts in zero-shot performance, showing that open-source models can reach quality levels competitive with commercial systems‚Äîgreatly advancing scalable alignment and generalization in multilingual AI assistants.

---

## 1. Executive Summary
This paper shows how to use `GPT-4` as an automatic data generator and judge to instruction-tune open-source language models, specifically `LLaMA` 7B. By replacing earlier GPT‚Äë3.5‚Äìgenerated data with 52K `GPT-4`‚Äìgenerated instruction‚Äìresponse pairs (and adding Chinese variants plus GPT‚Äë4‚Äìproduced feedback), the tuned models achieve stronger zero-shot performance on unseen tasks and competitive quality relative to larger or proprietary systems, as evidenced by human and GPT‚Äë4‚Äìbased evaluations (Sections 2‚Äì4; Figures 3‚Äì5).

## 2. Context and Motivation
- Problem addressed
  - Building language models that reliably follow natural-language instructions without expensive human-written data or feedback. The work targets scalable ‚Äúinstruction tuning‚Äù‚Äîsupervised fine-tuning on instruction‚Äìresponse pairs‚Äîusing machine-generated data (Abstract; Section 1).
- Why it matters
  - Instruction-following ability underpins practical assistants (question answering, writing, coding). High-quality supervision improves zero-shot generalization (i.e., performing new tasks without task-specific training), making models more useful in the real world (Introduction).
- Prior approaches and gaps
  - Self-Instruct methods bootstrap training data using a strong ‚Äúteacher‚Äù model (e.g., GPT‚Äë3.5) to produce instructions and responses (Wang et al., 2022a). Open-source models like Alpaca and Vicuna rely on GPT‚Äë3.5 outputs or ShareGPT logs, but these can be limited in response quality and breadth, and are costly to extend to cross-lingual settings or to gather comparison data for reward modeling (Introduction; Section 3.2).
- Positioning
  - This is, to the authors‚Äô knowledge, the first attempt to use `GPT‚Äë4` both as:
    - A data generator for instruction‚Äìresponse pairs in English and Chinese (Section 2; Algorithm 1).
    - A feedback source to score and compare model outputs for reward modeling and evaluation (Section 2 ‚ÄúComparison Data‚Äù; Section 3.2; Figure 2).
  - The paper provides a controlled comparison: instruction-tuning `LLaMA` 7B on the same 52K instructions as Alpaca but replacing the GPT‚Äë3.5 outputs with GPT‚Äë4 outputs, enabling direct measurement of the teacher swap (Section 2; Section 3.1).

## 3. Technical Approach
Step-by-step pipeline

- Data generation (Section 2; Algorithm 1)
  - Base instruction set: reuse the 52K unique instructions from Alpaca (English) to ensure an apples-to-apples test of teacher quality (GPT‚Äë3.5 vs GPT‚Äë4).
  - Prompting: two templates handle cases with and without extra input context. The unified template is:
    - ‚ÄúBelow is an instruction ‚Ä¶ Write a response that appropriately completes the request.‚Äù
    - It embeds `### Instruction`, optional `### Input`, and expects `### Response:` (Algorithm 1, lines 3‚Äì10).
  - GPT‚Äë4 call settings:
    - `model="gpt-4"`, `temperature=1.0`, `top_p=1.0`, `max_tokens=512` (Algorithm 1, lines 12‚Äì18).
    - Temperature and top‚Äëp allow diverse responses; `max_tokens=512` caps response length.
  - Chinese data: Translate the 52K instructions to Chinese using ChatGPT, then ask GPT‚Äë4 to respond in Chinese, producing a parallel Chinese instruction-following set (Section 2, item (2)).
  - Comparison data for feedback: For each prompt, collect responses from three systems (GPT‚Äë4, GPT‚Äë3.5, OPT‚ÄëIML) and have GPT‚Äë4 rate them on a 1‚Äì10 scale and provide pairwise comparisons, forming training data for a reward model (Section 2, item (3); Figure 2).
  - Unnatural Instructions test: Generate GPT‚Äë4 answers on a separate benchmark of 68K synthetic instruction‚Äìresponse triplets to measure alignment on ‚Äúunusual‚Äù instructions (Section 2, item (4)).

- Instruction-tuning (Section 3.1)
  - Models trained
    - `LLaMA-GPT4` (7B) on the 52K English instruction‚Äìresponse pairs from GPT‚Äë4.
    - `LLaMA-GPT4-CN` (7B) on the 52K Chinese instruction‚Äìresponse pairs from GPT‚Äë4.
  - Training schedule: follows Alpaca‚Äôs setup to isolate the effect of switching to GPT‚Äë4 data (Section 3.1).

- Reward modeling (Section 3.2; Figure 2)
  - Purpose: estimate user preference for responses to enable ranking or future RLHF.
  - Data: From GPT‚Äë4‚Äôs 1‚Äì10 ratings of multiple responses per prompt, convert to pairwise preferences (higher‚Äëscored `y_h` vs lower‚Äëscored `y_l`), producing many training pairs per prompt.
  - Model: `OPT 1.3B` used as the reward model `r_Œ∏`.
  - Loss: pairwise logistic preference loss encourages `r_Œ∏(x, y_h)` > `r_Œ∏(x, y_l)`:
    - In words: increase the score gap so that preferred responses receive higher scalar rewards.
    - In notation (Section 3.2): minimize `log(œÉ(r_Œ∏(x, y_h) ‚àí r_Œ∏(x, y_l)))`, where `œÉ` is the sigmoid.
  - Use in this paper: only for re-ranking multiple decoded samples per prompt during evaluation; no reinforcement learning is applied to the base model here (Section 4.3; Conclusions).

- Why these choices?
  - Reusing Alpaca‚Äôs 52K instructions isolates the effect of teacher quality (GPT‚Äë3.5 vs GPT‚Äë4) without confounding from instruction design or data size (Section 2).
  - GPT‚Äë4 as both generator and judge lowers the cost of high-quality supervision and preference data, addressing a key bottleneck in RLHF pipelines (Section 2; Section 3.2).
  - Chinese translation plus GPT‚Äë4 responses test cross-lingual generalization and yield a ready-made Chinese instruction-tuned model (Section 2, item (2)).

- Data characteristics (Figure 1)
  - The paper probes stylistic/content differences between GPT‚Äë3.5 and GPT‚Äë4 outputs by extracting root verb‚Äìdirect object noun pairs from each response.
    - Unique pairs: GPT‚Äë4 (5,229) vs GPT‚Äë3.5 (6,133) (Figure 1c).
    - GPT‚Äë4 tends to produce longer responses on average, but GPT‚Äë3.5 shows a longer tail in length distribution due to the Alpaca team‚Äôs iterative deduplication process not used here (Figure 1d; Section 2 ‚ÄúData Statistics‚Äù).

## 4. Key Insights and Innovations
- Using GPT‚Äë4 as a high-quality teacher for instruction tuning
  - What‚Äôs new: swap GPT‚Äë3.5 teacher outputs with GPT‚Äë4‚Äôs on the same 52K Alpaca instructions, producing English and Chinese datasets (Section 2).
  - Why it matters: This isolates the effect of teacher quality and shows notable gains in zero-shot helpfulness and competitive performance against stronger baselines (Figure 3a; Figure 4c‚Äìd).

- GPT‚Äë4‚Äìgenerated comparison data to train a reward model cheaply
  - What‚Äôs new: GPT‚Äë4 acts as an automatic rater and comparator across responses from multiple models (GPT‚Äë4, GPT‚Äë3.5, OPT‚ÄëIML), producing large-scale preference data (Section 2, item (3); Figure 2).
  - Why it matters: Preference data is expensive when collected from humans; this enables training a working reward model (`OPT` 1.3B) that aligns with GPT‚Äë4 judgments and improves decoding via re-ranking (Section 3.2; Figure 4a‚Äìb).

- Cross-lingual instruction tuning with Chinese data
  - What‚Äôs new: A parallel 52K Chinese instruction-following dataset and a Chinese-tuned model `LLaMA-GPT4-CN` (Section 3.1; Section 4.3 Figure 5c).
  - Why it matters: Demonstrates that the pipeline extends beyond English, and that a Chinese-tuned model substantially improves over an English-tuned model when evaluated in Chinese (Figure 5c: 64% vs 35% relative score vs GPT‚Äë4).

- Transparent evaluation with both humans and GPT‚Äë4
  - What‚Äôs new: Combine human HHH evaluation (helpful, honest, harmless) on user-oriented tasks with GPT‚Äë4 automatic pairwise scoring on challenging prompts (Section 4.2; Section 4.3).
  - Why it matters: Provides triangulated evidence. Human judges prefer the GPT‚Äë4‚Äìtuned model over the GPT‚Äë3.5‚Äìtuned Alpaca for helpfulness (Figure 3a), and GPT‚Äë4‚Äìbased evaluation ranks the GPT‚Äë4‚Äìtuned model above Alpaca and raw LLaMA (Figure 4c‚Äìd).

These are primarily methodological and empirical innovations (stronger teacher, automated feedback, cross-lingual extension) rather than theoretical advances.

## 5. Experimental Analysis
- Datasets and evaluation setup (Section 4.1)
  - Human evaluation: 252 ‚ÄúUser-Oriented-Instructions-252‚Äù prompts covering practical applications (writing, coding, etc.). MTurk interface enforces Helpful/Honest/Harmless comparisons between two models‚Äô outputs (Appendix A.1; Figure 7).
  - GPT‚Äë4 automatic evaluation: 80 challenging prompts synthesized in the Vicuna evaluation set (Section 4.1; Figure 4). GPT‚Äë4 assigns 1‚Äì10 scores to each model‚Äôs output in pairwise comparisons; the total over 80 items (max 800) is used. Results are reported relative to the opponent model‚Äôs total score (Figure captions).
  - Unnatural Instructions: 68,478 instruction‚Äìresponse triplets; 9K used for ROUGE‚ÄëL analysis, grouped by ground-truth response length (Figure 6).

- Baselines and comparators (Figures 3‚Äì5)
  - Open models: `LLaMA` (13B), `Alpaca` (13B, GPT‚Äë3.5 tuned), `Vicuna` (13B).
  - Commercial systems: `ChatGPT`, `Bard`, `GPT‚Äë4`.
  - This work: `LLaMA-GPT4` (7B), and a re-ranked variant `LLaMA-GPT4 (R1)` using the reward model‚Äôs top-1 selection from five decoded samples (Figure 4a‚Äìb).

- Human HHH evaluation results (Figure 3)
  - LLaMA‚ÄëGPT4 vs Alpaca (Figure 3a; 252 prompts):
    - Helpfulness:
      > LLaMA‚ÄëGPT4 wins 54.12%; Alpaca wins 19.74%; ties 26.14%.
    - Honesty:
      > LLaMA‚ÄëGPT4 31.39%; Alpaca 25.99%; ties 42.61%.
    - Harmlessness:
      > Alpaca 25.43%; LLaMA‚ÄëGPT4 16.48%; ties 58.10%.
    - Takeaway: Switching to GPT‚Äë4 responses notably improves perceived helpfulness and slightly honesty, while harmlessness is similar overall (high ties) with a slight edge to Alpaca.
  - LLaMA‚ÄëGPT4 vs GPT‚Äë4 (Figure 3b):
    - Helpfulness:
      > GPT‚Äë4 44.11%; LLaMA‚ÄëGPT4 42.78%; ties 13.11%.
    - Honesty:
      > LLaMA‚ÄëGPT4 37.88%; GPT‚Äë4 37.48%; ties 24.64%.
    - Harmlessness:
      > GPT‚Äë4 35.36%; LLaMA‚ÄëGPT4 31.66%; ties 32.98%.
    - Takeaway: On these user-oriented tasks, the 7B LLaMA tuned on GPT‚Äë4 data is surprisingly close to GPT‚Äë4 itself across HHH criteria.

- GPT‚Äë4 automatic evaluation on 80 challenging prompts (Figure 4)
  - Effect of reward-model re-ranking for `LLaMA-GPT4`:
    - Against ChatGPT (Figure 4a):
      > Baseline 609:666 (91%); top‚Äë1 re-ranked 624:667 (94%); others 85‚Äì92%.
    - Against GPT‚Äë4 (Figure 4b):
      > Baseline 606:726 (83%); top‚Äë1 re-ranked 631:722 (87%); others 83‚Äì85%.
    - Takeaway: The reward model‚Äôs top‚Äë1 selection yields consistent, modest gains (2‚Äì4 percentage points).
  - Overall standings against strong opponents (Figures 4c‚Äìd):
    - Against ChatGPT (Figure 4c):
      > LLaMA (13B) 72%; Alpaca (13B) 83%; Vicuna (13B) 99%; LLaMA‚ÄëGPT4 (7B) 91%; LLaMA‚ÄëGPT4 (7B, R1) 94%; Bard 96%; ChatGPT 100%; GPT‚Äë4 118%.
    - Against GPT‚Äë4 (Figure 4d):
      > LLaMA (13B) 71%; Alpaca (13B) 80%; Vicuna (13B) 89%; LLaMA‚ÄëGPT4 (7B) 83%; LLaMA‚ÄëGPT4 (7B, R1) 87%; Bard 88%; ChatGPT 91%; GPT‚Äë4 100%.
    - Takeaway: Despite using only 7B parameters, `LLaMA‚ÄëGPT4` outperforms raw LLaMA and Alpaca (both 13B) and closes part of the gap to Vicuna, Bard, and ChatGPT.

- Chinese evaluations (Figure 5)
  - When all models answer in English and outputs are translated to Chinese, compared against GPT‚Äë4‚Äôs translated Chinese outputs (Figure 5a):
    > LLaMA (13B) 67%; Alpaca (13B) 76%; Vicuna (13B) 93%; LLaMA‚ÄëGPT4 (7B) 87%; LLaMA‚ÄëGPT4 (R1) 89%; Bard 92%; ChatGPT 95%; GPT‚Äë4 100%.
  - When all models answer in English but compared to GPT‚Äë4‚Äôs Chinese outputs generated directly from Chinese prompts (Figure 5b):
    > Scores are slightly higher for GPT‚Äë4 itself (109%), and relative ordering remains consistent (e.g., LLaMA‚ÄëGPT4 (R1) 94%).
    - Insight highlighted in the text: GPT‚Äë4‚Äôs own translated answers outperform its native Chinese answers on this benchmark, consistent with stronger English capability (Section 4.3).
  - When all models are prompted and answer in Chinese (Figure 5c):
    > Alpaca (13B) 33%; LLaMA‚ÄëGPT4 (7B) 35%; LLaMA‚ÄëGPT4‚ÄëCN (7B) 64%; Vicuna (13B) 79%; GPT‚Äë4 92%.
    - Takeaway: Training directly on Chinese GPT‚Äë4 data (`LLaMA‚ÄëGPT4‚ÄëCN`) substantially improves over English‚Äëtuned `LLaMA‚ÄëGPT4` when evaluated in Chinese (64% vs 35%).

- Unnatural Instructions (Figure 6; 9K samples; ROUGE‚ÄëL)
  - Mean ROUGE‚ÄëL:
    > Alpaca 0.39; GPT‚Äë4 0.37; LLaMA‚ÄëGPT4 0.34.
  - Trend vs ground-truth response length:
    > For longer expected answers (length > 10), GPT‚Äë4 and LLaMA‚ÄëGPT4 close the gap or outperform, suggesting better handling of creative/long-form outputs; shorter answers favor Alpaca, likely because GPT‚Äë4‚Äëstyle chatty elaborations dilute n‚Äëgram overlap with concise reference answers (Section 4.3; Figure 6 bars and legend).
  - Takeaway: ROUGE‚ÄëL, which rewards literal overlap, can penalize high-quality but more verbose or stylistically different answers; the authors caution that lower ROUGE for GPT‚Äë4‚Äëstyle outputs may not reflect worse usefulness (discussion around Figure 6).

- Do the experiments support the claims?
  - Yes, for core claims:
    - Human and GPT‚Äë4 evaluations both favor the GPT‚Äë4‚Äìtuned model over GPT‚Äë3.5‚Äìtuned Alpaca, especially on helpfulness and overall pairwise quality (Figure 3a; Figure 4c‚Äìd).
    - Reward-model ranking provides consistent incremental gains (Figure 4a‚Äìb).
    - The Chinese-tuned model yields large gains in Chinese tasks (Figure 5c).
  - Caveats:
    - Heavy reliance on GPT‚Äë4 as the evaluator raises potential circularity (the judge agrees with the teacher). Human HHH results help mitigate this, but are limited to 252 prompts (Section 4.2; Figure 3).

## 6. Limitations and Trade-offs
- Data sourcing and scope
  - The 52K instructions are reused from Alpaca; there is no new instruction induction or iterative filtering, and the collection is a one-time generation rather than a self-expanding set (Section 2 ‚ÄúWe leave it as future work‚Ä¶‚Äù). This may limit instruction diversity.
  - The output distribution differs from Alpaca‚Äôs, partly because Alpaca iteratively removed similar instructions while this work did not (Figure 1d discussion).
- Model scale and training
  - Only the 7B `LLaMA` base is instruction-tuned, while some baselines are 13B and commercial systems are far larger, leaving potential headroom untested (Conclusions).
  - RLHF is not applied to update the policy; the reward model is used only for decoding-time ranking (Section 4.3; Conclusions). End-to-end RLHF might yield larger gains but was not explored.
- Evaluation design
  - GPT‚Äë4 provides much of the scoring in automatic evaluations (Figures 4‚Äì5), introducing bias toward its own style. Human evaluation adds balance but is smaller-scale (252 items) and mostly single-turn (Section 4.2).
  - ROUGE‚Äëbased analysis penalizes verbose, chat-like responses; results on Unnatural Instructions should be interpreted with this limitation (Figure 6).
- Language and task coverage
  - Chinese instruction set is produced by translating English instructions with ChatGPT before answering with GPT‚Äë4, so it may inherit translation artifacts; truly native Chinese instruction design is not covered (Section 2 item (2)).
  - Multi-turn dialogue, tool use, or grounded tasks are not addressed; all setups are single-turn instruction following (Algorithm 1; Section 3.1).

## 7. Implications and Future Directions
- How this changes the landscape
  - It validates a practical recipe: use a stronger proprietary model (`GPT‚Äë4`) to bootstrap high-quality instruction data and automatic preference labels, then fine-tune and re-rank open models to get competitive assistants at modest parameter counts (Sections 2‚Äì4; Figures 3‚Äì5).
  - It shows cross-lingual portability: the same pipeline yields meaningful Chinese instruction followers and highlights the benefit of native-language tuning (Figure 5c).

- Follow-up research enabled or suggested
  - Scale both data and models
    - Expand instruction‚Äìresponse pairs well beyond 52K and tune larger LLaMA variants (e.g., 13B and above). The paper anticipates combining GPT‚Äë4 data with multi-turn datasets like ShareGPT for further gains (Conclusions).
  - Full RLHF with machine feedback
    - Move beyond decoding-time ranking to train the base policy via reinforcement learning using the GPT‚Äë4‚Äìderived reward model (Section 3.2; Conclusions). Investigate how far ‚ÄúAI feedback‚Äù can substitute for human feedback and where it fails.
  - Diversify evaluators and metrics
    - Incorporate human evaluations at larger scale; use task-grounded metrics and robustness checks to reduce style bias from a GPT‚Äë4 judge. For short-answer tasks, employ exactness-focused metrics alongside semantic similarity to mitigate verbosity penalties (Figure 6 discussion).
  - Richer tasks and multilingual expansion
    - Extend to multi-turn dialogues, tool integration, and domain-specific settings (coding, math, safety-sensitive advice). Create native instructions in more languages rather than translated ones (Section 2 item (2)).
  - Data curation and quality control
    - Re-introduce iterative instruction generation and deduplication to expand diversity (noted as missing here relative to Alpaca), and analyze how instruction composition affects generalization (Figure 1d discussion).

- Practical applications
  - Organizations can train capable assistants on commodity hardware by:
    - Collecting instruction‚Äìresponse data with GPT‚Äë4 using the provided prompt template and settings (Algorithm 1).
    - Fine-tuning smaller open models (`LLaMA` 7B) and using a lightweight reward model (`OPT` 1.3B) to re-rank outputs, achieving substantial gains over untuned or GPT‚Äë3.5‚Äìtuned baselines (Figures 4c‚Äìd).
  - Cross-lingual deployments benefit from generating language-specific responses (Figure 5c), rather than relying solely on translation.

Overall, the work provides a clear, reproducible pathway to upgrade open models using GPT‚Äë4 as both teacher and critic, with evidence from human and automatic evaluations that this pipeline delivers tangible improvements, particularly in helpfulness and cross-lingual capability.

# Insights into DeepSeekâ€‘V3: Scaling Challenges and Reflections on Hardware for AI Architectures

**ArXiv:** [2505.09343](https://arxiv.org/abs/2505.09343)
**Authors:** Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Huazuo Gao, Jiashi Li, Liyue Zhang, Panpan Huang, Shangyan Zhou, Shirong Ma, Wenfeng Liang, Ying He, Yuqing Wang, Yuxuan Liu, Y.Â X.Â Wei
**Institutions:** (Not explicitly listed in the abstract)

## ğŸ¯ Pitch

This paper introduces DeepSeek-V3/R1, a novel hardware-model co-design strategy that allows large language models (LLMs) to be run efficiently on a 2,048-GPU cluster by targeting memory, computation, and bandwidth limitations through Multi-head Latent Attention and FP8 training. This breakthrough significantly reduces the cost and hardware requirements associated with LLMs, democratizing advanced AI capabilities for a broader range of organizations, and offering a pathway for more accessible AI innovation.

---

## 1. Executive Summary (2-3 sentences)
This paper presents a hardwareâ€“model co-design blueprint behind DeepSeekâ€‘V3/R1, showing how to train and serve state-of-the-art large language models (LLMs) on a relatively modest 2,048â€‘GPU H800 cluster by attacking the three dominant bottlenecks: memory, computation, and interconnect bandwidth. The work combines architectural choices (Multi-head Latent Attention, Mixture-of-Experts with hardware-aware routing, Multiâ€‘Token Prediction) with lowâ€‘precision FP8 training and a costâ€‘effective multiâ€‘plane network, and distills concrete requirements for nextâ€‘generation AI hardware.

## 2. Context and Motivation
- Problem this work targets
  - Modern LLMs are hitting the â€œAI memory wallâ€ and interconnect limits: memory for inference (especially Key/Value cache, or `KV cache`) scales with context length; compute efficiency falls when attention becomes memory bound; and Mixtureâ€‘ofâ€‘Experts (`MoE`) requires costly allâ€‘toâ€‘all communication.
  - Section 2.1 frames the memory trajectory: 
    > â€œLLMs generally require significant memory resources, with memory demands increasing by more than 1000% per year, while highâ€‘speed memory capacity â€¦ typically [grows] less than 50% per yearâ€ (Sec. 2.1).
  - For MoE, expert parallelism hinges on network bandwidth and latency; the interconnect often becomes the throughput limiter (Sec. 2.3.2).
- Why it matters
  - Cost and accessibility: stateâ€‘ofâ€‘theâ€‘art models commonly require clusters with â€œtens or even hundreds of thousands of GPUs or TPUsâ€ (Sec. 1.1). That bar excludes most organizations. This paper demonstrates how a smaller cluster can still reach top performance by coâ€‘designing models, software, and network fabric.
  - User experience and reasoning models: reasoningâ€‘style systems (o1/o3, DeepSeekâ€‘R1, etc.) rely on rapid token generation at long lengths; poor inference throughput directly degrades product usability (Sec. 2.3.4).
- Prior approaches and gaps
  - Dense models scale parameter count and compute uniformly, leading to huge training and serving costs.
  - KVâ€‘reduction techniques (GQA/MQA) compress cache size but donâ€™t tackle communicationâ€‘heavy MoE issues (Sec. 2.1.2).
  - Quantization is popular for inference (e.g., GPTQ/AWQ) but less so for training; FP8 training existed in vendor libraries but lacked open, largeâ€‘scale demonstrations and practical recipes for MoE (Sec. 3.1).
  - Datacenter networks usually use threeâ€‘tier fat trees or proprietary fabrics; costâ€‘effective, lowâ€‘latency alternatives that scale cleanly remain challenging (Sec. 5.1).
- How this paper positions itself
  - It is not a full model report (thatâ€™s [26]); instead it explains the coâ€‘designed mechanisms that made DeepSeekâ€‘V3/R1 efficient on 2,048 H800s: Multiâ€‘head Latent Attention (MLA) to shrink KV cache, MoE with routing tuned to the H800â€™s asymmetric bandwidths, FP8 fineâ€‘grained training, and a multiâ€‘plane, twoâ€‘layer fatâ€‘tree network (Fig. 1, Secs. 2â€“5). It also crystallizes hardware features the community should build next (Secs. 3.1.2, 3.2.2, 4.4.2, 4.5.2, 5.2.2, 6).

## 3. Technical Approach
At a glance (Fig. 1), DeepSeekâ€‘V3 integrates four pillars:
1) memoryâ€‘efficient attention (MLA), 2) sparse computation (DeepSeekMoE), 3) lowâ€‘precision training (FP8 with fineâ€‘grained scaling), and 4) inference accelerators (Multiâ€‘Token Prediction). These are then matched to hardware and network strategies (Secs. 4â€“5).

- Multiâ€‘head Latent Attention (`MLA`, Sec. 2.1.2; Fig. 1, bottom-left)
  - What it solves: the `KV cache` stores perâ€‘token Keys and Values from all attention heads during decoding; this becomes both memory and bandwidth bound. 
  - How it works: instead of caching headâ€‘wise K/V tensors, MLA compresses them into a much smaller â€œlatentâ€ vector using a trained projection. At inference, only this latent `c_t^KV` is cached (Fig. 1 shows latent vectors), and perâ€‘head K/V are reconstructed on the fly via learned projections.
  - Why this choice: it reduces perâ€‘token KV memory dramatically without rethinking the Transformer stack, and it shifts less traffic through memoryâ€‘bound GEMV paths.

- Mixtureâ€‘ofâ€‘Experts (`MoE`) with hardwareâ€‘aware routing (Sec. 2.2 and 4.3; Fig. 1, lower-right)
  - MoE recap: many â€œexpertsâ€ (Feedâ€‘Forward Networks) exist, but only a small subset is activated per token, guided by a gating router. This keeps compute per token low while growing total parameters.
  - DeepSeekMoE design highlights:
    - One shared expert plus multiple â€œrouted expertsâ€ (Fig. 1).
    - Nodeâ€‘Limited Routing (Sec. 4.3): in an 8â€‘node, 256â€‘expert setting (4 experts/GPU), each token is routed to up to 4 nodes. IB transfers to a node happen once, then intraâ€‘node NVLink forwards to the specific GPUs. This â€œdeduplicatesâ€ interâ€‘node traffic and exploits higher intraâ€‘node bandwidth.
    - Example (Sec. 4.3): if a token needs 9 experts (8 routed + 1 shared), naÃ¯vely spread across 8 nodes, interâ€‘node time is â€œ8tâ€. With nodeâ€‘limited routing and NVLink forwarding, it reduces to â€œMtâ€ where M â‰¤ 4.
  - Implementation: Expert Parallelism (`EP`) uses two allâ€‘toâ€‘all phasesâ€”`dispatch` (send token activations to experts) and `combine` (gather and reduce expert outputs). DeepSeekâ€™s `DeepEP` library overlaps these with compute (Sec. 2.3.1) and reaches nearâ€‘lineâ€‘rate bandwidth (Fig. 7).

- FP8 mixedâ€‘precision training with fineâ€‘grained scaling (Sec. 3.1; Fig. 1)
  - What is FP8: 8â€‘bit floating point formats (e.g., E4M3/E5M2) reduce memory/compute cost relative to BF16. 
  - How training is stabilized:
    - Highâ€‘precision accumulation (to internal registers) to curb rounding error.
    - Fineâ€‘grained scaling: tileâ€‘wise 1Ã—128 quantization for activations and blockâ€‘wise 128Ã—128 for weights.
    - Custom FP8 GEMMs (openâ€‘sourced as `DeepGEMM`, [77]).
  - Practical mapping: Fig. 1 annotates which forward/backward paths run in FP8 (e.g., attention and FFN core GEMMs) and which keep BF16/FP32 for numerically sensitive ops.

- Multiâ€‘Token Prediction (`MTP`) for inference speed (Sec. 2.3.3; Fig. 1, top)
  - What it does: adds shallow, singleâ€‘layer â€œheadsâ€ that predict the next 2â€“4 tokens cheaply. The main model then verifies these in parallel (a form of selfâ€‘drafting speculative decoding).
  - Why it helps: reduces sequential decoding steps, increasing effective tokens/sec while preserving accuracy.
  - Reported behavior:
    > â€œAn MTP module achieves an acceptance rate of 80%â€“90% for predicting the second subsequent token, increasing generation TPS by 1.8Ã—â€ (Sec. 2.3.3).

- Communication compression: FP8 dispatch and `LogFMT` exploration (Sec. 3.2)
  - Dispatch runs in FP8 (1 byte/element), halving traffic vs BF16; combine currently BF16 for accuracy, though FP8/E5M6/hybrids are being tested (Sec. 3.2).
  - `LogFMTâ€‘nBit` (novel): a blockâ€‘local logarithmic quantizer that maps |x| to logâ€‘space, linearly quantizes within the tileâ€™s dynamic range, and decodes with exp. 
    - Observations: LogFMTâ€‘8 outperforms E4M3/E5M2 on 7Bâ€‘scale tests for residualâ€‘branch simulation; LogFMTâ€‘10 approximates BF16 (Sec. 3.2).
    - Not deployed due to encode/decode overhead (50%â€“100%) on current GPUs (Sec. 3.2.1). The paper recommends native compression/decompression units in future NICs/I/O dies (Sec. 3.2.2).

- Hardware and parallelism choices for H800 (Sec. 4)
  - Context (Fig. 2): H800 has reduced NVLink bandwidth (400 GB/s total per node, ~160 GB/s achievable per direction cited in Sec. 4.3) vs H100, but each node includes eight 400 Gbps IB NICs.
  - Design choices (Sec. 4.2):
    - Avoid Tensor Parallelism during training (too NVLinkâ€‘heavy); optionally use for latencyâ€‘critical inference.
    - Use `DualPipe` pipeline parallelism to overlap computing with MoE comms and reduce bubbles.
    - Push Expert Parallelism (EP) hard over IB using DeepEP; allâ€‘toâ€‘all > 40 GB/s per GPU (Fig. 7).
  - Overlap strategy for throughput (Sec. 2.3.1): dual microâ€‘batch overlap decouples MoE/MLA compute and their respective dispatch/combine steps so communication is hidden behind another batchâ€™s compute; production also separates prefill (big batches) and decode (latencyâ€‘critical) onto different EP group sizes.

- Cluster network: Multiâ€‘Plane Twoâ€‘Layer Fatâ€‘Tree (`MPFT`) (Sec. 5.1; Figs. 3â€“6; Table 3)
  - Each GPUâ€‘NIC pair belongs to one of eight planes; crossâ€‘plane traffic is forwarded intraâ€‘node via NVLink/PCIe (Fig. 3).
  - The ideal design (Fig. 4) is NICs with multiple physical ports bonded into one logical interface with native outâ€‘ofâ€‘order placement; current ConnectXâ€‘7 falls short of this ideal, so the deployed MPFT uses perâ€‘GPU NICs per plane.
  - Why MPFT: a twoâ€‘layer topology lowers cost and latency vs a threeâ€‘layer fat tree while still scaling to >10k endpoints (Table 3).

- Lowâ€‘latency I/O path: InfiniBand GPUDirect Async (`IBGDA`) (Sec. 5.2.3)
  - GPUs directly post RDMA work requests and ring the NIC â€œdoorbell,â€ removing CPU proxy threads. This reduces controlâ€‘plane latency and improves manyâ€‘smallâ€‘packet sends.

## 4. Key Insights and Innovations
- MLA reduces KV cache by multiples without hurting generality (Sec. 2.1.2; Table 1)
  - Whatâ€™s new: a trained latent space that collapses all headsâ€™ K/V into a compact vector cached per token; perâ€‘head K/V are reconstructed on demand.
  - Why it matters: KV cache per token drops to 70 KB for DeepSeekâ€‘V3 vs 327 KB (Qwenâ€‘2.5â€‘72B) and 516 KB (LLaMAâ€‘3.1â€‘405B) (Table 1).
  - Quote:
    > â€œDeepSeekâ€‘V3 (MLA) 70.272 KB â€¦ Qwenâ€‘2.5 72B (GQA) 327.680 KB â€¦ LLaMAâ€‘3.1 405B (GQA) 516.096 KBâ€ (Table 1).

- Hardwareâ€‘aware MoE routing (Nodeâ€‘Limited Routing) that â€œdeduplicatesâ€ interâ€‘node traffic (Sec. 4.3)
  - Whatâ€™s new: tie the routerâ€™s Topâ€‘K expert selection to node groups so that IB traffic is minimized and highâ€‘bandwidth intraâ€‘node NVLink forwards within the node.
  - Why it matters: allâ€‘toâ€‘all is the dominant cost in EP; reducing IB fanâ€‘out directly boosts throughput. This is a fundamental systems innovation rather than a minor tuning.

- Practical FP8 training recipe for large MoE models with fineâ€‘grained scaling (Sec. 3.1; Fig. 1)
  - Whatâ€™s new: a full, open recipe that uses tile/blockâ€‘wise scaling, highâ€‘precision accumulation, and custom kernels (`DeepGEMM`) to make FP8 training robust in MoE.
  - Why it matters: halves activation/weight memory vs BF16 and lifts compute throughput; in small/medium ablations, accuracy loss is â‰¤0.25% (Sec. 2.4).

- Multiâ€‘Token Prediction (MTP) as a builtâ€‘in, trainingâ€‘time feature to enable speculative decoding (Sec. 2.3.3; Fig. 1)
  - Whatâ€™s new: lightweight oneâ€‘layer heads for nextâ€‘token(s) that can be validated in parallel; this is integrated and trained jointly.
  - Why it matters: realâ€‘world acceptance rate â€œ80â€“90%â€ for the second token yields â€œ1.8Ã—â€ tokens/sec, which is critical for reasoningâ€‘length outputs.

- Multiâ€‘Plane, twoâ€‘layer fat tree that matches singleâ€‘plane multiâ€‘rail performance at lower cost/latency (Sec. 5.1; Figs. 5â€“6; Table 3)
  - Whatâ€™s new: MPFT shows allâ€‘toâ€‘all performance comparable to MRFT thanks to NCCL PXN pathing (Figs. 5â€“6) while enabling >10k endpoints with two switching tiers.
  - Why it matters: achieves cost/latency comparable or better than threeâ€‘tier fat trees and competitive with Slim Fly (Table 3).

## 5. Experimental Analysis
- What is evaluated and how
  - Systemâ€‘level communication and training throughput, not endâ€‘task quality (those are in [26]). The paper measures NCCL allâ€‘toâ€‘all bandwidth/latency, DeepEP kernels under EP traffic, protocol latency, and endâ€‘toâ€‘end training throughput/MFU on 2,048 GPUs.
- Main quantitative findings
  - KV cache reduction via MLA (Table 1):
    > â€œ70.272 KB per tokenâ€ with MLA vs â€œ327.680 KBâ€ (Qwenâ€‘2.5â€‘72B, GQA) and â€œ516.096 KBâ€ (LLaMAâ€‘3.1â€‘405B, GQA).
  - Training compute cost advantage of MoE (Table 2; seq len 4096):
    > DeepSeekâ€‘V3 MoE: â€œ250 GFLOPs/tokenâ€ vs dense 72B: â€œ394 GFLOPs/tokenâ€ and dense 405B: â€œ2448 GFLOPs/token.â€
  - Theoretical decode upper bound (Sec. 2.3.2):
    > With 400 Gbps IB and dualâ€‘microbatch overlap: â€œ14.76 ms TPOT (â‰ˆ67 tok/s)â€; with GB200 NVL72â€‘class scaleâ€‘up bandwidth: â€œ>0.82 ms TPOT (â‰ˆ1200 tok/s).â€ These are analytical upper limits, not measured endâ€‘toâ€‘end.
  - MTP effectiveness (Sec. 2.3.3):
    > â€œ80â€“90% acceptance for the second token â€¦ 1.8Ã— TPS improvement.â€
  - Allâ€‘toâ€‘all and EP kernel performance (Figs. 5â€“7):
    - Fig. 5/6: MPFT vs MRFT allâ€‘toâ€‘all bandwidth/latency are â€œnearly identicalâ€ from 32 to 128 GPUs.
    - Fig. 7: DeepEP achieves >40 GB/s per GPU for both dispatch and combine across 16â€“128 GPUs, â€œnearly saturating the 400 Gbps NIC bandwidth.â€
  - Endâ€‘toâ€‘end training throughput on 2,048 GPUs (Table 4):
    > Tokens/day: â€œ272.80B (MPFT) vs 272.52B (MRFT)â€; MFU (causal): â€œ38.94% vs 38.90%â€; 1F1B time: â€œ13.95 s vs 14.00 s.â€ Differences are within noise.
  - Protocol latency (Table 5):
    > For 64â€‘byte messages, intraâ€‘leaf: IB â€œ2.8 Î¼sâ€ vs RoCE â€œ3.6 Î¼sâ€; crossâ€‘leaf: IB â€œ3.7 Î¼sâ€ vs RoCE â€œ5.6 Î¼s.â€ NVLink intraâ€‘node is â€œ3.33 Î¼s.â€
  - Network cost/scalability (Table 3):
    > MPFT: â€œ16,384 endpoints â€¦ $72M total â€¦ $4.39k per endpoint,â€ versus a 3â€‘layer fat tree at â€œ65,536 endpoints â€¦ $491M â€¦ $7.5k per endpoint.â€
- Robustness checks and ablations
  - FP8 training ablations show â‰¤0.25% relative loss on 16B and 230B DeepSeekâ€‘V2 models before integrating FP8 into V3 (Sec. 2.4).
  - LogFMT is validated on ~7B dense models (residualâ€‘branch simulation) and found superior to E4M3/E5M2 at 8 bits; 10â€‘bit approaches BF16 (Sec. 3.2), but not deployed due to encode/decode overhead.
- Do the experiments support the claims?
  - For system performance and cost effectiveness, yes: network microbenchmarks (Figs. 5â€“7), protocol latency (Table 5), and largeâ€‘scale training throughput (Table 4) directly support the MPFT + EP design and the routing/overlap strategies. The KV cache and GFLOPs/token tables (1â€“2) quantify MLA and MoE efficiency.
  - For endâ€‘task quality, this paper references the technical report [26]; it focuses on systems efficiency rather than benchmarks like MMLU or coding/math datasets.
  - The 14.76 ms and 0.82 ms TPOT are theoretical bestâ€‘cases under aggressive overlap assumptions (Sec. 2.3.2); they motivate network design but are not empirical endâ€‘toâ€‘end latency numbers.

## 6. Limitations and Trade-offs
- Architectural and hardware assumptions
  - The routing/coâ€‘design assumes strong intraâ€‘node bandwidth (NVLink) and multiple NICs per node (H800 with eight 400 Gbps IB NICs, Fig. 2). Other platforms with different scaleâ€‘up/scaleâ€‘out ratios may need different routing policies.
  - Dual microâ€‘batch overlap and prefill/decode disaggregation assume batched, mixed workloads (Sec. 2.3.1); singleâ€‘stream, lowâ€‘batch decoding will see less overlap benefit.
- FP8 constraints on current GPUs (Sec. 3.1.1)
  - Accumulation precision inside Tensor Cores is limited (e.g., 13 fraction bits accumulated into FP22 registers), which can hurt stability; the paper calls for configurable or FP32 accumulation (Sec. 3.1.2).
  - Fineâ€‘grained scaling introduces dequantization overhead as partial results move between Tensor Cores and CUDA cores (Sec. 3.1.1).
- Communication compression tradeâ€‘offs (Sec. 3.2)
  - LogFMT improves quantization quality at same bitâ€‘width but encode/decode adds â€œ50%â€“100%â€ overhead on current GPUs; thus not deployed (Sec. 3.2.1).
  - Combine remains BF16 for accuracy; pushing combine to FP8/E5M6 may incur minor quality loss (work in progress, Sec. 3.2).
- Scaleâ€‘up/scaleâ€‘out mismatch and SM contention (Sec. 4.4.1)
  - Due to unbalanced NVLink vs IB bandwidths, the software pipeline uses GPU SMs to forward, reduce, and manage data layouts, consuming â€œup to 20 SMsâ€ per H800 for communication chores during training. This steals compute from kernels.
- Bandwidth contention during inference (Sec. 4.5.1)
  - PCIe/NVLink traffic (e.g., CPUâ†”GPU KV cache movement) can contend with EP allâ€‘toâ€‘all, causing latency spikes. Dynamic prioritization is not well supported by todayâ€™s interconnects (Sec. 4.5.2).
- Multiâ€‘plane deployment gaps (Sec. 5.1)
  - The ideal MPFT requires NICs with multiple bonded ports and native outâ€‘ofâ€‘order placement (Fig. 4). ConnectXâ€‘7 lacks this; crossâ€‘plane traffic needs intraâ€‘node forwarding, adding extra hops/latency in some inference paths.
- Evaluation scope
  - This paper emphasizes systems efficiency. Comprehensive model quality results, safety, and longâ€‘context accuracy tradeâ€‘offs are in the technical report [26]; they are not reproduced here.

## 7. Implications and Future Directions
- How this work shifts the field
  - It demonstrates that with the right coâ€‘design, a 2,048â€‘GPU H800 cluster can train and serve frontierâ€‘class MoE models costâ€‘effectively by:
    - Slashing KV cache with MLA (Table 1).
    - Keeping compute/token low with MoE (Table 2).
    - Making FP8 training practical at scale (Sec. 3.1; Fig. 1).
    - Turning the network into a firstâ€‘class design element (Nodeâ€‘Limited Routing, MPFT, DeepEP; Secs. 4â€“5).
  - It reframes â€œbigger clusters onlyâ€ into â€œsmarter coâ€‘design,â€ broadening who can build advanced LLMs.
- Enabled applications
  - Onâ€‘prem and personal agents: MoE activates only a small parameter subset per token (e.g., V2: 21B active of 236B; V3: 37B active of 671B; Sec. 2.2.1), enabling high TPS on commodity servers; the paper notes nearly â€œ20 TPSâ€ on a ~$10k consumerâ€‘GPU server using KTransformers (Sec. 2.2.2).
  - Reasoning models and RL fineâ€‘tuning benefit from higher tokens/sec (Sec. 2.3.4).
- Concrete hardware directions distilled from the bottlenecks observed
  - Precision and quantization support (Secs. 3.1.2, 3.2.2)
    - Configurable or FP32 accumulation in Tensor Cores.
    - Native fineâ€‘grained scaling inside Tensor Cores (groupâ€‘scale GEMMs).
    - Builtâ€‘in compression/decompression units (FP8/custom formats like LogFMT) on NICs/I/O dies.
  - Unifying scaleâ€‘up and scaleâ€‘out fabrics (Sec. 4.4.2)
    - A â€œUnified Network Adapterâ€ or I/O die that speaks both intraâ€‘node (NVLinkâ€‘class) and interâ€‘node (IB/Ethernet) and can forward packets to specific GPUs with policy routing.
    - Dedicated communication coâ€‘processors to offload packet handling, memory copies, type casts, and reduce/broadcast to hardware.
    - Hardware synchronization primitives with memory semantics (acquire/release) to fix ordering without software fences (Sec. 4.4.2; expanded as â€œRegion Acquire/Releaseâ€ in Sec. 6.4).
    - Dynamic interconnect QoS and prioritization across EP/TP/KV traffic; integrate NICs on I/O dies and connect CPUâ†”GPU via scaleâ€‘up fabric to remove PCIe bottlenecks (Sec. 4.5.2).
  - Ethernet as a competitive AI fabric (Sec. 5.2.2)
    - Specialized lowâ€‘latency RoCE switches (e.g., Slingshotâ€‘like, Broadcom AIFHâ€‘style).
    - Adaptive routing (packet spraying) instead of pure ECMP; better congestion control (VOQ, RTTâ€‘based CC, programmable CC).
  - Inâ€‘network compute and compression (Sec. 6.5)
    - Hardware multicast for EP dispatch and smallâ€‘scope inâ€‘network reduce for EP combine.
    - LogFMTâ€‘class compression supported natively in the fabric.
  - Memoryâ€‘centric accelerators (Sec. 6.6)
    - DRAMâ€‘stacked accelerators (e.g., SeDRAMâ€‘like) for memoryâ€‘bound inference.
    - Systemâ€‘onâ€‘Wafer integration to push bandwidth and capacity for ultraâ€‘large models.
  - Reliability (Sec. 6.1)
    - Stronger error detection beyond ECC (checksums, hardware redundancy), plus vendorâ€‘supplied diagnostic toolkits to catch silent data corruption.

In sum, the paper provides a detailed, reproducible playbook for making large MoE LLMs efficient on todayâ€™s constrained hardware, and it translates concrete pain points into a clear set of hardware features the ecosystem can build toward. If you are designing nextâ€‘gen AI systems or fabrics, the recommendations in Secs. 3â€“6 form a prioritized roadmap grounded in real bottlenecks observed while training and serving DeepSeekâ€‘V3/R1.

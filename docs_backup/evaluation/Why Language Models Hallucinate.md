# Why Language Models Hallucinate

**ArXiv:** [2509.04664](https://arxiv.org/abs/2509.04664)
**Authors:** Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala, Edwin Zhang
**Institutions:** OpenAI, Georgia Tech

## ğŸ¯ Pitch

This paper provides a rigorous statistical framework to explain why large language models confidently produce false statements, highlighting the need to shift evaluation practices that incentivize guessing over uncertainty. By integrating confidence targets into benchmarks, the proposed solution realigns incentives, fostering the development of more reliable models crucial for applications in fields like medicine and law.

---

## 1. Executive Summary
This paper explains why large language models (LLMs) produce confident but false statements (â€œhallucinationsâ€) even when their training data are correct. It shows, with precise statistical arguments, that the standard pretraining objective and todayâ€™s evaluation practices together create strong incentives to guess rather than express uncertaintyâ€”and it proposes a concrete, scalable fix for evaluations to reverse that pressure.

## 2. Context and Motivation
- Problem addressed
  - LLMs often output plausible falsehoods instead of saying they donâ€™t know. The paper analyzes when and why this happens, from pretraining through postâ€‘training, and how to mitigate it at the evaluation level.
- Why it matters
  - Hallucinations undermine reliability in real applications (medicine, law, coding). They also reduce trust and can be hard to detect. The paper argues they are not mysterious: they follow from basic statistics and incentives (Section 1; Table 1 shows concrete failures on dissertation titles; the â€œletter countingâ€ prompt illustrates intrinsic mistakes).
- Prior approaches and gaps
  - Many causes have been proposed (overconfidence, exposure bias, long tails, spurious correlations). But there has been no general, endâ€‘toâ€‘end statistical account that:
    - connects generation errors to a simpler supervised problem,
    - handles prompts and abstentions (â€œIDKâ€), and
    - explains why postâ€‘training systems still hallucinate despite many mitigations.
- Positioning
  - The paper unifies generative errors with misclassification in binary supervised learning via a reduction (Sections 3.1â€“3.2). It strengthens a prior theoretical result on â€œarbitrary factsâ€ (birthdays) by including prompts and IDK (Section 3.3.1; Theorem 2) and offers a socioâ€‘technical diagnosis of postâ€‘training incentives (Section 4) with a practical remedy for mainstream benchmarks (Section 4.2; Table 2).

## 3. Technical Approach
At a high level, the paper shows:
1) Pretraining on crossâ€‘entropy naturally leads to some errors even with perfect data, because avoiding all generative errors is strictly harder than solving a related binary classification task.
2) Postâ€‘training evaluations usually score like exams that penalize uncertainty, so guessing raises measured performance.

Step by step:

A. Formalizing â€œvalid vs. erroneousâ€ generations (Sections 3.1, 3.2)
- The paper models the space of plausible strings (or promptâ€“response pairs) as a finite set `X`, partitioned into `V` (valid) and `E` (errors). The base model `pÌ‚` is a probability distribution over `X`.
- Generative error rate is `err = pÌ‚(E)` (Eq. 1).
- To analyze generation statistically, the paper defines a supervised classification problem named Isâ€‘Itâ€‘Valid (`IIV`): given an `x âˆˆ X`, predict whether it is `+` (valid) or `âˆ’` (error).
  - Training/test distribution `D` is a balanced mixture: half samples come from the training distribution over valid text (`p` restricted to `V`), and half are uniformly sampled errors from `E` (before prompts; Eq. 2). With prompts, this is extended by sampling a prompt `c` from a distribution `Î¼` and then a uniformly random erroneous response from `E_c` (Section 3.2).

B. Reduction: from generation to classification (Theorem 1; Corollary 1)
- Use the language model itself as a classifier by thresholding probabilities:
  - Predict â€œvalidâ€ if `pÌ‚(x) > 1/|E|` (or with prompts, `pÌ‚(r|c) > 1/min_c |E_c|`; Section 3.2).
- Main bound (with prompts, Theorem 1):
  - > Generative error rate `err` â‰¥ 2 Ã— `IIV` misclassification rate âˆ’ (max_c |V_c| / min_c |E_c|) âˆ’ Î´.
  - Here `Î´ = |pÌ‚(A) âˆ’ p(A)|` where `A` is the set of responses above the threshold. Intuitively, `Î´` measures miscalibration of the base model around that threshold.
- Interpretation:
  - If itâ€™s hard to classify valid vs. invalid (high `IIV` error), then generation must also make mistakes, roughly twice as often (up to small terms). Avoiding generative errors would require excellent discrimination and calibration.

C. Why Î´ is small after pretraining (Section 3.1; Fig. 2)
- Pretraining minimizes crossâ€‘entropy `L(pÌ‚) = E_{xâˆ¼p}[âˆ’log pÌ‚(x)]` (Eq. 3).
- Consider scaling up probabilities of all â€œaboveâ€‘thresholdâ€ items by a factor `s` and reâ€‘normalizing. The derivative of the loss in `s` at `s=1` equals `Î´`. If `Î´ â‰  0`, loss can be reduced by moving `s`, so local optimization makes `Î´` small (Section 3.1).
- Empirical evidence (reprinted calibration histograms for GPTâ€‘4): pretrained models are wellâ€‘calibrated while postâ€‘RLHF models may be less so (Fig. 2; left ECEâ‰ˆ0.007 vs. right ECEâ‰ˆ0.074).

D. Incorporating prompts and abstentions (IDK) (Section 3.2)
- With prompts, valid and erroneous responses per prompt are `V_c` and `E_c`. The same reduction applies, yielding Theorem 1 (as above). The threshold uses `min_c |E_c|`.
- The analysis supports IDK as a valid response and treats it explicitly in later results (Theorem 2).

E. Two canonical statistical regimes (Section 3.3)
1) Arbitrary facts (no learnable pattern)
   - Model (Definition 1): each prompt `c` has exactly one correct answer `a_c` drawn uniformly from a set `R_c`, answered with probability `Î±_c`, or IDK otherwise.
   - Define `singleton rate` `sr` as the fraction of prompts that appear exactly once with a nonâ€‘IDK answer in the Nâ€‘sample training data (Definition 2).
   - Main bound (Theorem 2): with high probability,
     - > `err â‰¥ sr âˆ’ 2/(min_c |E_c|) âˆ’ (35 + 6 ln N)/âˆšN âˆ’ Î´`.
     - When facts appear only once, they cannot be generalized: hallucination rate after pretraining is at least the share of such singletons (up to small terms).
   - Upper bound construction: there exists a calibrated `pÌ‚` (Î´=0) achieving
     - > `err â‰¤ sr âˆ’ sr/(max_c |E_c| + 1) + 13/âˆšN`.
   - Mechanism: this extends Goodâ€‘Turing â€œmissing massâ€ estimation to settings with IDK (Appendix B; Lemma 1) and shows how unseen or singleton facts force errors.

2) Poor models (misspecification or inadequate capacity)
   - Define a family of thresholdedâ€‘LM classifiers `G = {g_{Î¸,t}}` by varying model parameters `Î¸` and threshold `t` (Section 3.3.2).
   - If even the best classifier in `G` has high `opt(G)` (agnostic error), generation must err:
     - > `err â‰¥ 2Â·opt(G) âˆ’ (max_c |V_c| / min_c |E_c|) âˆ’ Î´` (from Theorem 1).
   - Special case: pure multipleâ€‘choice with exactly one correct response per prompt, `C` options (Theorem 3; proved more strongly as Theorem 4 in Appendix C):
     - > `err â‰¥ 2 (1 âˆ’ 1/C) Â· opt(G)`.
   - Example (Corollary 2): a trigram model must make â‰¥50% generation errors on a simple genderâ€‘agreement prompt pair because it cannot disambiguate the longâ€‘range dependency.

F. Additional error drivers (Section 3.4)
- Computational hardness: some prompts (e.g., decryption) are intractable; the reduction implies high error unless the model â€œbreaksâ€ the crypto (Appendix D; Observation 2).
- Distribution shift: outâ€‘ofâ€‘distribution prompts induce classificationâ€”and thus generationâ€”errors.
- GIGO (garbageâ€‘in, garbageâ€‘out): base models replicate errors in training data.

G. Postâ€‘training: why hallucinations persist (Section 4)
- Formalizing examâ€‘style grading (Section 4.1):
  - A binary grader `g_c` outputs 1 for correct, 0 otherwise; abstentions (`IDK`) receive 0 (by definition).
  - Decisionâ€‘theoretic result (Observation 1; proof in Appendix E):
    - > Under any distribution over such graders, the scoreâ€‘maximizing policy is never to abstain.
- Empirical metaâ€‘evaluation of benchmarks (Section 4; Table 2, Section F)
  - Most widely used benchmarks (GPQA, MMLUâ€‘Pro, MATH, SWEâ€‘bench, HLE, etc.) use binary grading; IDK gets no or worse credit than a risky guess. One exception, WildBench, offers minimal partial credit but can still reward confident bluffs.
- Proposed fix: explicit confidence targets in instructions (Section 4.2)
  - Append to each task a statement like:
    - > â€œAnswer only if you are > t confident; mistakes incur penalty t/(1âˆ’t), correct answers get 1, IDK gets 0.â€
  - This turns abstention into an optimal choice whenever confidence â‰¤ t. It makes the acceptable risk explicit and objective across benchmarks.
  - Introduces â€œbehavioral calibrationâ€: models should answer only when true correctness probability exceeds the stated threshold, measurable via accuracy vs. abstention curves.

## 4. Key Insights and Innovations
1) Reduction from generative modeling to binary classification (Theorem 1; Fig. 1)
   - Novelty: a general, modelâ€‘agnostic link that lowerâ€‘bounds generative error by misclassification error on a constructed `IIV` task, including prompts and IDK.
   - Significance: reframes hallucinations as ordinary statistical errors driven by learnability, calibration, and model capacityâ€”demystifying their origin.

2) Calibrationâ€“error tradeoff for base models (Section 3.1; Fig. 2)
   - Insight: minimizing crossâ€‘entropy encourages local calibration, which mathematically forces some generative errors when discrimination is imperfect. A perfectly â€œnonâ€‘hallucinatingâ€ base model would be miscalibrated (large `Î´`) unless it outputs IDK for everything.

3) Singletonâ€‘rate lower bound for arbitrary facts with IDK (Theorem 2; Appendix B)
   - Novelty: extends Goodâ€‘Turing â€œmissing massâ€ reasoning to prompts and abstentions, producing finiteâ€‘sample bounds both below and above.
   - Significance: gives a measurable predictor (singleton rate `sr`) for unavoidable hallucination on longâ€‘tail facts, even with clean data.

4) Benchmarkâ€‘driven guessing incentive and an explicit fix (Observation 1; Table 2; Section 4.2)
   - Innovation: formalizes why binary scoring makes abstention strictly suboptimal and documents that leading benchmarks overwhelmingly use such scoring.
   - Proposal: embed explicit confidence targets and penalties into existing mainstream evaluations (not separate hallucination tests), enabling â€œbehavioral calibration.â€ This is a leverage point for fieldâ€‘wide change.

## 5. Experimental Analysis
This work is primarily theoretical plus a benchmark metaâ€‘audit rather than a largeâ€‘scale empirical study.

- Evaluation methodology for metaâ€‘audit (Section 4; Table 2; Section F)
  - The paper inspects the primary metrics of widely used leaderboards:
    - HELM Capabilities (five scenarios), Open LLM Leaderboard v2 collection, SWEâ€‘bench, and HLE.
  - It checks whether abstentions can earn credit and whether grading is binary (0/1).
- Main findings (Table 2; Sections F.1â€“F.3)
  - > â€œThe vast majority of popular evaluations have binary gradingâ€ (Table 2).
  - Benchmarks providing no credit for IDK: GPQA, MMLUâ€‘Pro, BBH, MATH (L5), MuSR, SWEâ€‘bench, HLE, Omniâ€‘MATH (via equivalence grading).
  - IFEval aggregates binary subâ€‘scores; WildBench uses an LMâ€‘graded rubric but may score IDK lower than a â€œfairâ€ answer with hallucinations.
  - Additional detail: detailed reading of HELMâ€™s featured scenarios shows 4/5 clearly give no IDK credit; WildBench can still penalize abstention relative to flawed but â€œhelpfulâ€‘lookingâ€ answers (Section F.1).
- Supporting empirical illustrations
  - Real LLM failures on factual questions and counting letters (Section 1; Section 3.3.2).
  - Calibration evidence: GPTâ€‘4 calibration curves before vs. after RL (Fig. 2; reprinted from OpenAI, 2023a). ECE rises from ~0.007 (pretrain) to ~0.074 (postâ€‘RL), consistent with the crossâ€‘entropyâ€“calibration link and the idea that later training can distort it.
- Do these analyses support the claims?
  - The reductionâ€‘based theorems are mathematically proved (Sections 3; Appendices Aâ€“D).
  - The benchmark audit is descriptive but concrete: it maps the scoring rules that shape model incentives today (Table 2 and Sections F.1â€“F.3).
  - The proposed evaluation change is testable: adding explicit thresholds lets one measure â€œanswerâ€‘onlyâ€‘ifâ€‘confidentâ€ behavior via accuracy/coverage tradeâ€‘offs across `t`.

## 6. Limitations and Trade-offs
- Modeling assumptions (Section 5)
  - Finite â€œplausibleâ€ set `X` and a clean training distribution `p(V)=1`. Real corpora include noise; the authors note that noisy data would typically increase, not decrease, error lower bounds.
  - The â€œuniform random errorâ€ component in `D` for `IIV` simplifies analysis; real errors are structured.
  - The calibration term `Î´` is evaluated at a single threshold; richer calibration notions (ECE) vary across thresholds (Section 3.1).
- Scope limits (Section 5)
  - Openâ€‘ended, multiâ€‘fact generations are simplified to any falsehood being an error; degrees of hallucination are not modeled.
  - Hidden user intent or ambiguous context (â€œlatent contextâ€) is out of scope.
  - Search/RAG and chainâ€‘ofâ€‘thought are not panaceas under binary scoring; however, the paper does not benchmark these methods empirically here (Section 5).
- Practical tradeâ€‘offs
  - Adding explicit confidence targets introduces an accuracyâ€“coverage tradeâ€‘off: models may abstain more, reducing headline accuracy unless leaderboards accept this new metric.
  - Selecting the penalty parameter `t` is applicationâ€‘dependent; the paper suggests explicit but somewhat arbitrary thresholds (e.g., 0.5, 0.75, 0.9; Section 4.2).

## 7. Implications and Future Directions
- Fieldâ€‘level implications
  - Conceptual: Hallucinations are not exotic failures of generationâ€”they are standard statistical errors under capacity limits, long tails, and calibration. This reframes research toward classificationâ€‘style diagnostics and capacity/uncertainty management.
  - Practical: As long as leaderboards punish abstention, postâ€‘training will continue to produce good â€œtestâ€‘takersâ€ that guess. Adjusting mainstream benchmarks is a leverage point for safer systems (Section 4; Table 2).
- Methodological followâ€‘ups
  - Build â€œbehavioral calibrationâ€ dashboards: plot accuracy vs. abstention rate under explicit thresholds `t`, and audit perâ€‘domain calibration (Section 4.2).
  - Extend the reduction to graded hallucination severity and openâ€‘ended multiâ€‘fact outputs (Section 5).
  - Combine with RAG/reasoning: evaluate whether explicit thresholds improve retrieval/querying behavior (verifyâ€‘whenâ€‘uncertain pipelines).
  - Data curation: measure singleton rates `sr` per domain to predict unavoidable hallucination on longâ€‘tail facts (Theorem 2) and prioritize data collection.
- Applications
  - Highâ€‘stakes workflows (medical, legal, coding): deploy systems that abstain below explicit confidence targets; integrate fallback search or human handoff.
  - Benchmark design: retrofit GPQA, MMLUâ€‘Pro, SWEâ€‘bench, HLE with explicit confidence instructions and penalties to realign incentives without creating new niche evals.

In short, the paper delivers a principled explanation of why LLMs hallucinate and a concrete, scalable path to reduce it: change benchmark scoring so that â€œI donâ€™t knowâ€ is sometimes the optimalâ€”and rewardedâ€”answer.

# Efficient Memory Management for Large Language Model Serving with PagedAttention

**ArXiv:** [2309.06180](https://arxiv.org/abs/2309.06180)

## ðŸŽ¯ Pitch

This paper introduces PagedAttention, a novel attention algorithm inspired by operating systems, which enables key-value (KV) caches for large language models to be managed as non-contiguous fixed-size blocks rather than contiguous memory chunks. By dramatically reducing memory fragmentation and allowing KV cache sharing within and across requests, the authorsâ€™ vLLM system achieves up to 2â€“4Ã— higher serving throughput at the same latency as leading solutionsâ€”a breakthrough that removes a major bottleneck in high-throughput, cost-effective deployment of large language models.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces PagedAttention and vLLM, an operating-system-inspired memory subsystem and serving engine that store attention key/value (â€œKVâ€) states in fixed-size pages (blocks) rather than contiguous tensors. By eliminating fragmentation and enabling sharing of KV pages across sequences, vLLM sustains 2â€“4Ã— higher throughput at similar latency versus state-of-the-art systems across diverse models and workloads (Figures 12, 14, 16; Abstract).

## 2. Context and Motivation
- Problem addressed
  - Serving large language models (LLMs) efficiently requires batching many concurrent requests. The bottleneck is GPU memory, especially for the `KV cache`â€”the per-token key and value vectors used by attention during autoregressive generation (Â§2.2).
  - KV memory is large, grows/shrinks per request, and lengths are unknown in advance. Existing systems store each requestâ€™s KV cache in a contiguous chunk, causing (i) pre-reservation waste; (ii) internal fragmentation (unused space in over-provisioned chunks); and (iii) external fragmentation (allocator gaps) (Â§3.1, Figure 3).

- Why it matters
  - KV memory can consume >30% of GPU memory during serving (Figure 1 left) and scales linearly with tokens. For OPT-13B, a single tokenâ€™s KV cache is ~800 KB and a 2048-token sequence can reach ~1.6 GB (Â§3), severely limiting batch size and throughput.
  - GPU compute has grown faster than memory capacity; memory will remain a scaling bottleneck (Â§3).

- Shortcomings of prior approaches
  - FasterTransformer: highly optimized kernels but no fine-grained iteration-level scheduling; uses contiguous allocations that over-reserve memory (Baseline 1 in Â§6.1).
  - Orca: iteration-level scheduler improves compute utilization but still allocates per-request contiguous chunks, incurring significant fragmentation and preventing KV sharing (Â§3.1, Baseline 2 in Â§6.1).
  - Empirical evidence: in existing systems, only 20.4â€“38.2% of KV memory holds useful token states; the rest is waste (Figure 2 and Â§3.1).

- Positioning
  - The paper reframes KV memory management through the lens of virtual memory and paging: divide KV into fixed-size blocks and map â€œlogicalâ€ blocks of a sequence to â€œphysicalâ€ blocks in GPU memory dynamically. This enables near-zero fragmentation and block-level sharing across sequences (Â§4.1â€“Â§4.3).

## 3. Technical Approach
The system has two core parts: the `PagedAttention` algorithm (how attention reads from paged KV memory) and the `vLLM` engine (block allocators, scheduler, sharing mechanisms, and distributed execution).

1) KV cache and generation basics (Â§2.2)
- Serving consists of:
  - Prompt phase: process the whole user prompt in parallel; compute K and V for prompt tokens and logits for the first generated token.
  - Autoregressive phase: at each step, generate one new token using the query of the latest token and all past K/V. KV states grow token-by-token.
- The KV cache stores past K/V so each new token can attend to all previous tokens without recomputing them.

2) PagedAttention: attention over paged KV memory (Â§4.1; Figure 5)
- Key idea: Store KV states in fixed-size `KV blocks` of `B` tokens, and allow these blocks to be non-contiguous in physical GPU memory.
- Attention is computed block-by-block. For a query vector q_i at position i:
  - Instead of multiplying q_i by all previous keys as one long vector, compute attention scores and weighted sums per block (Eq. 4). In notation:
    - Compute A_ij (the slice of attention weights for tokens inside block j up to i),
    - Then combine V_j A_ij^T across j to form the output o_i.
- Why it works: the attention math is unchanged; only the memory access pattern is restructured to page in blocks. This lets the kernel fetch discontiguous blocks and still compute correct attention.

3) KV Cache Manager: logicalâ€“physical block mapping (Â§4.2; Figure 6)
- GPU block engine: pre-partitions a chunk of GPU DRAM into equal-sized `physical KV blocks`.
- Block tables: one per sequence; map `logical` block indices (0,1,2,â€¦) to physical block IDs and store a `#filled` count (how many token slots in the last logical block are occupied).
- Allocation strategy:
  - Allocate only the blocks needed so far. During prompt prefill, allocate enough blocks to hold the promptâ€™s tokens; in generation, allocate a new block only when the last one becomes full (Â§4.3).
  - This caps per-sequence waste to at most one partially filled block.

4) Decoding workflow with paged KV (Â§4.3; Figure 6â€“7)
- Prefill: compute prompt K/V; write them into logical blocks filled left-to-right. Map each logical block to any free physical block.
- Generation step t:
  - Read previous K/V via block table (PagedAttention kernel).
  - If the last block has space, append the new tokenâ€™s K/V there; otherwise allocate a new physical block, update block table.
- Batching: each iteration, the scheduler batches currently active sequences, allocates any needed blocks, concatenates that iterationâ€™s input tokens, runs the model, and writes new K/V into their mapped blocks (Â§4.3, last two paragraphs).

5) Memory sharing for advanced decoding (Â§4.4)
- Parallel sampling (multiple independent samples for one prompt):
  - Share prompt KV across samples by mapping their prompt logical blocks to the same physical blocks (Figure 8).
  - Use `copy-on-write` at block granularity: if a shared block needs modification (e.g., last prompt block when appending), allocate a new physical block, copy data, decrement reference count of the original, and write to the new block (Figure 8, â€œRef count: 2 â†’ 1â€).
- Beam search (keep top-k evolving candidates):
  - KV sharing changes dynamically as beams split and merge. vLLM represents beams as sequences whose logical blocks often point to shared physical blocks.
  - When some beams are discarded, their references are dropped and physical blocks with zero refcount are freed; new beams get new blocks (Figure 9).
  - This avoids large block copies across beams; copy-on-write is needed only when generating inside a shared block.
- Shared prefix (system prompt or few-shot exemplars reused across requests):
  - Pre-reserve physical blocks for common prefixes (like shared libraries). A new request maps its initial logical blocks to these, then continues from there (Figure 10; Â§4.4 â€œShared prefixâ€).

6) Scheduling, preemption, and recovery (Â§4.5)
- Scheduling policy: FCFS (first-come-first-served) for fairness; preempt the latest arrivals first under memory pressure.
- Eviction granularity: `all-or-nothing per sequence group` (e.g., all beams of a request together), because attention needs every block of a sequence. This fits paging semantics but exploits domain-specific â€œall blocks are used together.â€
- Recovery choices when GPU blocks are insufficient:
  - `Swapping`: move entire sequence groupsâ€™ KV blocks to CPU RAM using a CPU block allocator; later swap back. Swap space is bounded by the GPU KV allocation (Â§4.5 â€œSwappingâ€).
  - `Recomputation`: when resuming, re-run the prompt phase on the concatenated prompt + already-generated tokens to rebuild KV, which is faster than originally computing token-by-token during generation (Â§4.5 â€œRecomputationâ€).
- System behavior: after preempting and evicting some sequences, the system stops accepting new requests until all preempted sequences finish (ensures space to bring them back) (Â§4.5).

7) Distributed execution (Â§4.6)
- Works with Megatron-LM style tensor parallelism (SPMD across attention heads) so every shard needs KV for the same token positions.
- The centralized scheduler maintains one block table per sequence that all GPU workers use. At each iteration, the scheduler broadcasts input tokens and the block tables; workers run attention using those mappings; all-reduce combines intermediate results; sampled tokens return to the scheduler (Â§4.6).

8) Implementation and kernel optimizations (Â§5)
- vLLM is implemented with PyTorch and custom CUDA kernels; NCCL for communication (Â§5).
- Optimizations (Â§5.1):
  - Fused reshape + block write: split new KV into blocks and write them to physical locations in one kernel.
  - Fused block read + attention: extend FasterTransformerâ€™s attention kernel to read non-contiguous blocks based on block table; a warp reads one block; supports variable sequence lengths within the batch.
  - Fused block copy: batch many copy-on-write block copies into one kernel launch.

## 4. Key Insights and Innovations
- KV as virtual memory
  - Novelty: Treat a requestâ€™s KV states as a `paged` address space with fixed-size `blocks`; separate logical sequence order from physical placement (Â§4.1â€“Â§4.3).
  - Why significant: Eliminates external fragmentation entirely (all blocks same size), minimizes internal fragmentation (â‰¤ one block per sequence), and avoids long-lived over-reservation (Â§3.1, Figures 2â€“3). This frees memory to batch more requests, directly boosting throughput (Figures 12â€“13).

- Block-level sharing with copy-on-write
  - Novelty: Combine block-level reference counting and copy-on-write to share large KV regions across sequences from the same request (parallel sampling, beam search) and across requests (shared prefixes) (Â§4.4, Figures 8â€“10).
  - Why significant: Reduces duplication of prompt KV, and in beam search reduces repeated memory copies when beams branch/prune. Savings reach 37.6â€“55.2% in beam search on Alpaca and 44.3â€“66.3% on ShareGPT (Figure 15; Â§6.3).

- Domain-aware preemption and recovery
  - Novelty: All-or-nothing eviction at the sequence-group level and two recovery strategies (`swap` or `recompute`) tailored to LLM generation semantics (Â§4.5).
  - Why significant: Keeps the system responsive under memory pressure while bounding swap space and enabling recomputation that can be faster than swapping, especially with small blocks (Figure 19).

- Practical distributed serving with a centralized KV manager
  - Novelty: One scheduler maintains the global logicalâ†’physical mapping; all tensor-parallel workers execute using the same block tables (Â§4.6).
  - Why significant: Makes the paging abstraction practical for multi-GPU models (e.g., OPT-175B on 8Ã—A100-80GB in Table 1) without per-iteration synchronization beyond the broadcast of block tables.

## 5. Experimental Analysis
- Evaluation setup (Â§6.1)
  - Models: OPT-13B (1Ã—A100-40GB), OPT-66B (4Ã—A100-40GB), OPT-175B (8Ã—A100-80GB). Table 1 lists parameter sizes and KV memory budgets; e.g., OPT-13B has 12 GB for KV cache and max ~15.7K KV slots.
  - Workloads: Synthetic traces from ShareGPT (long prompts and outputs) and Alpaca (shorter), with Poisson arrivals. Length distributions shown in Figure 11 (ShareGPT: input mean 161, output mean 338; Alpaca: input 19, output 58).
  - Baselines:
    - FasterTransformer (FT) with a dynamic batching scheduler (Â§6.1).
    - Orca in three provisioning modes: `Max` (reserve to 2048 tokens), `Pow2` (next power-of-two), and `Oracle` (true lengths, unrealizable in practice) (Â§6.1).
  - Metric: `normalized latency` = end-to-end latency divided by output length (s/token). Systems are compared by how high a request rate they sustain before latency explodes (1-hour traces; 15 minutes for OPT-175B).

- Main results
  - Throughput gains with basic sampling (Figure 12; Â§6.2)
    - On ShareGPT, vLLM sustains 1.7â€“2.7Ã— higher request rates than Orca (Oracle) and 2.7â€“8Ã— than Orca (Max), at similar normalized latency across OPT-13B/66B/175B (Figure 12aâ€“c).
    - vLLM outperforms FT even more sharply (up to 22Ã— higher request rates) because FT lacks iteration-level scheduling and suffers from memory over-reservation (Â§6.2).
    - On Alpaca (short sequences), vLLM still wins (Figure 12dâ€“f), though with OPT-175B the advantage narrows because the setting becomes compute-bound (ample KV memory, short sequences, Â§6.2).
  - Batch size realized (Figure 13)
    - With OPT-13B on ShareGPT at 2 req/s, average concurrent batched requests: vLLM 30.42 vs Orca-Oracle 13.62, Orca-Pow2 9.81, Orca-Max 7.00 (Figure 13a).
    - On Alpaca at 30 req/s: vLLM 132.44 vs Orca-Oracle 72.75, Orca-Pow2 43.24, Orca-Max 7.00 (Figure 13b).
  - Memory utilization (Figure 2; Â§3.1)
    - Fragmentation analysis shows existing systems use only 20.4â€“38.2% of KV memory for actual token states, whereas vLLM achieves near-zero waste with 96.3% effective KV use (Figure 2).
    - Quote:
      > Only 20.4% â€“ 38.2% of the KV cache memory is used to store actual token states in existing systems (Â§3.1, Figure 2), while vLLM reduces waste to near zero (96.3% usage).
  - Parallel sampling and beam search (Figure 14; Â§6.3)
    - As parallelism grows, vLLMâ€™s advantage increases. With OPT-13B on Alpaca, normalized latency remains low at higher request rates than Orca across parallel sizes 2â€“6 and beam widths 2â€“6 (Figure 14aâ€“f).
    - Measured KV sharing savings: 6.1â€“9.8% for parallel sampling and 37.6â€“55.2% for beam search on Alpaca; 16.2â€“30.5% and 44.3â€“66.3% respectively on ShareGPT (Figure 15; Â§6.3).
  - Shared prefix reuse (Figure 16; Â§6.4)
    - For WMT16 Enâ€“De with LLaMA-13B and shared translation exemplars: vLLM attains 1.67Ã— (1-shot) and 3.58Ã— (5-shot) higher throughput than Orca (Oracle).
  - Chatbot scenario (Figure 17; Â§6.5)
    - With long histories truncated to 1024 tokens in OPT-13B, vLLM sustains 2Ã— higher request rates than all Orca variants; Orca variants behave similarly because buddy allocation reserves large output chunks regardless of predicted output length (Â§6.5).

- Ablations and diagnostics
  - Kernel microbenchmark (Â§7.1; Figure 18a):
    - PagedAttention kernels have 20â€“26% higher per-kernel latency than FTâ€™s attention kernel due to block-table indirection and variable-length handling, but end-to-end wins arise from much larger batch sizes.
  - Block size trade-off (Â§7.2; Figure 18b):
    - Best performance typically at block sizes 16â€“128 (ShareGPT) and 16â€“32 (Alpaca). Default is 16 to balance GPU utilization and low fragmentation.
  - Swap vs recompute (Â§7.3; Figure 19):
    - Swapping suffers with small blocks due to many tiny PCIe transfers; recomputation cost is roughly constant across block sizes.
    - Quote:
      > Recomputationâ€™s overhead is never higher than 20% of swappingâ€™s latency; for block sizes 16â€“64, both are comparable (Figure 19).

- Do results support the claims?
  - Yes. Gains are shown across multiple model sizes, hardware scales, datasets, and decoding regimes. The mechanismâ€“result link is clear: better KV memory utilization (Figure 2) translates into larger batches (Figure 13) and higher sustainable request rates (Figures 12, 14), especially in memory-bound regimes.

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Benefits are strongest when serving is memory-bound. When compute dominates (e.g., large GPU memory and short sequences: OPT-175B on Alpaca), throughput gains narrow (Â§6.2, Figure 12f).
  - The design assumes all KV blocks of a sequence must be concurrently accessible; hence eviction is â€œall-or-nothingâ€ at the sequence-group level (Â§4.5). This simplifies correctness but limits finer-grained memory reclamation.

- Overheads and engineering complexity
  - Attention kernel overhead: 20â€“26% slower microbenchmarks than FTâ€™s kernel (Figure 18a). End-to-end gains offset this but the overhead is real (Â§7.1).
  - Block size tuning: Too small harms GPU utilization; too large increases fragmentation and reduces sharing opportunities (Â§7.2). vLLM defaults to 16, but workloads may benefit from tuning.
  - Copy-on-write operates at block granularity. If many writes happen inside shared last blocks, extra block copies occur; sub-block COW is not supported (Â§4.4).

- Scheduling and preemption behavior
  - FCFS prioritization may not meet latency SLOs for heterogeneous request profiles. Moreover, once sequences are preempted and swapped out, the system pauses accepting new requests until they finish (Â§4.5), which is a conservative choice that may reduce admission under extreme load.

- Generality
  - The OS-like paging analogy is tailored to LLM KV access patterns. The paper cautions that similar techniques may not help compute-bound workloads or those with static tensor shapes (e.g., conventional DNN serving/training), and indirection overhead could hurt (Â§8).

- Not addressed
  - Cross-model multi-tenant scheduling, SLA-aware batching, and heterogeneous accelerators are outside scope.
  - Energy or cost analysis is not included; CPU memory capacity limits for swapping are bounded by GPU KV allocation but still consume host resources (Â§4.5).

## 7. Implications and Future Directions
- How this changes the landscape
  - Treating KV cache as paged virtual memory is a conceptual shift: it disentangles sequence order from physical placement. This unlocks near-zero fragmentation, sharing across diverse decoding methods, and scalable batching on fixed-memory GPUs (Figures 2, 12â€“16).
  - Quote:
    > vLLM improves throughput by 2â€“4Ã— with the same latency level compared to FasterTransformer and Orca (Abstract); improvements are more pronounced with longer sequences, larger models, and more complex decoding.

- Follow-up research
  - Adaptive block sizing: dynamically adjust block size per workload to optimize the utilization/fragmentation trade-off (Â§7.2).
  - SLA-aware scheduling: integrate deadlines or tail-latency constraints with the block allocator and preemption policies.
  - Predictive prefetching and admission control: use learned predictors of output lengths to pre-allocate blocks conservatively without hard over-reservation.
  - Deeper integration with optimized attention kernels (e.g., FlashAttention) that natively support paged access patterns.

- Practical applications
  - High-throughput chatbots, coding assistants, and translation services that need parallel sampling or beam search benefit from KV sharing (Figures 14â€“16).
  - Multi-tenant serving with shared system prompts or few-shot exemplars can cache and reuse large prefixes effectively (Figure 10, 16).
  - Cost-efficient deployment of very large models (e.g., 175B parameters) by increasing the number of concurrent users per GPU memory footprint (Table 1, Figures 12â€“13).

Overall, vLLMâ€™s PagedAttention rethinks memory as the primary scaling lever for LLM serving. By borrowing mature ideas from OS (paging, copy-on-write) and adapting them to KV access patterns, it turns fragmented, over-reserved memory into a shared, elastic pool that directly converts into throughput.

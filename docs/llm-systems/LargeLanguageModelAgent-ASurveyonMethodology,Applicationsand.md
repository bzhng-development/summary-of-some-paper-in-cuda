# Large Language Model Agent: A Survey on Methodology, Applications and Challenges

**ArXiv:** [2503.21460](https://arxiv.org/abs/2503.21460)

## ğŸ¯ Pitch

This paper introduces a unified, methodology-driven taxonomyâ€”spanning how LLM agents are built, collaborate, and evolveâ€”offering the first comprehensive architectural perspective that interlinks agent construction, coordination, memory, safety, evaluation, and real-world deployment. By systematizing a fragmented and rapidly advancing field, this survey equips researchers and practitioners with actionable frameworks to design, benchmark, and scale reliable, robust, and impactful LLM-based agentic systems across diverse applications.

---

## 1. Executive Summary (2â€“3 sentences)
This paper presents a methodologyâ€‘centered, endâ€‘toâ€‘end taxonomy of Large Language Model (LLM) agents that unifies how agents are built, how they collaborate, and how they evolve, and connects these to evaluation, tools, security, privacy, ethics, and applications. Its core significance is a coherent architectural lensâ€”the Buildâ€‘Collaborateâ€‘Evolve framework (Figures 1â€“2; Section 2)â€”that systematizes a fragmented literature into interoperable components and provides actionable guidance for designing, assessing, and deploying modern LLMâ€‘based agent systems at scale.

## 2. Context and Motivation
- Problem/gap addressed
  - The literature on LLM agents is rapidly expanding but fragmented: different works emphasize isolated pieces (e.g., tool use, planning, or multiâ€‘agent protocols) without a cohesive architectural methodology that maps design choices to capabilities and risks (Distinction from Previous Surveys, p. 2; Figure 1).
  - Existing surveys often focus on narrow slices (e.g., gaming, multiâ€‘modality, security) or provide highâ€‘level overviews without a detailed methodological taxonomy that connects construction, collaboration, and evolution (Section â€œDistinction from Previous Surveys,â€ p. 2).

- Why this matters
  - Practically: Agentic systems are moving from demos to production (e.g., research assistants, autonomous web agents, software automation). Designing robust, safe, and evolvable systems requires shared abstractions and evaluation scaffolds (Figure 1; Sections 2â€“4).
  - Theoretically: Understanding how memory, planning, tools, and multiâ€‘agent dynamics generate emergent behavior informs how to achieve reliability, generalization, and safety (Sections 2.1â€“2.3; 6.3).

- Prior approaches and limitations
  - Prior surveys concentrated on specific domains (e.g., games [11, 12]) or single dimensions (e.g., workflows [19], multiâ€‘agent interaction [18]). They lack an integrated framework that ties together individual agent design, multiâ€‘agent coordination, and learning/evolution (Distinction from Previous Surveys, p. 2).

- How this work positions itself
  - It proposes a methodologyâ€‘centered taxonomy anchored in three linked dimensionsâ€”Construction, Collaboration, Evolutionâ€”supplemented by Evaluation & Tools, Realâ€‘World Issues, and Applications (Figure 1; Section 2).
  - It provides concrete decomposition of agent internals (profile, memory, planning, action) and external structures (centralized/decentralized/hybrid collaboration) with curated exemplars (Figure 2; Sections 2.1â€“2.2), and ties these to security/privacy risks (Figure 4; Section 4) and benchmarks (Figure 3; Section 3).

## 3. Technical Approach
The paperâ€™s â€œapproachâ€ is a unifying architectural framework (Figures 1â€“2) plus a curated mapping of methods, tools, and risks. Below, each building block is explained in mechanismâ€‘level detail, with why/when to use it.

- Build: Agent Construction (Section 2.1; Figure 2)
  - `Profile definition` (Section 2.1.1)
    - What it is: The agentâ€™s operational identityâ€”role, goals, domain constraints, and communication protocol.
    - Two mechanisms:
      - Humanâ€‘curated static profiles: Manually specified, interpretable roles (e.g., developer/tester) with deterministic interaction rules. Useful for regulated domains and reproducibility. Examples: `CAMEL`, `AutoGen`, `MetaGPT`, `ChatDev`, `AFlow` (Section 2.1.1; Figure 2).
      - Batchâ€‘generated dynamic profiles: Programmatically sample diverse traits (personality, background) to create heterogeneous agent populations for simulation or robustness (e.g., `Generative Agents`, `RecAgent`), optionally optimized with `DSPy` (Section 2.1.1).
    - Why this design: Static profiles yield control and compliance; dynamic profiles yield diversity and emergent behaviors (Section 2.1.1).

  - `Memory mechanisms` (Section 2.1.2)
    - Shortâ€‘term memory: Conversation/state within the context window (pros: strong local coherence; cons: context length limits; requires summarization/compression). Used in `ReAct`, `ChatDev`, `Graph of Thoughts`, `AFlow` (Section 2.1.2).
    - Longâ€‘term memory: Persistent stores that transform transient reasoning into reusable assets:
      - Skill libraries (e.g., `Voyager` autoâ€‘discovers Minecraft skills; `GITM` knowledge base).
      - Experience repositories (e.g., `ExpeL` distilled successes/failures; `Reflexion` selfâ€‘improvement).
      - Tool synthesis (e.g., `TPTU` composes tools; `OpenAgents` selfâ€‘expands toolkits; `MemGPT` tiered memory) (Section 2.1.2).
    - Knowledge retrieval as memory (`RAG`, `GraphRAG`, `IRCoT`, `Llatrieval`, `KGâ€‘RAR`, `DeepRAG`): External corpora/graphs are queried inâ€‘loop, balancing parametric knowledge vs. external evidence (Section 2.1.2).
    - Why this design: Shortâ€‘term memory supports local reasoning; longâ€‘term/retrieval overcome forgetting and trainingâ€‘time limits, enabling cumulative competence (Section 2.1.2).

  - `Planning capability` (Section 2.1.3)
    - Task decomposition:
      - Chainâ€‘based (sequential): Planâ€‘andâ€‘solve, selfâ€‘consistency/ensemble voting, dynamic nextâ€‘step planning (`ReAct`). Simple but error accumulation risk (Section 2.1.3).
      - Treeâ€‘based: `ToT`, `Treeâ€‘planner`, `ReAcTree`, `ReSTâ€‘MCTS*`, enabling backtracking, exploration (e.g., MCTS) and trialâ€‘errorâ€‘correct loops; applied to robotics/embodied tasks (Section 2.1.3).
    - Feedbackâ€‘driven iteration: Incorporates environment signals (robotics), human feedback, selfâ€‘introspection, and multiâ€‘agent critique to refine plans (`BrainBodyâ€‘LLM`, `TrainerAgent`, `RASC`, `REVECA`, `AdaPlanner`, `AIFP`) (Section 2.1.3).
    - Why this design: Complex tasks require decomposition and iterative correction; tree search reduces premature commitment (Section 2.1.3).

  - `Action execution` (Section 2.1.4)
    - Tool use: Two subâ€‘problemsâ€”when to call tools (confidence/needâ€‘based decision) and which tool to select (documentation understanding; e.g., `EASYTOOL` simplifies docs, `GPT4Tools`, `TRICE`, `AvaTaR`) (Section 2.1.4).
    - Physical interaction: Embodied control must factor hardware, social norms, and multiâ€‘agent coordination (e.g., `DriVLMe` for driving, `ReAd`, `Collaborative Voyager`) (Section 2.1.4).
    - Why this design: Tools extend precision/coverage (math, APIs); embodiment requires grounded feedback loops (Section 2.1.4).

- Collaborate: Agent Collaboration (Section 2.2; Figure 2; Table 1)
  - Centralized control (Section 2.2.1): A controller decomposes tasks, assigns subgoals, and integrates results.
    - Explicit controllers: Human/LLM orchestration pipelines (`Coscientist` for experimental workflows; `LLMâ€‘Blender` ranks/fuses responses; `MetaGPT` manages software roles).
    - Differentiationâ€‘based: A single strong agent implicitly plays subâ€‘roles (`AutoAct` splits into plan/tool/reflect; `Metaâ€‘Prompting` assigns subtasks by metaâ€‘prompts; `WJudge` shows even weak controllers can help) (Section 2.2.1).
    - Tradeâ€‘off: Strong coordination and accountability; singleâ€‘point bottleneck/failure risk (Section 2.2.1).

  - Decentralized collaboration (Section 2.2.2): Peers interact directly; no single hub.
    - Revisionâ€‘based: Agents iteratively edit/critique a shared artifact to reach consensus (`MedAgents`, `ReConcile`, `METAL`, `DSâ€‘Agent`)â€”more deterministic outputs (Section 2.2.2).
    - Communicationâ€‘based: Open dialogues expose reasoning traces; good for dynamic social scenarios (`MAD`, `MADR`, `MDebate`, `AutoGen`) (Section 2.2.2).
    - Tradeâ€‘off: More flexible/robust but harder to coordinate and verify (Section 2.2.2).

  - Hybrid architectures (Section 2.2.3)
    - Static patterns: Predefined topologiesâ€”e.g., groups with roleâ€‘play and governance (`CAMEL`), threeâ€‘tier planner/negotiator/market (`AFlow`), canonical patterns (BUS/STAR/TREE/RING in `EoT`).
    - Dynamic systems: Topology adapts by performance feedbackâ€”`DiscoGraph` (learned collaboration graphs), `DyLAN` (importanceâ€‘aware restructuring), `MDAgents` (route by task complexity) (Section 2.2.3).
    - Tradeâ€‘off: Balance controllability with adaptivity; dynamic routing reduces waste and improves fit to task (Section 2.2.3).

- Evolve: Agent Evolution (Section 2.3; Figure 2; Table 2)
  - Autonomous selfâ€‘learning (Section 2.3.1)
    - Selfâ€‘supervised/pretraining refinements: `SE` (adaptive masking), evolutionary model merging [87], `DiverseEvol` (diverse data sampling).
    - Selfâ€‘reflection/correction: `SELFâ€‘REFINE`, `STaR`, `Vâ€‘STaR`, selfâ€‘verificationâ€”iteratively generate, critique, and improve outputs.
    - Selfâ€‘rewarding/RL: `Selfâ€‘Rewarding`, `RLCD`, `RLC` align models with internally generated rewards.
  - Multiâ€‘agent coâ€‘evolution (Section 2.3.2)
    - Cooperative: `ProAgent` (intent inference), `CORY` (cooperative RL fineâ€‘tuning), `CAMEL` (roleâ€‘based collaboration).
    - Competitive/adversarial: `Redâ€‘Team LLMs`, multiâ€‘agent debate (`MAD`, `MDebate`)â€”stress tests that improve robustness/reasoning.
  - Evolution with external resources (Section 2.3.3)
    - Knowledgeâ€‘enhanced planning: `KnowAgent` (action knowledge), `WKM` (world priors + dynamic local knowledge).
    - Feedbackâ€‘driven: `CRITIC` (toolâ€‘based selfâ€‘correction), `STE` (trialâ€‘andâ€‘error for tool mastery), `SelfEvolve` (generate + debug with execution feedback).

- Evaluation and tools (Section 3; Figure 3)
  - Benchmarks span general capability (e.g., `AgentBench`, `Mind2Web`), domainâ€‘specific (e.g., `MedAgentBench`, `LaMPilot`), realâ€‘world environments (`OSWorld`, `OmniACT`, `EgoLife`), and collaboration (e.g., enterpriseâ€‘style `TheAgentCompany`) (Sections 3.1.1â€“3.1.3).
  - Tools cover: what agents use (search, calculators, interpreters, APIs), what agents create (tool synthesis frameworks like `CRAFT`, `Toolink`, `CREATOR`, `LATM`), and what devs use to deploy/manage agents (`AutoGen`, `LangChain`, `LlamaIndex`, `Dify`, `Ollama`, `MCP`) (Section 3.2).

- Realâ€‘world issues (Section 4; Figure 4)
  - Security: Agentâ€‘centric (adversarial, jailbreak, backdoor, collaboration attacksâ€”Table 3) and dataâ€‘centric (prompt/indirect injections, poisoning, interactionâ€‘layer exploitsâ€”Table 4).
  - Privacy: Memorization (data extraction, membership, attribute inference) and IP (model/prompt stealing), with defenses like DP, distillation, watermarking, blockchain (Table 5).
  - Social impact: Benefits (automation, jobs, information access) vs. ethical concerns (bias, accountability, copyright, overreliance, environmental cost) summarized in Table 6 (Section 4.4).

## 4. Key Insights and Innovations
- Methodologyâ€‘centered Buildâ€‘Collaborateâ€‘Evolve taxonomy
  - Whatâ€™s new: A single framework that decomposes agent internals (profile, memory, planning, action) and externals (centralized/decentralized/hybrid) and ties them to evolution mechanisms (selfâ€‘learning, coâ€‘evolution, external resources) (Figures 1â€“2; Section 2).
  - Why it matters: It connects design choices to emergent behavior and evaluation/security realities, enabling principled system design rather than adâ€‘hoc pipelines.

- Unification of memory and retrieval as firstâ€‘class design axes
  - Whatâ€™s new: A threeâ€‘way classificationâ€”shortâ€‘term, longâ€‘term (skills/experience/tools), and retrievalâ€‘asâ€‘memory (`RAG`, `GraphRAG`, reasoningâ€‘integrated retrieval) (Section 2.1.2).
  - Why it matters: It clarifies how to transcend contextâ€‘window limits (noted constraints in Section 2.1.2) and build cumulative competenceâ€”critical for longevity and realâ€‘world deployment.

- Collaboration patterns distilled into explicit, decentralized, and hybrid topologies
  - Whatâ€™s new: A fineâ€‘grained split of decentralized styles (revision vs. communication) and a catalog of static vs. dynamic hybrid topologies with examples (`CAMEL`, `AFlow`, `EoT`, `DiscoGraph`, `DyLAN`, `MDAgents`) (Sections 2.2.2â€“2.2.3).
  - Why it matters: Designers can select appropriate organization patterns (determinism vs. flexibility vs. adaptivity) grounded in task needs.

- Endâ€‘toâ€‘end bridge from architecture to evaluation, tools, and risk
  - Whatâ€™s new: A single map that spans benchmarks (Figure 3; Sections 3.1.1â€“3.1.3), tool ecosystems (Section 3.2), and realâ€‘world risks (Figure 4; Section 4), with concrete scales (e.g., number of tasks, APIs, environments).
  - Why it matters: It enables coverageâ€‘aware testing (e.g., tool use via `Sealâ€‘Tools` with â€œ1,024 nested instances,â€ Section 3.1.1) and safetyâ€‘first deployment planning.

These are primarily integrative innovations (a coherent architecture and mappings) rather than a new algorithm; the novelty is in the synthesis and the actionable decomposition.

## 5. Experimental Analysis
While this is a survey (no new experiments), it assembles evaluation scaffolds with concrete scales, domains, and scenarios. Below are the key evaluation elements and what they support.

- Evaluation methodology (Section 3; Figure 3)
  - General capability
    - `AgentBench` offers â€œa unified test field across eight interactive environmentsâ€ (Section 3.1.1).
    - `Mind2Web` is â€œthe first generalist agent for evaluating 137 realâ€‘world websites with tasks spanning 31 domainsâ€ (Section 3.1.1).  
      > â€œMind2Web â€¦ proposing the first generalist agent for evaluating 137 realâ€‘world websites with different tasks spanning 31 domains.â€ (Section 3.1.1)
    - `MMAU` provides â€œmore than 3,000 crossâ€‘domain tasksâ€ with capability mapping (Section 3.1.1).
  - Scientific/vision/embodied
    - `BLADE` targets scientific workflows; `VisualAgentBench` spans GUI/visual design; `Embodied Agent Interface` gives fineâ€‘grained error classification; `CRAB` enables crossâ€‘platform testing with a unified Python interface (Section 3.1.1).
  - Dynamic/selfâ€‘evolving evaluation
    - `BENCHAGENTS` autoâ€‘creates benchmarks via agents; â€œSealâ€‘Tools (1,024 nested instances of tool calls)â€ and `CToolEval` (â€œ398 Chinese APIs across 14 domainsâ€) standardize toolâ€‘use evaluation (Section 3.1.1).  
      > â€œSealâ€‘Tools â€¦ 1,024 nested instances of tool calls â€¦ CToolEval (398 Chinese APIs across 14 domains)â€ (Section 3.1.1)
  - Domainâ€‘specific and realâ€‘world simulation
    - Healthcare: `MedAgentBench` (tasks by â€œ300 clinicians in an FHIRâ€‘compliant environmentâ€), `AI Hospital` simulates multiâ€‘agent clinical workflows (Section 3.1.2).
    - Autonomy: `LaMPilot` ties LLMs to selfâ€‘driving code generation (Section 3.1.2).
    - Data science & ML: `DSEval`, `DAâ€‘Code`, `DCAâ€‘Bench`, `MLAgentâ€‘Bench`, `MLEâ€‘Bench` (Section 3.1.2â€“3.1.3).
    - Real computers/web: `OSWorld` â€œsupports 369 multiâ€‘application tasks across Ubuntu/Windows/macOSâ€ (Section 3.1.2).  
      > â€œOSWorld â€¦ supports 369 multiâ€‘application tasks across Ubuntu/Windows/macOS.â€ (Section 3.1.2)
    - Web/task UX: `TurkingBench` (158 microâ€‘tasks), `OmniACT` (32K desktop/web instances), `EgoLife` (300â€‘hour egocentric multimodal dataset + EgoLifeQA), `GTA` (real tools + multiâ€‘modal inputs) (Section 3.1.2).
  - Collaboration/systemâ€‘level evaluation
    - Enterpriseâ€‘style `TheAgentCompany`, ML research/engineering (`MLRB`, `MLEâ€‘Bench`), and comparative frameworks (`AutoGen` vs. `CrewAI`) (Section 3.1.3).

- What the numbers show
  - Breadth and realism are increasing: multiâ€‘OS desktop tasks (369 tasks, `OSWorld`), large web coverage (137 sites, `Mind2Web`), deep toolâ€‘use testbeds (1,024 nested calls, `Sealâ€‘Tools`), and domainâ€‘specific rigor (FHIRâ€‘compliant medical tasks, `MedAgentBench`) (Sections 3.1.1â€“3.1.2).
  - Safety/stress testing is maturing: `AgentHarm` curates â€œ440 malicious agent tasks in 11 hazard categoriesâ€ (Section 3.1.2), enabling systematic multiâ€‘step harmfulness assessment.

- Do these convincingly support the paperâ€™s synthesis?
  - Yes for scope and structure: the curated benchmarks map naturally onto the Buildâ€‘Collaborateâ€‘Evolve design (e.g., planning and tool use can be stressâ€‘tested with `Mind2Web`, `Sealâ€‘Tools`; collaboration with enterprise/ML research settings). The paper is cautious not to claim stateâ€‘ofâ€‘theâ€‘art numbersâ€”it focuses on coverage and evaluation patterns (Sections 3.1â€“3.2).

- Ablations/failure cases/robustness checks
  - Being a survey, there are no new ablations. However, the security sections provide adversarial/poisoning stressors (Tables 3â€“4), and the reliability challenges (Section 6.3) discuss hallucinations and sensitivity to prompt changes, calling for verification pipelines.

- Conditional results and tradeâ€‘offs
  - Centralized vs. decentralized vs. hybrid collaboration each trade off controllability, flexibility, and scalability (Sections 2.2.1â€“2.2.3; Table 1).
  - Memory choices trade shortâ€‘term coherence vs. longâ€‘term accumulation vs. retrieval cost/latency (Section 2.1.2).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - The taxonomy assumes LLMâ€‘centric agents; specialized nonâ€‘LLM agents (symbolic planners, classic RL) are treated principally as tools/augmentations (Figure 1; Section 2), not primary citizens.
  - The survey aggregates but does not empirically compare methods headâ€‘toâ€‘head; performance conclusions rely on cited works and benchmark designs (Sections 3â€“5).

- Scenarios not fully addressed
  - Closedâ€‘loop safety for highâ€‘stakes settings (healthcare, finance) still lacks standardized verification triggers and humanâ€‘inâ€‘theâ€‘loop escalation points (Section 6.3).
  - Crossâ€‘modal embodied agents beyond desktop/web (e.g., multiâ€‘robot physical collaboration under uncertainty) are cataloged but not deeply formalized (Sections 2.1.4, 3.1.2).

- Computational and scalability constraints
  - Multiâ€‘agent systems with heavyweight LLMs face orchestration and cost bottlenecks; classic multiâ€‘agent infrastructures are not optimized for billionâ€‘parameter models (Section 6.1).
  - Memory scaling and relevance management (beyond vector DBs and naive summarization) remain open (Section 6.2).

- Open weaknesses and questions
  - Reliability/scientific rigor: susceptibility to hallucinations, sensitivity to prompts, and lack of standardized citation/grounding pipelines (Section 6.3).
  - Dynamic evaluation lags behind fastâ€‘moving models; contamination and overfitting to benchmarks remain concerns (Section 6.4).
  - Governance and ethics: actionable, auditable accountability pipelines and bias diagnostics across cultures and modalities need formalization (Sections 4.4, 6.5).

## 7. Implications and Future Directions
- How this changes the landscape
  - Provides a common design language for LLM agents: with Buildâ€‘Collaborateâ€‘Evolve, teams can reason about where to invest (e.g., memory vs. planning vs. topology) and how to evaluate and secure deployments (Figures 1â€“4; Sections 2â€“4).
  - Bridges design to evaluation: the mapped benchmarks (Section 3) help convert abstract capabilities (planning, tool use, collaboration) into measurable test plans.

- Followâ€‘up research enabled/suggested (Section 6)
  - Scalable coordination: hierarchical controllers with decentralized execution; learned collaboration graphs; costâ€‘aware agent routing (Section 6.1; dynamic hybrids in Section 2.2.3).
  - Longâ€‘horizon memory: hierarchical episodic/semantic memory plus autonomous knowledge compression and retrieval policies (Section 6.2).
  - Reliability pipelines: knowledgeâ€‘graph verification, retrievalâ€‘withâ€‘citation generation, selfâ€‘consistency ensembles, and standardized AI auditing logs (Section 6.3).
  - Dynamic evaluation: selfâ€‘evolving benchmarks, metaâ€‘probing evaluators, contaminationâ€‘resistant test generation (Section 6.4).
  - Safety and governance: topologyâ€‘aware defenses for multiâ€‘agent networks (Figure 4; Tables 3â€“4; Section 6.5), constitutional design for planning safety (`TrustAgent`, Section 4.1.4), psychologyâ€‘aware risk controls (`PsySafe`, Section 4.1.4).
  - Roleâ€‘play fidelity: improve coverage for underâ€‘represented roles, integrate realâ€‘world reasoning frameworks, and enhance dialogue diversity (Section 6.6).

- Practical applications and downstream use cases (Section 5; Table 7)
  - Science: autonomous hypothesis generation and experimental execution (`SciAgents`, `Curie`), chemistry toolâ€‘augmented synthesis (`ChemCrow`), materials/astronomy coâ€‘pilots (`AtomAgents`, CTA agents).
  - Medicine: virtual hospitals and patient simulators (`AgentHospital`, `AIPatient`), multimodal radiology agents with uncertainty reporting (`CXRâ€‘Agent`, `MedRAX`).
  - Productivity: software lifecycle automation (`ChatDev`, `MetaGPT`) and recommender systems with generative user/item agents (`Agent4Rec`, `AgentCF`, `MACRec`, `RecMind`).
  - Web/desktop autonomy: generalist web agents (`Mind2Web`), realâ€‘computer tasking (`OSWorld`), and APIâ€‘integration frameworks (`RestGPT`, `GraphQLRestBench`).

In short, this paperâ€™s value is architectural: it supplies a precise decomposition of LLM agent systems (what to build), maps collaboration patterns to task properties (how to organize), and catalogs evolution, evaluation, and safety (how to improve and deploy responsibly). Figures 1â€“4 and Tables 1â€“7 function as a design and testing checklist for anyone standing up real LLMâ€‘based agent systems.

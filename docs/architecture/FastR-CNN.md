# Fast R-CNN

**ArXiv:** [1504.08083](https://arxiv.org/abs/1504.08083)

## ğŸ¯ Pitch

Fast R-CNN presents a groundbreaking single-stage object detection framework that radically accelerates both training and inference while boosting detection accuracy. By introducing innovations like the RoI pooling layer and a streamlined, end-to-end trainable multi-task loss, Fast R-CNN efficiently shares computation across thousands of proposals and allows deep network fine-tuning throughout. This enables it to outperform earlier R-CNN and SPPnet models by orders of magnitude in speed and accuracyâ€”making accurate, scalable object detection practical for real-world vision applications.

---

## 1. Executive Summary (2-3 sentences)
Fast R-CNN introduces a single-stage, end-to-endâ€“trainable object detection system that shares computation across thousands of candidate boxes per image and learns classification and box refinement jointly. It delivers large speedups (up to 213Ã— faster at test-time with VGG16) while improving accuracy over prior R-CNN and SPPnet baselines on PASCAL VOC, mainly by introducing the RoI pooling layer, a multi-task loss, and an efficient training scheme (Sections 1â€“3; Tables 1â€“4; Fig. 1â€“2).

## 2. Context and Motivation
- Problem addressed:
  - Object detection must both classify objects and localize them precisely in an image. Two practical hurdles dominate (Section 1): (1) handling â€œnumerous candidate object locationsâ€ (proposals) efficiently, and (2) refining their rough locations to precise boxes.
- Why this matters:
  - Accurate detection underpins applications in autonomous systems, video analytics, image search, and robotics. Efficiency matters because detectors process thousands of regions per image; slow systems are impractical at scale or in time-sensitive settings.
- Prior approaches and their shortcomings (Section 1.1):
  - R-CNN: Classifies each proposal separately with a deep network.
    - Multi-stage training pipeline (fine-tune + SVMs + box regressors).
    - Costly training: extracting and caching per-proposal features â€œtakes 2.5 GPU-daysâ€ and â€œhundreds of gigabytes of storageâ€ for VGG16 on VOC07 (bulleted item 2).
    - Slow test-time: â€œ47s / image (on a GPU)â€ with VGG16 (bulleted item 3).
  - SPPnet: Shares convolutional computation by pooling features from a single image-level feature map (spatial pyramid pooling).
    - Still multi-stage training and disk caching.
    - Key limitation: could not backpropagate into convolutional layers before the pooling stage during fine-tuning, which â€œlimits the accuracy of very deep networksâ€ (end of Section 1.1).
- Positioning of Fast R-CNN (Section 1.2):
  - Single-stage, end-to-end training that updates all layers, removes feature caching, and improves accuracy and speed.
  - Concrete performance claims (Abstract; Section 4.4; Table 4):
    - Trains VGG16 9Ã— faster than R-CNN and 3Ã— faster than SPPnet.
    - Tests 213Ã— faster than R-CNN and 10Ã— faster than SPPnet (with truncated SVD).
    - Higher mAP on VOC07/10/12 (Tables 1â€“3).

## 3. Technical Approach
Fast R-CNN is a fully convolutional backbone that computes a single feature map per image, then extracts fixed-length features for each proposal via RoI pooling, followed by two prediction heads (Fig. 1; Section 2).

- Inputs and outputs (Section 2; Fig. 1):
  - Inputs: one image and a list of `R` object proposals (candidate boxes).
    - Proposal = a `Region of Interest (RoI)`: a rectangle on the feature map defined by `(r, c, h, w)` (top-left row/col and height/width).
  - Outputs per RoI:
    - A `softmax` probability distribution over `K` object classes plus background.
    - `K` sets of bounding-box refinements (4 values per class) that adjust the RoI to better fit the object.

- Step-by-step pipeline (Fig. 1; Sections 2â€“3):
  1. Compute the imageâ€™s convolutional feature map once (shared for all RoIs).
  2. For each RoI, apply `RoI pooling` (Section 2.1):
     - RoI pooling divides the RoI region on the feature map into an `H Ã— W` grid (e.g., `7 Ã— 7`) and applies max pooling in each cell, producing a fixed-size feature map for the RoI regardless of its original size.
     - This is a special case of spatial pyramid pooling with a single level (Section 2.1).
     - During backpropagation, gradients are routed through the â€œargmax switchesâ€ recorded during max pooling (Eq. 4), enabling end-to-end learning through the pooling operation.
  3. Pass the pooled features through fully connected layers (`fc6`, `fc7` in VGG16).
  4. Branch into two heads:
     - Classification head: outputs class probabilities `p = (p0, â€¦, pK)`.
     - Localization head: outputs class-specific box offsets `t^k = (t_x^k, t_y^k, t_w^k, t_h^k)` for each class `k`.

- Joint training with a multi-task loss (Section 2.3; Eqs. 1â€“3):
  - Intuition: learn classification and localization together so the shared features serve both tasks.
  - Loss per RoI:
    - `L(p, u, t^u, v) = L_cls(p, u) + Î» [u â‰¥ 1] L_loc(t^u, v)` (Eq. 1).
      - `u` is the ground-truth class (0 is background).
      - `v = (v_x, v_y, v_w, v_h)` is the ground-truth box target for class `u`.
      - `L_cls(p, u) = -log p_u` is softmax cross-entropy.
      - `L_loc` uses a robust `smooth L1` loss (Eq. 3) over the 4 box parameters, which avoids the instability (exploding gradients) of plain L2 for unbounded regression targets.
      - `Î» = 1` in all experiments; regression targets are normalized to zero mean/unit variance.
  - Box parameterization (as in R-CNN [9]): scale-invariant translation for center coordinates and log-space for width/height (Section 2.3).

- Efficient end-to-end fine-tuning (Section 2.3):
  - Challenge SPPnet faced: when each RoI comes from a different image, backpropagating through SPP becomes inefficient because each RoIâ€™s receptive field covers most of the imageâ€”forward/backward would need to process large image regions per RoI.
  - Fast R-CNN resolves this with hierarchical mini-batching:
    - Sample `N` images per SGD step and then sample `R/N` RoIs from each image (default `N = 2`, `R = 128`; 64 RoIs per image).
    - RoIs from the same image share the convolutional computation and memory in forward and backward passes, making training fast (about 64Ã— faster than sampling one RoI from 128 different images; Section 2.3).
  - RoI sampling (Section 2.3, â€œMini-batch samplingâ€):
    - 25% of sampled RoIs are foreground (IoU â‰¥ 0.5 with a ground-truth box).
      - IoU (Intersection-over-Union) measures overlap between two boxes.
    - 75% are background, chosen with max IoU in [0.1, 0.5), which acts as simple hard-negative mining.
    - Data augmentation: horizontal flip with probability 0.5.
  - Optimization hyperparameters (Section 2.3):
    - Global LR 0.001 for 30k iterations, then 0.0001 for 10k (VOC07/12); larger schedules for larger datasets.
    - Momentum 0.9, weight decay 0.0005. Per-layer LR: 1Ã— for weights, 2Ã— for biases.
    - New fully connected layers initialized from Gaussians (std 0.01 for classifier, 0.001 for regressor), biases to 0.

- Scale handling (Section 2.4; Section 5.2):
  - Single-scale (â€œbrute forceâ€): resize each image so its shortest side is `s = 600` pixels, cap longest side at 1000.
  - Multi-scale: use an image pyramid with 5 scales `s âˆˆ {480, 576, 688, 864, 1200}` and assign each RoI to the scale where its area is closest to `224^2` (Section 3). Used primarily for smaller backbones due to GPU memory limits.

- Test-time detection (Section 3):
  - Run a forward pass for all RoIs to get `p_k` and class-specific box refinements.
  - Score a detection as `p_k`; apply per-class non-maximum suppression to remove duplicates (same as R-CNN).
  - Optional acceleration: compress large fully connected layers via `truncated SVD` (Section 3.1; Eq. 5) without re-training:
    - Factorize weight matrix `W â‰ˆ U Î£_t V^T` and replace one large FC with two smaller FCs (no nonlinearity between), cutting parameters and latency.
    - Example (Fig. 2): keep 1024 singular values for `fc6` and 256 for `fc7` in VGG16.

- What RoI pooling enables (design choice rationale):
  - Fixed-length features for arbitrary box sizes allow a single set of fully connected heads to process all proposals.
  - Sharing convolutional computation across proposals eliminates the per-proposal forward pass that made R-CNN slow, while the RoI poolingâ€™s max locations allow clean gradient flow (Eq. 4) to update all earlier layers.

## 4. Key Insights and Innovations
- RoI pooling with end-to-end fine-tuning through shared features (Sections 2.1â€“2.3; Eq. 4; Fig. 1):
  - Difference from SPPnet: trains through the pooling layer into all convolutional layers efficiently by batching many RoIs per image. This is crucial for very deep nets: freezing conv layers drops VGG16 mAP from 66.9% to 61.4% (Table 5).
  - Significance: enables both accuracy (learn task-specific convolutional features) and speed (one conv pass per image).

- Single-stage, multi-task loss for joint classification and localization (Section 2.3; Eqs. 1â€“3; Table 6):
  - Difference from R-CNN/SPPnet: replaces a multi-step pipeline (fine-tune, then SVMs, then regressors) with one training objective and no feature caching.
  - Evidence of benefit: multi-task training improves pure classification accuracy by +0.8 to +1.1 mAP over training classification alone (Table 6, comparing â€œclassification-onlyâ€ vs â€œmulti-task but no bbox at testâ€ columns).

- Practical speed innovation: truncated SVD on large FC layers (Section 3.1; Eq. 5; Fig. 2):
  - Compresses `fc6` and `fc7` to shrink compute on the dominant per-RoI FC stages (45% of time before compression, Fig. 2 left).
  - With VGG16, detection time drops from 320 ms/image to 223 ms/image with only a 0.3-point mAP loss (Fig. 2).

- Empirical clarification on proposals: more is not always better (Section 5.5; Fig. 3):
  - Increasing selective search proposals beyond ~2k/image causes mAP to first rise then fall (blue curve), and dense sliding-window boxes (~45k/image) perform worse (52.9% mAP) than sparse proposals.
  - Average Recall (AR) does not predict mAP well when the number of proposals changes (red curve in Fig. 3), cautioning against proposal metrics used in isolation.

These are fundamental in that they change how detectors are trained (single-stage, joint loss, end-to-end through pooling) and how they are engineered for speed (shared conv features + FC compression). They also deliver strong empirical clarity on the role of proposal count.

## 5. Experimental Analysis
- Evaluation setup (Section 4.1):
  - Backbones: CaffeNet/AlexNet-like (`S`), VGG CNN M 1024 (`M`), and VGG16 (`L`).
  - Datasets and splits:
    - VOC07, VOC10, VOC12 with standard trainval/test splits.
    - Combined training sets: `07+12` (union of VOC07 trainval & VOC12 trainval) and `07++12` (VOC07 trainval + VOC07 test + VOC12 trainval).
  - Metric: mean Average Precision (mAP) at IoU â‰¥ 0.5 (VOC protocol).
  - Unless noted, single-scale training/testing at `s = 600` (Section 4.1; Section 5.2).

- Main quantitative results:
  - Accuracy (Tables 1â€“3):
    - VOC07 (Table 1, VGG16):
      - Fast R-CNN: 66.9% mAP (single-scale).
      - R-CNN: 66.0%; SPPnet: 63.1% (five-scale).
      - Removing â€œdifficultâ€ examples: FRCN 68.1%.
      - With more data (`07+12`): FRCN 70.0%.
    - VOC10 (Table 2, VGG16):
      - FRCN 66.1%; with `07++12`: 68.8%.
      - SegDeepM (uses segmentation + MRFs): 67.2%. FRCN surpasses it when trained on `07++12`.
      - R-CNN: 62.9%.
    - VOC12 (Table 3, VGG16):
      - FRCN 65.7%; with `07++12`: 68.4%.
      - R-CNN: 62.4%.
  - Speed and training cost (Table 4; Fig. 2):
    - VGG16 test time per image:
      - R-CNN: 47.0s; SPPnet: 2.3s; Fast R-CNN: 0.32s; with SVD: 0.22s.
      - Speedups vs R-CNN: 146Ã— (no SVD), 213Ã— (with SVD).
    - VGG16 training time (VOC07):
      - R-CNN: 84h; SPPnet: 25.5h; Fast R-CNN: 9.5h.
      - No disk caching of features for FRCN (saves 100s of GB).
    - Timing breakdown (Fig. 2):
      - Before SVD: conv 46.3%, fc6 38.7%, fc7 6.2%.
      - After SVD: conv 67.8%, fc6 17.5%, fc7 1.7% (total 223ms/image).

- Ablations and design studies:
  - Which layers to fine-tune? (Table 5)
    - Freezing all conv layers (fine-tuning â‰¥ fc6) drops VGG16 mAP from 66.9% to 61.4%.
    - Updating from `conv3_1` upward yields near-best mAP (66.9%) with manageable memory/training time; updating from `conv2_1` gives a small +0.3 point gain but adds 1.3Ã— training time and exhausts memory if starting at `conv1_1`.
  - Multi-task vs. stage-wise training (Table 6):
    - Pure classification models are improved by joint training: +0.8 to +1.1 mAP when comparing â€œclassification-onlyâ€ vs. â€œmulti-task but no bbox at test.â€
    - Full multi-task training with test-time bbox regression yields the best mAP across S/M/L backbones (fourth column per group).
  - Single vs multi-scale (Table 7):
    - Multi-scale improves mAP modestly (+~1â€“1.5) at a large speed cost:
      - For model `S`, single-scale 57.1% at 0.10s/image vs multi-scale 58.4% at 0.39s/image.
      - For model `M`, single-scale 59.2% at 0.15s/image vs 60.7% at 0.64s/image.
    - VGG16 uses single-scale due to GPU limits yet still leads (66.9%).
  - Softmax vs SVM (Table 8):
    - In Fast R-CNN, softmax slightly outperforms post-hoc one-vs-rest SVMs:
      - VGG16: 66.9% (softmax) vs 66.8% (SVM).
      - Similar small margins for `S` and `M`.
  - Number and type of proposals (Section 5.5; Fig. 3):
    - With selective search, mAP increases then decreases as proposals grow from 1k to 10k per image (blue curve).
    - Dense boxes (~45k/image) hurt: 52.9% mAP with softmax; 49.3% with SVM.
    - Replacing each SS box with its nearest dense box reduces mAP by only ~1 point (to 57.7%), implying dense sets can cover SS but their distribution harms learning/inference.
  - More training data helps (Section 5.3):
    - On VOC07, mAP rises from 66.9% to 70.0% when training on `07+12`; similar gains on VOC10/12 with `07++12`.

- Additional benchmark (Section 5.6):
  - On MS COCO â€œtest-dev,â€ PASCAL-style mAP: 35.9%; COCO-style AP (averaged over IoUs): 19.7%.

- Do the experiments support the claims?
  - Yes: speedups are quantified (Table 4; Fig. 2), accuracy is improved across datasets (Tables 1â€“3), and ablations isolate the impact of (i) fine-tuning conv layers (Table 5), (ii) multi-task learning (Table 6), (iii) scale choices (Table 7), and (iv) classifier choice (Table 8).
  - The proposals study (Fig. 3) strengthens a nuanced claim: proposal quantity/quality trade-offs are non-trivial, and AR alone can mislead when proposal counts vary.

## 6. Limitations and Trade-offs
- Reliance on external proposals (Sections 3, 5.5):
  - The reported 0.3s/image test time excludes proposal generation (Abstract; Sections 1, 3). Overall runtime depends on the proposal method (e.g., selective search can be slow).
  - Accuracy depends on proposal recall/distribution; dense sliding windows underperform despite high coverage (Fig. 3).
- Memory/computation constraints for very deep nets:
  - Multi-scale training/testing for VGG16 is not feasible under the presented implementation due to GPU memory (Table 7 note). Single-scale may miss very small objects.
- Class-specific box regressors:
  - The localization head predicts class-specific refinements, which increases parameters for large `K` and assumes class labels are reliable before box regression.
- Training heuristics and choices:
  - Foreground/background sampling thresholds (IoU â‰¥ 0.5; background in [0.1, 0.5)) are heuristic (Section 2.3). Different tasks or datasets may need re-tuning.
  - Single augmentation (horizontal flip) is modest; robustness to domain shifts or occlusions is not specifically studied.
- Speed after FC compression:
  - After SVD, convolution dominates latency (Fig. 2 right, 67.8%), so further speed gains require accelerating conv layers or using lighter backbones.
- Not a fully proposal-free detector:
  - The approach does not learn to generate proposals; later work (e.g., integrating a region proposal network) is required to eliminate proposal overhead.

## 7. Implications and Future Directions
- Field impact:
  - Fast R-CNN established the now-standard paradigm of end-to-end training over pooled region features with shared convolutional computation. Its `RoI pooling` abstraction directly influenced later advances such as proposal networks (for proposal-free pipelines), feature pyramids, and instance segmentation heads (Mask-style heads build on pooled region features).
- Follow-up research it enables/suggests:
  - Integrate proposal generation into the network to remove external proposal cost and enable joint optimization (the training/inference scaffolding here makes such integration natural).
  - Explore alternatives to max-based RoI pooling (e.g., bilinear interpolation) to improve localization accuracy and gradient flow for small objects.
  - Revisit proposal metrics: design evaluation protocols that better correlate with detector mAP when the number of proposals varies (insight from Fig. 3).
  - Develop lighter backbones or conv acceleration to shift the latency bottleneck beyond convolutions (Fig. 2 right).
  - Study class-agnostic vs class-specific box regression at scale to reduce parameters while maintaining accuracy.
- Practical applications:
  - Near real-time detection pipelines in video analytics, robotics, and mobile/embedded settings where per-image latency must be well under a second and disk-heavy preprocessing is impractical.
  - A faster/cleaner training workflow for detection tasks in industry (no feature caching, single-stage objective), easing iteration and large-scale experimentation.

> Representative headline results (with VGG16): â€œprocesses images 0.32s each (0.22s with SVD), trains in 9.5h on VOC07, and achieves 66.9% mAP on VOC07, 66.1% on VOC10 (68.8% with extra data), and 65.7% on VOC12 (68.4% with extra data)â€ (Tables 1â€“4; Fig. 2).

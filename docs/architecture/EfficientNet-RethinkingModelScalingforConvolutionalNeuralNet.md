# EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks

**ArXiv:** [1905.11946](https://arxiv.org/abs/1905.11946)

## ðŸŽ¯ Pitch

EfficientNet introduces a breakthrough method for scaling convolutional neural networks by jointly and proportionally increasing their depth, width, and input resolution using a single compound coefficient. This innovative 'compound scaling' framework achieves state-of-the-art accuracy on major benchmarks like ImageNet while using dramatically fewer parameters and compute, making high-performance vision models far more accessible and efficient for real-world applications.

---

## 1. Executive Summary
EfficientNet introduces a principled way to scale convolutional neural networks by jointly increasing depth, width, and input resolution with a single â€œcompoundâ€ coefficient `Ï†`. Using this method and a NAS-designed baseline (`EfficientNet-B0`), the paper builds a family `B0â€“B7` that achieves stateâ€‘ofâ€‘theâ€‘art ImageNet accuracy with far fewer parameters and FLOPs; for example, `EfficientNetâ€‘B7` reaches 84.3% topâ€‘1 with 8.4Ã— fewer parameters and 6.1Ã— lower CPU latency than the prior best (Figures 1, 5; Tables 2, 4).

## 2. Context and Motivation
- Problem addressed
  - How to scale a convolutional network when more compute or memory is available. Practitioners typically increase only one dimensionâ€”`depth` (more layers), `width` (more channels), or `resolution` (larger input images). Section 1 notes that this is â€œarbitraryâ€ and requires â€œtedious manual tuning,â€ often giving subâ€‘optimal accuracy/efficiency.
- Why it matters
  - Accuracy improvements have historically come from ever larger models (Section 2, â€œConvNet Accuracyâ€), culminating in systems like GPipe with 557M parameters that require specialized pipeline parallelism. However, hardware memory/latency constraints make such brute-force scaling impractical for many applications (â€œwe have already hit the hardware memory limit,â€ Section 2).
- Shortcomings of prior approaches
  - Singleâ€‘dimension scaling exhibits diminishing returns: Figure 3 shows topâ€‘1 accuracy quickly saturates when only width, depth, or resolution is increased.
  - Manual attempts to balance multiple dimensions exist (e.g., NASNet, AmoebaNet; Section 3.3) but require substantial human tuning and do not offer a general rule.
- Positioning
  - The paper formulates scaling as an optimization problem (Equation 2) and proposes a simple, universal ruleâ€”`compound scaling`â€”that balances width, depth, and resolution via constant ratios learned once on a small baseline (Section 3.3). It then pairs this rule with a NASâ€‘designed baseline architecture (`EfficientNetâ€‘B0`) to build a family of models (Section 4).

## 3. Technical Approach
Step-by-step overview of how EfficientNet works:

- Problem formulation (Section 3.1)
  - A ConvNet is viewed as a sequence of stages, each with repeated layers. Each stage `i` has:
    - `LÌ‚_i`: number of layer repeats (depth within the stage),
    - `HÌ‚_i Ã— WÌ‚_i`: spatial resolution,
    - `CÌ‚_i`: number of channels (width).
  - The layer operators `FÌ‚_i` (the micro-architecture) stay fixed; scaling chooses new global multipliers for depth, width, and resolution for the whole network. This reduces the design space from per-layer choices to three numbers.

- Why single-dimension scaling fails (Section 3.2; Figure 3)
  - Depth alone: vanishing returns and training difficulty; accuracy gains level off for very deep models.
  - Width alone: easier to train and captures fine details, but shallowâ€‘wide networks struggle with high-level features.
  - Resolution alone: higher resolution adds detail, but without more depth/width the network cannot exploit it; returns diminish at very high resolutions.

- Compound scaling rule (Section 3.3; Equation 3)
  - Use one user-specified compute scale `Ï†` and three constants `Î±, Î², Î³`:
    - depth multiplier: `d = Î±^Ï†`
    - width multiplier: `w = Î²^Ï†`
    - resolution multiplier: `r = Î³^Ï†`
    - Constraint: `Î± Â· Î²^2 Â· Î³^2 â‰ˆ 2`, with `Î±, Î², Î³ â‰¥ 1`
  - Intuition in plain language:
    - If you supply more input pixels (higher `r`), the model needs a larger receptive field (more `d`) and more channels (more `w`) to capture and process the extra fineâ€‘grained patterns. Balancing all three maintains representational capacity aligned to the input scale.
  - Why the constraint: FLOPs of convolution scale roughly with `d Â· w^2 Â· r^2`. Choosing `Î± Â· Î²^2 Â· Î³^2 â‰ˆ 2` means each +1 step in `Ï†` roughly doubles compute, i.e., FLOPs â‰ˆ `2^Ï†` (Section 3.3).

- How `Î±, Î², Î³` are chosen (Section 4)
  - Two-step procedure:
    - Step 1 (small grid search at `Ï† = 1`): find constant ratios on the small baseline `EfficientNetâ€‘B0`. Best values: `Î± = 1.2`, `Î² = 1.1`, `Î³ = 1.15` under the constraint `Î± Â· Î²^2 Â· Î³^2 â‰ˆ 2`.
    - Step 2 (scale with `Ï†`): keep `Î±, Î², Î³` fixed and vary `Ï†` to obtain the family `B1, â€¦, B7` (Table 2 lists their resulting sizes and accuracies).

- The baseline architecture `EfficientNetâ€‘B0` (Table 1; Section 4)
  - Obtained by a multi-objective neural architecture search (NAS) that optimizes accuracy and FLOPs: objective `ACC(m) Ã— [FLOPs(m)/T]^w` with `w = âˆ’0.07` and FLOPs target `T` = 400M.
  - Building blocks:
    - `MBConv` (mobile inverted bottleneck): an efficient block using depthwise separable convolutions and an expansion phase (from MobileNetV2). It prioritizes compute efficiency.
    - `Squeeze-and-Excitation (SE)`: a small gating mechanism that reweights channels based on global context to improve representational quality.
  - Skeleton (Table 1): A standard conv stem, then eight MBConv stages with varying kernel sizes (3Ã—3/5Ã—5), expansion ratio 6, and increasing channels while spatial resolution decreases; ends with a 1Ã—1 conv and classifier.

- Why this design (Figures 3â€“4, 8; Section 6)
  - Figure 4 empirically shows that width scaling improves much more when baseline depth and resolution are also increased, reinforcing the need for coordinated scaling.
  - Figure 8 compares scaling strategies on the same baseline: compound scaling gives up to +2.5% topâ€‘1 over singleâ€‘dimension scaling at similar FLOPs.

## 4. Key Insights and Innovations
- A universal scaling law for ConvNets (fundamental)
  - Novelty: Equation 3 provides a simple, generalizable rule to scale depth, width, and resolution together with one knob `Ï†`, rather than adâ€‘hoc perâ€‘dimension choices.
  - Why it matters: It turns model scaling into a predictable, computeâ€‘aware process (FLOPs â‰ˆ `2^Ï†`) that empirically yields better accuracy at the same cost (Figure 8).

- Empirical diagnosis that balanced scaling outperforms single-dimension scaling (fundamental/diagnostic)
  - Evidence: Figure 3 shows diminishing returns for width-only, depth-only, and resolution-only scaling; Figure 4 shows width scaling benefits more when baseline is deeper and higher resolution.
  - Significance: Establishes the need to co-scale all dimensions to harvest gains from larger inputs and models.

- EfficientNet family from a small NAS baseline plus compound scaling (methodological)
  - Design pattern: Do a single small grid search for `Î±, Î², Î³` on a compact NAS-designed `B0`, then scale to a whole family (`B1â€“B7`) by increasing `Ï†` (Section 4; Table 2).
  - Payoff: Consistent accuracy/efficiency dominance across a wide operating range, from mobile to large models (Figures 1 and 5; Table 2).

- Strong transfer learning with orders-of-magnitude fewer parameters (applied)
  - Result: On 5/8 datasets, EfficientNet variants set or match SOTA with large parameter reductions (Table 5). Example: Flowers 98.8% with `B7`, and CIFARâ€‘100 91.7% with `B7`, using 8.7Ã— fewer parameters than GPipe.

## 5. Experimental Analysis
- Evaluation setup
  - Datasets and tasks:
    - ImageNet classification (1.28M train, 50K val): primary benchmark (Tables 2, 4; Figures 1, 5).
    - Transfer learning on eight datasets (Table 6): CIFARâ€‘10/100, Birdsnap, Stanford Cars, Flowers, FGVC Aircraft, Oxfordâ€‘IIIT Pets, Foodâ€‘101 (Table 5; Figure 6).
  - Metrics:
    - Topâ€‘1 and topâ€‘5 accuracy on ImageNet (Table 2).
    - Parameter count, FLOPs (Table 2) and singleâ€‘core CPU latency (Table 4).
  - Training details (Section 5.2):
    - Optimizer: RMSProp (decay 0.9, momentum 0.9); BN momentum 0.99; weight decay 1eâ€‘5; initial LR 0.256 with exponential decay; SiLU/Swishâ€‘1 activation; AutoAugment; stochastic depth (survival prob 0.8); dropout increases from 0.2 (`B0`) to 0.5 (`B7`). Early stopping uses a 25K â€œminivalâ€ split from the training set, then evaluates the chosen checkpoint on the official validation set.

- Baselines and comparisons
  - Classic and modern ConvNets: ResNet, DenseNet, Inception-v3/v4, Xception, ResNeXtâ€‘101, SENet, NASNet, AmoebaNet, PNASNet, GPipe (Table 2).
  - Controlled scaling ablations on existing models: MobileNetV1/V2 and ResNetâ€‘50 with width-only, depth-only, resolution-only, and compound scaling (Table 3).
  - Scaling ablation on EfficientNetâ€‘B0: Figure 8 (compound vs single-dimension).

- Main quantitative results (all single-model, single-crop)
  - Dominance in accuracyâ€“efficiency tradeâ€‘off (Table 2; Figures 1, 5):
    - â€œSmallâ€ regime:
      - `B1` 79.1% topâ€‘1 with 7.8M params and 0.70B FLOPs.
        - > â€œResNetâ€‘152 77.8% with 60M params and 11B FLOPsâ€ (Table 2). CPU latency: `B1` 0.098s vs ResNetâ€‘152 0.554s (5.7Ã— faster; Table 4).
    - â€œMediumâ€ regime:
      - `B3` 81.6% with 12M params and 1.8B FLOPs.
        - > â€œResNeXtâ€‘101 80.9% with 84M params and 32B FLOPsâ€ (Table 2; Figure 5 shows 18Ã— FLOPs reduction).
      - `B4` 82.9% with 19M params and 4.2B FLOPs.
        - > â€œSENet 82.7% with 146M params and 42B FLOPsâ€ (Table 2).
    - â€œLargeâ€ regime:
      - `B7` 84.3% with 66M params and 37B FLOPs.
        - Matches â€œGPipe 84.3% with 557M paramsâ€ (8.4Ã— smaller; Table 2). CPU latency: `B7` 3.1s vs GPipe 19.0s (6.1Ã— faster; Table 4).
  - Scaling existing networks (Table 3):
    - MobileNetV1 at similar FLOPs (~2.2â€“2.3B):
      - Width-only: 74.2%; Resolution-only: 72.7%; Compound (`d=1.4, w=1.2, r=1.3`): 75.6%.
    - MobileNetV2 at ~1.1â€“1.3B FLOPs:
      - Depth-only (`d=4`): 76.8%; Width-only (`w=2`): 76.4%; Resolution-only (`r=2`): 74.8%; Compound: 77.4%.
    - ResNetâ€‘50 at ~16â€“17B FLOPs:
      - Depth-only: 78.1%; Width-only: 77.7%; Resolution-only: 77.5%; Compound: 78.8%.
    - Takeaway: compound scaling consistently yields the best accuracy at comparable compute.
  - Ablation of scaling strategy (Figure 8):
    - Compound scaling adds up to +2.5% topâ€‘1 over singleâ€‘dimension scaling at similar FLOPs when scaling `B0`.
  - Qualitative evidence (Figure 7):
    - Class Activation Maps show compoundâ€‘scaled models attend to more relevant, detailed regions than depthâ€‘only, widthâ€‘only, or resolutionâ€‘only scaled counterparts (Table 7 lists matchedâ€‘FLOPs comparisons).
  - Transfer learning (Table 5; Figure 6):
    - > â€œOur scaled EfficientNet models achieve new stateâ€‘ofâ€‘theâ€‘art accuracy for 5 out of 8 datasets, with 9.6Ã— fewer parameters on average.â€
    - Examples:
      - CIFARâ€‘100: `B7` 91.7% vs GPipe 91.3% with 8.7Ã— fewer parameters.
      - Flowers: `B7` 98.8% (best reported).
      - Oxfordâ€‘IIIT Pets: `B6` 95.4% vs GPipe 95.9% with 14Ã— fewer parameters (slightly below the best).
  - Sanity check on ImageNet test server (Appendix Table 8):
    - Validation and test accuracies closely match for all `B0â€“B7` variants (e.g., `B7` val 84.26% vs test 84.33%).

- Do results support the claims?
  - Yes: Multiple, strong baselines across size regimes; consistent improvements in both parameters and FLOPs (Table 2; Figures 1, 5), matchedâ€‘FLOPs/latency comparisons (Table 4), and ablations on both thirdâ€‘party baselines (Table 3) and their own baseline (Figure 8). The qualitative CAMs (Figure 7) align with the central intuition about balanced receptive fields and fineâ€‘grained detail capture.

## 6. Limitations and Trade-offs
- Dependence on the baseline and constants
  - `Î±, Î², Î³` are found via a small grid search on `B0` at `Ï† = 1` (Section 4). The paper notes it â€œis possible to achieve even better performance by searching for `Î±, Î², Î³` directly around a large model,â€ but that becomes â€œprohibitively more expensive.â€ This raises the question of how optimal the fixed ratios remain for very large scales or for very different architectures.
- Approximate compute model
  - The `Î± Â· Î²^2 Â· Î³^2 â‰ˆ 2` constraint assumes convoÂ­lutions dominate cost and that FLOPs scale as `d Â· w^2 Â· r^2` (Section 3.3). Real hardware performance also depends on memory bandwidth, kernel sizes, and implementation details; Table 4 gives CPU latency but no GPU/mobile latency, and the search objective optimizes FLOPs, not latency.
- Uniform scaling across all stages
  - The method applies global multipliers to all stages (Equation 2). Some architectures might benefit from perâ€‘stage or perâ€‘block scaling (e.g., earlier layers may need more resolution emphasis).
- Domain/task scope
  - Experiments focus on image classification and classificationâ€‘style transfer. Object detection/segmentation are mentioned as contexts where high resolution helps (Section 3.2), but not evaluated here.
- Absolute model size at the high end
  - While efficient, `B7` still has 66M parameters and 37B FLOPs (Table 2). Training such models remains compute-intensive and requires robust regularization (Section 5.2).
- Training recipe influence
  - Gains reflect a combination of architecture, scaling, and training choices (SiLU, AutoAugment, stochastic depth, dropout schedule; Section 5.2). The paper does not fully disentangle their relative contributions beyond the scaling ablations (Figure 8).

## 7. Implications and Future Directions
- How it changes the field
  - Establishes a simple, broadly applicable principle: scale CNNs by balancing depth, width, and resolution with fixed ratios tied to a compute budget. This offers a reproducible knob (`Ï†`) to navigate the accuracyâ€“efficiency frontier, replacing adâ€‘hoc scaling (Figures 1, 5, 8).
- Follow-up research it enables
  - Automated discovery of perâ€‘architecture or perâ€‘stage scaling laws (learn `Î±, Î², Î³` jointly with architecture search; or make them layerâ€‘wise instead of global).
  - Latencyâ€‘aware or hardwareâ€‘aware compound scaling (replace FLOPs with deviceâ€‘specific latency models during the small grid search).
  - Extending compound scaling ideas beyond CNNs (e.g., jointly scaling depth/width/patchâ€‘resolution in vision transformers) and to multiâ€‘task settings.
  - Theoretical analysis connecting receptive field growth, information density at higher input resolutions, and the observed need to co-scale width and depth.
- Practical applications
  - Building model families for deployment tiers (mobile â†’ edge â†’ server) by simply changing `Ï†`.
  - Efficient transfer learning: Table 5 shows strong results across diverse datasets with far fewer parameters, reducing training and inference costs.
  - As a design template: start with a compact, NASâ€‘found baseline, run one small search to set `Î±, Î², Î³`, then scale to the desired budgetâ€”yielding predictable gains without exhaustive manual tuning.

> In short, EfficientNet contributes both a general scaling law (Equation 3) validated by extensive ablations (Figure 8; Table 3) and a practical model family (`B0â€“B7`) that dominates in accuracy per parameter/FLOP across sizes (Table 2; Figures 1, 5), with competitive realâ€‘world latency on CPU (Table 4) and strong transfer performance (Table 5).

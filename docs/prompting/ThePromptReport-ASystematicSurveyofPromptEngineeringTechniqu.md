# The Prompt Report: A Systematic Survey of Prompt Engineering Techniques

**ArXiv:** [2406.06608](https://arxiv.org/abs/2406.06608)

## ðŸŽ¯ Pitch

This paper delivers the most comprehensive, evidence-based synthesis of prompt engineering to date by conducting a PRISMA-guided systematic review that maps 58 text-based and 40 multimodal/multilingual prompting techniques into an actionable taxonomy, paired with a unified vocabulary. By benchmarking prompting strategies and providing empirical case studiesâ€”including high-stakes domains like suicidality detectionâ€”it empowers both practitioners and researchers to navigate and standardize best practices in a field whose inconsistent terminology and scattered methods have previously impeded reliable, secure, and effective AI deployment.

---

## 1. Executive Summary (2â€“3 sentences)
This paper systematizes the rapidly expanding area of prompt engineering by (a) performing a PRISMAâ€‘guided systematic literature review that yields a taxonomy of 58 textâ€‘based prompting techniques (plus 40 multimodal/multilingual variants) and a consolidated vocabulary of 33 terms, and (b) validating practice through two empirical studies: a technique benchmark on MMLU and a realâ€‘world promptâ€‘engineering case study for detecting suicidal â€œentrapment.â€ It matters because prompting is the primary interface to modern generative models; inconsistent terminology, scattered techniques, and unclear best practices hinder reliable deployment, evaluation, and safety.

## 2. Context and Motivation
- Problem/gap addressed
  - Prompt engineering has grown ad hoc with conflicting terminology and overlapping techniques, which makes it hard to know what works, when, and why (Section 1; Figure 1.3 for terminology; Figure 2.2 for technique map).
  - There is no consolidated, evidenceâ€‘based guide that spans text, multilingual, and multimodal prompting, while also covering safety (prompt hacking) and evaluation (Sections 3â€“5).
  - Practical guidance has lacked empirical case studies demonstrating how experts iterate prompts on real tasks (Section 6).

- Why this is important
  - Prompts are the operational handle on LLM behavior in consumer, enterprise, and research settings (Section 1). Better prompts consistently improve performance across tasks (Section 1 citing Wei et al., 2022b; Liu et al., 2023b).
  - Security and safety failures (e.g., prompt injection and jailbreaking) create real business risk (Section 5.1); poor calibration, bias, and ambiguity hurt reliability (Section 5.2).

- Prior approaches and where they fall short
  - Earlier surveys covered prompting broadly (e.g., including cloze or soft prompts) or specific subareas (reasoning, multimodal, etc.). They did not provide an upâ€‘toâ€‘date, PRISMAâ€‘grounded synthesis focused on modern hard, prefix prompts, nor did they pair a field map with actionable empirical case studies (Section 7).

- Positioning
  - Scope is deliberately focused on standard, deployable prompting: hard (discrete) `prefix` prompts, not `cloze` or soft prompts, and taskâ€‘agnostic techniques (Section â€œScope of Studyâ€). The work unifies vocabulary, organizes techniques, quantifies usage, and demonstrates practice through benchmarking and a detailed realâ€‘world prompt engineering process.

## 3. Technical Approach
This paper has three pillars: a systematic review to build a taxonomy, an empirical benchmark, and a realâ€‘world case study.

- Systematic review (PRISMA pipeline)
  - Data sources: arXiv, Semantic Scholar, ACL; 44 promptingâ€‘related keywords (Appendix A.4).
  - Process (Figure 2.1; Section 2.1.1):
    - Start with 4,247 unique records after deduplication.
    - Human review on 1,661 arXiv titles/abstracts with 92% interâ€‘annotator agreement; criteria focused on novel prompting techniques using hard, prefix prompts; fineâ€‘tuning papers excluded.
    - LLMâ€‘assisted screening of remaining records using a GPTâ€‘4 prompt (Appendix A.5); validated at 89% precision and 75% recall (F1 = 81%).
    - Final corpus: â€œ1,565 quantitative records included in analysisâ€ (Figure 2.1).
  - Output:
    - A terminology chart capturing components of prompts and related artifacts (Figure 1.3).
    - A taxonomy of textâ€‘based prompting techniques grouped into six families (Figure 2.2).
    - Extensions for multilingual and multimodal prompting (Figures 3.1 and 3.2).
    - A structured treatment of security, safety, and evaluation (Sections 4â€“5).

- Terminology and prompt components
  - A `prompt` is any input (text, image, audio, etc.) used to guide a generative modelâ€™s output; prompts often come from `prompt templates`, which are functions with variables that render into concrete prompts (Section 1.1; Figure 1.2).
  - Typical components (Section 1.2.1):
    - `Directive` (the task, e.g., â€œClassifyâ€¦â€).
    - `Exemplars` (a.k.a. â€œshotsâ€).
    - `Output formatting` and `style` constraints.
    - `Role` (a persona to influence style or behavior).
    - `Additional Information` (domain facts, constraints).

- Taxonomy of text techniques (Figure 2.2; Sections 2.2.1â€“2.2.5)
  - `Inâ€‘Context Learning (ICL)`: learning from exemplars or instructions inside the prompt; includes `fewâ€‘shot` prompts and `zeroâ€‘shot` instruction prompts (Figures 2.4â€“2.6).
  - `Thought generation`: inducing explicit reasoning, e.g., `Chainâ€‘ofâ€‘Thought (CoT)` and its zeroâ€‘shot, fewâ€‘shot, and table/analogical variants (Section 2.2.2; Figure 2.8).
  - `Decomposition`: dividing complex problems into subâ€‘problems (`Leastâ€‘toâ€‘Most`, `Treeâ€‘ofâ€‘Thought`, `Programâ€‘ofâ€‘Thoughts`) (Section 2.2.3).
  - `Ensembling`: produce multiple answers using prompt variations or sampling, then aggregate (`Selfâ€‘Consistency`, `DENSE`, `USP`) (Section 2.2.4).
  - `Selfâ€‘criticism`: generate, critique, and revise answers (`Selfâ€‘Refine`, `COVE`, `Selfâ€‘Verification`) (Section 2.2.5).

- Formalization and answer engineering
  - Prompts are treated as conditioning mechanisms on a language model `p_LM`, optionally via a template `T(x)` and fewâ€‘shot exemplars `X` (Appendix A.8, Eqs A.1â€“A.5).
  - Prompt optimization is maximizing a score function `S` over a dataset, sometimes jointly with an `answer extractor` `E` that parses LLM outputs into canonical answers (Appendix A.8, Eqs A.6â€“A.8; Section 2.5).
  - `Answer engineering` decisions: answer `shape` (token/span), `space` (allowed values), and `extractor` (regex, verbalizer, or a separate LLM) (Section 2.5; Figure 2.13).

- Empirical benchmark (Section 6.1)
  - Task: MMLU subset (2,800 questions; 20% per category; sensitive â€œhuman_sexualityâ€ excluded).
  - Model: `gpt-3.5-turbo`.
  - Prompting conditions (Figure 6.2): `Zeroâ€‘Shot`; `Zeroâ€‘Shot CoT` with three thought inducers; `Zeroâ€‘Shot CoT` with `Selfâ€‘Consistency` (3 samples); `Fewâ€‘Shot`; `Fewâ€‘Shot CoT`; and `Fewâ€‘Shot CoT` with `Selfâ€‘Consistency`.
  - Two question formats (Figures 6.3 and 6.4).
  - Decoding: temperature 0.5 for `Selfâ€‘Consistency`, 0.0 otherwise (Section 6.1.3).
  - Answer parsing: patternâ€‘based extraction of choices; variations tested (Section 6.1.4).

- Realâ€‘world case study: suicidal â€œentrapmentâ€ detection (Section 6.2)
  - Data: 221 Reddit r/SuicideWatch posts labeled by two trained coders for presence/absence of â€œentrapmentâ€ (Krippendorffâ€™s Î± = 0.72) (Section 6.2.2).
    - `Entrapment` is defined as â€œa desire to escape from an unbearable situation, tied with the perception that all escape routes are blockedâ€ with concrete phrasing cues (Figure 6.7).
  - Goal: promptâ€‘engineer a binary classifier using only prompting (no fineâ€‘tuning).
  - Process (~20 hours, 47 steps) included:
    - Guardrail conflicts: initial models responded with crisis advice instead of labels; switching to `GPTâ€‘4â€‘32K` allowed oneâ€‘word labels (Section 6.2.3.2).
    - Iterative technique exploration: zeroâ€‘shot with definition, fewâ€‘shot, `CoT`, `contrastive CoT`, targeted answer extraction, ensembling, and a new method, `Automatic Directed CoT (AutoDiCoT)` (Figures 6.12â€“6.16).
  - `AutoDiCoT` (Figure 6.12):
    - For each training item, elicit a reasoning chain `r_i` that either justifies a correct label or explains why the earlier label was wrong; store triplets `(q_i, r_i, a_i)`.
    - Use selected triplets as exemplars (including an â€œincorrect reasoningâ€ one) to steer reasoning on new inputs (Figures 6.13, 6.16).
  - Observations:
    - Seemingly innocuous context duplication (pasting the same context email twice) improved performance, and deâ€‘duplication hurt it (Section 6.2.3.3: â€œFull Context Only,â€ â€œDeâ€‘Duplicating Emailâ€).
    - Overâ€‘restricting to â€œexplicitâ€ entrapment raised precision but crashed recallâ€”misaligned with the clinical goal of minimizing false negatives (Section 6.2.3.3).

- Evaluation frameworks and safety (Sections 4.2 and 5)
  - LLMâ€‘asâ€‘evaluator designs: prompt roles, CoT, modelâ€‘generated guidelines, output formats (binary, Likert, JSON/XML), and frameworks like `LLMâ€‘EVAL`, `Gâ€‘EVAL`, and `ChatEval` (Section 4.2.1â€“4.2.3).
  - Security taxonomy: `prompt hacking` âŠ‡ `prompt injection` (override developer instructions) and `jailbreaking` (coax unsafe actions with no developer instruction present), plus risks (data leakage, package hallucination, chatbot liability), and defenses (detectors, guardrails) (Section 5.1; Figure 5.1).

## 4. Key Insights and Innovations
- A fieldâ€‘wide, PRISMAâ€‘grounded taxonomy and vocabulary
  - Novelty/significance: integrates 58 textâ€‘based techniques into six coherent families (Figure 2.2) and provides a consistent terminology (Figure 1.3), reducing confusion across papers that use overlapping or conflicting names (Section 1.2).
  - Difference from prior: earlier reviews were broader or less structured; here the focus on deployable hard, prefix prompts plus a formal prompt definition and answer engineering pipeline (Appendix A.8; Section 2.5) is practiceâ€‘oriented.

- A practical, formal view of `answer engineering`
  - Whatâ€™s new: elevates output parsing as a firstâ€‘class design spaceâ€”answer `shape`, `space`, and `extractor` (Section 2.5; Figure 2.13)â€”and unifies it with prompt optimization via Eq. A.8 (Appendix A.8).
  - Why it matters: many failures in real systems come from brittle postâ€‘processing rather than modeling; formalizing this makes evaluations repeatable and prompts more robust.

- `Automatic Directed CoT (AutoDiCoT)` for reasoning control
  - What it is: a simple algorithm to build CoT exemplars that explicitly steer reasoning away from observed error modes by including â€œwhat not to doâ€ alongside correct rationales (Figure 6.12; Oneâ€‘Shot contrastive example in Figure 6.13).
  - Impact: on the entrapment task, a `10â€‘Shot AutoDiCoT` prompt reached the best development F1 (0.53) with recall 0.86 and precision 0.38 (Section 6.2.3.3; Figure 6.16 and Figure 6.6). This demonstrates targeted reasoning control without fineâ€‘tuning.

- Empirical clarity on technique performance tradeâ€‘offs
  - Benchmark result (MMLU; `gptâ€‘3.5â€‘turbo`): `Fewâ€‘Shot CoT` outperforms `Zeroâ€‘Shot` and `Zeroâ€‘Shot CoT`, while `Selfâ€‘Consistency` helps zeroâ€‘shot but not fewâ€‘shot in this setup (Section 6.1.5; Figure 6.1).
  - Quote of main numbers:
    > Zeroâ€‘Shot 0.627; Zeroâ€‘Shot CoT 0.547; Zeroâ€‘Shot CoT + Selfâ€‘Consistency 0.574; Fewâ€‘Shot 0.652; Fewâ€‘Shot CoT 0.692; Fewâ€‘Shot CoT + Selfâ€‘Consistency 0.691 (Figure 6.1).

- A consolidated treatment of prompt security and alignment
  - Security: clear distinctions between `prompt injection` and `jailbreaking`, concrete risks (e.g., trainingâ€‘data leakage, package hallucination), and layered defenses (detectors, guardrails) (Section 5.1; Figure 5.1).
  - Alignment: practical promptâ€‘level mitigations for prompt sensitivity, miscalibration, sycophancy, bias, and ambiguity (Section 5.2; Figure 5.2).

## 5. Experimental Analysis
- Evaluation methodology
  - Benchmark (Section 6.1):
    - Dataset: MMLU subset (2,800 items; 20% per category).
    - Model: `gpt-3.5-turbo`.
    - Prompt settings: 6 technique families with 2 question formats; temperature 0.5 for `Selfâ€‘Consistency`, else 0.0.
    - Parsing: patternâ€‘based extraction rules (Section 6.1.4).
  - Case Study (Section 6.2):
    - Dataset: 221 Reddit posts; 121 for development, 100 for test.
    - Metric: F1, plus precision and recall reported throughout.
    - Iterative exploration across ~20 prompting variants (Figures 6.5, 6.6).

- Main quantitative results
  - Benchmark headline (Figure 6.1):
    > Accuracy: `Fewâ€‘Shot CoT` 0.692 â‰ˆ `Fewâ€‘Shot CoT + Selfâ€‘Consistency` 0.691, both better than `Fewâ€‘Shot` 0.652 and `Zeroâ€‘Shot` 0.627; `Zeroâ€‘Shot CoT` alone drops to 0.547; `Zeroâ€‘Shot CoT + Selfâ€‘Consistency` recovers to 0.574.
  - Case study progression (Figures 6.5â€“6.6):
    - Starting point: zeroâ€‘shot with definition (Section 6.2.3.3 â€œZeroâ€‘Shot + Contextâ€) achieved F1 0.40 with recall 1.00 and precision 0.25.
    - Best development result: `10â€‘Shot AutoDiCoT` (with duplicated context) reached F1 0.53, recall 0.86, precision 0.38 (Figure 6.16; Figure 6.6).
    - Sensitivity: removing duplicated context (â€œDeâ€‘Duplicating Emailâ€) reduced F1 to 0.45 (recall 0.74; precision 0.33).
    - Test set via automated prompt optimization (`DSPy`, BootstrapFewShotWithRandomSearch): 
      > â€œF1 0.548 (precision 0.385, recall 0.952)â€ without using the email or the â€œexplicit entrapmentâ€ constraint (Figure 6.19).
  
- Do the experiments support the claims?
  - The MMLU benchmark systematically compares common techniques under a controlled setup and reveals a nonâ€‘obvious outcome: `Zeroâ€‘Shot CoT` can hurt performance unless stabilized by `Selfâ€‘Consistency` (Figure 6.1). This supports the paperâ€™s caution that technique effectiveness is contextâ€‘dependent (Section 6.1.5).
  - The entrapment case study convincingly demonstrates the realities of prompt engineering: sensitivity to context, the value of exemplar selection and reasoning control (`AutoDiCoT`), and the importance of aligning prompt objectives to domain goals (high recall in clinical screening) (Section 6.2.4).

- Ablations, failures, robustness
  - Answerâ€‘extraction choices mattered; parsing only the â€œfirst charactersâ€ improved F1 over â€œexact matchâ€ during early steps (Section 6.2.3.3).
  - Ensembling (`10â€‘Shot AutoDiCoT Ensemble + Extraction`) unexpectedly degraded performance due to unstructured outputs requiring additional extraction (Section 6.2.3.3).
  - Overly strict instruction (â€œonly explicit entrapmentâ€) improved precision but sharply reduced recall, revealing value misalignment for the clinical use case (Section 6.2.3.3).
  - The paper quantifies model/dataset citation frequency to characterize technique adoption (Figures 2.9â€“2.11). For example:
    > Figure 2.11 shows `Chainâ€‘ofâ€‘Thought` and `Fewâ€‘Shot` methods among the most cited in the corpus.

- Conditionality and tradeâ€‘offs
  - `Selfâ€‘Consistency` improves zeroâ€‘shot CoT but has little added value for fewâ€‘shot CoT in this benchmarkâ€”an interaction effect worth checking per task/model (Figure 6.1).
  - Reasoning control (`AutoDiCoT`) increased recall at some cost in precisionâ€”beneficial for highâ€‘risk screening, but perhaps not for precisionâ€‘critical tasks.

## 6. Limitations and Trade-offs
- Scope constraints
  - Focused on `hard` (discrete) `prefix` prompts; excludes soft prompts and gradientâ€‘based prompt tuning (Section â€œScope of Studyâ€).
  - Taskâ€‘agnostic techniques only; domainâ€‘specific prompting is out of scope (Section â€œScope of Studyâ€).

- Empirical limits
  - Benchmark uses a single model (`gptâ€‘3.5â€‘turbo`), one dataset (MMLU subset), and limited variants (Section 6.1), so generalization across models/tasks is not guaranteed.
  - Parsingâ€‘based evaluation can misjudge answers if the modelâ€™s formatting drifts (Section 6.1.4; Section 2.5).
  - Citation counts as a proxy for usage (Figures 2.9â€“2.11) reflect research discourse, not necessarily industry adoption.

- Prompt sensitivity and brittleness
  - Small format variations (e.g., duplicated context email) significantly affected results (Section 6.2.3.3), echoing broader sensitivity findings (Section 5.2.1).
  - Overâ€‘constraint or poorly aligned instructions can degrade metrics most valued by stakeholders (e.g., recall in clinical screening) (Section 6.2.3.3).

- Compute, cost, and latency
  - Techniques like `Selfâ€‘Consistency`, ensembling, `Treeâ€‘ofâ€‘Thought`, or agentic toolâ€‘use increase API calls, latency, and cost (Sections 2.2.4, 4.1), which may limit realâ€‘time or largeâ€‘scale use.

- Security defenses remain partial
  - Promptâ€‘based defenses can mitigate but not eliminate prompt hacking; detectors/guardrails reduce risk but are not foolproof (Section 5.1.3).

## 7. Implications and Future Directions
- How this work changes the landscape
  - Provides a shared map and vocabulary for the field (Figures 1.3, 2.2, 3.1, 3.2), making it easier to reason about design choices, compare techniques, and teach best practices.
  - Bridges prompting research with safety and evaluation, promoting endâ€‘toâ€‘end thinking: prompts, answer extraction, evaluation pipelines, and defenses (Sections 2.5, 4.2, 5).

- Followâ€‘up research enabled
  - Multiâ€‘model, multiâ€‘dataset replications of the benchmark to test interaction effects (e.g., when `Zeroâ€‘Shot CoT` helps vs. hurts).
  - Programmatic methods for `answer engineering` (learned extractors, structured decoding) that reduce formatting brittleness (Section 2.5).
  - Generalized `Directed CoT` methods: algorithmic selection of â€œwhat not to doâ€ exemplars, with uncertaintyâ€‘aware sampling or RL.
  - Robust prompting under adversarial or noisy inputs; formal safety metrics for guards/detectors (Section 5.1.3).

- Practical applications and downstream use cases
  - Enterprise prompt design playbooks: exemplar selection (Figure 2.3), role/style instructions (Section 2.2.1.3), ensembling with cost control (Section 2.2.4), and builtâ€‘in answer extraction (Section 2.5).
  - Safetyâ€‘critical screening workflows (healthcare, trust & safety): prioritize highâ€‘recall prompts, add calibration prompts (Section 5.2.2), and include humanâ€‘inâ€‘theâ€‘loop confirmation.
  - Agentic systems: tool routing and retrievalâ€‘augmented reasoning patterns (`MRKL`, `ReAct`, `IRCoT`) for tasks needing factuality and planning (Section 4.1; Figure 4.1).
  - LLMâ€‘asâ€‘evaluator: adopt `Gâ€‘EVAL`/`LLMâ€‘EVAL` styles with CoT and modelâ€‘generated guidelines for consistent, auditable assessments (Section 4.2).

> Overall, the work offers a coherent blueprint for designing, evaluating, and securing promptâ€‘driven systemsâ€”from vocabulary and taxonomy (Figures 1.3, 2.2) through empirical guidance (Figure 6.1) and realâ€‘world procedure (Figures 6.12â€“6.16)â€”while candidly surfacing sensitivity and safety pitfalls that practitioners must manage.

# Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?

**ArXiv:** [2202.12837](https://arxiv.org/abs/2202.12837)

## ğŸ¯ Pitch

This paper offers a surprising empirical analysis showing that large language models do not depend on the correct inputâ€“label mappings in demonstrations to achieve strong in-context learning performance on classification and multi-choice tasks. Instead, the key drivers are simply showing the model the label space, in-distribution inputs, and a consistent inputâ€“label pair formatâ€”randomizing the labels within demonstrations barely reduces accuracy. These findings fundamentally reshape our understanding of what LMs 'learn' at inference time, suggesting new ways to construct prompts, improve zero-shot performance without labeled data, and set more accurate expectations for the capabilities and limitations of in-context learning.

---

## 1. Executive Summary (2-3 sentences)
This paper investigates what, exactly, makes in-context learning (ICL) work in large language models (LMs). It shows that correct inputâ€“label pairings in demonstrations are surprisingly unimportant for many classification and multiple-choice tasks: replacing the gold labels with random labels only marginally reduces performance (typically 0â€“5% absolute), while the main benefits come from providing the label space, in-distribution inputs, and a consistent inputâ€“label pair format (Figures 1, 3, 5, 8â€“10; Sections 4â€“5).

## 2. Context and Motivation
- Problem addressed:
  - In-context learning (ICL) is when a language model performs a new task at inference time by conditioning on a prompt that contains a few inputâ€“label examples, called `demonstrations`, without updating model weights (Figure 2; Introduction).
  - The open question: What aspects of the demonstrations actually drive ICLâ€™s performance? Is the model truly â€œlearningâ€ the inputâ€“label mapping from the examples, or are other factors at play (Sections 1, 4â€“5)?
- Why it matters:
  - Understanding ICLâ€™s mechanism:
    - Theoretically: clarifies whether LMs â€œlearn at test timeâ€ vs. recall and adapt pretraining knowledge (Section 6, â€œDoes the model learn at test time?â€).
    - Practically: guides how to construct prompts to get the most benefit, potentially elevating zero-shot baselines without labeled data (Sections 4â€“5, 6).
- Prior approaches and gaps:
  - Prior ICL research optimized prompt formats, example selection, and â€œchain-of-thoughtâ€ prompting; others studied brittleness and sensitivity (Related Work).
  - There was limited empirical analysis of why ICL beats zero-shot; particularly, the role of ground-truth inputâ€“label correspondences was not isolated (Related Work; Sections 1, 4).
- Positioning:
  - This paper provides a systematic empirical decomposition of demonstrations into four componentsâ€”`inputâ€“label mapping`, `input distribution`, `label space`, and `format`â€”and quantifies their contributions (Figure 7; Sections 4â€“5). It evaluates 12 models (774Mâ€“175B parameters) across 26 datasets, including GPT-3 and meta-trained LMs (Table 1; Section 3), delivering a cross-model, cross-task perspective.

## 3. Technical Approach
The paper is empirical, designed around controlled interventions that isolate specific aspects of in-context demonstrations. The overall logic is: start from standard k-shot ICL, then swap or remove components to test what changes performance.

- Core setup
  - Demonstrations: sequences of k inputâ€“label pairs `(x1, y1), â€¦, (xk, yk)` concatenated before a test input `x`, prompting the model to predict a label `y` (Figure 2; Section 3).
  - Two inference methods (scoring strategies) per model (Section 3; Figure 3):
    - `Direct` method: choose the label `y` from the label set `C` that maximizes `P(y | D, x)` where `D` is the demonstration prompt.
    - `Channel` (noisy-channel) method: flip the conditional and choose `y` that maximizes `P(x | D, y)` (the paper notes you can â€œflip x and yâ€ to convert Direct â†” Channel; Section 4.1). This is often used to reduce â€œlabel biasâ€ when labels are short tokens.
  - Models: 12 decoder-only LMs from 774M to 175B parameters, including GPT-2, GPT-J, fairseq 6.7B/13B, GPT-3, and `MetaICL` (a GPT-2 Large model meta-trained for ICL on many supervised datasets; Table 1; Section 3).
  - Tasks and metrics: 26 low-resource classification and multiple-choice datasets spanning sentiment, paraphrase detection, NLI, hate speech, QA, and sentence completion (Table 2; Section 3). Metrics are Macro-F1 (classification) and Accuracy (multiple-choice).
  - Demonstration size and sampling: default k = 16, uniformly sampled from training data; 5 random seeds (3 seeds for GPT-3/fairseq-13B due to cost; Section 3).
  - Prompt templates: minimal templates by default; also evaluate hand-written â€œmanualâ€ templates from prior work (Section 4.2, Figure 6; Table 3).

- Key experimental variants
  - Baselines to isolate inputâ€“label mapping (Section 4.1; Figure 3):
    - `No demonstrations` (zero-shot with label set `C`).
    - `Demos w/ gold labels`: standard k-shot ICL with `(xi, yi)`.
    - `Demos w/ random labels`: pair each input `xi` with a random label `á»¹i âˆˆ C` drawn uniformly; predict with `P(Â· | x1, á»¹1, â€¦, xk, á»¹k, x)` (Section 4.1).
  - Ablations (Sections 4.2, 5):
    - Fractional correctness: mix of correct and incorrect pairs to reach `a%` correctness (Algorithm 1; Figure 4).
    - Vary k: number of examples per prompt from 0 to 32 (Figure 5).
    - Templates: minimal vs. manual (Figure 6).
  - Decomposing demonstration components (Section 5; Figure 7):
    - `Input distribution`: Replace in-prompt inputs with out-of-distribution (OOD) sentences sampled from a news corpus (CC-News) while keeping label tokens and format intact (â€œOOD + Random Labelsâ€; Figure 8; Appendix B).
    - `Label space`: Replace labels with random English words of the same cardinality as `C`, destroying knowledge of the true label space but preserving format and inputs (Figure 9).
    - `Format`: Remove the inputâ€“label pairing structure:
      - `No labels`: concatenate only inputs `x1 â€¦ xk` before test input.
      - `Labels only`: concatenate only labels `y1 â€¦ yk` before test input.
      - These serve as â€œno-formatâ€ counterparts to the above variants (Figure 10).
    - Additional controls (Appendix C.3):
      - Replace labels with a constant token like â€œanswerâ€ (hurts performance).
      - Replace all inputs with the same test input paired with random labels (also hurts), suggesting that breaking the pattern of distinct inputâ€“label pairs disrupts the â€œformatâ€.

- How choices isolate mechanisms
  - By holding three components fixed and changing the fourth (Figure 7), each ablation shows the marginal impact of that component:
    - Swap gold â†’ random labels: tests reliance on the inputâ€“label mapping itself.
    - Use OOD inputs: tests contribution of in-distribution inputs.
    - Use random English labels: tests how much knowing the actual label set matters.
    - Remove inputâ€“label pairing: tests whether the paired format cues the model to continue the pattern.

## 4. Key Insights and Innovations
- Novel contribution 1: Ground-truth inputâ€“label mapping often matters little in ICL
  - Insight: Performance with demonstrations that pair inputs with random labels is close to performance with gold-labeled demonstrations across many models and datasets (Figure 3; Section 4.1).
  - Evidence:
    - â€œReplacing the labels in demonstrations barely hurts performance on a range of classification and multi-choice tasksâ€¦ consistently over 12 different models including GPT-3.â€ (Abstract; Figure 3).
    - Typical drops are 0â€“5% absolute; multi-choice is especially robust (â‰ˆ1.7% drop on average) compared to classification (â‰ˆ2.6%; Section 4.1; Figure 3).
    - `MetaICL` shows near-zero sensitivity: only 0.1â€“0.9% absolute drop (Section 4.1).

- Novel contribution 2: The main gains come from label space + in-distribution inputs + paired format
  - Decomposition (Figure 7; Section 5) shows three drivers:
    - Knowing the label space: Direct models lose 5â€“16% absolute when labels are replaced with random English words, even though the overall paired format is kept (Figure 9; Section 5.2). Channel models are less affected (0â€“2%).
    - Conditioning on in-distribution inputs: Replacing prompt inputs with OOD sentences reduces performance by 3â€“16% absolute for several model/task pairs; in one case it is worse than no demonstrations (Figure 8; Section 5.1).
    - Preserving the inputâ€“label pair format: Removing the pairing (inputs-only or labels-only) is typically no better than no demonstrations, despite keeping inputs or labels present (Figure 10; Section 5.3).
  - Significance: The benefits come from â€œcontextualizingâ€ the model on the right input distribution and label set while giving the model an inputâ€“label pair pattern to imitate, rather than from learning new mappings (Sections 5, 6).

- Novel contribution 3: Meta-training for ICL amplifies reliance on simple aspects and de-emphasizes true mapping
  - Observation: `MetaICL` (meta-trained to do ICL) is even less sensitive to inputâ€“label correctness and more sensitive to format (Section 5.4).
  - Example: With Direct `MetaICL`, inputâ€“label mapping and input distribution have â€œnearly zero influenceâ€ in some settings, implying the model keys heavily off the label space and format (Section 5.4; Figures 8â€“10).
  - Significance: This suggests meta-training encourages models to exploit the easiest-to-use properties (format, familiar token distributions) rather than infer true mappings at test time (Section 5.4).

- Novel contribution 4: Stronger zero-shot baselines without any labeled data
  - Finding: You can achieve nearly k-shot performance by pairing unlabeled inputs with random labels to form â€œfakeâ€ demonstrationsâ€”this raises the zero-shot baseline substantially (Section 6, â€œSignificantly improved zero-shot performanceâ€).
  - Quote:
    > â€œIt is possible to achieve nearly k-shot performance without using any labeled data, by simply pairing each unlabeled input with a random label and using it as the demonstrations.â€ (Section 6)

These are fundamental insights (not just incremental improvements) because they challenge the common assumption that ICLâ€™s advantage comes primarily from â€œlearningâ€ the inputâ€“label mapping in the prompt.

## 5. Experimental Analysis
- Evaluation methodology (Section 3):
  - Models (Table 1): `GPT-2 Large` (774M), `MetaICL` (774M, meta-trained for ICL), `GPT-J` (6B), fairseq 6.7B and 13B, and `GPT-3` (175B, â€œDavinci baseâ€ API).
  - Tasks (Table 2): 26 datasets in six categories: sentiment, paraphrase detection, NLI, hate speech detection, question answering, sentence completion. All are classification or multiple-choice (no open-ended generation).
  - Prompting:
    - Minimal templates (Table 3) by default; â€œmanualâ€ templates tested for robustness (Figure 6).
    - Demonstrations are k = 16 by default, uniform sampling; 5 random seeds (3 for GPT-3/13B); Macro-F1 (classification) and Accuracy (multi-choice).
    - Direct and Channel scoring are both evaluated for each model (Figure 3).
  - Reproducibility: Code referenced in Appendix B; data via Hugging Face (Section 3; Appendix A/B).

- Main results (Sections 4, 5; Figures 3â€“6, 8â€“10, 11â€“12):
  - Gold vs. random labels (Section 4.1; Figure 3):
    - Average drop replacing gold with random labels: â‰ˆ2.6% absolute (classification), â‰ˆ1.7% (multiple-choice).
    - `MetaICL` is exceptionally robust (0.1â€“0.9% drop).
    - Some exceptions noted (Section 4.1 footnote): for a few Direct setups (GPT-2, GPT-J, fairseq 6.7B) the k-shot performance with gold labels was weak on many datasets; Channel fairseq-13B had zero-shot > k-shot in some cases.
  - Fractional correctness (Section 4.2; Figure 4):
    - Performance is relatively insensitive to fraction of correct labels. Even with 0% correct labels (all incorrect pairings), performance often remains far above no-demos baselines.
    - Examples:
      - With MetaICL (classification), 0% correct retains â‰ˆ92% of the improvement over no-demos (Figure 4 caption).
      - With MetaICL (multi-choice), 0% correct retains â‰ˆ100% of improvement; with GPT-J (multi-choice), â‰ˆ97% (Figure 4 caption).
      - GPT-J (classification) is more sensitive: â‰ˆ10% absolute drop at 0% correctâ€”but still much better than no-demos (Section 4.2; Figure 4).
  - Varying k (Section 4.2; Figure 5):
    - Using demonstrations helps even for small k (e.g., k=4).
    - Performance saturates beyond k â‰ˆ 8; increasing k to 16 or 32 brings limited gains for both gold and random labels (Figure 5).
    - The goldâ†’random label performance gap remains small across k (â‰ˆ0.8â€“1.6% absolute), with one outlier (â‰ˆ4.4% at k=4, likely high variance).
  - Templates (Section 4.2; Figure 6):
    - Manual templates confirm the trend: random labels â‰ˆ gold labels. Manual templates are not consistently superior to minimal ones.
  - Decomposition into four aspects (Section 5; Figures 8â€“10):
    - Input distribution (Figure 8):
      - Replace in-prompt inputs with OOD sentences (while keeping format and random labels): performance drops 3â€“16% absolute for Channel MetaICL, Direct GPT-J, and Channel GPT-J across classification and multiple-choice.
      - In one setting (Direct GPT-J multi-choice), OOD + random labels is worse than no demonstrations.
      - Direct MetaICL is an exception (little sensitivity), attributed to meta-training (Section 5.4).
    - Label space (Figure 9):
      - Replace labels with random English words (same cardinality): Direct models lose 5â€“16% absolute; Channel models drop 0â€“2% or even slightly improve.
      - Interpretation: Direct scoring needs to generate the correct tokens; Channel scoring conditions on them and is less sensitive to their identity (Section 5.2).
    - Format of inputâ€“label pairs (Figure 10):
      - Inputs-only and labels-only (no pairing) are generally no better than no demos, showing format matters.
      - Keeping format enables strong performance even when using only inputs or only labels:
        - Direct MetaICL classification: using OOD inputs + random labels retains 95% of the improvement of full in-context learning (gold labels) (Figure 10 caption).
        - Direct MetaICL multi-choice: retains 82%.
        - Channel models:
          - Channel MetaICL classification: pairing training inputs with random English words retains 82% of gains.
          - Channel GPT-J classification: 87%.
          - Channel MetaICL multi-choice: 86%.
          - Channel GPT-J multi-choice: 75%.
  - Dataset-level breakdown and label distribution effects (Appendix C.2; Figure 12):
    - Sampling random labels from the true label frequency distribution reduces the goldâ†’random gap further (classification, channel scoring):
      - Channel MetaICL: gap shrinks from 1.9% to 1.3% absolute.
      - Channel GPT-J: 5.0% to 3.5% absolute.
    - Some datasets show larger sensitivity (outliers):
      - For Channel GPT-J, financial_phrasebank shows nearly 14% absolute drop with random labels in one setting (Figure 12).
  - Additional controls that break format degrade performance (Appendix C.3):
    - Using a constant label token (â€œanswerâ€) or repeating the test input for all demo inputs both hurt, likely because they distort the pairing pattern.

- Do experiments support claims?
  - Yes, convincingly for classification and multiple-choice tasks under both Direct and Channel scoring across many models:
    - The sheer breadth (12 models, 26 datasets; Tables 1â€“2) and systematic isolation of components (Figures 7â€“10) support the central claim that mapping correctness is not the main driver.
    - Multiple robustness checks (fractional correctness, k, templates, label distribution sampling, OOD inputs) triangulate the conclusion.
  - Caveats:
    - Mixed results in a few model/dataset combinations (Section 4.1 footnote; Figure 12) indicate task- and model-specific variability.
    - Results are confined to classification/multi-choice; open-ended generation may behave differently (Limitations).

## 6. Limitations and Trade-offs
- Scope limitations (Section 6: Limitation; Appendix C):
  - Task types: Only classification and multiple-choice. Extending to open-ended generation is nontrivial because itâ€™s hard to â€œbreakâ€ inputâ€“output correspondences while preserving the output distribution (Section 6; â€œExtensions to generationâ€).
  - Dataset variability: While averages show small gaps, some datasets (e.g., financial text, certain hate speech datasets) show larger sensitivity to correct labels (Appendix C.2; Figure 12).
  - Synthetic tasks: Prior reports suggest that in synthetic settings with limited inputs, correct labels can matter more (Limitations; citing Rong 2021).
- Assumptions:
  - Access to unlabeled in-distribution inputs for demonstrations is often assumedâ€”even when using random labels. The paper explicitly treats this as an acceptable â€œzero-shotâ€ enhancement (Section 6, footnote 10).
  - Label set `C` is known for classification. When `C` is unknown, using random English words helps if the format is preserved (Sections 5.2â€“5.3; Figure 10).
- Computational constraints:
  - For the largest models (GPT-3, fairseq-13B), experiments use fewer datasets (6) and seeds (3) due to cost (Section 3).
- Model training factors are not exhaustively explored:
  - The effect of pretraining data and objectives on the observed phenomena is discussed as future work (review response lines 055â€“061).
- Interpretation limits:
  - The study is empirical; it does not provide a formal theory of ICL mechanisms, though it references Bayesian interpretations in prior work (Related Work).

## 7. Implications and Future Directions
- Reframing what ICL â€œlearnsâ€ at test time (Section 6):
  - Narrow definition (â€œlearns a new inputâ€“label mapping from demonstrationsâ€): evidence suggests LMs typically do not learn this at test time for the evaluated tasks.
  - Broader definition (â€œadapts to input distribution, label space, and formatâ€): LMs do adapt in this sense; demonstrations act as cues to retrieve and apply pretraining knowledge aligned with the task (Section 6).
- Practical prompting guidance:
  - To boost zero-shot performance without labels:
    - Use in-distribution inputs as demonstrations and pair them with random labels, preserving the inputâ€“label pair format (Sections 4â€“5). This can approach k-shot performance (Section 6).
  - When label tokens are unknown or unsuitable:
    - Use random words as stand-ins, but keep the inputâ€“label pair format (Figure 10).
  - Prefer Channel scoring when label identity is ill-defined or biased, as it is less sensitive to label token choices (Figure 9).
- Model training strategy:
  - Meta-training for ICL (`MetaICL`) makes the model rely even more on simple prompt aspects (format, label tokens), reducing reliance on true inputâ€“label mapping (Section 5.4). This can be beneficial for robustness to wrong labels but may risk over-reliance on superficial patterns.
  - Future work: Analyze how pretraining corpora and objectives shape the modelâ€™s ability to internalize â€œinputâ€“label correspondencesâ€ that ICL can then surface (Section 6).
- Extending to other paradigms:
  - Instruction following: Evidence elsewhere suggests models can perform well even with irrelevant or misleading instructions (Webson & Pavlick, 2022). This paperâ€™s findings suggest instructions may function like demonstrationsâ€”triggering latent capabilities rather than conveying new mappings (Section 6).
  - Chain-of-thought and rationales: Early evidence shows some kinds of wrong rationales harm, while others do not (Appendix citing Madaan & Yazdanbakhsh, 2022). A â€œfour-aspectâ€ decomposition (input distribution, label space, format, mapping) could inform which rationale properties matter.
- Research directions:
  - Identify tasks where correct inputâ€“label mapping is truly needed for ICL (e.g., financial or nuanced hate speech tasks; Figure 12).
  - Develop prompting or scoring methods that better extract latent inputâ€“label associations when present, or detect when they are absent and require fine-tuning.
  - Extend the decomposition framework to open-ended generation and structured prediction, designing interventions that preserve output distributions while breaking mappings.

Block-quoted highlights to anchor takeaways:
> â€œReplacing the labels in demonstrations barely hurts performance on a range of classification and multi-choice tasks, consistently over 12 different models including GPT-3.â€ (Abstract; Figure 3; Sections 4.1, 5)

> â€œIt is possible to retain 95% and 82% of improvements from in-context learning â€¦ by simply sampling random sentences from a corpus and randomly pairing them with the label setâ€ (Direct MetaICL, classification and multi-choice; Figure 10 caption; Section 5.3)

> â€œRemoving the format is close to or worse than no demonstrations, indicating the importance of the format.â€ (Section 5.3; Figure 10)

> â€œPerformance with random labels is very close to performance with gold labelsâ€ (Figure 3; Sections 4.1â€“4.2)

By disentangling what demonstrations contribute, this paper shifts the fieldâ€™s understanding of ICL from â€œlearning mappings from a few examplesâ€ toward â€œformat- and distribution-based cueing of pretrained capabilities,â€ and provides concrete, widely applicable prompting strategies for practitioners.

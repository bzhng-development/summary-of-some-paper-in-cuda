# NVILA: Efficient Frontier Visual Language Models

**ArXiv:** [2412.04468](https://arxiv.org/abs/2412.04468)

## ğŸ¯ Pitch

NVILA introduces a new family of open visualâ€“language models that set a new benchmark for both accuracy and efficiency by employing a 'scale-then-compress' approachâ€”scaling up input resolutions to capture richer information, then aggressively compressing visual tokens to minimize computational overhead. This innovation is bolstered by data pruning, low-precision training, and quantized inference throughout the model lifecycle, allowing NVILA to meet or surpass top visualâ€“language models on image and video benchmarks while slashing training times and inference latency by up to 5Ã—. The result is a high-performing, resource-friendly VLM architecture poised to unlock real-world deployment in fields like robotics, edge devices, and medical AIâ€”where both performance and speed are critical.

---

## 1. Executive Summary (2â€“3 sentences)
This paper introduces `NVILA`, a family of open visualâ€“language models (VLMs) designed to set an â€œefficient frontierâ€ by jointly improving accuracy and endâ€‘toâ€‘end efficiency across training, fineâ€‘tuning, and deployment. The core idea is a â€œscaleâ€‘thenâ€‘compressâ€ pipeline: first raise spatial/temporal resolution to capture more information, then aggressively compress visual tokens so computation remains low, complemented by data pruning, lowâ€‘precision training, and quantized inference. As a result, `NVILA` matches or surpasses leading open/proprietary VLMs on image and video benchmarks while reducing training time 1.9â€“5.1Ã— and inference latency/throughput by 1.6â€“2.8Ã— (Figure 1aâ€“c; Figure 5).

## 2. Context and Motivation
- Problem addressed:
  - Strong VLMs exist, but efficiency has lagged behind accuracy. VLMs are expensive to train (hundreds of GPUâ€‘days even at 7â€“8B scale), memoryâ€‘intensive to fineâ€‘tune, and slow to deploy on limited hardware (Section 1). The reference baseline `VILA-1.5` used 448Ã—448 images and 8â€“14 frames, discarding detail and underperforming on textâ€‘heavy images and long videos (Section 2.1; Tables 8â€“9).
- Why it matters:
  - Realâ€‘world applications (robotics, mobile/edge devices, medical imaging) are computeâ€‘ and latencyâ€‘constrained. Efficient training lowers entry barriers; efficient inference enables responsive user experiences (Section 1; Figure 5).
- Prior approaches and shortcomings:
  - Many models focus on accuracy via larger backbones or more data; few provide a systematic efficiency methodology across the lifecycle (training â†’ fineâ€‘tuning â†’ deployment). Token reduction methods exist (e.g., Token Merging, TokenLearner, Perceiver Resampler), but have not been shown to move the frontier for VLMs at high resolution/long videos or within a cohesive endâ€‘toâ€‘end recipe (Section 5.2).
- Positioning:
  - `NVILA` builds on `VILA` but redesigns the vision path to support highâ€‘resolution and longâ€‘context inputs efficiently, and adds systemâ€‘level accelerations (FP8 training, quantized vision+LLM at inference), curated data pruning, and fineâ€‘tuning guidelines. It presents a complete stack that improves both quality and speed (Sections 2â€“4; Figure 3; Tables 1â€“7).

## 3. Technical Approach
`NVILA` is an autoâ€‘regressive VLM with three standard components (Figure 3):
- A vision encoder (SigLIP, a Vision Transformer variant) that converts images/video frames to embeddings.
- A projector (2â€‘layer MLP) that aligns vision embeddings to the language space.
- A token processor (`Qwen2` LLM of different sizes) that produces text outputs conditioned on visual and textual tokens.

The novelty lies in how the vision path is scaled and then compressed, plus lifecycleâ€‘wide efficiency techniques.

A. Spatial â€œscaleâ€‘thenâ€‘compressâ€ for images (Section 2.1.1; Table 1)
- Challenge: Raising resolution improves accuracy but increases tokens and quadratic attention cost.
- Step 1 â€” Scale with `S2` tiling and `Dynamic-S2`:
  - `S2` (multiâ€‘scale tiling) processes the image at several scales by splitting into 448Ã—448 tiles, running the encoder per tile, stitching feature maps per scale, then concatenating across scales (Section 2.1.1).
  - Problem: `S2` forces square resizing, distorting unusual aspect ratios.
  - `Dynamic-S2` fixes distortion by keeping the largest scale close to the original aspect ratio with tileâ€‘aligned dimensions; all scales are interpolated to the largest scale and concatenated (Section 2.1.1).
  - Effect: Substantial gains on textâ€‘heavy benchmarks. Table 1 shows moving from the `VILA-1.5` baseline to â€œScale (Dynamicâ€‘S2)â€ lifts AI2D 87.0â†’90.1, DocVQA 61.3â†’91.1, TextVQA 67.5â†’77.0, and IMâ€‘10 average 61.2â†’71.5.
- Step 2 â€” Compress spatial tokens:
  - Use `spatial-to-channel (STC)` reshaping (e.g., 3Ã—3) to reduce spatial tokens by packing local patches into channels. A naive increase in STC ratio hurts accuracy (roughly âˆ’10% DocVQA if done directly, Section 2.1.1).
  - Remedy: Add a `visual encoder preâ€‘training (VEP)` stage (Table 7, Stage 2) that jointly tunes the vision encoder and projector under the compressed setup. This recovers most accuracy while yielding 2.4Ã— speedups in training/inference (Section 2.1.1; Table 1 â€œScale + Compress + VEPâ€: IMâ€‘10 rises from 67.1â†’70.8).
  - Why not TokenLearner/Perceiver? With the same token reduction, these learnable compression modules did not outperform simple STC in this training recipe; likely an optimization/training stability issue (Table 1, â€œAlternative Designsâ€).

B. Temporal â€œscaleâ€‘thenâ€‘compressâ€ for video (Section 2.1.2; Table 2)
- Step 1 â€” Scale frames:
  - Uniformly sample more frames (e.g., 8â†’32) and add videoâ€‘supervised instruction tuning to teach the model to use longer temporal contexts. This improves Videoâ€‘MME overall 55.7â†’61.0 (Table 2).
- Step 2 â€” Compress temporally:
  - Use `temporal averaging` (a simple pooling) within frame groups to exploit redundancy across adjacent frames, reducing tokens 4Ã— with only modest accuracy loss: â€œScale + Compress (32 frames, 4Ã— pooling)â€ keeps overall Videoâ€‘MME 61.0â†’60.1 (Table 2).
  - The same approach scales to 256 frames with 8Ã— pooling while improving accuracy relative to 32â€‘frame setups (Table 2, last row: overall 64.0).

C. Efficient training (Section 2.2)
- `Dataset pruning with DeltaLoss` (Section 2.2.1; Figure 4; Table 3):
  - Goal: Prune supervised fineâ€‘tuning (SFT) data to remove examples that are too easy or distractingly hard, keeping those most informative for learning.
  - Mechanism (Eq. 1): Compute perâ€‘example score log[p_large(x)/p_small(x)] on answer tokens across subâ€‘datasets, pick topâ€‘K per subset. Intuition:
    - Near 0: both models agree (either both right or both wrong) â†’ low value.
    - Negative: small model right but large wrong â†’ distracting.
    - Positive: large right but small wrong â†’ challenging yet learnable (helpful).
  - Results: Pruning 50% with `DeltaLoss` maintains accuracy almost unchanged relative to 100% data and beats random/cluster pruning on IMâ€‘10, MMMU, DocVQA, TextVQA (Table 3; e.g., IMâ€‘10 75.6â†’75.5 vs random 74.0).
- `FP8 training with COAT` (Section 2.2.2; Table 4):
  - `FP8` is an 8â€‘bit floatingâ€‘point format supported on NVIDIA Hopper/Blackwell GPUs.
  - Training setup uses FP8 for both weights and activations (COAT) and leverages that VLM batches have highly variable sequence lengths; underâ€‘utilized batches benefit from larger batch sizes (Table 4).
  - Measured on 64Ã— H100: without gradient checkpointing (GC), FP8 raises batch size 4â†’16 and roughly doubles throughput (199â†’390 it/s) with similar accuracy. With GC, FP8 still adds ~1.2Ã— throughput (492â†’580 it/s) at unchanged accuracy (MMMU and Videoâ€‘MME nearly equal).
- Other system choices:
  - FlashAttentionâ€‘2, DeepSpeed sharding, functionalâ€‘preserving sequence packing, and a 5â€‘stage training curriculum (Table 7) together streamline compute and memory.

D. Efficient fineâ€‘tuning (Section 2.3; Table 5)
- Recipe insights:
  - Use different learning rates for ViT vs LLM; ViT benefits from 5â€“50Ã— smaller LR than LLM.
  - Tuning only ViT LayerNorm parameters (much cheaper than LoRA) can match LoRAâ€™s performance for many tasks. Best practice: LoRA (or QLoRA) on LLM + ViT LayerNorm with a small LR, selecting LR ratios from {1,5,10,50} per task (Table 5).
  - Memory/throughput: QLoRA halves memory vs LoRA (e.g., 11.1 GB vs 20.1 GB) at modest throughput cost while preserving accuracy after LR tuning.

E. Efficient deployment (Section 2.4; Figure 5; Table 6)
- Two inference phases:
  - `Prefilling`: encode all inputs (vision + prompt) and build attention caches; computeâ€‘bound.
  - `Decoding`: generate tokens stepâ€‘byâ€‘step; memoryâ€‘bound.
- Optimizations:
  - Compress visual tokens (as above), then the vision tower becomes the prefill bottleneck (>90% latency). Apply `W8A8` quantization to the vision tower to reduce Timeâ€‘toâ€‘Firstâ€‘Token (TTFT), and `W4A16` (AWQ) to the LLM for decoding, with an improved GEMM kernel using FP16 accumulation for an extra 1.7Ã— kernel speedup (Section 2.4; Figure 5).
  - Quality/latency tradeâ€‘off: Table 6 shows `W4A16` on the LLM causes small accuracy drops (e.g., MMMU 50.7â†’49.2), while `W8A8` on ViT is nearly lossless.
- Measured speedups vs `Qwen2â€‘VL` on a single RTX 4090 (Figure 5):
  - Prefilling TTFT speedup up to 2.22Ã— for video and 1.55Ã— for images.
  - Decoding throughput speedup up to 2.84Ã— for video and 1.24Ã— for images.

F. Training curriculum and data (Section 3.1; Table 7; Table A1)
- 5 stages (Table 7): projector init â†’ visual encoder preâ€‘training (`VEP`) â†’ token processor preâ€‘training â†’ image instruction tuning â†’ video instruction tuning (extends longâ€‘video capability).
- Implementation: PyTorch 2.3, Transformers 4.46, DeepSpeed 0.9.5, FlashAttentionâ€‘2, gradient checkpointing, sequence packing; trained on 128Ã— H100 with global batch 2048 (Section 3.1).
- Data mixture: curated across recaptioned corpora, documents/OCR, interleaved multiâ€‘modal, chart/diagram, general VQA, textâ€‘only instruction, medical, and video SFT (Table A1).

## 4. Key Insights and Innovations
- â€œScaleâ€‘thenâ€‘compressâ€ vision path (Sections 2.1.1â€“2.1.2; Tables 1â€“2)
  - Whatâ€™s new: Treat high spatial/temporal resolution as a firstâ€‘class capability, then compress tokens to keep cost low. The addition of `Dynamic-S2` addresses aspect ratio distortion, and `VEP` stabilizes training under heavy spatial compression.
  - Why it matters: Large accuracy gains on textâ€‘rich images and long videos with roughly the same token budget as lowâ€‘resolution baselines. Table 1 shows up to ~30â€‘point gains on DocVQA (61.3â†’91.1) before compression; with compression and `VEP`, most gains remain while tokens are cut substantially.
- Lifecycleâ€‘wide efficiency recipe (Sections 2.2â€“2.4; Tables 3â€“6; Figure 5)
  - Data: `DeltaLoss` pruning removes 50% of SFT data with negligible accuracy loss (Table 3).
  - Training: FP8 with COAT increases batch size/throughput without hurting accuracy (Table 4).
  - Inference: Joint `W8A8` (ViT) + `W4A16` (LLM) with custom kernels meaningfully improves TTFT and decoding throughput (Figure 5; Table 6).
  - Significance: The combined stack reduces training time by 1.9â€“5.1Ã— and inference latency/throughput by 1.6â€“2.8Ã— (Figure 1aâ€“b).
- Simple compression beats complex modules here (Table 1)
  - STC reshaping (with `VEP`) outperforms TokenLearner and Perceiver Resampler at the same reduction ratio in this recipeâ€”highlighting optimization/stability as central for learnable compressors at scale.
- New capabilities via efficient long context (Section 4; Tables 10â€“12)
  - Temporal localization with discrete time tokens (Table 10), robotics navigation at 1 Hz on a laptop GPU (Figure 6; Table 11), and medical multiâ€‘tasking when paired with expert models (Table 12).

## 5. Experimental Analysis
- Evaluation setup
  - Image benchmarks: AI2D, ChartQA, DocVQA, InfoVQA, MathVista, MMMU (zeroâ€‘shot CoT), RealWorldQA, SEED, TextVQA, VQAv2 (Section 3.2.1; Table 8).
  - Video benchmarks: ActivityNetâ€‘QA, LongVideoBench, MLVU, MVBench, NExTâ€‘QA, Videoâ€‘MME (with/without subtitles) (Section 3.2.2; Table 9).
  - Efficiency: Speed comparisons vs LLaVAâ€‘OneVision (training) and Qwen2â€‘VL (inference), measured on H100 for training and single RTX 4090 for inference (Figure 1aâ€“b; Figure 5).
- Main results (selected highlights)
  - Endâ€‘toâ€‘end efficiency:
    - > â€œNVILA trains image and video models 5.1Ã— and 1.9Ã— faster than LLaVAâ€‘OneVisionâ€ (Figure 1a).
    - > â€œPrefilling 1.6â€“2.2Ã— faster; decoding 1.2â€“2.8Ã— faster than Qwen2â€‘VLâ€ (Figure 1b; Figure 5).
  - Image accuracy (Table 8):
    - `NVILA-8B` achieves AI2D 92.3 (best among opens), DocVQA 93.7, TextVQA 68.6, VQAv2 85.4. `NVILA-15B` further improves AI2D 94.1, ChartQA 86.9, DocVQA 94.0, InfoVQA 73.5.
    - Against strong opens (InternVL2â€‘8B, Qwen2â€‘VLâ€‘8B), `NVILA-8B` is competitive or better on several image and OCR tasks; vs proprietary models, `NVILA-15B` is competitive on multiple datasets.
  - Video accuracy (Table 9):
    - `NVILA-8B (256 frames)` reaches LongVideoBench 3.7, MLVU val 57.7/test 58.7, MVBench 70.1, NExTâ€‘QA 68.1, Videoâ€‘MME 64.2 (w/o subtitles) and 70.0 (with subtitles), surpassing or matching prior open models of similar/larger sizes; notably close to GPTâ€‘4o mini on several metrics.
- Ablations that support claims
  - Spatial ablations (Table 1):
    - â€œScaleâ€ via `Dynamic-S2` alone substantially improves textâ€‘heavy tasks (e.g., DocVQA +29.8 points). â€œScale + Compressâ€ drops some performance, but â€œ+ VEPâ€ recovers most of it while achieving a 2.4Ã— system speedup.
  - Temporal ablations (Table 2):
    - 8â†’32 frames improves Videoâ€‘MME overall 55.7â†’61.0; compressing 4Ã— keeps 60.1 overall; scaling to 256 frames with 8Ã— compression further raises overall to 64.0.
  - Data pruning (Table 3):
    - At 50% data, `DeltaLoss` maintains average IMâ€‘10 (75.6â†’75.5) and beats random/cluster pruning across DocVQA/TextVQA/MMMU.
  - FP8 training (Table 4):
    - Throughput doubles without GC (199â†’390 it/s) and rises 2.5â†’2.9Ã— with GC (492â†’580) while keeping benchmark scores within Â±0.9 points.
  - Quantization (Table 6):
    - `W4A16` on LLM lowers TTFT (0.90â†’0.77 s) with small accuracy drop (e.g., Videoâ€‘MME 63.9â†’62.0). Adding `W8A8` on ViT further reduces TTFT to 0.65 s with negligible additional loss.
  - Inference profiling (Figure 5):
    - The vision tower dominates prefill after token compression; quantizing it (`W8A8`) is key to TTFT speedups. Decoding throughput benefits primarily from `W4A16` + FP16â€‘accumulating kernels.
- Do the experiments support the claims?
  - Yes. The paper provides clear before/after ablations for each design element, strong headâ€‘toâ€‘head efficiency comparisons (vs Qwen2â€‘VL and LLaVAâ€‘OV), and broad benchmark coverage. Note that Figure 1c shows accuracy normalized to each benchmarkâ€™s best score, while Tables 8â€“9 report absolute numbers; both views are provided.
- Additional capabilities (Section 4):
  - Temporal localization (ActivityNetâ€‘RTL): Mean IoU improves from 32.1 (VILAâ€‘1.5â€‘8B) to 34.8 with `NVILA-8B` (Table 10) by adding discrete time tokens and smoothed crossâ€‘entropy training.
  - Robotics (VLNâ€‘CE, R2R Valâ€‘Unseen): Navigation success rate improves to 53.3 with lower navigation error (NE 5.43), outperforming reported baselines (Table 11); realâ€‘time 1 Hz demo on laptop GPU (Figure 6).
  - Medical (`NVILAâ€‘M3`): Across VQA/report/classification, `NVILAâ€‘8B` paired with medical experts beats both general Medâ€‘Gemini and taskâ€‘specific SOTA on several tasks (Table 12).

## 6. Limitations and Tradeâ€‘offs
- Token compression accuracy tradeâ€‘off:
  - Spatial STC beyond 2Ã—2 hurts unless `VEP` is added (Table 1). Even with `VEP`, compressed models can be a few points below the â€œscaleâ€‘onlyâ€ peak on some OCR tasks.
- Vision encoder cost after scaling:
  - Multiâ€‘scale tiling (`Dynamic-S2`) and many frames make the vision tower the prefill bottleneck (Figure 5). The paper mitigates this with `W8A8` quantization, but this assumes compatible hardware and a custom engine (Section 2.4).
- Data pruning dependency:
  - `DeltaLoss` requires evaluating both a â€œlargeâ€ and a â€œsmallâ€ model over subâ€‘datasets to score examples (Eq. 1), introducing extra compute and relying on the choice of teacher/student models (Section 2.2.1). The method is validated on their data mixture (Table 3), but generalization across other corpora might need reâ€‘scoring.
- FP8 training assumptions:
  - Gains are measured on H100s with COAT (Table 4). Portability to other accelerators or to mixed hardware is not demonstrated; stability on very long sequences or different LLM backbones may require tuning.
- Quantization accuracy:
  - `W4A16` on the LLM introduces a small accuracy drop (Table 6). For the strictest accuracy targets, some deployments may prefer higher precision at the cost of latency.
- Benchmark scope and context:
  - While coverage is broad, some reported video metrics note with/without subtitles; datasets beyond those listed, or nonâ€‘English OCR/scene text scenarios, are not discussed. Figure 1c normalizes scores (helpful for visualization) but should be interpreted alongside absolute values in Tables 8â€“9.

## 7. Implications and Future Directions
- How this changes the landscape:
  - `NVILA` shows that highâ€‘resolution images and long videos need not be at odds with efficiency if compression is staged and training/inference are optimized endâ€‘toâ€‘end. The work provides a reproducible blueprintâ€”data pruning, FP8 training, flexible fineâ€‘tuning, and dualâ€‘path quantizationâ€”that others can adopt (Tables 3â€“7; Figure 5).
- Followâ€‘up research enabled/suggested:
  - Learnable compression that trains stably at high reduction ratios (where TokenLearner/Perceiver underâ€‘performed here) could push efficiency further (Table 1).
  - Dynamic token budgets conditioned on input complexity (e.g., contentâ€‘aware tiling/frame selection rather than uniform multiâ€‘scale/temporal sampling).
  - Joint optimization of vision and language quantization under accuracy constraints, including perâ€‘layer/adaptive precision.
  - Broader evaluation: nonâ€‘English OCR, egocentric/robotic longâ€‘horizon videos, safety/robustness under adversarial degradations.
- Practical applications:
  - Edge deployment of multimodal assistants (robots, AR, mobile) where TTFT matters (Figure 5; Figure 6).
  - Document understanding and chart/diagram QA at high accuracy and lower cost (Table 8; Table 1).
  - Medical imaging workflows when paired with expert models (Table 12) and longâ€‘video analytics for surveillance or instructional content (Table 9).
  
Key citations to ground the above:
- Architecture and paradigm: Figure 3; Sections 2.1.1â€“2.1.2.
- Spatial results/ablations: Table 1.
- Temporal results/ablations: Table 2.
- Dataset pruning: Equation (1), Figure 4, Table 3.
- FP8 training: Section 2.2.2, Table 4.
- Fineâ€‘tuning: Section 2.3, Table 5.
- Quantization and inference: Section 2.4, Table 6, Figure 5.
- Training curriculum and implementation: Section 3.1, Table 7, Table A1.
- Image benchmarks: Section 3.2.1, Table 8.
- Video benchmarks: Section 3.2.2, Table 9.
- Additional capabilities: Section 4; Tables 10â€“12; Figure 6.
- Aggregate efficiency/accuracy overview: Figure 1aâ€“c.

# Language Models are Few-Shot Learners

**ArXiv:** [2005.14165](https://arxiv.org/abs/2005.14165)

## ğŸ¯ Pitch

This paper introduces GPT-3, a 175-billion-parameter language model that achieves remarkable 'in-context' learning: it performs a wide array of NLP tasks with high accuracy simply by seeing a few examples or natural-language promptsâ€”no fine-tuning or task-specific gradient updates required. By scaling up model size, the authors demonstrate that a single, general-purpose model can approach or surpass state-of-the-art benchmarks across diverse domains, suggesting a fundamentally new paradigm for building adaptable, versatile AI that learns more like humans and less like traditional supervised systems.

---

## 1. Executive Summary
This paper introduces GPTâ€‘3, a 175â€‘billionâ€‘parameter transformer language model trained to predict the next token in text, and shows that simply prompting it with naturalâ€‘language instructions and a few examples enables strong â€œinâ€‘contextâ€ learning on many tasksâ€”often without any gradient updates. The significance is twofold: (1) scaling model size leads to smooth, predictable gains in fewâ€‘shot performance across diverse NLP tasks (Figure 3.1), and (2) at sufficient scale, a single, taskâ€‘agnostic model can approach or match fineâ€‘tuned systems on several benchmarks while also demonstrating novel onâ€‘theâ€‘fly reasoning abilities (Sections 3.2â€“3.9).

## 2. Context and Motivation
- Problem gap
  - Modern NLP relied on â€œpretrain then fineâ€‘tuneâ€: models are trained on large unlabeled corpora and then fineâ€‘tuned on thousands of labeled examples for each downstream task. This demands taskâ€‘specific datasets, risks overfitting to narrow distributions, and doesnâ€™t match how humans often learn from brief instructions or a few examples (Section 1).
- Why it matters
  - Practical impact: many useful tasks (e.g., grammar correction, novel word use) lack large labeled datasets.
  - Scientific impact: if a single model can adapt to new tasks from text alone, we move toward more general and flexible language systems (Figures 1.1â€“1.3).
- Prior approaches and limits
  - Fineâ€‘tuned SOTA models (e.g., BERT/T5/RoBERTa) achieve strong performance but require perâ€‘task supervised data and retraining (Section 2, Figure 2.1).
  - Earlier â€œzero-/fewâ€‘shotâ€ attempts with smaller LMs showed promise but trailed far behind fineâ€‘tuning (Section 1; e.g., low accuracy on Natural Questions).
- Positioning
  - This work tests whether massive scale aloneâ€”without taskâ€‘specific gradient updatesâ€”yields strong taskâ€‘agnostic, fewâ€‘shot performance, and whether the â€œinner loopâ€ of adaptation can happen inside the forward pass via â€œinâ€‘context learningâ€ (Figure 1.1).

Terminology (defined only when uncommon or paperâ€‘specific):
- `Inâ€‘context learning`: specifying a task to the model by writing instructions and a few inputâ€‘output examples directly in the prompt; the model adapts within its forward pass (no weight updates).
- `Fewâ€‘shot`/`oneâ€‘shot`/`zeroâ€‘shot`: respectively, many (10â€“100), one, or no demonstrations in the prompt (Figure 2.1).
- `Closedâ€‘book QA`: answering questions using only knowledge in the modelâ€™s parameters (no retrieval).
- `Openâ€‘domain QA`: answering with an external retrieval system over documents.

## 3. Technical Approach
Stepâ€‘byâ€‘step overview of how GPTâ€‘3 is built and evaluated:

1) Model architecture (Section 2.1; Table 2.1)
- A transformer language model (decoderâ€‘only) with 8 sizes from 125M to 175B parameters.
- GPTâ€‘3 (175B) uses 96 layers, model dimension 12,288, 96 attention heads, 2048â€‘token context window, and alternates dense and locally banded sparse attention patterns (to reduce compute while preserving longâ€‘context capacity).
- Training uses model parallelism across depth and width to fit large matrices on GPUs.

2) Training data and filtering (Section 2.2; Table 2.2; Appendix A)
- Base corpus is Common Crawl (2016â€“2019) filtered for quality using a logistic regression classifier trained to resemble â€œhighâ€‘qualityâ€ reference corpora (WebText, Wikipedia, books). Documents are retained with a Paretoâ€‘weighted sampling favoring higher classifier scores.
- Fuzzy deduplication removes nearâ€‘duplicates within and across datasets to reduce redundancy and overfitting.
- Final mixture by sampling weight (not proportional to corpus size):
  - 60% filtered Common Crawl (~410B tokens),
  - 22% WebText2 (19B),
  - 8% Books1 (12B),
  - 8% Books2 (55B),
  - 3% Wikipedia (3B).
- Each model trains for 300B tokens total; some datasets are seen <1 epoch, others multiple times (Table 2.2).

3) Training process (Section 2.3; Appendix B; Figure 2.2)
- Optimizer: Adam; cosine LR decay; warmup; gradient clipping; weight decay.
- Batch sizes scale with model size (up to 3.2M tokens for 175B; Table 2.1).
- Total compute grows massively with size; for 175B the estimate is ~3,640 PFâ€‘days (Appendix D; Figure 2.2).

4) Evaluation methodology (Section 2.4; Figure 2.1)
- For each task, construct a prompt with K demonstrations (fewâ€‘shot), 1 demonstration (oneâ€‘shot), or just instructions (zeroâ€‘shot). K is limited by the 2048â€‘token context (typically 10â€“100 examples).
- Scoring:
  - Multiple choice: compute the conditional log probability of each candidate answer given the prompt; usually normalize per token. On ARC, OpenBookQA, and RACE, an additional normalization divides by the unconditional probability of the completion to reduce length and frequency bias.
  - Freeâ€‘form generation: use beam search (beam=4, length penalty Î±=0.6; Section 2.4) and score with F1, BLEU, or exact match as standard.
- Example of â€œprogramming by promptingâ€:
  - LAMBADA is recast as a fillâ€‘inâ€‘theâ€‘blank cloze with oneâ€‘word targets (Table 3.2; Figure 3.2), letting the model infer the task constraints from the prompt examples.

5) Measuring contamination (Section 4; Appendix C)
- Postâ€‘hoc, identify benchmark overlap with pretraining data using conservative Nâ€‘gram matching and reâ€‘evaluate on the â€œcleanâ€ subset to estimate inflation (Figure 4.2).
- Two tasks flagged with small effects: PIQA (âˆ’4% relative) and Winograd (âˆ’2.6 points; Section 4).

Why this approach?
- The design isolates the role of scale and prompting. By avoiding fineâ€‘tuning and taskâ€‘specific architectures, any gains can be attributed to emergent inâ€‘context learning ability and the breadth of the pretrained distribution (Section 2; Figures 1.2, 3.1).

## 4. Key Insights and Innovations
1) Scaling enables effective inâ€‘context learning (fundamental)
- Accuracy increases smoothly with model size and with number of inâ€‘prompt examples (Figures 1.2, 3.1, 3.8). The gap between zeroâ€‘, oneâ€‘, and fewâ€‘shot widens with scale, indicating the larger model exploits demonstrations more effectively.

2) â€œPrompting as programmingâ€ is a universal interface (conceptual + practical)
- Many tasks are solvable by writing instructions and showing a handful of formatted examplesâ€”no gradient steps or taskâ€‘specific heads (Figure 2.1; Section 2.4). This reframes task specification from â€œretrain the modelâ€ to â€œauthor a prompt,â€ demonstrated from translation (Table 3.4) to cloze (Table 3.2) to arithmetic and word manipulation (Figures 3.10â€“3.11).

3) Broad, emergent abilities beyond standard benchmarks (novel capability)
- Without taskâ€‘specific training, GPTâ€‘3 carries out multiâ€‘digit arithmetic to substantial accuracy, scrambles/unscrambles words, and solves SAT analogies (Section 3.9). For example:
  > â€œ2â€‘digit addition: 100%; 3â€‘digit addition: 80.4%; 2â€‘digit multiplication: 29.2%â€ (Table 3.9).  
  > â€œRandomâ€‘insertion unscrambling (fewâ€‘shot): 67.2%â€ (Table 3.10).

4) Systematic contamination measurement at scale (methodological)
- The paper builds an explicit cleanâ€‘subset evaluation to quantify training/test overlap and its impact (Section 4; Figure 4.2), a practice that becomes essential at web scale. Most benchmarks show negligible differences between full vs. clean subsets; PIQA and Winograd are noted with asterisks due to small effects.

5) Human detectability of modelâ€‘generated news drops near chance (societal signal)
- With short (~200 words) or longer (~500 words) articles, human accuracy at distinguishing GPTâ€‘3 outputs is ~52%â€”barely above random guessing (Tables 3.11, 3.12; Figure 3.13), while weaker models are easier to detect. This highlights both quality and potential misuse risks.

## 5. Experimental Analysis
Evaluation design
- Breadth: >40 datasets across 9 categories (Sections 3.1â€“3.9), always reporting zeroâ€‘, oneâ€‘, and fewâ€‘shot and scaling across 8 model sizes (Appendix H).
- Metrics: taskâ€‘standard (accuracy, F1, BLEU, perplexity); careful scoring details for multipleâ€‘choice and generation (Section 2.4).
- Baselines: compare to fineâ€‘tuned SOTAs (e.g., T5â€‘11B, RoBERTa) and, for QA, to both closedâ€‘book and retrievalâ€‘augmented systems like RAG (Table 3.3).

Headline quantitative results (few highlights; see cited tables/figures for details)
- Language modeling and cloze/completion (Section 3.1)
  - PTB (zeroâ€‘shot perplexity):  
    > â€œ20.5â€ vs prior zeroâ€‘shot 35.8 (Table 3.1).
  - LAMBADA:  
    > â€œFewâ€‘shot 86.4% accuracyâ€ (Table 3.2, Figure 3.2), +18 points over prior SOTA; zeroâ€‘shot 76.2%; oneâ€‘shot 72.5%.
  - HellaSwag:  
    > â€œFewâ€‘shot 79.3%,â€ exceeding fineâ€‘tuned GPTâ€‘2â€‘style baselines but below overall fineâ€‘tuned SOTA 85.6% (Table 3.2).
  - StoryCloze:  
    > â€œFewâ€‘shot 87.7%,â€ still ~4 points behind fineâ€‘tuned SOTA 91.8% (Table 3.2).

- Closedâ€‘book QA (Section 3.2; Table 3.3; Figure 3.3)
  - TriviaQA:  
    > â€œZeroâ€‘shot 64.3%, oneâ€‘shot 68.0%, fewâ€‘shot 71.2%,â€ which matches or exceeds fineâ€‘tuned openâ€‘domain RAG (68.0%).
  - WebQuestions:  
    > â€œFewâ€‘shot 41.5%,â€ approaching fineâ€‘tuned closedâ€‘book T5â€‘11B+SSM (44.7%).
  - Natural Questions:  
    > â€œFewâ€‘shot 29.9%,â€ below fineâ€‘tuned closedâ€‘book T5â€‘11B+SSM (36.6%); large gains from zeroâ€‘shot suggest distribution/style mismatch mitigated by demonstrations.

- Translation (Section 3.3; Table 3.4; Figure 3.4)
  - One/fewâ€‘shot performance approaches or surpasses prior unsupervised NMT into English (e.g., Roâ†’En fewâ€‘shot 39.5 BLEU vs mBART 30.5), but from English to Romanian lags (Enâ†’Ro 21.0 BLEU fewâ€‘shot). Performance scales smoothly with size; translation into English is consistently stronger.

- Winogradâ€‘style coreference (Section 3.4; Table 3.5; Figure 3.5)
  - Winograd (WSC273):  
    > â€œ~88â€“90% across zero/one/fewâ€‘shot,â€ near human level, but flagged for contamination with a small measured effect (Section 4).
  - WinoGrande (adversarial):  
    > â€œFewâ€‘shot 77.7%,â€ below fineâ€‘tuned SOTA 84.6% but competitive with fineâ€‘tuned RoBERTaâ€‘large.

- Commonsense reasoning (Section 3.5; Table 3.6; Figure 3.6)
  - PIQA:  
    > â€œFewâ€‘shot 82.8% (testâ€‘server),â€ surpassing fineâ€‘tuned RoBERTa SOTA 79.4% (marked with an asterisk due to small potential contamination).
  - ARCâ€‘Challenge:  
    > â€œFewâ€‘shot 51.5%,â€ well below UnifiedQA SOTA 78.5%.
  - OpenBookQA:  
    > â€œFewâ€‘shot 65.4%,â€ below SOTA 87.2% but similar to fineâ€‘tuned BERTâ€‘large baselines.

- Reading comprehension (Section 3.6; Table 3.7; Figure 3.7)
  - CoQA:  
    > â€œFewâ€‘shot 85.0 F1,â€ close to human/SOTA (~90â€“91).
  - SQuAD 2.0:  
    > â€œFewâ€‘shot 69.8 F1,â€ modest.
  - DROP:  
    > â€œFewâ€‘shot 36.5 F1,â€ far below numericalâ€‘reasoning SOTAs (~89).
  - RACE:  
    > â€œHigh school 46.8% acc,â€ comparatively weak.

- SuperGLUE (Section 3.7; Table 3.8; Figure 3.8)
  - Overall fewâ€‘shot test score:  
    > â€œ71.8,â€ competitive with fineâ€‘tuned BERTâ€‘Large (69.0) but below fineâ€‘tuned SOTA (89.0).
  - Strong tasks: COPA (92.0), ReCoRD F1 (91.1); Weak task: WiC (49.4%, at chance).

- NLI (Section 3.8; Figure 3.9; Appendix H)
  - ANLI Round 3 dev:  
    > â€œFewâ€‘shot 40.2%,â€ only a modest improvement over chance (33%).

- Synthetic reasoning and pattern tasks (Section 3.9; Tables 3.9â€“3.10; Figures 3.10â€“3.12)
  - Arithmetic up to 5 digits shows partial competence; one/fewâ€‘shot helps substantially.
  - Word scrambling: robust on several variants in fewâ€‘shot (up to ~67%).
  - SAT analogies:  
    > â€œFewâ€‘shot 65.2%,â€ above historical human average (57%).

- Human detection of synthetic news (Section 3.9.4; Tables 3.11â€“3.12; Figure 3.13)
  - Mean human accuracy drops from 86% on a deliberately bad control model to ~52% on GPTâ€‘3, trending toward chance as model size grows.

Robustness, ablations, and caveats
- Scaling law holds: validation loss follows a power law in compute/parameters (Figure 3.1).
- Fewâ€‘shot benefits increase with K examples and model size (Figure 3.8).
- Contamination analysis shows small or negligible inflation for most tasks; LAMBADA had large overlap but nearâ€‘zero measured effect; PIQA and Winograd show small effects and are marked (Section 4; Figure 4.2).
- No componentâ€‘wise ablations (e.g., attention variants) are presented; the study isolates the effect of scaling and prompting rather than architectural tweaks.

Overall assessment
- The experimental evidence strongly supports the central claim: scale makes inâ€‘context learning broadly effective. However, results are mixed: GPTâ€‘3 approaches fineâ€‘tuned SOTA on some tasks (TriviaQA, COPA, ReCoRD, CoQA) but lags on others (DROP, ARCâ€‘Challenge, WiC, RACE). The breadth of tasks and the consistent scaling trends make the case convincing despite these gaps.

## 6. Limitations and Trade-offs
Assumptions and scope
- Assumes task specification fits within a 2048â€‘token prompt; complex tasks may exceed this limit, constraining K (Section 2.4).
- Uses a unidirectional LM objective; tasks that benefit from bidirectional context (e.g., WiC, some NLI, and spanâ€‘based comprehension) may be disadvantaged (Section 5).

Data and contamination
- Webâ€‘scale training risks trainâ€“test overlap; despite best efforts, a filtering bug left some overlaps (Section 4). Cleanâ€‘subset analysis mitigates but cannot guarantee unbiased estimates.
- Training data are predominantly English (93% by word count), limiting multilingual performance, especially from English into some languages (Section 3.3).

Computation and efficiency
- Training is extremely computeâ€‘intensive (â‰ˆ3,640 PFâ€‘days for 175B; Appendix D; Figure 2.2). Inference is also heavy and prompts consume context budget; distillation or retrieval could reduce costs (Section 5; 6.3).

Capabilities and failure modes
- Weak on tasks requiring:
  - Fineâ€‘grained sentence comparison or wordâ€‘sense discrimination (WiC; Table 3.8).
  - Multiâ€‘step discrete reasoning and numeracy (DROP; Table 3.7).
  - Knowledgeâ€‘dense, Wikipediaâ€‘style factual detail in closedâ€‘book QA (NQ; Table 3.3).
- On some synthetic tasks, oneâ€‘shot/zeroâ€‘shot trails fewâ€‘shot, indicating dependence on explicit demonstrations (Tables 3.9â€“3.10).

Bias and safety
- Bias analyses reveal stereotypical associations by gender, race, and religion (Section 6.2):
  > Occupations more likely to be followed by male identifiers; female coâ€‘occurrences skew toward appearance words like â€œbeautiful/gorgeousâ€ (Table 6.1).  
  > Sentiment trends: â€œAsianâ€ highest, â€œBlackâ€ lowest across models (Figure 6.1).  
  > For â€œIslam,â€ words like â€œterrorism/violent/terroristâ€ appear among highly favored coâ€‘occurrences (Table 6.2).
- Human detection experiments show nearâ€‘chance detectability of GPTâ€‘3â€‘generated news (Tables 3.11â€“3.12), raising misuse concerns (Section 6.1).

Open questions
- Does the model learn new tasks on the fly vs. recognize seen patterns? The paper notes the ambiguity and suggests it likely varies by task (Section 5).
- How much can retrieval, grounding, or RLâ€‘based objectives complement pure nextâ€‘token prediction (Section 5)?

## 7. Implications and Future Directions
Shifts in the field
- Validates â€œprompting as an interfaceâ€: instead of building bespoke fineâ€‘tuned models, users can steer a single model with instructions and examples (Figure 2.1). This foreshadows instructionâ€‘following and alignment trends.
- Reinforces scaling laws: predictable gains motivate systematic exploration of larger modelsâ€”while foregrounding the need for energyâ€‘ and dataâ€‘efficient training (Figure 3.1; Section 6.3).

Research avenues
- Architectural and objective augmentations:
  - Bidirectional or encoderâ€‘decoder pretraining combined with inâ€‘context learning to improve comparison/entailment tasks (Section 5).
  - Retrievalâ€‘augmented pretraining/inference (as contrasted with RAG in Table 3.3) to boost knowledgeâ€‘heavy QA.
  - Multiâ€‘modal grounding (vision, action) to improve â€œcommonsense physicsâ€ and world modeling (Section 5).
  - Learning objectives that prioritize entities/relations or goalâ€‘directed behavior beyond nextâ€‘token prediction (Section 5).
- Prompt engineering and tooling:
  - Methods to optimize prompts automatically and exploit demonstration order and formatting.
  - Extended context windows and memory mechanisms to admit larger K or longer documents.
- Efficiency and safety:
  - Distillation to reduce inference cost while retaining fewâ€‘shot abilities (Section 5).
  - Robust, standardized contamination detection; benchmark design less likely to appear verbatim on the web (Section 4).
  - Systematic bias auditing and mitigation integrated into pretraining and prompting (Section 6.2).
  - Detection and provenance tools for synthetic text (Section 3.9.4).

Applications
- Highâ€‘leverage, lowâ€‘label settings: rapid prototyping of classifiers, content transformation (summarization, style transfer), data augmentation, code and writing assistance, conversational agents.
- Knowledge tasks where closedâ€‘book fewâ€‘shot is sufficient (e.g., TriviaQAâ€‘like) and creative generation where prompts can steer style and content (Sections 3.2, 3.9.4, F).

> Bottom line: The paper demonstrates that sheer scale, coupled with carefully designed prompts, yields a single model that can perform a wide array of tasks without taskâ€‘specific training. The approach doesnâ€™t replace fineâ€‘tuning everywhere yet, but it decisively establishes inâ€‘context learning as a core capability and a practical interface for generalâ€‘purpose language models.

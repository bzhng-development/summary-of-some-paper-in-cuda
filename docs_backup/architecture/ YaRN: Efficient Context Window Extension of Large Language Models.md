# YaRN: Efficient Context Window Extension of Large Language Models

**ArXiv:** [2309.00071](https://arxiv.org/abs/2309.00071)
**Authors:** Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole
**Institutions:** 

## ğŸ¯ Pitch

YaRN introduces a novel method to extend the usable context window of transformer language models using rotary position embeddings, achieving reliable operation at up to 128k tokens with minimal fine-tuning. This approach significantly enhances the practical applicability of large models in tasks requiring long-span attention, such as legal or technical document processing, by offering a compute-efficient solution that maintains benchmark performance and compatibility with fast attention kernels.

---

## 1. Executive Summary (2-3 sentences)
The paper introduces YaRN (â€œYet another RoPE extensioNâ€), a computeâ€‘efficient method for extending the usable context window of transformer language models that use rotary position embeddings (`RoPE`). It combines a targeted frequencyâ€‘aware interpolation of positional frequencies with a simple attentionâ€‘logit scaling, enabling LLaMA/Llamaâ€‘2 and Mistral models to operate reliably at 64kâ€“128k tokens with minimal fineâ€‘tuning, while maintaining standard benchmark performance and compatibility with fast attention kernels (Section 3.4; Tables 2â€“3; Appendix B.4).

## 2. Context and Motivation
- The specific problem:
  - Transformer LLMs trained with `RoPE` encode token positions as rotations, but they typically fail to generalize beyond the maximum context length used in pretraining; perplexity spikes and retrieval degrades when going past this limit (Section 1; Figure 1).
- Why it matters:
  - Many real tasks (e.g., long-document summarization, codebases, legal/technical documents) require attending to tens or hundreds of thousands of tokens. Extending context postâ€‘pretraining avoids the cost of retraining large models from scratch and broadens practical usability (Section 1).
- Prior approaches and gaps:
  - `Position Interpolation (PI)` scales positions by a factor `s` to â€œstretchâ€ a fixed context but tends to:
    - Lose highâ€‘frequency positional detail (hurting local token order) and slightly degrade shortâ€‘context performance (Section 3.1).
    - Require substantial fineâ€‘tuning tokens (billions) to work well (Section 2.2; [9]).
  - `NTK-aware` interpolation rescales the RoPE frequency base so high frequencies are compressed less, improving zeroâ€‘finetune extrapolation but:
    - Slightly extrapolates some dimensions â€œoutâ€‘ofâ€‘bounds,â€ making its stated `s` not match the effective scale and yielding weaker results with fineâ€‘tuning compared to PI (Section 3.1; Def. 1; Eq. 16).
  - `Dynamic NTK` (inferenceâ€‘time dynamic scaling of `s`) improves zeroâ€‘finetune behavior but does not address the frequencyâ€‘targeting or shortâ€‘context degradation (Section 3.3).
  - Methods that alter attention (e.g., ReRoPE, LMâ€‘Infinite) can extend length but incur incompatibilities (e.g., not working with FlashAttention 2, or requiring two passes) and are not pure embeddingâ€‘level approaches (Section 2.4).
- This paperâ€™s position:
  - It consolidates and refines prior ideas into a single, lightweight recipeâ€”`YaRN`â€”that:
    1) targets which RoPE frequencies to interpolate and by how much (â€œby partsâ€);
    2) adds a costâ€‘free attentionâ€‘logit scaling that stabilizes training/inference at long lengths; and
    3) optionally uses dynamic scaling at inference to â€œgracefullyâ€ extend past trained limits (Sections 3.2â€“3.4).

## 3. Technical Approach
At a high level, RoPE encodes each token position `m` as a rotation in the complex plane applied to query/key vectors. The dot product between a query at position `m` and a key at position `n` depends only on their relative distance `mâˆ’n` (Section 2.1; Eqs. 1â€“9). YaRN modifies how those rotations are parameterized across dimensions and how the attention logits are scaled.

Step 0 â€” Background: How `RoPE` encodes position
- Each pair of hidden dimensions is interpreted as a complex dimension and rotated by an angle proportional to token position. Different pairs use different frequencies `Î¸_d = b^(-2d/|D|)`, where `b=10000` (Section 2.1).
- Wavelength `Î»_d = 2Ï€/Î¸_d = 2Ï€ b^(2d/|D|)` grows exponentially with dimension index `d` (Eq. 13). Lowâ€‘index dimensions are highâ€‘frequency (short wavelength), highâ€‘index dimensions are lowâ€‘frequency (long wavelength).

Step 1 â€” Baseline extension: `Position Interpolation (PI)`
- Idea: scale down positions by `s` when building RoPE, i.e., use `g(m)=m/s`, keeping `Î¸_d` the same (Section 2.2; Eq. 10 rewritten as Eq. 12).
- Limitation: compresses all frequencies equally, shrinking local angular differences among nearby tokens, which removes highâ€‘frequency detail needed for precise local order (Section 3.1).

Step 2 â€” Preserve high frequencies: `NTK-aware` base change
- Idea: instead of scaling positions, change the RoPE base from `b` to a new base `b'` so that highâ€‘frequency dimensions are compressed less and lowâ€‘frequency dimensions more (Section 3.1; Def. 1).
- Concretely, keep `g(m)=m` and modify `h(Î¸_d)` by replacing `b` with `b' = b * s^(|D|/(|D|âˆ’2))` (Eq. 16). This redistributes the â€œinterpolation burdenâ€ across dimensions.
- Tradeâ€‘off: some dimensions slightly extrapolate beyond pretraining ranges, which is helpful without fineâ€‘tuning but suboptimal when fineâ€‘tuning (Section 3.1).

Step 3 â€” Target interpolation where it helps: `NTK-by-parts`
- Motivation: Treating all dimensions the same (â€œblindâ€ interpolation) harms local relationships. Instead, interpolate only where the model relies more on absolute than relative position, and leave truly local (highâ€‘freq) channels untouched (Section 3.2).
- Mechanism:
  - Define `r(d) = L / Î»_d`, the ratio of the pretraining context `L` to the wavelength at dimension `d` (Eq. 17).
  - Choose thresholds `Î±` and `Î²` to partition dimensions:
    - If `r(d) < Î±`: wavelength â‰¥ L (very low frequency). Interpolate fully (like PI) by using `Î¸_d/s`.
    - If `r(d) > Î²`: wavelength â‰ª L (very high frequency). Do not interpolate; keep `Î¸_d`.
    - If `Î± â‰¤ r(d) â‰¤ Î²`: interpolate partially using a linear ramp `Î³(r)` from 0 to 1 (Eq. 18).
  - Implement as a convex combination over the RoPE frequency (Def. 2; Eqs. 19â€“20):
    - `h(Î¸_d) = (1âˆ’Î³(r(d))) * (Î¸_d/s) + Î³(r(d)) * Î¸_d`.
- Typical hyperparameters: for LLaMA family, `Î± = 1`, `Î² = 32` worked well (Section 3.2).

Intuition with a toy analogy:
- Think of RoPE dimensions as rulers of different granularity. Fine rulers measure local token order; long rulers measure global position. PI shrinks all rulers equallyâ€”making fine rulers too blunt. NTKâ€‘byâ€‘parts leaves the fine rulers unchanged, only stretching the long rulers and gradually blending for medium ones.

Step 4 â€” Stabilize attention at long lengths: YaRNâ€™s attention scaling
- Observation (Appendix A.2; Figures 2â€“4): When sequences get longer (large `s`), scaling the attention logits before softmax improves perplexity consistently across documents and token positions.
- Mechanism (Section 3.4):
  - Replace the attention softmax with a temperature `t`: `softmax(q^T k / (t âˆš|D|))` (Eq. 21).
  - Equivalently (and cheaply), scale both `q` and `k` by `âˆš(1/t)` via the RoPE embedding (the â€œlength scalingâ€ trick). This preserves compatibility with fast attention kernels and adds virtually no overhead.
  - Set the scale empirically as a simple function of `s` that fits several LLaMA variants: `1/t = 0.1 ln(s) + 1` (Eq. 22). This is surprisingly consistent across models and token positions.
- Why this helps:
  - Interpolation compresses angular separations between nearby tokens, which tends to concentrate softmax probabilities. A mild temperature (>1) deconcentrates them, counteracting overconfidence and restoring effective capacity at long ranges.

Step 5 â€” Optional inferenceâ€‘time boost: `Dynamic Scaling`
- In generation or streaming inference, current sequence length grows from 1 to the target maximum. Instead of fixing `s = L'/L`, update `s = max(1, l'/L)` at each forward pass, where `l'` is the current length (Section 3.3).
- Benefits:
  - Avoids premature degradation below the trained limit, and bends (rather than breaks) beyond it (Section 3.3; Appendix B.3, Figure 5).
- Implementation note with KVâ€‘cache:
  - Cache key/value tensors before applying RoPE, because RoPE depends on `s`, which changes across steps (Section 3.3).

Putting it together â€” `YaRN`
- Definition (Section 3.4; Def. 3): YaRN = `NTK-by-parts` interpolation (targeted per dimension) + attention scaling (Eq. 21), with optional `Dynamic Scaling` at inference.
- Training recipe used in the paper (Section 4.1):
  - Base models: Llamaâ€‘2 7B and 13B; later, Mistral 7B v0.1 (Appendix B.4).
  - Data: PG19 book corpus chunked into 64k sequences for Llamaâ€‘2 (Section 4.1); Longâ€‘Dataâ€‘Collections for Mistral (Appendix B.4).
  - Hyperparams: AdamW, lr 2eâ€‘5, Î²1=0.9, Î²2=0.95, no weight decay, 20 warmup steps, FlashAttentionâ€‘2 + FSDP (Section 4.1).
  - Schedule: train `s=16` for 400 steps (global batch 64). Then start from that checkpoint and train `s=32` for 200 more steps (Section 4.1).

## 4. Key Insights and Innovations
- Targeted, frequencyâ€‘aware interpolation (â€œby partsâ€), not oneâ€‘sizeâ€‘fitsâ€‘all:
  - Novelty: Uses the wavelength ratio `r(d)` to decide how much each RoPE dimension should be interpolated, preserving highâ€‘frequency (local) channels and stretching lowâ€‘frequency (global) ones (Section 3.2; Eqs. 17â€“20).
  - Significance: Maintains local order sensitivity and avoids the shortâ€‘context degradation observed with blind interpolation like PI (Section 3.2). This is a conceptual shift from uniform to targeted positional scaling.
- Zeroâ€‘overhead attention scaling tied to context extension:
  - Novelty: A simple preâ€‘softmax scaling `t` implemented as reâ€‘scaling of RoPE output, with the empirical rule `1/t = 0.1 ln(s) + 1` (Section 3.4; Eq. 22).
  - Significance: Robustly improves perplexity across documents and token positions without changing the attention kernel or adding inference cost (Appendix A.2; Figures 2â€“4).
- Dynamic scaling at inference:
  - Incremental but practical improvement: Adjust `s` with current length to gracefully extrapolate and prevent sharp failures at or beyond trained limits (Section 3.3; Appendix B.3).
  - Significance: Particularly helpful for zeroâ€‘finetune scenarios and compatible with KVâ€‘caching if implemented carefully (Section 3.3).
- Computeâ€‘efficient training and transfer:
  - Novelty: Shows effective 64k and 128k extensions with roughly 0.1% of original pretraining tokens and only 400â€“600 fineâ€‘tuning steps (Section 4; 4.1).
  - Significance: 10Ã— fewer tokens and 2.5Ã— fewer steps than prior PIâ€‘based methods (e.g., [9]) while outperforming them, enabling longer contexts under tight compute budgets (Abstract; Section 4).

## 5. Experimental Analysis
Evaluation setup
- Metrics and datasets:
  - Longâ€‘sequence modeling via slidingâ€‘window perplexity (window S=256) on Proofâ€‘Pile and GovReport (Sections 4.3.1; B.1; Figure 1; Tables 1â€“2, 4).
  - Passkey retrieval accuracy: synthetic task placing a 5â€‘digit key at random positions up to 128k (Section 4.3.2; Table 5).
  - Standard benchmarks: ARCâ€‘Challenge (25â€‘shot), HellaSwag (10â€‘shot), MMLU (5â€‘shot), TruthfulQA (0â€‘shot) (Section 4.3.3; Table 3).
- Baselines:
  - PIâ€‘based Together 32k; â€œNTKâ€‘awareâ€ Code Llama 100k; original Llamaâ€‘2; for Mistral, base v0.1 and MistralLite (NTKâ€‘aware) (Sections 4.3.1; Appendix B.4).

Main quantitative results
- Longâ€‘sequence perplexity (Proofâ€‘Pile; Table 2; Figure 1):
  - 7B:
    - `YaRN s=32 (128k)` shows perplexity 2.45 at 65k, 2.36 at 98k, and 2.37 at 128k.
    - â€œNTKâ€‘awareâ€ Code Llama 100k: 2.55 at 65k, 2.54 at 98k, rising to 2.71 at 128k.
    - Together 32k fails beyond 32k (perplexity explodes >10Â² at 65k+).
  - 13B:
    - `YaRN s=32 (128k)`: 2.31 at 65k, 2.23 at 98k, 2.24 at 128k.
    - Code Llama 100k: 2.41 at 65k, 2.37 at 98k, degrades to 2.54 at 128k.
- Shortâ€‘toâ€‘medium lengths (Proofâ€‘Pile; Table 1):
  - Extending Llamaâ€‘2 7B from 4kâ†’8k:
    - At 8,192 tokens: PI 3.34, NTKâ€‘aware 3.59, `YaRN` 3.35.
    - At 10,240 tokens (beyond trained window): `YaRN` 6.04 vs PI 8.07 vs NTKâ€‘aware 6.24.
  - Takeaway: `YaRN` matches PI at the target length and is more stable beyond it with fewer training steps and tokens (Table 1 vs Section 4).
- GovReport 32k perplexity (Table 4):
  - 7B: `YaRN s=16` achieves 3.59 vs Together 32k at 3.67 and Code Llama 100k at 4.44.
  - 13B: `YaRN s=16` at 3.35 vs Code Llama 100k at 4.22.
- Passkey retrieval (Table 5):
  - 7B: `YaRN s=32` achieves 99.4% through 128k; Code Llama 100k achieves 94.3% at up to ~112k.
  - 13B: `YaRN s=32` achieves 99.4% through 128k; Code Llama 100k 99.4% at 128k.
  - Authors note that passkey accuracy can remain high even when perplexity worsens, suggesting perplexity alone is not a full measure of longâ€‘context usability (Appendix B.2).
- Standard benchmarks (Table 3):
  - 7B:
    - Llamaâ€‘2 baseline: ARC 53.1, HellaSwag 77.8, MMLU 43.8, TruthfulQA 39.0.
    - `YaRN s=16`: 52.3, 78.8, 42.5, 38.2.
    - `YaRN s=32`: 52.1, 78.4, 41.7, 37.3.
    - Code Llama 100k: markedly worse (e.g., HellaSwag 60.8, MMLU 31.1).
  - 13B:
    - Llamaâ€‘2 baseline: 59.4, 82.1, 55.8, 37.4.
    - `YaRN s=16`: 58.1, 82.3, 52.8, 37.8.
    - `YaRN s=32`: 58.0, 82.2, 51.9, 37.3.
    - Code Llama 100k: much lower (e.g., HellaSwag 63.4, MMLU 32.8).
  - Takeaway: `YaRN` preserves general knowledge/task performance with minimal degradation versus baseline Llamaâ€‘2, unlike some prior longâ€‘context models.
- Mistral extension (Appendix B.4; Figure 6; Table 6):
  - Base Mistral v0.1 (8k) and MistralLite (16k) fail at long lengths (>16k), while `YaRN s=16 (128k)` achieves 2.24 at 65k and 2.19 at 128k.

Ablations and robustness checks
- Attentionâ€‘scaling ablation across positions/documents (Appendix A.2; Figures 2â€“4):
  - Shows a broad, consistent improvement from the preâ€‘softmax scaling factor, with best `1/âˆšt` around the rule in Eq. 22, across multiple token positions.
- Dynamic scaling without any fineâ€‘tuning (Appendix B.3; Figure 5):
  - For Llamaâ€‘2 at 4k pretrain length, `Dynamic-YaRN` prevents the perplexity blowâ€‘up beyond 4k and outperforms `Dynamic-PI`.
- Missing or limited ablations:
  - No detailed study varying `Î±, Î²` thresholds, or the exact ramp shape in `NTK-by-parts`.
  - Limited analysis of how `t` generalizes beyond LLaMA/Llamaâ€‘2/Mistral families or outside the tested scale factors.

Do the experiments support the claims?
- Yes, for the primary claims:
  - YaRN yields lower perplexity at long lengths and sustains capability to 128k for Llamaâ€‘2 7B/13B and Mistral 7B, surpassing prior RoPEâ€‘based extensions (Tables 2, 6; Figure 1).
  - It preserves standard benchmark performance (Table 3) and shows strong retrieval (Table 5).
  - It achieves this with far fewer training tokens/steps than prior methods (Sections 4, 4.1).
- Caveats:
  - Perplexity and passkey retrieval do not fully capture all aspects of longâ€‘context reasoning/composition.
  - Comparisons depend on particular datasets and training choices; more diverse tasks would further validate generality.

Representative quotes (verbatim figures/tables/sections)
> â€œYaRN reaches state-of-the-art performances in context window extensions after fine-tuning on less than ~0.1% of the original pre-training dataâ€¦ Dynamic-YaRN allows for more than 2x context window extension without any fine-tuning.â€ (Abstract; Sections 3.3â€“3.4)

> â€œYaRN (s = 32) modelsâ€¦ show continued declining perplexity through 128k, despite the fine-tuning data being limited to 64k tokens in length.â€ (Section 4.3.1; Table 2)

> â€œMinimal performance degradation between the YaRN models and their respective Llama 2 baselines.â€ (Section 4.3.3; Table 3)

## 6. Limitations and Trade-offs
- Scope limitation: Requires models that use `RoPE`. It does not directly apply to architectures with other positional schemes (e.g., pure ALiBi, learned absolute embeddings) without adaptation (Section 1; 2.1).
- Heuristic choices:
  - Thresholds `Î±, Î²` for `NTK-by-parts` (e.g., Î±=1, Î²=32 for LLaMA) are empirical; sensitivity and optimality are not fully explored (Section 3.2).
  - The temperature rule `1/t = 0.1 ln(s) + 1` is fit empirically on LLaMA variants; its universality is suggested but not theoretically derived (Section 3.4; Appendix A.2).
- Metrics vs capabilities:
  - Perplexity improvements and passkey retrieval success do not guarantee improvements in complex longâ€‘range reasoning or instruction following; broader evaluation suites would strengthen claims (Appendix B.2 discussion).
- Extrapolation limits:
  - Although `Dynamic Scaling` and `YaRN` extend well beyond trained lengths, behavior at extremely long contexts (>128kâ€“355k) is not exhaustively evaluated here for Llamaâ€‘2; Code Llama reports larger scales but with attention modifications and different setups (Section 4.2; Table 2 discussion).
- Implementation detail with KVâ€‘cache:
  - Must cache preâ€‘RoPE tensors; otherwise dynamic adjustments to `s` are incorrect (Section 3.3). This is manageable but is a footgun in custom inference stacks.
- Fineâ€‘tuning is still needed for best results:
  - Zeroâ€‘finetune `Dynamic-YaRN` helps but does not match fineâ€‘tuned YaRN at very long lengths; some training remains necessary to achieve the headline results (Appendix B.3 vs Tables 1â€“2).

## 7. Implications and Future Directions
- Practical impact:
  - Makes longâ€‘context LLMs (64kâ€“128k) accessible with minor code changes and minimal training cost, while remaining compatible with FlashAttentionâ€‘2 and standard inference stacks (Section 3.4; 4.1).
  - Immediate applications: longâ€‘document QA/summarization (GovReport), codebaseâ€‘level reasoning, legal/financial analysis, multiâ€‘file retrieval, and multiâ€‘session conversational memory.
- Conceptual shift:
  - Demonstrates that positional extension should be frequencyâ€‘aware and that lightâ€‘touch attention scaling can robustly stabilize longâ€‘range behavior.
- Research directions:
  - Theory: formalize why the attention temperature rule scales approximately with `ln(s)`; analyze the interplay of frequency targeting and softmax calibration.
  - Generality: validate `Î±, Î²` and `t(s)` across more architectures (e.g., larger Llamaâ€‘2 70B, diverse pretrain corpora) and tasks (longâ€‘horizon reasoning, program synthesis).
  - Tooling: automated selection of thresholds and ramp shapes; adaptive or learned interpolation during fineâ€‘tuning.
  - Integration: combine YaRN with memory modules, retrieval augmentation, or hierarchical attention to stretch beyond 128k with stronger reasoning guarantees.
  - Evaluation: develop richer longâ€‘context benchmarks beyond perplexity/passkeyâ€”e.g., multiâ€‘hop narrative reasoning across 100k+ tokens, longâ€‘range contradiction detection, crossâ€‘document synthesis.

In short, YaRN offers a principled yet simple recipeâ€”targeted frequency interpolation plus attentionâ€‘logit scalingâ€”that turns standard RoPEâ€‘based models into reliable longâ€‘context models with very low additional compute, while retaining mainstream performance on common benchmarks (Sections 3â€“4; Tables 2â€“3; Figure 1).

# Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

**ArXiv:** [2104.04473](https://arxiv.org/abs/2104.04473)

## ðŸŽ¯ Pitch

This paper introduces a system that effectively combines tensor, pipeline, and data parallelism to enable the training of trillion-parameter language models on thousands of GPUs, achieving unprecedented throughput and practical training times. By developing a novel interleaved pipelining schedule and optimizing communication patterns, the authors demonstrate 52% of A100 GPU peak efficiency at 502 petaFLOP/s, drastically reducing the barrier to large-scale language model research and deployment. This breakthrough makes the training of massively-scaled models feasible for real-world applications, accelerating progress and innovation in natural language processing.

---

## 1. Executive Summary
This paper shows how to train very large Transformer language models efficiently across thousands of GPUs by composing three kinds of parallelismâ€”pipeline, tensor, and dataâ€”into a single system they call PTD-P. It introduces a new interleaved pipeline schedule and communication optimizations that, together with careful engineering, deliver 52% of A100â€™s theoretical peak throughput while training a 1-trillion-parameter GPT on 3,072 GPUs (502 petaFLOP/s aggregate; Table 1).

## 2. Context and Motivation
- Problem addressed
  - Training state-of-the-art language models is constrained by GPU memory and by the sheer number of compute operations required. Large models no longer fit on a single GPU, and even on multi-GPU servers memory and time become prohibitive (Â§1).
  - Pure data parallelism fails at extreme scales because per-GPU batch sizes become too small (hurting utilization and raising communication overhead) and because parallelism is capped by the batch size (Â§1).

- Why this matters
  - Practical training of trillion-parameter models enables downstream applications like code completion, search, and dialogue systems at new levels of capability (Â§1).
  - Efficient scaling reduces time-to-train from years to months, making research and product iteration feasible (training time estimates derived from Eq. 4 and Table 1).

- Prior approaches and gaps
  - Tensor (intra-layer) model parallelism splits large matrix multiplications across GPUs and works well within a node, but it breaks down across nodes due to slower interconnects and the need for frequent all-reduce synchronization (Â§1, Â§2.3, Â§3.2).
  - Pipeline (inter-layer) model parallelism splits layers across devices and pipelines microbatches. However, to preserve strict synchronous optimizer semantics, it requires â€œpipeline flushes,â€ causing idle time (â€œpipeline bubblesâ€) and reduced throughput, especially at small batch sizes (Â§2.2, Fig. 3 and Fig. 4).
  - Naively combining these methods leads to poor scaling at thousands of GPUs (abstract, Â§1).

- Positioning
  - The paper composes pipeline, tensor, and data parallelism into PTD-P and contributes both a new pipeline schedule (interleaved 1F1B) and communication/computation optimizations. It also analyzes how the three dimensions interact, offering practical heuristics for choosing them (Â§1, Â§3, Â§4).

## 3. Technical Approach
PTD-P: composing three parallelisms

- Definitions and notation (Â§3.1)
  - `p`: pipeline-parallel size (number of pipeline stages).
  - `t`: tensor-parallel size (number of shards per layer).
  - `d`: data-parallel size (number of data-replicated groups).
  - `n`: total GPUs with `p * t * d = n`.
  - `B`: global batch size; `b`: microbatch size; `m = (B/d) / b`: number of microbatches per pipeline.

- Pipeline model parallelism, memory-friendly schedules, and â€œpipeline bubblesâ€
  - Pipeline parallelism shards the modelâ€™s layers over devices. A batch is split into `m` microbatches which move through the pipeline (Â§2.2).
  - â€œPipeline bubbleâ€ = periods when devices are idle at the beginning and end of a batch because some stages wait for others (Â§2.2, Fig. 3â€“4).
  - Schedules:
    - GPipe (all-forward then all-backward): bubble time fraction `(p-1)/m`, but requires storing activations for all `m` microbatchesâ€”high memory at large `m` (Â§2.2.1, Fig. 3).
    - 1F1B with flush (PipeDream-Flush): warm up, then alternate one forward and one backward per microbatch. Same bubble as GPipe but stores activations for at most `p` in-flight microbatchesâ€”much lower memory (Â§2.2.1, Fig. 4 top).
    - Interleaved 1F1B (new): assign each device `v` â€œchunksâ€ (subsets of layers) so each device acts as multiple pipeline stages; reduces bubble by factor `v`:
      > Bubble time fraction becomes `((p-1)/m) * (1/v)` (Â§2.2.2, Fig. 4 bottom).
      - Trade-off: communication increases by factor `v` (Â§2.2.2). The paper counters this with the scatter/gather optimization (Â§4.1).

- Tensor model parallelism: how layers are split (Â§2.3, Fig. 5)
  - MLP block: split first weight `A` by columns and second weight `B` by rows to avoid synchronization between the two GEMMs. Only two all-reduces in forward and two in backward are needed (operators `g` and `f` in Fig. 5a).
  - Self-attention: split attention heads across devices; output projection is row-partitioned, allowing each shard to operate locally with limited synchronization (Fig. 5b).
  - This design minimizes communication per layer while preserving correctness.

- Data parallelism
  - Replicates model shards across `d` groups; gradients are aggregated by all-reduce once per batch (Â§2.1).

- Why this composition and how to choose degrees
  - Analytical trade-offs (Â§3):
    - Pipeline bubble decreases as `d` grows: with `t=1`, bubble size `(n - d) / (B/b)` (Â§3.3.1, Fig. 6 shows bubble vs. `d`).
    - Tensor parallelism adds all-reduces per layer per microbatch; communication becomes expensive across nodes and shrinks GEMM sizes, lowering GPU utilization (Â§3.2).
  - Heuristic guidance (â€œTakeawaysâ€):
    - Use tensor parallelism within a node (e.g., up to 8 on DGX A100), then use pipeline across nodes to scale (Â§3.2, Takeaway #1).
    - Use the smallest `t * p` (total model-parallel degree) that fits memory; use data parallelism to scale out further (Â§3.3.2, Takeaway #2).
    - Optimize microbatch size `b` to balance utilization (larger `b`) against pipeline bubble (smaller `b`): throughput time estimate
      > `(B'/b + p - 1) * (t_f(b) + t_b(b))`, where `B' = B/d` (Â§3.4, Eq. (1); Fig. 7â€“8 illustrate the trade-off and optimal `b`).

- Memory management: activation recomputation (Â§3.5)
  - Store only inputs to a stage and recompute forward activations just before backward; optimal checkpointing every 1â€“2 Transformer layers in practice.

- Communication optimization: scatter/gather across nodes (Â§4.1, Fig. 9)
  - Problem: with tensor parallelism of size `t`, adjacent pipeline stages send the same activation tensor redundantly `t` times across nodes (Fig. 9a).
  - Solution: scatter the tensor into `t` chunks at sender; each GPU sends only its chunk over InfiniBand; at receiver, all-gather over faster NVLink to reconstruct the tensor (Fig. 9b).
  - Quantitatively reduces per-microbatch inter-stage traffic from `b*s*h` to `(b*s*h)/t` (Â§4.1). This is essential for the interleaved scheduleâ€™s higher intra-batch communication.

- Computation optimizations (Â§4.2)
  - Data layout change `[b, s, a, h] â†’ [s, b, a, h]` to enable strided batched GEMMs (fewer transposes).
  - JIT-fused elementwise kernels (bias+GeLU; bias+dropout+add).
  - Custom fused softmax kernels (scale+mask+softmax) for general and causal masking.

- FLOPs accounting and training-time estimation
  - Parameter count: Eq. (2).
  - Iteration FLOPs lower bound (GEMMs plus logit layer) with activation recomputation: Eq. (3).
  - End-to-end training time estimate: 
    > `â‰ˆ 8*T*P / (n*X)` given tokens `T`, parameters `P`, GPUs `n`, achieved per-GPU throughput `X` (Â§5.1, Eq. (4)).

## 4. Key Insights and Innovations
- Interleaved 1F1B pipeline schedule (Â§2.2.2, Fig. 4)
  - Novelty: assigns multiple smaller â€œchunksâ€ of layers to each device and interleaves their forward/backward execution.
  - Why it matters: reduces the pipeline bubble by `v` without increasing activation memory beyond standard 1F1B. It improves throughput at small to mid batch sizes where bubbles dominate (Fig. 12).

- Scatter/gather cross-node communication (Â§4.1, Fig. 9)
  - Novelty: exploits tensor-parallel replication to split inter-node transfers and reassemble them intra-node with faster NVLink all-gathers.
  - Why it matters: cuts inter-node point-to-point bandwidth demand by `t`, making interleaved scheduling viable at scale. Results: up to 11% per-GPU throughput improvement on communication-heavy regimes (Fig. 18).

- Practical â€œ3Dâ€ composition with clear heuristics (Â§3, Takeaways)
  - Contribution: an actionable guide to pick `t`, `p`, `d`, and `b` based on communication patterns, memory fit, and compute efficiency. Example insight:
    > â€œUse tensor parallelism up to the number of GPUs per node, then add pipeline parallelism across nodes; scale with data parallelism afterwardâ€ (Takeaway #1 and Â§5.4.1).

- System-level kernel and layout fusion (Â§4.2, Â§5.8)
  - Significance: keeps most of the graph compute-bound and reduces memory traffic. Throughput gains of 11â€“19% depending on model size (Â§5.8).

- Demonstrated record efficiency at scale (Â§5.1, Table 1)
  - Not just scaling to 1T parameters, but doing so with 52% of device peak and linear-ish weak scaling across 3,072 A100s (52% vs. prior 36% reported elsewhere in related work Â§6).

## 5. Experimental Analysis
- Setup (Â§5)
  - Hardware: NVIDIA Selene, nodes with 8Ã—80GB A100s (NVLink/NVSwitch), 8Ã—200 Gbps InfiniBand per node (Â§5).
  - Software: PyTorch + NCCL; mixed precision; activation recomputation; interleaved schedule with scatter/gather on by default for large runs (Â§5.1).
  - Models: GPT-family with sequence length `s=2048`, vocabulary `V=51,200`. Size ranges from 1.7B to 1T parameters by varying hidden size `h`, heads, and layers `l` (Table 1).
  - Metric: achieved teraFLOP/s per GPU and aggregate petaFLOP/s, derived from Eq. (3); includes all overheads: communication, data processing, and optimizer (Â§5.1).

- Main results (weak scaling across model sizes; Table 1)
  > â€œAt 1T parameters on 3,072 GPUs: 163 TFLOP/s per GPU (52% of peak 312 TFLOP/s), 502 PFLOP/s aggregateâ€ (Table 1).
  > â€œAt 175B on 1,536 GPUs: 148 TFLOP/s per GPU (47% of peak), 227 PFLOP/s aggregateâ€ (Table 1).
  - Efficiency generally improves with model size due to larger GEMMs and good comm/compute balance (Â§5.1).

- Training time estimates (Â§5.1, Eq. (4))
  > â€œ175B model on 1,024 A100s at 140 TFLOP/s per GPU with batch 1,536 would take 34 days for 300B tokensâ€ (derived below Table 1).
  > â€œ1T model on 3,072 A100s at 163 TFLOP/s per GPU for 450B tokens would take 84 daysâ€ (same section).

- Comparison to ZeRO-3 without model parallelism (Â§5.2, Table 2, Fig. 10)
  - Baseline: DeepSpeed ZeRO-3 sharding, no tensor/pipeline parallelism, same global batch.
  - Findings:
    > â€œPTD-P yields 6% (175B) to 24% (530B) higher per-GPU throughput at lower GPU countsâ€ (Table 2).
    > â€œWhen doubling GPUs (fixed batch), PTD-P outperforms ZeRO-3 by ~70% for both 175B and 530B because ZeRO-3â€™s cross-node communication dominatesâ€ (Fig. 10 and Â§5.2).
  - Note: 530B ZeRO-3 run needed more GPUs (640 vs 560) and a larger batch to fit memory (starred row in Table 2), indicating scaling/memory pressure.

- Pipeline parallelism behavior (Â§5.3)
  - Weak scaling with pipeline depth: throughput per GPU decreases as `p` grows at small batch sizes, consistent with bubble fraction `(p-1)/m` (Fig. 11).
  - Interleaved vs non-interleaved: interleaved schedule delivers higher throughput at small to mid batch sizes; the gap narrows as batch grows due to (1) bubble shrinking naturally and (2) interleavedâ€™s extra communication (Fig. 12).

- Parallel-composition trade-offs (Â§5.4)
  - Tensor vs pipeline at fixed 64 GPUs, 162B model: best performance when `t` equals GPUs per node (8) and pipeline spans nodesâ€”tensor-only or pipeline-only are inferior (Fig. 13).
  - Pipeline vs data (5.9B model): throughput drops as `p` increases; data parallelism is the preferred way to scale once the model fits (Fig. 14).
  - Tensor vs data (5.9B model): larger `t` hurts throughput due to frequent all-reduces per microbatch and smaller per-GPU GEMMs; data parallelism is friendlier when batch is sufficient (Fig. 15).

- Microbatch size effects (Â§5.5)
  - Throughput vs `b` is non-monotonic due to opposing forces: larger `b` improves arithmetic efficiency but reduces `m` and increases bubble. Example: optimum `b=2` for a 91B model with `(t,p)=(8,8)` (Fig. 16). Eq. (1) approximates this trade-off.

- Activation recomputation (Â§5.6)
  - For small batches, recomputation reduces sequences/sec by up to ~33%; however, it enables much larger batches that reduce bubble and double end-to-end throughput relative to the best non-recompute batch (Fig. 17).

- Communication optimization impact (Â§5.7)
  - Scatter/gather yields up to 11% per-GPU throughput gain on the 175B model using the interleaved schedule at larger batch sizes (Fig. 18).

- Fused operator impact (Â§5.8)
  > â€œ175B: +19% (113 â†’ 135 TFLOP/s per GPU). 530B: +11% (133 â†’ 148 TFLOP/s per GPU)â€ (both with fusion vs without).

- System bandwidth at scale (Â§5.9)
  > â€œEffective bisection bandwidth measured: 892 GB/s for pipeline point-to-point; 12.9 TB/s for data-parallel all-reduceâ€ on the 1T/3,072-GPU run.

- I/O for checkpoints (Â§5.10)
  > â€œ1T model checkpoint size is 13.8 TB; parallel filesystem peaks at ~1 TB/s read; writes at ~273 GB/s (40% of peak)â€â€”practical constraints for saving/loading.

- Do the experiments support the claims?
  - The core claimsâ€”efficient trillion-parameter training with >50% of device peak and practical end-to-end timeâ€”are backed by comprehensive throughput measurements (Table 1), ablations (interleaving, scatter/gather, fusion), and analytical models that predict observed trends (bubble vs `p`, `d`, `b` in Â§3 and Figs. 6â€“8, 11â€“16).
  - Limitations in baselines: ZeRO-3 is not combined with tensor/pipeline (authors note this explicitly in Â§5.2).

## 6. Limitations and Trade-offs
- Hardware assumptions
  - Results rely on high-bandwidth intra-node (NVLink/NVSwitch) and multiple 200 Gbps InfiniBand NICs per node. Clusters with fewer NICs or slower networks may not sustain the measured bisection bandwidths (892 GB/s p2p; 12.9 TB/s all-reduce in Â§5.9), reducing the benefit of interleaving and tensor parallelism across nodes.

- Communication vs bubble trade-off
  - Interleaved schedule reduces bubble by `v` but increases inter-stage communication by `v` (Â§2.2.2). It is advantageous primarily when batch size is not large enough to amortize bubbles (Fig. 12); at very large batches, the non-interleaved schedule can catch up.

- Strict synchronous semantics and pipeline flushes
  - The design preserves exact synchronous optimizer semantics (flush at batch boundaries; Â§2.2), avoiding asynchrony but leaving some bubble overhead on the table compared to bounded-staleness methods (Â§2.2 mentions PipeDream-2BW and others).

- Heuristics vs. auto-search
  - PTD-P uses manually derived heuristics to choose `t`, `p`, `d`, and `b` (Â§3 â€œTakeawaysâ€). It does not automatically search the large configuration space (acknowledged in Â§1 and Â§6), so suboptimal choices remain possible for novel architectures or hardware.

- Evaluation focus
  - Experiments focus on throughput (FLOP/s) rather than time-to-accuracy or convergence behavior. Although strict semantics help preserve standard training behavior, no end-to-end accuracy curves are reported here.

- Checkpointing and memory
  - Activation recomputation incurs extra compute (up to 33% fewer seq/s at small batches; Fig. 17). Checkpoints for trillion-parameter models are multi-terabyte (13.8 TB), imposing operational costs on storage and I/O (Â§5.10).

- Baseline coverage
  - ZeRO-3 is evaluated without model parallelism; combining ZeRO with tensor/pipeline could be a stronger baseline (not covered in Â§5.2).

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that trillion-parameter dense LMs can be trained within months with high hardware efficiency when 3D parallelism is engineered carefully (Table 1, Eq. 4). This changes the practical feasibility of scaling dense models and informs the design of training clusters and frameworks.

- Practical applications
  - Any large-model training regime (LMs, multi-modal Transformers) that must span many nodes can adopt PTD-Pâ€™s composition and interleaved scheduling. The scatter/gather pattern is broadly applicable wherever tensor-parallel replicas emit identical tensors across node boundaries.

- Research directions
  - Automated configuration search: extend FlexFlow/PipeDream/DAPPLE-style systems to include interleaved schedules, activation recomputation, and scatter/gather, exploring `t/p/d/b` jointly (Â§6 â€œRelated workâ€ and Â§3).
  - Adaptive or elastic interleaving: dynamically adjust `v` (chunks per device) based on observed network congestion and batch composition.
  - Integration with optimizer/state sharding: combine PTD-P with ZeRO variants (e.g., ZeRO-2/3/Infinity) to reduce optimizer memory/compute without excessive communication (Â§6).
  - Asynchronous or bounded-staleness pipelines: explore whether the gains from removing flush bubbles outweigh convergence penalties for very large models (Â§2.2 mentions PipeMare, PipeDream-2BW).
  - Hardware co-design: network topologies and NIC counts per node that best accommodate both all-reduce and point-to-point traffic suggested by Â§5.9 measurements.

> Bottom line: By carefully composing pipeline, tensor, and data parallelism and introducing interleaved execution plus communication-aware optimizations, this paper provides a practical and analytically grounded path to efficient trillion-parameter training on contemporary GPU clusters.

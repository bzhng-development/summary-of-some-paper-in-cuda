# 2Â OLMoÂ 2Â Furious

**ArXiv:** [2501.00656](https://arxiv.org/abs/2501.00656)
**Authors:** TeamÂ OLMo, PeteÂ Walsh, LucaÂ Soldaini, DirkÂ Groeneveld, KyleÂ Lo, ShaneÂ Arora, AkshitaÂ Bhagia, YulingÂ Gu, ShengyiÂ Huang, MattÂ Jordan, NathanÂ Lambert, DustinÂ Schwenk, OyvindÂ Tafjord, TairaÂ Anderson, DavidÂ Atkinson, FaezeÂ Brahman, ChristopherÂ Clark, PradeepÂ Dasigi, NouhaÂ Dziri, MichalÂ Guerquin, HamishÂ Ivison, PangÂ WeiÂ Koh, JiachengÂ Liu, SaumyaÂ Malik, WilliamÂ Merrill, LesterÂ JamesÂ V.Â Miranda, JacobÂ Morrison, TylerÂ Murray, CrystalÂ Nam, ValentinaÂ Pyatkin, AmanÂ Rangapur, MichaelÂ Schmitz, SamÂ Skjonsberg, DavidÂ Wadden, ChristopherÂ Wilhelm, MichaelÂ Wilson, LukeÂ Zettlemoyer, AliÂ Farhadi, NoahÂ A.Â Smith, HannanehÂ Hajishirzi
**Institutions:** AllenÂ InstituteÂ forÂ AI (AI2)

## ğŸ¯ Pitch

OLMo 2 revolutionizes open language models by providing not only the weights but also a complete reproducible recipe that includes training data and code, addressing major gaps in transparency and scientific study. Its stability-first design and innovative two-stage training place it at the forefront of performance for its compute level, making high-quality research more accessible and enabling deeper understanding of training dynamics and safety.

---

## 1. Executive Summary (2â€“3 sentences)
OLMo 2 is a fully open family of language models at 7B, 13B, and 32B parameters that releases everything needed to reproduce the models: weights, complete training data, training code/recipes, logs, and intermediate checkpoints. It introduces a stabilityâ€‘first architecture and a twoâ€‘stage training recipe (â€œmidâ€‘trainingâ€ with a curated Dolmino Mix 1124) that together place the models on the Pareto frontier of performance vs. training compute while remaining fully transparent (Figure 1; Table 6).

## 2. Context and Motivation
- The problem/gap:
  - Many â€œopenâ€ models release weights, but not the full recipe or training data, limiting scientific study and reproducibility. Fully open efforts existed (e.g., OLMo 1, Pythia, Amber), but typically underperformed compared to recent openâ€‘weights models (Introduction; Figure 1).
  - Largeâ€‘scale training is fragile: loss spikes and gradient explosions cause costly divergences, especially at larger scales (Section Â§3; Figure 2). This instability undermines performance and wastes compute.
  - Data quality and curriculum choices late in training can significantly shift downstream capabilities (e.g., math), but principled, costâ€‘efficient methods to compose such curricula are underdeveloped (Sections Â§4, Â§4.4).

- Why it matters:
  - Transparent, reproducible models enable research on training dynamics, memorization, scaling laws, and safetyâ€”areas that require access to pretraining data and logs (Introduction).
  - Trainingâ€‘stability fixes and efficient curricula reduce cost and risk while improving capability, widening access to highâ€‘quality open models.

- Prior approaches and their limits:
  - Prior fully open suites (OLMoâ€‘0424, Pythia, Amber, DCLM) advanced openness but were either less competitive on benchmarks or did not resolve stability at larger scales (Figure 1; Table 6).
  - Contemporary openâ€‘weights models (e.g., Llama 3.1, Gemma 2, Qwen 2.5) are strong, but their datasets and exact recipes are not fully disclosed, limiting reproducibility and study (Introduction; Table 6 notes on openness).

- Positioning:
  - OLMo 2 targets both stability and capability with:
    - Architectural and optimizer changes that measurably suppress loss/gradient spikes (Section Â§3; Figures 2â€“10).
    - A twoâ€‘stage training process with a late â€œmidâ€‘trainingâ€ curriculum (Dolmino Mix 1124) that patches specific deficits (especially math) and upgrades general skills (Sections Â§2.3, Â§4; Tables 5, 9, 11â€“13).
    - A postâ€‘training pipeline (TÃ¼lu 3 + RLVR) using permissively licensed data only (Section Â§5; Tables 15â€“16).

## 3. Technical Approach
This section unpacks how OLMo 2 is built and trained, emphasizing the concrete mechanisms behind its stability and capability gains.

- Model architecture (Section Â§2.1; Table 1; Table 3):
  - Base: decoderâ€‘only transformer with no biases and SwiGLU activation.
  - Stabilityâ€‘oriented changes:
    - `RMSNorm` replaces the prior nonâ€‘parametric LayerNorm to normalize activations (Section Â§2.1; Table 1).
    - `Postâ€‘norm` block layout: normalize the outputs of attention and MLP blocks (not the inputs). The block computes:
      - h := x + RMSNorm(Attention(x))
      - hout := h + RMSNorm(MLP(h))
      (Equations (1)â€“(2), Section Â§2.1). This change mitigates gradient amplification in deep stacks (Section Â§3.3.2; Figure 7).
    - `QKâ€‘norm`: apply RMSNorm to queries and keys before dotâ€‘product attention to prevent overly large attention logits (Section Â§3.3.2).
    - `zâ€‘loss` regularization: add 10^-4 Â· log2 Z to the loss (Z is the softmax normalizer) to discourage very large logits (Section Â§3.3.3).
    - Larger `RoPE Î¸ = 5e5` to increase positional encoding resolution (Section Â§2.1).
    - 32B uses `GQA` (grouped query attention) to reduce KV cache costs while retaining multiâ€‘head queries (Section Â§2.3; Table 3).
  - Why these choices: Ablations and diagnostics show these normalizations and regularization reduce gradient/loss spikes (Figures 2, 7â€“9) and keep gradient/activation scales healthy through depth (Figures 5â€“6).

- Tokenizer (Section Â§2.2; Table 2):
  - Switch to `cl100k` vocabulary (used by GPTâ€‘3.5/4) with a few legacy special tokens retained for backward compatibility.
  - Tested at 1B scale: small but consistent improvements across OLMES generative and MC tasks (Table 2).

- Twoâ€‘stage training recipe (Section Â§2.3; Table 3):
  - Stage 1: long pretraining (90â€“95% FLOPs) with cosine LR schedule after a 2,000â€‘step warmup. Tokens: 3.90T (7B), 5.0T (13B), 6.06T (32B) (Section Â§2.3 and Â§2.4; Table 4).
  - Stage 2 (â€œmidâ€‘trainingâ€): short late stage (5â€“10% FLOPs) with linearly decaying LR to zero and a targeted, higherâ€‘quality/specialized mixture, â€œDolmino Mix 1124â€ (Sections Â§2.3, Â§4; Tables 5, 13).

- Data pipeline and mixes:
  - Pretraining mix â€œOLMo 2 Mix 1124â€ (Table 4):
    - ~95% web (DCLM baseline 1.0), plus permissive code (StarCoder subset), academic corpora (peS2o; arXiv), Wikipedia, and math web/proofs (OpenWebMath, Algebraic Stack).
    - Data cleaning for stability: filter documents with â‰¥32 repeated nâ€‘gram sequences; additional trainingâ€‘time masking for such spans (Section Â§3.1; Figure 3).
  - Midâ€‘training mix â€œDolmino Mix 1124â€ (Table 5):
    - Highâ€‘quality web filtered by two classifiers (DCLM FastText; FineWeb Edu), plus encyclopedic/academic sources and StackExchange Q&A.
    - Mathâ€‘centric synthetic/filtered sets (TuluMath, DolminoSynthMath, `TinyGSMâ€‘MIND`, MathCoder2â€‘synthetic, filtered Metamath and CodeSearchNet, GSM8Kâ€‘train) (Section Â§4.4.1; Table 5).
    - Composition varies by 50B / 100B / 300B token budgets, keeping relative proportions approximately constant by repeating sources as needed (Section Â§4.5; Table 13).

- Stability interventions (Section Â§3; Figures 2â€“10):
  - `Repeated nâ€‘gram` removal/masking lowers the frequency of gradient spikes (Section Â§3.1; Figure 3).
  - `Initialization`: simple normal init with std=0.02 (no layerâ€‘scaled init). This preserves gradient/activation norms across depth better (growth exponent near 0), enabling stable lowâ€‘precision training; spike score drops from 0.40â†’0.03 in tests (Section Â§3.2; Figures 4â€“6).
  - `AdamW Îµ`: reduce from 1eâ€‘5 to 1eâ€‘8 to allow larger early updates, stabilizing gradient norms sooner (Section Â§3.4.1; Figure 9).
  - `No weight decay on embeddings`: avoids vanishing embedding norms that otherwise induce large early gradients via normalization Jacobians (Section Â§3.4.2; Figure 10).

- Learningâ€‘rate annealing behavior (Section Â§4.1; Figure 11; Table 8):
  - Higher peak LRs win early but are overtaken later; after a short midâ€‘training to LR=0, variants converge to nearly identical losses and similar downstream averages (Table 8). A higher LR can yield slightly better math (GSM8K +2.8) when the midâ€‘training data itself is mathâ€‘focused.

- â€œMicroâ€‘annealsâ€ to choose math data cheaply (Section Â§4.4.2; Table 12):
  - Method: brief 50/50 runs mixing a candidate math subset with a standard web mix; linearly anneal LR to zero to evaluate quality quickly (<10B tokens visible effects).
  - Findings:
    - Even a small fraction of domain data yields big gains (GSM* rises from 28.5â†’61 with only 10% math; Table 12).
    - Limited duplication of scarce math data (2Ã—) can help (GSM* 61â†’66; Table 12).
    - Rewriting codeâ€‘style math (TinyGSM) into natural language (â€œMINDâ€ prompts) dramatically improves outcomes (GSM* 25â†’65.5; 2Ã— to 70.0), showing representation matters (Table 12).

- Model â€œsoupsâ€ (weight averaging) (Section Â§4.5; Table 14):
  - Average multiple midâ€‘training runs with different data orders; across six mixes, a 3â€‘run soup matches or beats the best single run on both MC and generative averages and on GSM* (Table 14).

- Postâ€‘training with TÃ¼lu 3 and RLVR (Section Â§5):
  - SFT: carefully curated permissive instruction data plus largeâ€‘scale personaâ€‘driven synthetic questions; small variants of the TÃ¼lu 3 mix (Section Â§5; Table 17 notes).
  - DPO (preference tuning): onâ€‘policy prompts from OLMo 2 SFT variants, plus responses from a pool of permissibly licensed models; GPTâ€‘4o judges pairwise preferences on helpfulness/truthfulness/honesty/instructionâ€‘following (Section Â§5; Table 16; Appendix D: Table 25 & Table 27).
  - RLVR (Reinforcement Learning with Verifiable Rewards): use PPO to reward only verifiably correct generations (e.g., exact numeric math answers). 7B/13B use reward models; 32B uses GRPO (no RM) (Section Â§5; Figures 13â€“15; Table 18).
  - Multiâ€‘stage RLVR for 13B (GSM8K + MATH + constraints â†’ GSM8K only â†’ MATH only) steadily increases math and average scores (Figure 13).

- Infrastructure and efficiency (Section Â§6):
  - Training on two H100 clusters (Cirrascale â€œJupiterâ€ and Google Cloud â€œAugustaâ€) orchestrated by Ai2â€™s Beaker (Sections Â§6.1â€“6.2).
  - Practical speedups: PyTorch `torch.compile`, avoiding hostâ€“device syncs, asynchronous logging/checkpointing via a separate backend, and coordinated garbage collectionâ€”all to stabilize and speed large distributed jobs (Section Â§6.4; Figure 16).
  - Environmental accounting (energy, carbon, water) reported from logged telemetry and local grid intensities (Section Â§6.5; Table 19).

## 4. Key Insights and Innovations
- Stabilityâ€‘first transformer stack that scales smoothly (Section Â§3; Figures 2â€“10; Table 1):
  - Whatâ€™s new: the specific combinationâ€”postâ€‘norm RMSNorm inside residual branches, RMSNorm on queries/keys, zâ€‘loss, a simple unscaled initialization, reduced AdamW Îµ, and turning off embedding weight decayâ€”systematically reduces gradient and loss spikes.
  - Why it matters: stable, lowâ€‘precision training at larger scales with fewer restarts and better final minima. Evidence: growth exponents near zero (Figure 5), improved gradient/activation scaling vs. width (Figure 6), significant drop in spike scores, and smoother losses (Figure 2).

- Costâ€‘effective â€œmidâ€‘trainingâ€ with Dolmino Mix 1124 and â€œmicroâ€‘annealsâ€ (Sections Â§4.1â€“Â§4.5; Tables 5, 9, 11â€“14):
  - Whatâ€™s new: treat late training as a targeted curriculum stage, and select math sources via extremely short â€œmicroâ€‘annealsâ€ that are cheap yet predictive at full run scale (Table 12).
  - Why it matters: dramatic gains where the base model is weakâ€”e.g., GSM8K jumps 24.1â†’67.5 (7B) and 37.3â†’75.1 (13B) after midâ€‘training (Table 9)â€”without expensive fullâ€‘run sweeps for each data variant.

- Fully open, computeâ€‘efficient models on the Pareto frontier (Figure 1; Table 6):
  - Whatâ€™s new: endâ€‘toâ€‘end transparencyâ€”weights, code, complete data, logs, and thousands of checkpointsâ€”at performance competitive with popular openâ€‘weightsâ€‘only models, often using fewer approximate training FLOPs (Figure 1; Table 6).
  - Why it matters: unlocks research into data/recipe effects, memorization, and scaling that closed or partially open projects cannot support.

- Multiâ€‘stage RL with verifiable rewards at scale (Section Â§5; Table 16; Figures 13â€“15):
  - Whatâ€™s new: integrate TÃ¼lu 3 SFT+DPO with RLVR in multiple stages (for 13B), initializing PPOâ€™s value function from learned reward models; 32B uses GRPO to remove the need for an RM.
  - Why it matters: systematic, measurable gains from SFTâ†’DPOâ†’RLVR on reasoning/math while using permissive data (Table 16; Figures 13â€“15).

## 5. Experimental Analysis
- Evaluation methodology (Appendix A; Table 20; Section Â§2.5):
  - Base models: OLMES suite with standardized prompts and scoring across 5 multipleâ€‘choice and 2 generative development tasks, plus 4 heldâ€‘out tasks (AGIEval, GSM8K, MMLUâ€‘Pro, TriviaQA). OLMES uses two MC formats and reports the better (Multipleâ€‘Choice vs. Cloze/Completion), consistent shots (often 5â€‘shot), and F1 for generative tasks (Appendix A; Table 20).
  - Instruct models: TÃ¼lu 3 evaluation settings on knowledge recall, reasoning, math, instruction following, and safety (Section Â§5; Table 15).

- Main quantitative results:
  - Base models (Table 6; Figure 1):
    - OLMo 2 7B achieves avg 62.9 on 10 dev benchmarks; OLMo 2 13B: 68.3; OLMo 2 32B: 73.3.
    - These are competitive with openâ€‘weights models of similar size while requiring fewer training FLOPs; Figure 1 plots average performance vs. approximate FLOPs, placing OLMo 2 on the Pareto frontier among fully open models.
  - Gains from midâ€‘training (Table 9):
    - Average dev improvements: +10.6 (7B), +10.3 (13B), +7.0 (32B).
    - Math improves most: GSM8K +43.4 (7B: 24.1â†’67.5), +37.8 (13B: 37.3â†’75.1), +22.6 (32B: 56.2â†’78.8).
    - Reading comprehension also jumps (NQ, DROP), and general MC (MMLU, ARCâ€‘C) improves.
  - Midâ€‘training mix comparisons (Table 11):
    - Highâ€‘quality web filters (DCLM FT topâ€‘7% + FineWeb Edu â‰¥2) beat simple LR anneal (OLMES avg 75.2 vs. 74.0; MMLU 63.1 vs. 61.8).
    - Adding math boosts both math (GSM* 52.0) and generative tasks (OLMESâ€‘Gen +6.2 over PT Mix).
    - Adding instruction data alongside math keeps broad gains (OLMESâ€‘Gen best at 70.2; math slightly below pure math mix).
  - Microâ€‘anneals (Table 12):
    - Even 10% math materially helps (GSM* ~61). Doubling math subset helps (to ~66), with diminishing returns at 4Ã—.
    - Rewriting codeâ€‘style TinyGSM to natural language yields the biggest jump (to ~65.5; 2Ã— to 70.0).
  - Model soups (Table 14):
    - In 6 mixes, a 3â€‘run average consistently equals or beats the best single run across OLMES averages and frequently on GSM*.
  - Instruct models (Table 7; Table 16):
    - Stagewise gains are consistent. Example (13B): SFT avg 56.6 â†’ DPO 62.0 â†’ RLVR 63.4 (Table 16). GSM8K for 13B reaches 87.4; MATH 39.2.
    - Against openâ€‘weights peers: OLMo 2â€‘13Bâ€‘Instruct approaches Qwen 2.5â€‘14B and surpasses Llama 3.1â€‘8B and TÃ¼lu 3â€‘8B on the averaged suite (Table 7).
    - 32Bâ€‘Instruct averages 68.8; with strong GSM8K (87.6) and MATH (49.7) (Table 7), aided by RLVR/GRPO (Figure 14).

- Stability evidence:
  - Training curves before vs. after interventions show many fewer spikes and steadier gradients (Figure 2).
  - Quantitatively, the initialization change reduces spike score of gradient norm from 0.40 to 0.03 in stress tests (Section Â§3.2). QKâ€‘norm + postâ€‘norm reduces gradient spike score from 0.108â†’0.069 (Figure 7). AdamW Îµ lowering smooths early gradients (Figure 9). Disabling embedding weight decay prevents embedding norm collapse and reduces spike frequency (Figure 10).

- Learningâ€‘rate invariance:
  - Different peak LRs converge after annealing; downstream averages across nine tasks are within ~0.5 points (Table 8). Slight math advantage appears when highâ€‘LR pretraining is followed by mathâ€‘focused midâ€‘training (+2.8 GSM8K).

- Robustness/heldâ€‘out:
  - The project maintains heldâ€‘out tasks (AGIEval, GSM8K test, MMLUâ€‘Pro, TriviaQA) not used for development (Table 6; Appendix A.1). Midâ€‘training gains transfer to these heldâ€‘outs (Table 9, rightmost columns), indicating generalization of the approach.

- Do the experiments support the claims?
  - Yes, on two fronts:
    - Stability: multiple targeted ablations/diagnostics link each intervention to reduced spikes or healthier norms (Figures 3â€“10).
    - Capability: a clean, staged comparison (Table 9) ties midâ€‘training to large, broad improvements; microâ€‘anneals + soups show repeatable and composable dataâ€‘selection gains (Tables 12 & 14). Stagewise instruct training (Table 16; Figures 13â€“15) further adds consistent improvements.

## 6. Limitations and Trade-offs
- Assumptions and scope:
  - Midâ€‘training relies on the availability of â€œverifiableâ€ domains (math, constrained tasks) for RLVR and on highâ€‘quality filtered web/math corpora; transfer to other domains (e.g., multilingual, code) is not evaluated here (Tables 5, 15).
  - The tokenizer switch was validated at 1B scale with small gains (Table 2); effects at very large scales are inferred but not directly isolated.

- Scenarios not addressed:
  - Code specialization is not a target in this release; code datasets are a small fraction of pretraining and optional in midâ€‘training (Tables 4â€“5, 10).
  - The 1B model struggles to escape nearâ€‘random MC accuracy without midâ€‘training/postâ€‘training (Appendix B; Table 22), pointing to capacity limits at very small sizes.

- Computational/data constraints:
  - Although computeâ€‘efficient relative to peers, total tokens are still large (up to 6.6T; Section Â§2.3). Training requires H100â€‘class clusters with highâ€‘speed interconnect (Section Â§6.1).
  - zâ€‘loss implementation details matter: different fused vs. PyTorch implementations had mismatched backward behavior, forcing reâ€‘training from a safe checkpoint (Section Â§3.3.3; Figure 8), suggesting fragility in some fused kernels.

- Evaluation tradeâ€‘offs:
  - OLMES chooses the better of two MC formats per task (Appendix A), which aligns with widely used practice but may slightly inflate â€œbestâ€‘ofâ€‘formatâ€ numbers compared to fixedâ€‘format-only reports.
  - While FLAN was decontaminated from evaluation sets (Section Â§4.3), broader decontamination across all sources is intractable; some contamination risk remains typical of webâ€‘scale pretraining.

- Environmental impact:
  - Despite efficiency, pretraining energy remains substantial (e.g., 7B ~131 MWh; 13B ~257 MWh; Table 19). Carbon and water use are reported but still nonâ€‘trivial.

## 7. Implications and Future Directions
- Field impact:
  - OLMo 2 demonstrates that fully open models can reach the performance/compute frontier (Figure 1; Table 6) when training stability and a principled lateâ€‘stage curriculum are prioritized. The complete release (data, code, logs, checkpoints) sets a high bar for transparency and will likely catalyze research on training dynamics, memorization, and data governance.

- Followâ€‘up research enabled/suggested:
  - Stability: Formalize spikeâ€‘scoreâ€‘driven earlyâ€‘warning systems and investigate theoretical links between postâ€‘norm + QKâ€‘norm + zâ€‘loss and signal propagation (Sections Â§3.2â€“Â§3.3).
  - Data curricula: Extend â€œmicroâ€‘annealsâ€ to other domains (code, multilingual, retrievalâ€‘augmented recipes) and automate source selection with better proxies and learned scorers (Sections Â§4.3â€“Â§4.4.2).
  - RLVR: Build rewardable datasets for other verifiable tasks (unit tests for code, structured QA with validators); study GRPO vs. RMâ€‘based PPO tradeâ€‘offs at scale (Section Â§5; Figure 14).
  - Small models: Address the 1B capacity cliff using distillation, mixtureâ€‘ofâ€‘experts, or taskâ€‘targeted SFT curricula (Appendix B).

- Practical applications:
  - Deployable â€œInstructâ€ models for general assistance with strong math and reasoning (Table 7), especially where transparent training data is required (regulated domains, research).
  - A reproducible blueprint for organizations to train specialized models with tighter budgets: adopt stability toolkit (Section Â§3), twoâ€‘stage midâ€‘training (Section Â§4), and soup averaging (Section Â§4.5).
  - Infrastructure practices (Section Â§6.4) are directly actionable for teams running large distributed training on PyTorch.

> Key quantitative takeaways:
> - â€œMidâ€‘trainingâ€ raises GSM8K by +43.4 (7B) and +37.8 (13B) and improves general benchmarks (Table 9).
> - Final base model averages: 7B 62.9, 13B 68.3, 32B 73.3 (Table 6), with efficient FLOPs positioning on the Pareto frontier (Figure 1).
> - Instruct models gain consistently through SFTâ†’DPOâ†’RLVR (Table 16); 13Bâ€‘Instruct reaches GSM8K 87.4 and competitive averages vs. similarly sized peers (Table 7).

Overall, OLMo 2â€™s contribution is twofold: a stabilityâ€‘first recipe that unlocks reliable, computeâ€‘efficient scaling, and a transparent, reproducible endâ€‘toâ€‘end release that advances the science of language model training as much as it advances the models themselves.

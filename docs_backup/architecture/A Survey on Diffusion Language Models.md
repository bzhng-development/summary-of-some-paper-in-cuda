# A Survey on Diffusion Language Models

**ArXiv:** [2508.10875](https://arxiv.org/abs/2508.10875)
**Authors:** Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
**Institutions:** not specified on arXiv

## ğŸ¯ Pitch

This paper systematizes the field of Diffusion Language Models (DLMs), introducing a taxonomy and performance benchmarks that highlight their potential to surpass autoregressive models in speed and multimodal capabilities. By enabling parallel text generation and bidirectional context, DLMs promise significant real-world applications in latency-sensitive tasks, bridging gaps in open-ended and structured generation while uncovering new avenues for robust reasoning and multimodal integration.

---

## 1. Executive Summary
This survey systematizes the fastâ€‘growing field of Diffusion Language Models (DLMs)â€”models that generate text by iteratively denoising masked or noisy sequences in parallel rather than predicting tokens one by one. It builds a comprehensive taxonomy (continuous, discrete, hybrid, and multimodal variants), explains core training and inference mechanics, consolidates performance evidence across benchmarks, and analyzes practical accelerations and limitations (Sections 2â€“8; Figs. 1â€“6, Tables 1â€“2).

## 2. Context and Motivation
- Problem addressed
  - Large language models based on autoregression (AR) generate text sequentially (token-by-token), which creates an inferenceâ€‘time bottleneck and limits parallelism (Section 1). DLMs promise parallel generation and bidirectional context but the literature is fragmented across formulations (continuous vs. discrete), training recipes, postâ€‘training for reasoning, and inference accelerations.
- Why it matters
  - Realâ€‘world latency: Parallel denoising can accept or revise multiple tokens per step, potentially yielding much higher throughput than AR decoding (Section 1; Section 4).
  - New capabilities: Bidirectional conditioning supports infilling, structured editing, and unified multimodal generation/understanding in a single framework (Sections 2.2â€“2.4, 5).
- Prior approaches and gaps
  - AR LLMs excel in quality but are inherently sequential (Section 2.1.2; Eq. (3)â€“(4)). Masked Language Models (BERTâ€‘style) capture bidirectional context but are not designed for openâ€‘ended generation (Section 2.1.1; Eq. (1)).
  - Early DLMs were small or imageâ€‘centric; recent works scale to 7â€“8B parameters and extend to multimodal tasks (Fig. 1; Table 1). However, there was no single, upâ€‘toâ€‘date, endâ€‘toâ€‘end synthesis of principles, models, training/postâ€‘training, inference, and applications.
- Positioning relative to existing work
  - The survey offers: (i) a unified taxonomy spanning continuous/discrete/hybrid paradigms (Fig. 3), (ii) detailed mechanisms for training and inferenceâ€”including caching, parallel decoding, and guidance (Section 4; Fig. 5), (iii) a consolidated performance view across tasks (Section 6; Fig. 6), and (iv) a critical assessment of limitations and future directions (Section 8).

## 3. Technical Approach
This section explains â€œhow DLMs work,â€ organized from foundational formulations to practical training and inference. Citations refer to where each mechanism is defined or exemplified.

- Recap of modern language modeling paradigms (Section 2.1)
  - Masked Language Modeling (MLM): predict randomly masked tokens using surrounding context (Eq. (1)); useful for understanding but not designed for freeform generation.
  - Autoregressive (AR): factorize sequence probability leftâ€‘toâ€‘right (Eq. (3)â€“(4)); great for generation but sequential at inference.
  - Permutation LM (XLNet): trains on random factorization orders (Eq. (5)); captures bidirectional context yet retains AR decoding.

- Continuous-space DLMs (Section 2.2; Eqs. (6)â€“(9))
  - Idea in plain language: map tokens to continuous vectors (â€œembeddingsâ€), repeatedly â€œadd noiseâ€ to training data, then learn to â€œremove noiseâ€ stepâ€‘byâ€‘step; at test time, start from noise and denoise back to a clean embedding sequence; finally â€œroundâ€ embeddings to tokens.
  - Forward (noising) process: a Markov chain gradually corrupts clean embeddings x0 into xt (Eq. (6)â€“(8)); Î±t, bt control how much of x0 vs. Gaussian noise appears at time t.
  - Reverse (denoising) process: a Transformer fÎ¸(xt, t) predicts a target (e.g., noise or clean data) and is trained by a simple regression loss (Eq. (9)).
  - Decoding to tokens: after denoising, map the final embeddings back to words by nearest neighbor or a classifier head.
  - Representative mechanisms
    - Classifier-free guidance for controllable generation (adopted from image diffusion; Section 4.3; Eq. (13)).
    - Continuous logitâ€‘space diffusion (TESS/TESSâ€‘2) that diffuses over probability simplices rather than embeddings (Section 2.2).

- Discrete-space DLMs (Section 2.3; Eq. (10))
  - Idea in plain language: operate directly on tokens. The forward process replaces some tokens with a special `[MASK]` symbol; the model learns to recover original tokens from partially masked sequences.
  - Training objective: compute crossâ€‘entropy only on positions that are masked at time t (Eq. (10)); t is sampled, and xt is produced by randomly masking x0.
  - Inference by iterative maskâ€‘predict (Fig. 4 middle-right; Section 2.3; Section 4.2)
    - Start with a fully masked response of desired length.
    - At each step: predict all positions; â€œunmaskâ€ highâ€‘confidence tokens; â€œremaskâ€ lowâ€‘confidence or uncertain spans; repeat until no masks remain.
  - Key design levers: noise schedules (how many positions are masked at each step), unmask/remask policies (thresholds, confidence ranking), and acceptance rules (Section 4.1â€“4.2).

- Hybrid ARâ€“Diffusion (blockâ€‘wise) models (Section 2.4; Eq. (11); Fig. 4 bottom)
  - Idea: generate â€œblocksâ€ of tokens autoregressively to preserve longâ€‘range dependency, but fill tokens within each block in parallel via diffusion steps.
  - Objective (BD3â€‘LM, Eq. (11)): sum denoising losses over blocks b=1..B, conditioning each block on previous blocks x< b while denoising masked positions inside the current block xb.
  - Benefit: enables effective Keyâ€“Value (KV) caching for finished blocks and variableâ€‘length outputs while keeping intraâ€‘block parallelism (Sections 2.4, 4.4).

- Postâ€‘training to elicit reasoning (Section 3.2; Tables 2)
  - Challenge: AR RL methods use exact sequence logâ€‘likelihood; DLM likelihood is intractable because generation is iterative and nonâ€‘factorized.
  - Solutions summarized and mechanized:
    - Diffusionâ€‘ofâ€‘Thought (DoT): fineâ€‘tune on reasoning traces but refine them in parallel through denoising; uses scheduled/coupled sampling to expose model to its own mistakes (Section 3.2.1).
    - DCoLT: treat each denoising step as a â€œlatent thinkingâ€ action and optimize the whole trajectory by outcomeâ€‘based RL; adds an Unmasking Policy Module to decide reveal order (Section 3.2.1).
    - SEPO: derive stable policyâ€‘gradient updates by importanceâ€‘sampling the â€œscore entropy,â€ enabling PPO/GRPOâ€‘like updates for discrete diffusion (objective in Eq. (12); Section 3.2.2).
    - diffuâ€‘GRPO (d1) and coupledâ€‘GRPO (DiffuCoder): estimate sequence/perâ€‘token logâ€‘probabilities via masked forward passes; use complementary masking to reduce variance (Section 3.2.2).
    - VRPO (LLaDA 1.5): varianceâ€‘reduced preference optimization for DPOâ€‘style training in DLMs using optimal sampling across timesteps and antithetic sampling against a reference policy (Section 3.2.3).

- Inference accelerations and controls (Section 4; Fig. 5)
  - Parallel decoding: accept many tokens per step based on confidence or auxiliary checks; e.g., Fastâ€‘dLLM thresholds (Section 4.1).
  - Unmasking/remasking: revisit previously accepted tokens to improve quality (ReMDM), or schedule slowâ€‘thenâ€‘fast acceptance (Section 4.2).
  - Guidance: classifierâ€‘free guidance (CFG) combines conditioned and unconditioned scores to push outputs toward a prompt with tunable strength Î» (Eq. (13); Section 4.3).
  - Caches and step distillation (Section 4.4):
    - KV caches adapted for semiâ€‘AR/block schedules or with delayed commits (Fastâ€‘dLLM DualCache; dKVâ€‘Cache).
    - Feature caches reuse stable intermediate activations across steps (dLLMâ€‘Cache; FreeCache).
    - Step distillation collapses many denoising steps into a few or even one (Di4C for discrete; DLMâ€‘One reports oneâ€‘step continuous generation).

- Multimodal DLMs (Section 5)
  - Visionâ€‘conditioned DLMs: plug a vision encoder and project features to the text space (LLaDAâ€‘V, LaViDa, Dimple).
  - Unified token spaces: tokenize images as discrete codes (VQâ€‘VAE) and train a single diffusion Transformer for both text and image inputs/outputs (MMaDA, UniDisc, Muddit; Section 5).

## 4. Key Insights and Innovations
- A unified taxonomy and timeline that clarifies the DLM landscape
  - Fig. 3 organizes paradigms (continuous, discrete, hybrid), training/postâ€‘training, inference, and applications. Fig. 1 and Fig. 2 trace how early continuous models gave way to large discrete and multimodal DLMs. This consolidation is significant for orienting researchers in a rapidly evolving area.

- Mechanistic, sideâ€‘byâ€‘side depiction of training/inference across paradigms
  - Fig. 4 contrasts AR training with discrete/continuous diffusion and hybrid block diffusion, making the generative loops concrete (masking strategies, attention patterns). This helps practitioners reason about engineering tradeâ€‘offs (e.g., feasibility of caching).

- Systematization of DLMâ€‘specific inference tooling
  - Section 4 and Fig. 5 synthesize a toolkitâ€”parallel decoding, remasking, guidance, KV/feature caches, step distillationâ€”showing how each piece slots into the iterative denoising loop. The survey highlights where large speedups are achieved (e.g., â€œup to 27Ã—â€ from Fastâ€‘dLLM; â€œup to 34Ã—â€ with SlowFast+cache; â€œup to 500Ã—â€ with oneâ€‘step distillation) and what is traded off.

- Clear articulation of the â€œParallel Decoding Curseâ€ with concrete evidence
  - Section 8.1 explains and Fig. 7 visualizes how aggressively accepting many tokens per step can break global consistency (e.g., math problems solved correctly only when unmasking 1â€“2 tokens per step; wrong or incoherent answers with fewer steps). This frames a central open problem unique to DLMs.

- Bridge from ARâ€‘style reasoning/RL to diffusion
  - Section 3.2 and Table 2 collect and explain how CoT, PPO/GRPO, and DPOâ€‘like methods are reâ€‘engineered for DLMs (e.g., SEPO objective in Eq. (12), VRPO sampling design). This is a conceptual advance that opens the door to robust reasoningâ€‘aligned DLMs.

## 5. Experimental Analysis
- Evaluation setup as compiled by the survey (Section 6; Fig. 6)
  - Benchmarks cover general language understanding (PIQA, HellaSwag), mathematical reasoning (GSM8K), code generation (HumanEval), and multimodal understanding/generation (GenEval, MME, MMMU, GQA).
  - Comparisons are sizeâ€‘matched where possible (from <1B to â‰ˆ8B parameters), with AR baselines like LLaMAâ€‘2/3, Qwen, Mistral shown alongside DLMs such as LLaDA, Dream, DiffuLLaMA, DiffuCoder, MMaDA, Dimple, and unified models (Fig. 6).

- Main quantitative takeaways (Fig. 6; Section 6; Table 1 for model context)
  - General language understanding: DLMs like `LLaDA-8B` are â€œslightly below or on parâ€ with similarâ€‘sized AR models on PIQA/HellaSwag.
  - Math/Science: DLMs often shine. The survey notes consistent gains on GSM8K for `LLaDA`/`Dream`, especially after reasoning postâ€‘training (Section 3.2; Fig. 6).
  - Code: `DiffuCoder-7B` is competitive on HumanEval among openâ€‘source models; `Mercury Coder` (industrial) reports ARâ€‘beating throughput â€œup to 10Ã—â€ at comparable quality (Sections 5, 7.2; Table 1).
  - Multimodal: `MMaDA` and `LLaDAâ€‘V` outperform several ARâ€‘VLMs on understanding (MME, GQA) and achieve strong T2I/I2T quality; unified discrete diffusion (`UniDisc`, `Muddit`) also performs competitively (Section 5; Fig. 6).

- Acceleration evidence (Section 4)
  - Parallel decoding and caches:
    - â€œUp to 27.6Ã— speed-upsâ€ (Fastâ€‘dLLM; Section 4.1).
    - â€œUp to 34Ã— accelerationâ€ with SlowFast sampling plus caching (Section 4.1).
    - KV caches tailored to DLMs (dKVâ€‘Cache): â€œ2â€“10Ã— speed-upsâ€ with negligible quality drop (Section 4.4).
  - Step distillation:
    - `Di4C` compresses discrete diffusion to 4â€“10 steps at â‰ˆteacher quality (~2Ã— faster; Section 4.4).
    - `DLMâ€‘One` trains a oneâ€‘step continuous DLM achieving â€œup to 500Ã— accelerationâ€ with nearâ€‘teacher quality (Section 4.4).

- Ablations and robustness highlighted by the survey
  - Unmasking/remasking policies matter: confidenceâ€‘aware acceptance and revisiting tokens (ReMDM) improve quality/compute tradeâ€‘offs (Section 4.2).
  - Parallelismâ€“quality tradeâ€‘off is real and visual: Fig. 7 shows correct math reasoning only under conservative perâ€‘step acceptance; aggressive parallelism induces inconsistencies (Section 8.1).
  - Trainingâ€“inference discrepancy remedies: twoâ€‘step diffusion or schedule tweaks improve discrete DLMsâ€™ inference behavior (Section 3.1).

- Do the results support the claims?
  - The aggregated plots (Fig. 6) and concrete speedup reports (Section 4) credibly support that DLMs are now competitive with similarly sized AR models on many tasks while unlocking strong parallelâ€‘time accelerationâ€”conditional on careful decoding/caching and, for the largest speedups, step distillation.

## 6. Limitations and Trade-offs
- Parallelism vs. coherence (â€œParallel Decoding Curseâ€) (Section 8.1; Fig. 7)
  - Accepting many tokens per step ignores interâ€‘position dependencies; even if each local prediction is highâ€‘probability, the joint sequence can be inconsistent (toy ABAB example in Section 8.1). Fig. 7 confirms quality drops when steps are too few or acceptance too aggressive.

- Infrastructure maturity (Section 8.1)
  - Unlike AR LLMs (with widely used serving stacks like HuggingFace Transformers and vLLM), DLMs lack standardized, optimized openâ€‘source serving infrastructure, complicating deployment.

- Long sequences and dynamic lengths (Section 8.1)
  - Many DLMs are trained for fixed context windows (often â‰¤4k). Extrapolation techniques common in AR models are underexplored. Inference complexity can be cubic in length: O(N^2) per step with full attention times O(N) steps â‰ˆ O(N^3), unless mitigated by KVâ€‘cache or block designs.

- Scalability and data (Section 8.1)
  - The largest open DLMs are â‰ˆ8B parameters; todayâ€™s strongest AR systems scale to tens/hundreds of billions or MoE trillions. Many highâ€‘performing DLMs are adapted from AR checkpoints (Table 1), leaving â€œfromâ€‘scratch at scaleâ€ relatively untested.

- Assumptions and edge cases
  - Discrete DLMs often assume a good mask schedule and reliable confidence calibration for acceptance. Continuous DLMs require robust rounding from embeddings/logits to tokens; mismatch can harm fluency (Section 2.2â€“2.3).
  - Reasoning RL for DLMs relies on approximate likelihood estimates (e.g., masked forward passes); bias/variance tradeâ€‘offs remain (Section 3.2).

## 7. Implications and Future Directions
- How this work changes the landscape
  - By clarifying mechanisms and limits sideâ€‘byâ€‘side (Figs. 3â€“5), the survey makes DLM design choices tractable: when to prefer discrete vs. continuous, how to mix AR and diffusion, and which inference tools deliver real speed at acceptable quality.
  - It surfaces a coherent agenda for making DLMs practically competitive at scale: robust caching, principled acceptance/remasking, and distillation pathways (Sections 4, 8).

- Followâ€‘up research enabled or suggested (Section 8.2)
  - Training efficiency: improve token utilization (e.g., complementary masking as in LaViDa, Section 3.1) and reduce trainâ€“inference mismatch.
  - Lowâ€‘bit and sparsity: quantization/binarization and pruning for DLMs are largely open, with large upside for latency and memory.
  - Compression: distillation for fewer steps or smaller students (Di4C; DLMâ€‘One) and hybridization with AR for cacheâ€‘friendly decoding.
  - Unified multimodal reasoning: expand models like MMaDA/UniDisc to reason across textâ€“image jointly with RLâ€‘aligned objectives (Section 5).
  - DLMâ€‘based agents: exploit iterative refinement and bidirectionality for tool use, planning, and selfâ€‘correction loops.

- Practical applications
  - Highâ€‘throughput code generation (Mercury Coder; DiffuCoder) where parallel revision is handy (Sections 5, 7.2).
  - Structured editing, infilling, and constrained generation via remasking and DFAâ€‘based control (DINGO; Section 4.3).
  - Unified multimodal systems capable of both understanding and generation, with controllable formats and fewer modalityâ€‘specific components (MMaDA, UniDisc, Muddit; Section 5).

> In sum, the survey shows that DLMs have moved from small proofsâ€‘ofâ€‘concept to 7â€“8Bâ€‘scale models competitive with AR peers on several fronts, especially math and multimodal tasks (Section 6, Fig. 6), while offering inference speedups from smart parallel decoding, caching, and distillation (Section 4). The remaining bottlenecksâ€”coherence under high parallelism, longâ€‘context scaling, and ecosystem maturityâ€”define the next phase of research (Section 8).

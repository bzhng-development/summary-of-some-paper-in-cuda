# Segment Anything

**ArXiv:** [2304.02643](https://arxiv.org/abs/2304.02643)

## ğŸ¯ Pitch

Segment Anything introduces the revolutionary Segment Anything Model (SAM), a foundation model for image segmentation that can generate high-quality object masks from a wide variety of promptsâ€”including points, boxes, masks, and even textâ€”without task-specific training. By leveraging a model-in-the-loop data engine, the authors created SA-1B, the largest segmentation dataset ever (1.1 billion masks over 11 million images), enabling SAM to perform impressive zero-shot transfer across numerous vision tasks. This work establishes a new paradigm for segmentation, unlocking practical, generalized, and reusable segmentation capabilities that significantly lower the barrier for computer vision applications in diverse, real-world domains.

---

## 1. Executive Summary
Segment Anything introduces a new way to do image segmentation called promptable segmentation and a model, `SAM`, that returns highâ€‘quality masks for virtually any prompt (points, boxes, masks, and even text). Using a modelâ€‘inâ€‘theâ€‘loop â€œdata engine,â€ it builds the largest segmentation dataset to date, `SAâ€‘1B` (11M images, 1.1B masks), enabling strong zeroâ€‘shot transfer to many tasks (Section 1; Fig. 1).

## 2. Context and Motivation
- Problem addressed
  - Traditional segmentation models are trained for fixed label sets and tasks (semantic, instance, panoptic) and typically require taskâ€‘specific training and data. There was no â€œfoundationâ€ segmentation model that could generalize to new data distributions and tasks via lightweight prompting (Section 1).
  - There is no webâ€‘scale source of segmentation masks to train such a general model; existing datasets are two to three orders of magnitude smaller (Fig. 6).
- Why it matters
  - A single, reusable segmentation component that can be prompted for many goals (e.g., segment an object from a single point or box; generate proposals; support edge detection) reduces the need for taskâ€‘specific models and labels, improving practicality for new domains where annotated data is scarce (Sections 1â€“2).
- Prior approaches and gaps
  - Interactive segmentation methods can refine a mask from clicks but are not designed to be composed inside larger systems, nor to always output a valid mask after any promptâ€”including ambiguous ones (Section 2).
  - Multiâ€‘task segmentation systems (e.g., joint semantic/instance/panoptic) still assume the test task matches the training task set (Section 2, â€œRelated tasksâ€).
  - Visionâ€‘language pretraining (e.g., CLIP) shows zeroâ€‘shot capabilities for classification, but segmentation problems need detailed perâ€‘pixel masks and lack abundant training masks (Section 1).
- Positioning
  - This paper proposes promptable segmentation as the pretraining task and uses it to build a general, composable segmentation component. It contributes a model (`SAM`) and the data engine that collected `SAâ€‘1B`, enabling zeroâ€‘shot transfer across tasks and image domains (Fig. 1; Sections 2â€“5).

## 3. Technical Approach
The work consists of three tightly connected parts: a task, a model, and a data engine.

- Task: `promptable segmentation` (Section 2; Fig. 1a)
  - Goal: Given a prompt that specifies â€œwhat to segment,â€ return a valid segmentation mask.
  - Prompts can be sparse (points, boxes, text) or dense (masks). A â€œvalidâ€ mask means that even if the prompt is ambiguous (e.g., a point on a shirt could refer to the shirt or the person), the output should be a reasonable mask for at least one valid object (Fig. 3).
  - Why: This task encourages generality and composability; downstream tasks become â€œprompt engineeringâ€ problems (Section 2, â€œZeroâ€‘shot transferâ€).

- Model: `SAM` (Section 3; Fig. 4)
  - Highâ€‘level flow
    1. An `image encoder` computes a oneâ€‘time, highâ€‘resolution embedding of the image.
    2. A `prompt encoder` embeds the user prompt (points/boxes/text/masks).
    3. A fast `mask decoder` fuses image and prompt embeddings to produce one or more masks and a confidence score for each.
  - Key design choices and how they work
    - Image encoder: A Vision Transformer (ViT, MAEâ€‘pretrained) adapted for highâ€‘res inputs (1024Ã—1024), producing a 16Ã— downscaled embedding (64Ã—64Ã—C). A 1Ã—1 conv then 3Ã—3 conv reduce channels to 256 with layer norms (Appendix Â§A â€œImage encoderâ€).
      - Rationale: Heavy computation is done once; subsequent prompts reuse the embedding, enabling amortized realâ€‘time interaction (~50 ms per prompt on CPU in browser; Section 3, â€œEfficiencyâ€).
    - Prompt encoder: 
      - Points/boxes: add a positional encoding of coordinates to a learned embedding that marks point type (foreground/background) or box corner (topâ€‘left/bottomâ€‘right) (Appendix Â§A â€œPrompt encoderâ€).
      - Dense mask prompts: downsample and embed via small convs and add elementâ€‘wise to the image embedding.
      - Text: use CLIPâ€™s text encoder to produce a text embedding (Section 3 â€œPrompt encoderâ€).
    - Mask decoder (lightweight Transformer; Fig. 14)
      - Two decoder layers with:
        - Token selfâ€‘attention (over prompt tokens),
        - Tokenâ€‘toâ€‘image crossâ€‘attention,
        - Perâ€‘token MLP,
        - Imageâ€‘toâ€‘token crossâ€‘attention (updates image embedding with prompt information).
      - After decoding: upsample image embedding by 4Ã— with transposed convs; an MLP maps a learned `output token` to a dynamic linear classifier that predicts the perâ€‘pixel foreground probability by dotâ€‘product with the upsampled embedding (Appendix Â§A â€œLightweight mask decoderâ€).
      - Positional encodings are added wherever the image embedding attends; the original prompt tokens are reâ€‘added at attention layers to keep strong geometric grounding (Appendix Â§A).
    - Ambiguity handling: predict multiple masks per prompt (default 3), using different `output tokens`. During training, compute loss against each candidate and backprop only the minimum (â€œminimum lossâ€ or â€œmultiple choiceâ€ training; Section 3 â€œResolving ambiguityâ€). A small head predicts an `IoU` score (estimated overlap with the true mask) to rank the candidates.
        - Definition: `IoU` (Intersectionâ€‘overâ€‘Union) measures overlap between prediction and ground truth: area of intersection divided by area of union. `mIoU` is the mean across examples.
    - Training objective and schedule (Section 3 â€œLosses and trainingâ€; Appendix Â§A)
      - Mask loss: focal loss + dice loss (20:1 weight) on the predicted mask logits.
      - IoU head: mean squared error between predicted IoU and true IoU of the chosen mask.
      - Interactive simulation: train in 11 â€œroundsâ€ per maskâ€”start with a point or box, then sample next points from the error region between prediction and ground truth, and feed back the previous mask logits as an additional prompt (Section 3; Appendix Â§A â€œTraining algorithmâ€).
      - Initialization: MAEâ€‘pretrained ViT; AdamW optimizer with learningâ€‘rate warmup and steps; large batch and distributed training (Appendix Â§A â€œTraining recipeâ€).

- Data engine to build `SAâ€‘1B` (Section 4; Fig. 1c)
  - Motivation: masks arenâ€™t available on the web at scale. The data engine iteratively improves the model while using it to accelerate or automate labeling.
  - Three stages (Sections 4; B for details):
    1. Assistedâ€‘manual: professional annotators click foreground/background points; SAM runs inâ€‘browser at ~50 ms/prompt using precomputed embeddings. Over iterations, SAM is retrained on collected masks; annotation time dropped from 34 s to 14 s per mask; 4.3M masks on 120k images were collected (Section 4 â€œAssistedâ€‘manual stageâ€).
    2. Semiâ€‘automatic: a generic box detector (trained on stageâ€‘1 masks as â€œobjectâ€) preâ€‘fills confident masks; annotators focus on missing/difficult objects; 5.9M additional masks on 180k images were collected (Section 4 â€œSemiâ€‘automatic stageâ€).
    3. Fully automatic: SAM is prompted with a 32Ã—32 grid of points (also overlapping zoomed crops) and returns multiple masks per point; stable, confident masks are kept; duplicates are filtered with `NMS` (nonâ€‘maximum suppression removes overlapping duplicates). This produced 1.1B masks across 11M images (Sections 4 â€œFully automatic stageâ€; B â€œCropping, Filtering, Postprocessingâ€).
       - â€œStable maskâ€ heuristic: keep a mask only if thresholding its probability map at 0.5âˆ’Î´ and 0.5+Î´ yields very similar binary masks (IoU â‰¥ 95.0; Appendix Â§B).
       - Confidence filter: keep masks whose predicted IoU â‰¥ 88.0 (Appendix Â§B).
       - NMS threshold: 0.7 within and across crops (Appendix Â§B).

## 4. Key Insights and Innovations
- Promptable segmentation as a pretraining target (Section 2)
  - Novelty: train the model to respond to arbitrary prompts with a valid mask, rather than to a fixed task definition or label set.
  - Significance: makes the model composableâ€”downstream tasks are solved by prompting rather than retraining (e.g., â€œbox â†’ instance mask,â€ â€œsingle point â†’ object,â€ â€œgrid of points â†’ segment everythingâ€).
- Ambiguityâ€‘aware multiâ€‘mask prediction (Section 3 â€œResolving ambiguityâ€; Fig. 3)
  - Novelty: instead of forcing a single answer to an ambiguous prompt, predict a small set of plausible masks and rank them with an IoU head.
  - Significance: boosts singleâ€‘point performance and realism; with an â€œoracleâ€ choice among the three, SAM surpasses prior interactive methods across all 23 datasets (Fig. 9a).
- Lightweight, bidirectional decoder with amortized computation (Section 3; Fig. 4, Fig. 14)
  - Novelty: crossâ€‘attention in both directions (tokenâ†’image and imageâ†’token) to fuse prompts and image features; heavy encoding computed once; prompt processing is fast (~50 ms on CPU).
  - Significance: supports realâ€‘time interactive use and scalable, automated mask generation in browsers and pipelines.
- Scalable data engine and SAâ€‘1B (Sections 4â€“5; Figs. 2, 5â€“7)
  - Novelty: threeâ€‘stage, modelâ€‘inâ€‘theâ€‘loop pipeline that ends in fully automatic, ambiguityâ€‘aware mask generation with quality filters and NMS.
  - Significance: `SAâ€‘1B` offers 1.1B highâ€‘quality masks on 11M imagesâ€”about 400Ã— more masks than previous largest dataset (Open Images; Fig. 6 legend). Human verification shows 94% of automatic masks have â‰¥90% IoU with professionally corrected versions (Section 5 â€œMask qualityâ€).
- Textâ€‘prompting without text supervision via CLIP alignment (Section 7.5)
  - Novelty: during training, replace text with CLIP image embeddings of the masked region; at inference, feed CLIP text embeddings instead (both are aligned by CLIP).
  - Significance: enables early â€œtextâ€‘toâ€‘maskâ€ capability without assembling text annotations (Fig. 12).

## 5. Experimental Analysis
- Evaluation setup (Sections 7; D)
  - Datasets
    - A suite of 23 segmentation datasets spanning many domains (egocentric, underwater, Xâ€‘ray, aerial, simulation, etc.; Fig. 8; Table 7).
    - SAâ€‘1B analysis includes geographic distribution and representation (Fig. 7; Table 1).
  - Metrics (defined where needed)
    - `mIoU`: mean IoU across objects.
    - `AP`: average precision for instance segmentation (COCO, LVIS).
    - `AR@1000`: average recall using 1000 proposals per image (LVIS; object proposals).
    - Edge detection: ODS, OIS, AP, R50 on BSDS500 (Section 7.2; Table 3). ODS/OIS are dataset-/imageâ€‘level Fâ€‘score optima; `R50` is recall at 50% precision.
  - Baselines
    - Interactive segmentation: RITM, FocalClick, SimpleClick (Section 7.1; D.1).
    - Instance segmentation and proposals: ViTDetâ€‘H with Cascade Mask Râ€‘CNN (Sections 7.3â€“7.4).
  - Human ratings protocol
    - Professional annotators rate single masks on a 1â€“10 quality scale using multiâ€‘view panels, with guidance and QA (Appendix Â§E, Figs. 19â€“20). Rating distributions shown in Fig. 11 and Fig. 18; statistical significance in Table 8.

- Main results
  - Singleâ€‘point promptable segmentation across 23 datasets (Section 7.1)
    - Quantitative (Fig. 9a):
      - SAM beats RITM on 16/23 datasets with the modelâ€™s topâ€‘ranked mask. With an oracle selecting the best of SAMâ€™s three masks, it outperforms RITM on all 23 datasets.
    - Human quality ratings (Fig. 9b; Table 8):
      - On seven diverse datasets, SAMâ€™s masks receive substantially higher ratings than RITM. The differences are statistically significant:
        > Table 8 shows pâ€‘values â‰¤ 7eâ€‘26 across datasets and 99% confidence intervals for mean score differences that exclude zero (e.g., on LVIS v0.5, improvement CI99 is (1.40, 1.84)).
    - More points (Figs. 9câ€“9d):
      - With 1â€“9 points, SAM dominates at 1 point and remains competitive as points increase; gaps shrink as the task becomes easier. Under random point sampling, SAMâ€™s advantage grows further (Fig. 9d).
  - Zeroâ€‘shot edge detection (Section 7.2; Fig. 10; Table 3)
    - Pipeline: regular point grid â†’ many masks â†’ Sobel gradients of mask probability maps â†’ edge NMS (D.2).
    - Performance on BSDS500:
      > Table 3: SAM ODS .768, OIS .786, AP .794, R50 .928.
      - Not SOTA vs. trained edge detectors (e.g., EDTER ODS .840), but far ahead of classical zeroâ€‘shot edges (e.g., Canny ODS .600).
  - Zeroâ€‘shot object proposals (Section 7.3; Table 4)
    - Setup: generate masks with a dense point grid and NMS; evaluate AR@1000 on LVIS v1 (D.3).
    - Results:
      > Table 4: SAM overall AR@1000 59.3 vs. ViTDetâ€‘H 63.0; for medium objects 81.6 vs. 80.8 (SAM higher); for rare/common categories 65.8/63.9 vs. 58.3/63.3 (SAM higher); it trails on small objects (45.5 vs. 51.7) and frequent categories (59.1 vs. 63.1).
      - Multiâ€‘mask ability matters: a singleâ€‘output ablation drops overall AR to 54.9.
  - Zeroâ€‘shot instance segmentation via boxes (Section 7.4; Table 5; Fig. 16â€“11)
    - Setup: use ViTDet boxes as prompts; SAM predicts the mask (D.4).
    - AP:
      > Table 5: On COCO, SAM 46.5 AP vs. ViTDetâ€‘H 51.0; on LVIS v1, SAM 44.7 vs. 46.6.
    - Human ratings on LVIS boxes (Fig. 11):
      > SAM mean 8.1 Â± 0.07 vs. ViTDetâ€‘H 7.9 Â± 0.08; LVIS GT 8.6 Â± 0.06; COCO GT 7.6 Â± 0.12.
      - Interpretation: SAMâ€™s masks are often visually sharper and more faithful (Fig. 16), while ViTDet benefits from training on datasetâ€‘specific mask biases (e.g., LVIS polygon rules).
  - Textâ€‘toâ€‘mask (Section 7.5; Fig. 12; D.5)
    - Training trick: Replace text with CLIP image embeddings of the target region; at inference, use CLIP text embeddings (both are aligned).
    - Qualitative results show success on simple phrases (â€œa wheelâ€) and more nuanced ones (â€œbeaver tooth grilleâ€); when text alone is ambiguous, adding a point disambiguates (Fig. 12).
  - Ablations and scaling (Section 7.6; Fig. 13)
    - Data engine stages: using only fully automatic masks is within ~0.5 mIoU of using all stages (Fig. 13 left), simplifying training.
    - Data volume: training with ~1M images (~10% of SAâ€‘1B, â‰ˆ100M masks) reaches performance comparable to full 11M (Fig. 13 middle); 0.1M images degrades notably.
    - Model scale: ViTâ€‘H improves substantially over ViTâ€‘B; gains from ViTâ€‘L to ViTâ€‘H are marginal (Fig. 13 right).
  - Responsible AI analysis (Section 6; Table 1; Table 2; Fig. 7; C)
    - Dataset geography and income (Table 1; Fig. 7):
      > SAâ€‘1B has higher proportions from Europe and Asia & Oceania and more middleâ€‘income countries vs. COCO/Open Images; all regions have â‰¥28M masks.
    - People segmentation fairness (Table 2):
      > mIoU with 1 point is similar across perceived gender presentation (feminine 54.4, masculine 55.7) and across skin tones (range ~51.5â€“56.7); with 3 points, all groups are ~90â€“92 mIoU.
    - Clothing segmentation shows a gap by perceived gender presentation at 1 point (masculine higher), which narrows at 3 points (Appendix Table 6).

- Do the experiments support the claims?
  - The 23â€‘dataset study with both automatic metrics and human ratings (Figs. 9aâ€“b) strongly supports the core claim that SAM returns valid, highâ€‘quality masks from minimal prompts and transfers zeroâ€‘shot across domains. The proposal and edge experiments demonstrate composability beyond the training task. Human studies explain AP gaps by dataset annotation biases (Fig. 11 with Table 5).

## 6. Limitations and Trade-offs
- Assumptions and scope (Section 8 â€œLimitationsâ€)
  - `Promptable segmentation` promises a valid mask for a prompt; it does not enforce semantic consistency across an image (e.g., panoptic labeling) nor provide closedâ€‘vocabulary semantics unless combined with other modules.
  - Designed for generality and speed of prompting, not for maximizing IoU after many clicks; specialized interactive segmenters can surpass SAM in that regime (Section 8; Figs. 9câ€“d).
- Quality limitations (Section 8)
  - Misses fine structures and can hallucinate small disconnected components; boundaries are not as crisp as â€œzoomâ€‘inâ€ methods that spend computation per region (e.g., FocalClick; see also Fig. 16 examples).
- Computational constraints
  - The image encoder is heavy (ViTâ€‘H); while prompt processing is ~50 ms on CPU (Section 3 â€œEfficiencyâ€), endâ€‘toâ€‘end performance depends on the oneâ€‘time image embedding.
- Text prompting
  - The textâ€‘toâ€‘mask pathway is preliminary; it relies on CLIP alignment and shows qualitative promise but lacks rigorous quantitative evaluation (Section 7.5).
- Dataset biases
  - Despite geographic improvements over prior datasets, regions like Africa and lowâ€‘income countries remain underrepresented relative to Europe and North America (Table 1). Clothing segmentation shows a genderâ€‘presentation gap at 1 point (Appendix Table 6).
- Automatic mask generation heuristics
  - Fully automatic stage depends on stability thresholds, IoU prediction accuracy, and NMS settings (Appendix Â§B). While human audits are strong (94% â‰¥90% IoU; Section 5), heuristic choices may still bias which masks are kept.

## 7. Implications and Future Directions
- How this changes the landscape
  - Establishes segmentation as a `promptable` capability akin to language prompting and CLIP zeroâ€‘shot classificationâ€”one reusable model becomes a versatile component across tasks (Section 2 Discussion; Section 8 â€œCompositionalityâ€).
  - SAâ€‘1B provides a new pretraining substrate for segmentation research at scale (Section 5). This can catalyze new foundation models that go beyond SAM.
- Followâ€‘up research directions
  - Richer prompts and interfaces: combine text, gaze, gestures, and multiple context masks to reduce ambiguity; better multiâ€‘mask ranking and uncertainty estimation.
  - Toward semantic/panoptic outputs via prompting: design prompt sets or composition strategies that induce consistent sceneâ€‘level labeling without retraining.
  - Improved fineâ€‘structure and boundary quality: integrate efficient â€œzoomâ€‘inâ€ refinement while keeping amortized cost low (Section 8).
  - Stronger textâ€‘toâ€‘mask: explicit language grounding, multiâ€‘modal pretraining with paired textâ€“mask supervision, or improved use of CLIPâ€‘like alignment (Section 7.5).
  - Fairness and coverage: targeted data enrichment for underrepresented regions/attributes; taskâ€‘specific audits when composing SAM into larger systems (Section 6).
- Practical applications
  - Rapid dataset labeling and bootstrapping segmentation in new domains (Section 4; fully automatic stage).
  - Plugâ€‘andâ€‘play component in perception stacks: instance segmentation from boxes, edge maps, and object proposals (Sections 7.2â€“7.4).
  - Humanâ€‘inâ€‘theâ€‘loop tools in browsers with realâ€‘time feedback; accessibility tools where users indicate objects by pointing, speaking, or looking (Section 3 â€œEfficiencyâ€, Section 8 â€œCompositionalityâ€).

In short, Segment Anything reframes segmentation as a general prompting problem and demonstrates that a single, ambiguityâ€‘aware model trained on SAâ€‘1B can transfer zeroâ€‘shot to a wide range of tasks and distributions, with strong humanâ€‘perceived quality. The methodâ€™s scalability and composability are its primary strengths; crispness on fine structures, text grounding, and certain fairness/coverage aspects remain open areas for improvement.

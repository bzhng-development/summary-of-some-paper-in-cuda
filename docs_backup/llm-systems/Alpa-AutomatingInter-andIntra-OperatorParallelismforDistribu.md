# Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning

**ArXiv:** [2201.12023](https://arxiv.org/abs/2201.12023)

## ğŸ¯ Pitch

Alpa presents a novel compiler and runtime that fully automates the discovery and orchestration of efficient distributed training plans for large-scale deep learning models, unifying data, operator, and pipeline parallelism. By hierarchically decomposing the problem into intra-operator (tensor/weight sharding) and inter-operator (pipeline/model slicing) levels and optimizing each with tractable algorithms, Alpa eliminates the need for manual, expert-intensive parallelizationâ€”enabling scalable, out-of-the-box training of massive models across heterogeneous GPU/TPU clusters. This breakthrough accelerates both machine learning research and deployment by removing a pervasive bottleneck in scaling up model size and cluster utilization.

---

## 1. Executive Summary (2-3 sentences)
Alpa is a compiler and runtime that automatically finds and executes efficient distributed training plans for very large deep learning models by jointly combining data, operator/tensor, and pipeline parallelism. Its key idea is to recast the planning problem into two hierarchical levelsâ€”â€œintra-operatorâ€ (how to shard each operator across devices) and â€œinter-operatorâ€ (how to slice the model into pipeline stages and map them to device groups)â€”and to optimize each level with tractable algorithms. This matters because it removes much of the hand-engineering needed to train multiâ€‘billionâ€‘parameter models efficiently on GPU/TPU clusters.

## 2. Context and Motivation
- Problem addressed:
  - Efficiently training very large models requires carefully mixing several parallelization techniques: data parallelism, operator/tensor model parallelism, and pipeline parallelism. Choosing the â€œrightâ€ combination for a specific model and cluster can change performance by an order of magnitude, but it is hard and brittle to do by hand (Â§1, Â§2.1).
- Why itâ€™s important:
  - State-of-the-art models (e.g., large language models, Mixtureâ€‘ofâ€‘Experts) are too big for single devices and too complex for a single parallelization strategy. Automating this planning lowers the barrier for researchers and practitioners to train large models, accelerates iteration, and improves hardware utilization (Â§1).
- Prior approaches and gaps:
  - Hand-tuned systems (e.g., Megatron-LMâ€™s â€œ3D parallelismâ€) prescribe a limited plan for specific model families and cluster types; they often assume repeated layers and fixed pipeline splits (Â§2.1, Fig. 1b).
  - Auto-parallel systems typically optimize only one dimension: intra-operator sharding (e.g., Tofu) or pipeline placement (e.g., DAPPLE), missing cross-technique synergies (Â§2.1, Fig. 1câ€“d).
  - Some solutions rely on strong assumptions (uniform models/layers, preassigned devices), or do not scale to large graphs/clusters (Â§2.1, Â§5.1).
- Positioning:
  - Alpa reframes the search space as a hierarchical composition of inter- and intra-operator parallelisms (Fig. 1e). It introduces optimization passes and a runtime that jointly plan sharding within stages and pipelining across stages on heterogeneous cluster topologies (Â§3, Â§4â€“Â§6).

## 3. Technical Approach
Alpaâ€™s method consists of three major components (Fig. 3): an intra-operator pass, an inter-operator pass, and a runtime orchestration pass. You use a simple decorator (`@parallelize`) around your training step (Fig. 4), and Alpa traces the function, compiles, profiles, and executes the distributed plan automatically.

1) Concepts and definitions (used throughout)
- `Device mesh`: a logical 2D grid view of physical devices (e.g., GPUs) used to express collective communications along mesh axes; different logical layouts (e.g., 4Ã—4 or 1Ã—16) can be considered over the same physical cluster (Â§4.1 â€œDevice meshâ€).
- `Sharding spec`: a short code describing how a tensor is partitioned and replicated across mesh axes, e.g., `RS0` means the first tensor axis is replicated (R), the second is sharded (S) along mesh axis 0 (Table 1).
- `Resharding`: changing a tensorâ€™s sharding layout between operators; this may trigger collectives like allâ€‘gather, allâ€‘reduce, or allâ€‘toâ€‘all (Table 2).
- `SPMD`: single program multiple data; all devices execute the same program on different tensor shards (Â§4).
- `1F1B`: a synchronous pipeline schedule that alternates one forward and one backward microbatch per stage and reduces memory vs. GPipe while keeping latency the same (Â§2.2).

2) Intra-operator pass (how to shard operators within a stage)
- Goal:
  - For every operator (node) in a stageâ€™s computational graph, choose one parallel algorithm (a specific sharding layout and communication pattern) to minimize total time = compute + communications + resharding across edges (Â§4.2).
- How the search space is represented:
  - Each operator has a set of candidate parallel algorithms with known output/input sharding specs and communication costs (e.g., batched matmul alternatives in Table 3). Resharding costs between specs are enumerated (Table 2).
- Cost model and ILP formulation:
  - The objective sums per-operator compute/communication costs (`dv + cv`) and edge resharding costs (`Rvu`) (Eq. (1)). Decision variables select one algorithm per node, and pairwise choices determine resharding; quadratic terms are linearized to fit an ILP (Â§4.2).
  - Compute costs are set to zero in the model for tractability because heavy ops are evenly divided across devices (no redundant compute) and light opsâ€™ compute costs are negligible (Â§4.2).
- Practicalities:
  - Graph simplification merges trivial ops (like element-wise and transpose) into neighbors to shrink the ILP size (Â§4.2).
  - After selecting sharding, Alpa applies communicationâ€‘reducing rewrites (e.g., replacing allâ€‘reduce with reduceâ€‘scatter + allâ€‘gather) to realize weightâ€‘update sharding (ZeRO) where applicable (Â§4.2).

3) Inter-operator pass (how to split into pipeline stages and map to meshes)
- Goal:
  - Given the full model graph and a device cluster, find a slicing into stages and assign each to a submesh such that total pipeline latency is minimized (Â§5.1â€“Â§5.2).
- Latency model:
  - Pipeline latency for `B` microbatches is:
    - â€œFillâ€‘andâ€‘drainâ€ time (sum of stage times) + â€œsteadyâ€‘stateâ€ time for remaining microbatches (bounded by the slowest stage):
    - T* = min over stage/mesh choices of Î£ ti + (B âˆ’ 1) Â· max{ti} (Eq. (2), Fig. 5).
- Device mesh choices:
  - To ensure submeshes tile the full cluster (no idle devices), Alpa restricts candidate submesh shapes to a set that always covers the cluster (proof in Appendix A): (i) 1Ã—(1, 2, 4, â€¦, M) and (ii) (2..N)Ã—M (Â§5.2).
- Dynamic programming (DP) over `tmax`:
  - The pass enumerates a candidate `tmax = max{ti}` (slowest stage time), then computes the minimal total Î£ ti subject to each stage time â‰¤ `tmax`.
  - Subproblem `F(s, k, d; tmax)` = minimal time to slice operators `ok..oK` into `s` stages using `d` devices with stage times â‰¤ `tmax`; recurrence in Eq. (3). The final objective is Eq. (4) (Â§5.2).
- Stage cost queries come from the intra-op pass:
  - For each subgraph candidate (a contiguous operator span) and submesh, the intra-op pass compiles and profiles the stage to get time and memory; the result is only valid if `memstage + s Â· memact â‰¤ memdevice` under the 1F1B schedule (Eq. (5)) (Â§5.2).
- Scaling optimizations:
  - Early pruning of `tmax` enumeration and discretization (Îµâ€‘spacing) keeps DP tractable while bounding suboptimality by `BÂ·Îµ` (Â§5.2 â€œPerformance optimization #1â€).
  - Operator clustering: an auxiliary DP merges neighboring light ops to reduce problem size while controlling perâ€‘layer FLOPs; recurrence in Eq. (6) (Â§5.2 â€œPerformance optimization #2â€).

4) Runtime orchestration (how execution is stitched together)
- Crossâ€‘mesh resharding:
  - Adjacent pipeline stages may use different mesh shapes and sharding specs, so their boundary tensors require manyâ€‘toâ€‘many multicast between meshes (Â§6).
  - Alpa generates a twoâ€‘pass plan: (i) build P2P send/recv between source/destination tiles; (ii) when the destination has replication, rewrite to a single interâ€‘mesh transfer plus a fast intraâ€‘mesh allâ€‘gather (the â€œlocal allâ€‘gatherâ€ optimization), shifting load to highâ€‘bandwidth local links (Fig. 6bâ€“c, Â§6).
- Execution model:
  - The runtime is MPMD (multiple programs, multiple data): each mesh receives a static instruction list for its stage(s), including allocations, compute, interâ€‘stage comms, and sync, avoiding centralized orchestration during steady state (Â§6).

5) Why these design choices?
- Hierarchical split leverages cluster structure: intraâ€‘op sharding prefers highâ€‘bandwidth local links (within nodes), while pipeline stage edges cross lowerâ€‘bandwidth links (across nodes). This maps naturally to typical cluster hierarchies (Â§1â€“Â§3).
- ILP and DP isolate two otherwise entangled problems into tractable subproblems with nearâ€‘optimal local solutions that compose well empirically (Â§3, Â§4, Â§5).
- The reduced submesh set trades negligible optimality loss for guaranteed full coverage and faster search (Â§5.2 and Appendix A).

## 4. Key Insights and Innovations
- Hierarchical parallelism decomposition (fundamental):
  - Reâ€‘categorizes the vast plan space into intraâ€‘operator vs. interâ€‘operator parallelism (Fig. 1câ€“e, Â§2.2). This reframing matches network asymmetry (fast within nodes, slower across nodes), enabling a principled mapping of sharding and pipelining to appropriate links (Â§1â€“Â§3).
- ILP-based auto-sharding for SPMD within stages (significant capability + performance):
  - Formulates operatorâ€‘level sharding as an ILP over enumerated algorithm choices, including resharding edges, with an communicationâ€‘aware cost model (Eq. (1), Table 2â€“3). This unifies data parallelism, ZeRO, and Megatronâ€‘style tensor parallelism under one solver (Â§4.1â€“Â§4.2).
- DP-based joint stage slicing and mesh assignment with profiling (significant capability):
  - Introduces a latencyâ€‘aware DP that simultaneously chooses stage boundaries, mesh shapes, and feasible intraâ€‘op plans (via queries) under memory constraints (Eq. (2)â€“(5), Alg. 1). Operator clustering keeps this tractable for large graphs (Â§5).
- Cross-mesh resharding with â€œlocal all-gatherâ€ (practical systems innovation):
  - Generalizes the equalâ€‘mesh â€œscatterâ€‘gatherâ€ trick beyond identical meshes (Fig. 6aâ€“c). By pushing replication to the destination mesh, it reduces slow crossâ€‘mesh traffic and leverages fast local collectives (Â§6).
- MPMD runtime that composes SPMD intra-op with pipelined inter-op (systems design):
  - Generates static instruction streams per mesh to avoid runtime coordination overheads while accommodating different stage shapes and programs (Â§6).

## 5. Experimental Analysis
- Setup:
  - Cluster: 8Ã— p3.16xlarge nodes (64 GPUs total), NVLink within nodes; 25 Gbps crossâ€‘node bandwidth (Â§8).
  - Models (Table 4): GPTâ€‘3â€‘style LMs up to 39B params (FP16), GShard MoE up to 70B (FP16), Wideâ€‘ResNet up to 13B (FP32).
  - Metric: total PFLOPS over the cluster; weak scaling (model size grows with GPU count), warmâ€‘up then measure; variability <0.5% (Â§8 â€œEvaluation metricsâ€).
  - Baselines: Megatronâ€‘LM for GPT (Â§8.1), DeepSpeed for MoE (Â§8.1), and a PPâ€‘DP baseline (pipeline+data parallel only) for Wideâ€‘ResNet. Also show â€œinterâ€‘op onlyâ€ and â€œintraâ€‘op onlyâ€ using Alpa (Â§8.1).

- Main results (Fig. 7):
  - GPTâ€‘3 (Fig. 7a):
    - Alpa matches or slightly exceeds Megatronâ€‘LM across 1â€“64 GPUs, with nearâ€‘linear or superâ€‘linear weak scaling.
    - Insight: the best Megatronâ€‘LM plans tend to avoid tensor parallelism except when memoryâ€‘bound; Alpa rediscovers similar strategies and additionally shards weight updates (ZeROâ€‘style) inside stages, explaining small gains (Â§8.1).
  - MoE (Fig. 7b):
    - Alpa scales across nodes and outperforms DeepSpeed substantially: 
      > â€œ3.5Ã— speedup on 2 nodes and 9.7Ã— on 4 nodesâ€ (text in Â§8.1 and abstract).
    - Reason: DeepSpeed combines intraâ€‘op techniques (expert parallelism + ZeRO + tensor parallel) but lacks interâ€‘operator pipelining; crossâ€‘node bandwidth becomes the bottleneck. Alpa uses interâ€‘op stages to contain crossâ€‘node traffic (Â§8.1).
  - Wideâ€‘ResNet (Fig. 7c):
    - Large heterogeneous CNN without manual plans: Alpa achieves good scaling; 
      > â€œ80% linear scaling efficiency on 32 GPUsâ€ (Â§8.1 and abstract).
    - Baselines run OOM (PPâ€‘DP and interâ€‘op only) or fail to scale (intraâ€‘op only), highlighting the importance of mixing stageâ€‘level pipelining with selective operator sharding (Â§8.1).

- Ablation studies:
  - Intraâ€‘op search (Fig. 8aâ€“c):
    - ILP (â€œAutoâ€‘shardingâ€) consistently beats heuristic, ZeROâ€‘2/3, and vanilla data parallel; the latter often OOMs. When gradients dominate, ZeRO variants communicate large gradient tensors every step and fall behind (Â§8.2).
  - Interâ€‘op stage DP (Fig. 9):
    - Full DP outperforms naive â€œequal operatorâ€ clustering and â€œequal layers,â€ especially on heterogeneous Wideâ€‘ResNet (2.6Ã— over equalâ€‘operator, 1.6Ã— over equalâ€‘layer on 32 GPUs) (Â§8.3).
  - Compilation time (Fig. 10, Table 5):
    - Scales roughly linearly with model and GPU count; for GPTâ€‘39B on 64 GPUs, total time â‰ˆ 2393 s (â‰ˆ40 min) with profiling, down from >40 hours without the accelerations (Table 5). Most time is in profiling stageâ€‘mesh pairs; distributed compilation and a simple cost model accelerate this (Â§8.4).
  - Crossâ€‘mesh resharding (Fig. 11):
    - The â€œlocal allâ€‘gatherâ€ optimization yields â‰ˆ 2.0Ã— speedup on 32â€‘GPU Wideâ€‘ResNet vs. naive send/recv; the â€œsignal send/recvâ€ curve shows the upper bound with negligible interâ€‘mesh payload (Â§8.5).
  - Case study (Fig. 12):
    - On 16 GPUs, Alpa splits Wideâ€‘ResNet into 3 stages with 4/4/8 GPUs. Early stages favor batchâ€‘axis partitioning (activations large), later stages shard channels/weights (weights dominate). These nonâ€‘uniform, layerâ€‘dependent choices are difficult to design manually (Â§8.6).

- Assessment of evidence:
  - The evaluation spans homogeneous (Transformers) and heterogeneous (CNN) models, includes competitive baselines where available, and provides ablations isolating both passes. Reported gains are substantial (up to 9.7Ã—) and consistent with the design rationale (placing communication on fast links, balancing memory/time across stages).
  - One caveat: the metric is PFLOPS on synthetic data; convergence behavior is unchanged by design but not empirically verified here (explicitly noted in â€œEvaluation metricsâ€).

## 6. Limitations and Trade-offs
- Modeling and search assumptions:
  - Crossâ€‘stage communication is not modeled in the ILP/DP cost; the paper argues it is small compared to intraâ€‘stage collectives, but this may not hold for all architectures/datasets (Â§7).
  - Microbatch count `B` is a hyperparameter, not optimized inside the DP; different `B` affects both memory and pipeline bubbles (Â§7).
  - The pipeline schedule is fixed (synchronous 1F1B); dynamic or branchâ€‘parallel schedules are out of scope (Â§7).
  - The approach targets static computation graphs with known shapes; dynamic control flow or variable shapes are not handled (Â§7).
- Practical constraints:
  - Profiling many stageâ€‘mesh pairs is still the dominant compilation cost (Table 5), although manageable for the evaluated scales (Â§8.4).
  - The intraâ€‘op cost model ignores compute time and uses bytes/bandwidth for communication; while justified (heavy ops equally partitioned), architectures with irregular kernels might deviate (Â§4.2).
  - Submesh shapes are restricted to guarantee coverage ((1Ã—powers of two) or (nÃ—M)); some exotic topologies could prefer other partitions (Â§5.2 and Appendix A).
- Scope:
  - No explicit optimization for overlapping communication with computation (Â§7).
  - Heterogeneous device capabilities or highly irregular network topologies are not modeled explicitly.

## 7. Implications and Future Directions
- Impact on the field:
  - Establishes a general, automated path to combine data, tensor, and pipeline parallelism with strong empirical performance, reducing reliance on manual, modelâ€‘specific plans. This can democratize largeâ€‘model training for diverse architectures (Â§10).
- What it enables:
  - Outâ€‘ofâ€‘theâ€‘box scaling for new models (e.g., heterogeneous CNNs, custom MoE mixes) without bespoke system engineering (Fig. 7c, Fig. 12).
  - A unifying framework that can incorporate complementary techniques (rematerialization, mixed precision, gradient compression) at either level (Â§9 â€œTechniques for training largeâ€‘scale modelsâ€).
- Promising research directions:
  - Optimize microbatch schedule jointly with stage/mesh choices; model and minimize crossâ€‘stage communication directly in the DP.
  - Design cost models that include compute kernels and overlap comm/compute; integrate topologyâ€‘aware mapping and heterogeneous resources.
  - Develop optimal crossâ€‘mesh resharding strategies beyond â€œlocal allâ€‘gather,â€ potentially using multicast trees or topologyâ€‘aware collectives (Â§6, Â§8.5).
  - Extend to dynamic graphs and dynamic schedules; explore branchâ€‘parallel interâ€‘op planning and asynchronous variants while maintaining convergence guarantees.
- Practical applications:
  - Training and fineâ€‘tuning large LMs/MoEs on commodity multiâ€‘node GPU clusters; scaling vision or multimodal models with uneven layer characteristics; rapid prototyping of novel architectures using JAX via a simple decorator API (Fig. 4).

> In short, the core ideaâ€”optimize intraâ€‘operator sharding and interâ€‘operator pipelining in a hierarchy that mirrors cluster communication asymmetryâ€”proves powerful. The system achieves parity with the best handâ€‘tuned setups on their home turf (GPTâ€‘style Transformers) and outperforms specialized baselines where those lack a dimension of parallelism (MoE across nodes), while generalizing to architectures that lack manual strategies (Wideâ€‘ResNet).

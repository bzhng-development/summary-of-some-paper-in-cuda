# SmolVLM: Redefining small and efficient multimodal models

**ArXiv:** [2504.05299](https://arxiv.org/abs/2504.05299)

## ğŸ¯ Pitch

SmolVLM pioneers a new class of ultra-efficient vision-language models that deliver robust image and video understanding while using less than 1GB of GPU memoryâ€”making them practical for edge devices, smartphones, and browsers. By systematically rethinking architecture, tokenization, and training recipes for compactness rather than merely scaling down large models, SmolVLM closes the performance gap with models hundreds of times larger, unlocking real-world, privacy-preserving multimodal AI for resource-constrained scenarios.

---

## 1. Executive Summary (2-3 sentences)
SmolVLM introduces a family of small, memoryâ€‘efficient visionâ€“language models (VLMs) that run on as little as 0.8â€“1.2 GB of GPU RAM while matching or surpassing much larger systems on many image and video benchmarks. The workâ€™s significance is twofold: it distills a concrete recipe for building strong compact VLMs (architecture, tokenization, training data, and prompts) and validates it with extensive ablations and endâ€‘toâ€‘end results, including onâ€‘device and browser deployments (Figures 1, 3â€“7, 9; Table 1).

## 2. Context and Motivation
- Problem addressed:
  - Most VLMs achieve high accuracy by being very large, which makes them impractical on phones, browsers, or lowâ€‘end GPUs due to memory and compute constraints (Introduction, p.2).
  - Smaller VLMs often copy design choices from large models (e.g., very dense image tokenization), which keeps memory cost high and nullifies the benefits of being small (Abstract; Introduction, p.2).

- Why it matters:
  - Realâ€‘world need for onâ€‘device and edge inference: lower latency, privacy, offline use, and energy efficiencyâ€”especially challenging for video tasks, which inflate token counts (Introduction, p.2; Finding 4 in Â§2.3).
  - Practicality is affected not only by parameter count but by sequence length and tokenization. Efficient perâ€‘token processing is crucial because modern LMs generate many tokens at inference (Introduction, p.2).

- Prior approaches and gaps:
  - Large early VLMs like Flamingo and Idefics (80B) set strong performance but huge memory cost (Introduction, p.2; Related Work Â§5.1).
  - â€œSmallerâ€ lines (e.g., Qwen2â€‘VL 1â€“2B, InternVL 2.x) still carry heavy overheads or reserve vision only for large models (Introduction, p.2; Related Work Â§5.2).
  - Efficient models exist (e.g., Moondream, MiniCPMâ€‘V), but the field lacked a systematic, endâ€‘toâ€‘end recipe for compact VLMs that jointly optimizes architecture, tokenization, and data for strict memory budgets.

- Positioning:
  - The paper offers a principled exploration and a unifying design: balanced visionâ€“language compute for small scales, extended context, aggressive but controlled visual token compression, learned positional tokens, and carefully balanced training data (Sections 2â€“3). It releases open weights, code, and demos, and demonstrates onâ€‘device use (Abstract; Â§4.4; Figure 9).

## 3. Technical Approach
SmolVLM is a pipeline that compresses visual information into a small number of tokens and interleaves them with text tokens for a compact LM to process. The system is instantiated at three sizes: `SmolVLM-256M`, `SmolVLM-500M`, and `SmolVLM-2.2B` (Â§4, p.7).

- Architecture and token path (Figure 2; Â§2):
  1. Image/video ingestion
     - Images can be split into tiles (â€œsubâ€‘imagesâ€), and videos are sampled into frames (Â§2.3). Image splitting provides both a highâ€‘resolution tiled view and a downscaled global image so the model sees details without losing global context.
  2. Vision encoder
     - Uses `SigLIP` variants: `SigLIPâ€‘B/16` (93M params) for the 256M and 500M models; `SigLIPâ€‘SO400M` (~400M) for the 2.2B model (Â§4, p.7).
     - SigLIP is a CLIPâ€‘like image encoder trained with a sigmoid contrastive loss (Related Work Â§5.2; Zhai et al., 2023).
  3. Pixel shuffle (spaceâ€‘toâ€‘depth) compression
     - Pixel shuffle rearranges spatial features into channels, reducing token count by rÂ² for shuffle ratio `r` (Figure 4; Â§2.2). Example: 2Ã—2 shuffle turns 4 adjacent patches into 1 token with 4Ã— channels.
     - This lowers attention cost while trying to preserve information density.
  4. MLP projection
     - A small MLP maps vision features to the LMâ€™s embedding space to form â€œvisual tokensâ€ (Figure 2).
  5. Concatenation and LLM
     - Visual tokens are concatenated/interleaved with text tokens and fed to a compact `SmolLM2` language model (135M, 360M, or 1.7B params; Â§2.2) with an extended context window (Â§2.2).

- Compute allocation between vision and language (Â§2.1; Figure 3, left):
  - Design choice: use smaller encoders with smaller LMs; use a larger encoder only when the LM is large enough to exploit it.
  - Evidence (Figure 3, left): pairing a large encoder (428M) with the tiniest LM (135M) reduces performance; the 360M LM benefits modestly from the big encoder but at high parameter cost; only at 1.7B LM scale does the large encoder add value with a small totalâ€‘parameter penalty.

- Longâ€‘context capability (Â§2.2; Figure 3, middle):
  - The team increases the `RoPE` base (rotary positional embedding base) from 10k to 273k to enable stable longâ€‘context attention (to 8kâ€“16k tokens), then fineâ€‘tunes on a mix of longâ€‘context and shortâ€‘context text corpora.
  - Stability limit: 1.7B LM variant trains to 16k tokens; 135M/360M are stable up to 8k tokens (text + vision tokens).

- Aggressive visual token compression (Â§2.2; Figure 3, middleâ€‘right, and Figure 4):
  - Many VLMs choose shuffle ratio `r=2` to protect OCR and localization. Here, small models often benefit from `r=4`, because the reduced token count decreases attention overhead and improves longâ€‘context modeling (Figure 3, middleâ€‘right).

- Image and video handling (Â§2.3; Figure 3, right):
  - Image splitting helps small models keep detail without exploding tokens (tiles + small global image).
  - Video â€œframe averagingâ€ (averaging multiple frames into one feature) hurts performance as the averaging factor grows (2â†’4â†’8), so it is avoided (Figure 3, right). Instead, frames are rescaled to the encoderâ€™s input resolution.

- Positional encoding for split images and media segmentation (Â§3.1â€“3.2; Figures 5â€“6):
  - Learned positional tokens versus literal string tags: using raw strings such as `<row_1_col_2>` causes unstable plateaus in training (â€œOCR loss plagueâ€) for small models; learned tokens stabilize optimization and improve OCR and overall scores (Figure 5, left and center).
  - Media intro/outro markers and concise system prompts disambiguate where visual content begins/ends and what the modelâ€™s role is; both improve zeroâ€‘shot performance, especially on video (Figure 6). During supervised fineâ€‘tuning, masking user prompts (training only on completions) boosts generalization (Figure 6, right).

- Training data composition (Â§3.3â€“3.5; Figure 7; Â§4.1, Figure 8):
  - Two stages: Vision stage (heavy on OCR/docs, charts, tables, VQA, and some reasoning; Figure 8 left) and Video fineâ€‘tuning stage (33% video, 35% image, 12% multiâ€‘image, 20% text; Figure 8 right). Maintain only ~14% pure text to avoid overwhelming compact models with nonâ€‘visual data (Â§4.1).
  - Avoid reusing LLM SFT text (â€œSmolTalkâ€): it reduces image and video scores on small VLMs by 3.7% and 6.5% on average (Figure 7, left).
  - Use very sparse Chainâ€‘ofâ€‘Thought (CoT): small fractions (0.02â€“0.05%) help slightly; higher fractions hurt, especially on image tasks (Figure 7, middle).
  - Moderate video durations (â‰ˆ3.5 minutes) during training improve results; longer yields diminishing returns (Figure 7, right).

- Implementation note on context scaling:
  - Extending context used RoPE base scaling (Liu et al., 2024c). The `SmolVLM-2.2B` uses a 16k limit; smaller variants use an 8k limit (Â§2.2).

## 4. Key Insights and Innovations
- Balanced encoderâ€“LM capacity for small VLMs (Finding 1; Â§2.1; Figure 3, left)
  - Novelty: Rather than defaulting to a powerful vision encoder, small LMs pair better with smaller encoders; larger encoders become beneficial only once the LM has enough capacity (â‰¥1.7B).
  - Why it matters: Avoids overâ€‘investing in vision capacity that the LM cannot utilize, saving parameters and memory at small scales.

- Aggressive but targeted visual token compression (Findings 2â€“4; Â§2.2â€“Â§2.3; Figures 3â€“4)
  - Novelty: For compact models, using pixel shuffle with `r=4`â€”more aggressive than the common `r=2`â€”improves performance by reducing attention load (Figure 3, middleâ€‘right). Combined with a longer context window, this supports higher resolutions without exploding memory.
  - Significance: This departs from prior art that warns against strong compression due to OCR/localization; SmolVLM shows how to compensate (image splitting, learned positional tokens).

- Learned positional tokens and structured prompting for stability and OCR (Finding 5â€“6; Â§3.1â€“3.2; Figures 5â€“6)
  - Novelty: Replacing stringâ€‘based positional tags with learned embeddings eliminates the â€œOCR loss plagueâ€ and improves both image and video scores (Figure 5).
  - Prompting/segmentation tokens and masking user inputs during SFT yield consistent gains, especially on video (Figure 6).
  - Significance: Turns previously brittle training dynamics into stable ones for small multimodal models.

- Data curation rules for small VLMs (Findings 7â€“9; Â§3.3â€“3.5; Figure 7; Â§4.1, Figure 8)
  - Novelty: Counterintuitive empirical rulesâ€”do not reuse LLM SFT text blends; keep CoT minimal; limit average video duration to ~3.5 minutesâ€”optimize capacity usage for small VLMs (Figure 7).
  - Significance: Provides a tested recipe for training compact VLMs without saturating them with textâ€‘heavy or overly long video data.

These are mostly practical innovations grounded in systematic ablations rather than new theory; the novelty lies in the recipe and its interactions.

## 5. Experimental Analysis
- Evaluation setup (Â§4.2):
  - Toolkit: `VLMEvalKit` (Duan et al., 2024) for reproducibility.
  - Leaderboard: `OpenVLM` (by OpenCompass) and many benchmarks (31 total for the leaderboard; Figure 1). The paper emphasizes RAM usage as a more meaningful proxy for deployment cost than parameter count (Â§4.2).
  - Image preprocessing size: longest edge 1920 for `256M`/`500M`; 1536 for `2.2B` (Â§4.2).

- Benchmarks and metrics (Table 1):
  - Singleâ€‘image tasks: OCRBench (OCR), AI2D (science diagrams), ChartQA, TextVQA, DocVQA, ScienceQA.
  - Multiâ€‘task: MMMU (collegeâ€‘level), MathVista (visual math), MMStar (multidisciplinary).
  - Video: Videoâ€‘MME (general), MLVU (movie QA + MSRVTT caption), MVBench (multiview), WorldSense (temporal/physics), TempCompass (temporal).
  - Metrics are standard task accuracies or CIDEr where appropriate (Figure 3 caption notes averaging CIDEr and accuracy in their analyses).

- Main quantitative results (Table 1; Â§4.3):
  - Average across 14 benchmarks:
    > `SmolVLM-256M`: 44.0% | `SmolVLM-500M`: 51.0% | `SmolVLM-2.2B`: 59.8%
  - Memory usage (batch size 1):
    > 0.8 GB (256M), 1.2 GB (500M), 4.9 GB (2.2B) vs. 27.7 GB for `MolmoEâ€‘A1Bâ€‘7B` (efficient large baseline in the table).
  - Selected singleâ€‘image tasks:
    - OCRBench:
      > 52.6% (256M) â†’ 61.0% (500M) â†’ 72.9% (2.2B) vs. 54.7% `MolmoEâ€‘A1Bâ€‘7B`.
    - DocVQA:
      > 58.3% â†’ 70.5% â†’ 80.0% vs. 77.7% `MolmoEâ€‘A1Bâ€‘7B`.
    - ScienceQA:
      > 73.8% â†’ 80.0% â†’ 89.6% vs. 87.5% `MolmoEâ€‘A1Bâ€‘7B`.
    - MMMU (hard reasoning):
      > 29.0% â†’ 33.7% â†’ 42.0% (close to 33.9% baseline at small scales; improves with the 2.2B variant).
  - Selected video tasks:
    - Videoâ€‘MME:
      > 33.7% â†’ 42.2% â†’ 52.1% vs. 45.0% `InternVL2â€‘2B`.
    - WorldSense:
      > 29.7% â†’ 30.6% â†’ 36.2% vs. 32.4% `Qwen2VLâ€‘7B`.
    - MVBench (challenging for SmolVLM at small scale):
      > 32.7% â†’ 39.7% â†’ 46.3% vs. 60.2% `InternVL2â€‘2B`.
  - Scaling trend:
    - Almost all tasks improve with model size; even `256M` often beats far larger historical models (Figure 1; Â§4.3).

- Throughput and onâ€‘device viability (Â§4.4; Figure 9):
  - A100 GPU:
    > `256M`: 0.8 â†’ 16.3 examples/s (batch 1â†’64)  
    > `500M`: 0.7 â†’ 9.9 examples/s  
    > `2.2B`: 0.6 â†’ 1.7 examples/s
  - NVIDIA L4 (edge server GPU):
    > Peaks: `256M` ~2.7 ex/s (batch 8); `500M` ~1.4 ex/s; `2.2B` ~0.25 ex/s
  - Browser/WebGPU on MacBook Pro (M4 Max):
    > Up to ~80 decode tokens/s for `256M`.
  - These support the claim that RAM (and perâ€‘token cost) is a better deployment proxy than parameters (Â§4.2; Figure 9).

- Ablations and training strategy evidence:
  - Encoderâ€“LM balance (Figure 3, left): large encoder hurts with 135M LM; only helps clearly at 1.7B.
  - Context length gains (Figure 3, middle): accuracy increases up to 16k tokens for the large model; small models are stable up to 8k.
  - Pixel shuffle ratio (Figure 3, middleâ€‘right): `r=4` can outperform `r=2` in compact regimes.
  - Frame averaging (Figure 3, right): hurts video performance as averaging factor increases; thus excluded.
  - Learned positional tokens (Figure 5): fix training stalls (â€œOCR loss plagueâ€) and yield higher scores than string tags.
  - Prompting and masking (Figure 6): system prompts + media intro/outro + userâ€‘prompt masking each add gains; most pronounced on video.
  - Data mix (Figure 7): avoid LLM SFT text; keep CoT tiny; target ~3.5 min videos for training.

- Do the experiments support the claims?
  - Yes, for the paperâ€™s scope. The combination of broad benchmark coverage (Table 1), explicit memory measurements, and many targeted ablations (Figures 3â€“7) makes a strong case that the proposed recipe yields compact, practical VLMs with competitive accuracy. Where results are mixed (e.g., MVBench), the paper is transparent, and the trends align with known difficulty of longâ€‘range temporal reasoning for small models.

## 6. Limitations and Trade-offs
- Spatial detail vs. compression:
  - Aggressive pixel shuffle (`r=4`) reduces spatial resolution in the token sequence (Â§2.2). Although compensated with image splitting and learned positional tokens, fineâ€‘grained localization tasks (e.g., dense OCR, small object localization) remain sensitive; the paper hints at this tradeâ€‘off when discussing why prior work defaults to `r=2` (Figure 4; Â§2.2).

- Video longâ€‘range reasoning:
  - On MVBench, even the `2.2B` model reaches 46.3% vs. 60.2% for `InternVL2â€‘2B` (Table 1). This suggests that very long, complex spatioâ€‘temporal reasoning remains a weakness for compact setups (also consistent with avoiding frame averaging and relying on rescaled frames; Â§2.3).

- Context stability constraints:
  - Smaller LMs (135M/360M) are stable up to 8k tokens rather than 16k (Â§2.2). This caps how many visual tokens (e.g., tiles + frames) can be processed at once for the smallest variants.

- Data sensitivity:
  - Small VLMs are sensitive to data compositionâ€”LLM SFT text hurts, and excessive CoT degrades performance (Â§3.3â€“3.4; Figure 7). This increases curation burden and may limit reuse of popular instructionâ€‘tuning corpora.

- Throughput at larger size:
  - `SmolVLMâ€‘2.2B` offers strong accuracy but comparatively low throughput on modest GPUs (e.g., ~0.25 ex/s on L4; Figure 9). For strict realâ€‘time applications on edge hardware, the `256M/500M` variants are preferable.

- Evaluation scope:
  - While broad, the study focuses on open benchmarks in `VLMEvalKit` and OpenVLM. Domainâ€‘specific edge cases (e.g., industrial inspection, complex multiâ€‘page forms at 4K+) are not directly evaluated. The image resolution was capped (1920 or 1536 longest edge; Â§4.2).

## 7. Implications and Future Directions
- Field impact:
  - SmolVLM strengthens the case that carefully engineered small VLMs can be practical and competitive, shifting emphasis from parameter count to token efficiency and memory use (Figure 1; Â§4.3â€“4.4). It provides a reproducible recipeâ€”balanced encoderâ€“LM capacity, long context, aggressive compression with compensating mechanisms, lean data mixesâ€”that others can adopt.

- Enabled followâ€‘ups:
  - Adaptive tokenization: dynamic pixel shuffle or learned, contentâ€‘aware compression that preserves detail where needed while keeping sequence length short elsewhere.
  - Memoryâ€‘efficient video: alternatives to frame averaging (e.g., temporal keyframe selection, learned segment pooling) that retain temporal cues without degrading performance (contrast Figure 3, right).
  - Stability at longer contexts for tiny LMs: optimization and architecture tricks to push 135M/360M models beyond 8k safely (Â§2.2).
  - Taskâ€‘adaptive prompting: learn media segmentation markers and system prompts endâ€‘toâ€‘end rather than handâ€‘engineering (Figures 5â€“6).
  - Mixtureâ€‘ofâ€‘experts at small scale: route only a subset of experts per token to preserve memory while expanding capacity (suggested by comparisons to `MolmoEâ€‘A1Bâ€‘7B` in Table 1).

- Practical applications (demonstrated and suggested):
  - Onâ€‘device assistants and accessibility tools: demonstrated smartphone app (HuggingSnap) and browser deployment via WebGPU (Figure 9; Â§4.4).
  - Document AI at the edge: â€œSmol Doclingâ€ uses specialized tags for document conversion with just 256M parameters (Â§4.5).
  - Retrieval and QA for enterprise documents: â€œColSmolVLMâ€ and connections to efficient document retrieval (ColPali) (Â§4.5).
  - Domainâ€‘specific VQA (e.g., biomedical â€œBioVQAâ€) where compute is constrained (Â§4.5).
  - Robotics and embedded systems: lowâ€‘VRAM inference widens feasibility for onboard perceptionâ€‘language reasoning.

In short, SmolVLM contributes a validated blueprint for building capable, genuinely small VLMs. Its most durable insights are the interplay between token budget and model capacity, and the importance of training/data strategies tailored to compact settingsâ€”principles that will likely guide future efficient multimodal systems.

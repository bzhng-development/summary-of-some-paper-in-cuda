# Deep Residual Learning for Image Recognition

**ArXiv:** [1512.03385](https://arxiv.org/abs/1512.03385)

## üéØ Pitch

This paper introduces the groundbreaking concept of residual learning, where deep neural networks are structured to learn residual functions via identity-based shortcut connections, enabling the training of extremely deep architectures. This simple yet powerful framework overcomes the notorious optimization difficulties of deep models, allowing networks over 100 layers to achieve record-breaking accuracy in image classification, detection, and segmentation‚Äîsparking a transformative leap in the scalability and effectiveness of deep learning systems.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces residual learning, a simple architectural idea that lets very deep neural networks train reliably by reformulating what each stack of layers learns as a residual function added to its input. Using identity ‚Äúshortcut‚Äù connections that add no parameters or extra computation, the work trains up to 152-layer image classifiers on ImageNet and achieves state-of-the-art accuracy, and the same principle boosts detection and segmentation performance.

## 2. Context and Motivation
- Specific problem or gap:
  - Deep networks should improve with depth, but in practice deeper ‚Äúplain‚Äù networks (layers stacked without skip connections) become harder to optimize. Training error starts to increase as depth grows, even when vanishing/exploding gradients are addressed with modern initialization and Batch Normalization (BN). This ‚Äúdegradation problem‚Äù is visualized on CIFAR-10 in Fig. 1 and on ImageNet in Fig. 4 (left), where a 34-layer plain network has higher training error than an 18-layer plain network.
- Why it matters:
  - Depth is a primary driver of representation power in vision; contemporary best results on ImageNet use very deep models (e.g., VGG with 16‚Äì19 layers, Inception with 22+) and many downstream tasks (detection, segmentation) benefit from deep features. Making much deeper models trainable promises better accuracy across tasks (Introduction; Sec. 1).
- Prior approaches and their limits:
  - Improved initialization and BN enable training tens of layers but do not prevent degradation when going further (Sec. 1).
  - Highway Networks add gated shortcuts; however, gates are parameterized, can ‚Äúclose,‚Äù and did not yet show consistent gains beyond ~100 layers (Sec. 2).
  - VGG/GoogLeNet scale depth but rely on carefully engineered modules, not a general fix for optimization difficulty (Fig. 3; Tables 3‚Äì5).
- Positioning:
  - The paper proposes a generic architectural reformulation‚Äîresidual learning with identity shortcuts‚Äîthat:
    - Adds no parameters or computational cost for most skips (Sec. 3.2; Eqn. 1).
    - Maintains end-to-end differentiability and compatibility with standard training (Sec. 3.2‚Äì3.4).
    - Addresses optimization degradation and unlocks accuracy gains at much greater depths (Fig. 4 right; Tables 3‚Äì5).

## 3. Technical Approach
Step-by-step methodology and design:

- Core mechanism: residual mapping
  - Goal: Instead of having a stack of layers learn a direct mapping `H(x)` from input `x` to output, have them learn a residual mapping `F(x) = H(x) - x` and then compute `y = F(x) + x`. Intuition: if the desired mapping is close to identity, it is easier to learn small deviations around identity than to reproduce identity precisely with multiple nonlinear layers (Sec. 3.1; Eqn. 1).
  - Implementation: Add an identity ‚Äúshortcut connection‚Äù that carries `x` forward and sum it elementwise with the output of the stacked layers `F(x)`. Apply the nonlinearity after the addition (Fig. 2; Sec. 3.2).

- Shortcut types and dimension handling
  - Identity shortcut: `y = F(x) + x` (Eqn. 1). No parameters or FLOPs are added; only elementwise addition.
  - Projection shortcut: When the number of channels or spatial size changes (e.g., due to downsampling), match dimensions via a learned linear projection `W_s` (1√ó1 convolution): `y = F(x) + W_s x` (Eqn. 2). This is used sparingly; the paper tests three options:
    - Option A: identity plus zero-padding for channel increase (parameter-free).
    - Option B: projection only when dimensions increase.
    - Option C: projection for all shortcuts.
    - Results show B slightly outperforms A; C brings only marginal extra gains while adding parameters (Table 3; Sec. Identity vs. Projection).

- Residual block designs
  - Basic 2-layer block (used up to 34 layers): two 3√ó3 convolutions with BN and ReLU, plus an identity shortcut, and ReLU applied after the addition (Fig. 2; Fig. 3 right).
  - Bottleneck 3-layer block (used for 50/101/152 layers): a 1√ó1 conv reduces channels, a 3√ó3 conv processes features, and another 1√ó1 conv restores channels; identity shortcut spans the block (Fig. 5 right). This design preserves compute while allowing much deeper networks (Table 1 lists stage/block counts; Sec. ‚ÄúDeeper Bottleneck Architectures‚Äù).

- Full-network architectures
  - ImageNet baseline (plain vs. residual):
    - VGG-like ‚Äúplain‚Äù net: stacks 3√ó3 convs with stage-wise downsampling by stride-2 convs, followed by global average pooling and a 1000-way linear layer (Fig. 3 middle; Table 1).
    - Residual counterpart: insert a shortcut for every pair of 3√ó3 convs; when stage transitions require size/channel changes, use option A/B/C (Fig. 3 right; Table 1; Sec. 3.3).
  - Deeper models: Replace 2-layer blocks with bottlenecks to form ResNet-50/101/152 while keeping FLOPs in check (Table 1; Sec. ‚ÄúDeeper Bottleneck Architectures‚Äù).
  - CIFAR-10 setup: Input 32√ó32; first 3√ó3 conv, then 3 stages each with `2n` residual blocks (all 3√ó3 convs), with channel sizes {16, 32, 64}; total depth `6n+2` layers (Sec. 4.2). Identity shortcuts are used throughout (option A), so residual and plain nets have identical parameter counts.

- Training pipeline
  - ImageNet: Scale augmentation (short side uniformly in [256, 480]), random 224√ó224 crops and horizontal flips, per-pixel mean subtraction, color augmentation; BN after each conv and before ReLU; SGD with batch size 256; initial LR 0.1 with step decay; weight decay 1e-4; momentum 0.9; no dropout (Sec. 3.4). Testing uses 10-crop; best single-model uses fully convolutional multi-scale inference (short side in {224, 256, 384, 480, 640}).
  - CIFAR-10: Simple flip-and-crop augmentation (pad 4 pixels each side; random 32√ó32 crop); batch size 128; LR 0.1 with step decays at 32k and 48k iterations, stop at 64k; for 110-layer net, a short warmup at LR 0.01 improves early convergence (Sec. 4.2).
  - Detection (Appendix A/B): Faster R-CNN on PASCAL/COCO with ResNet-50/101 as backbone. Shared feature maps up to stride 16 (conv1‚Äìconv4_x), RoI pooling before conv5_1; BN layers fixed during fine-tuning to reduce memory (Appendix A, ‚ÄúPASCAL VOC‚Äù and ‚ÄúMS COCO‚Äù).
  - Localization (Appendix C): A per-class Region Proposal Network (RPN) and an R-CNN head trained in a per-class regression/classification manner; dense multi-scale testing (Appendix C).

- Why this design?
  - Identity shortcuts give the optimizer an easy path to propagate information and gradients and provide an explicit identity reference, making it easier to learn small residuals rather than full mappings (Sec. 3.1; Fig. 7 shows residual responses are small).
  - Bottlenecks enable depth without heavy compute/memory by confining the 3√ó3 conv to a reduced channel dimension (Fig. 5; Table 1 shows FLOPs: 3.8/7.6/11.3√ó10^9 for ResNet-50/101/152 vs 19.6√ó10^9 for VGG-19).

## 4. Key Insights and Innovations
- Residual learning with identity shortcuts (fundamental innovation)
  - What‚Äôs new: Reformulate layer stacks to learn `F(x)` such that outputs are `F(x)+x` (Eqn. 1), realized via identity shortcut and addition (Fig. 2).
  - Why it matters: Directly fixes the degradation problem‚Äîdeeper residual nets show lower training error than shallower ones (Fig. 4 right), unlike plain nets (Fig. 4 left). It enables substantial depth increases with consistent accuracy gains (Tables 3‚Äì4).

- Empirical demonstration that identity shortcuts are sufficient and efficient (innovation in design economy)
  - Finding: Zero-parameter identity shortcuts (option A) already outperform plain nets; using projections only for dimension increases (option B) yields small extra gains; projections everywhere (option C) add parameters with marginal benefits (Table 3).
  - Significance: Keeps complexity low while achieving large improvements, particularly important for bottleneck designs (Sec. ‚ÄúIdentity vs. Projection Shortcuts‚Äù).

- Bottleneck residual block for very deep networks (practical architectural innovation)
  - What‚Äôs new: 1√ó1‚Äì3√ó3‚Äì1√ó1 structure reduces/increases channels around the 3√ó3 conv (Fig. 5 right).
  - Why it matters: Allows training of 50/101/152-layer models with competitive or lower FLOPs than VGG-16/19, and brings clear accuracy gains with increasing depth (Table 1 for FLOPs; Tables 3‚Äì4 for accuracy).

- Evidence that residual functions have small magnitude (explanatory insight)
  - Observation: Standard deviations of block outputs (after BN, before nonlinearity) are lower for residual nets than plain nets, and decrease as depth increases (Fig. 7).
  - Significance: Supports the hypothesis that learning perturbations around identity is easier and that residuals remain small, which acts as an implicit preconditioning of the optimization problem (Sec. 3.1; Fig. 7).

## 5. Experimental Analysis
- Evaluation methodology
  - Datasets and metrics:
    - ImageNet 2012 classification: 1.28M training, 50k validation, 100k test; metrics are top-1 and top-5 error (Sec. 4.1).
    - CIFAR-10: 50k train, 10k test; error rate (%) with standard data augmentation (Sec. 4.2).
    - Object detection: PASCAL VOC 2007/2012 (mAP@.5), MS COCO (mAP@.5 and mAP@[.5, .95]) using Faster R-CNN (Tables 7‚Äì9).
    - ImageNet detection and localization tasks (mAP@.5 for DET; top-5 localization error for LOC) (Tables 12‚Äì14).
  - Baselines: Plain networks (same depth/parameters as residual), VGG-16/19, GoogLeNet/Inception, PReLU-net, BN-Inception (Tables 3‚Äì5).

- Main results (selected, with citations)
  - Residual vs plain, same depth:
    - On ImageNet validation (10-crop), 34-layer plain top-1 error 28.54% vs ResNet-34 25.03% (option A) (Table 2), with training curves showing lower training error for ResNet (Fig. 4 right).
    - 18-layer plain and residual are similar in final accuracy but ResNet converges faster (Fig. 4).
  - Scaling depth with bottlenecks:
    - ImageNet validation (10-crop): ResNet-50/101/152 achieve top-1/top-5 errors of 22.85/6.71, 21.75/6.05, 21.43/5.71 respectively (Table 3). Single-model multi-scale validation top-5 error reaches 4.49% for ResNet-152 (Table 4).
    - Ensemble on test set: top-5 error 3.57% (Table 5), winning ILSVRC 2015 classification.
  - CIFAR-10:
    - Plain nets degrade with depth (training error increases; Fig. 6 left). Residual nets improve with depth; ResNet-110 achieves 6.43% test error (best run) vs 6.97% for ResNet-56 (Table 6; Fig. 6 middle).
    - Extremely deep ResNet-1202 trains to <0.1% training error but tests at 7.93% (Table 6; Fig. 6 right), suggesting overfitting on small data without stronger regularization.
  - Object detection (Faster R-CNN baseline, same system, backbone swap only):
    - PASCAL VOC: mAP improves from 73.2%‚Üí76.4% (07 test) and 70.4%‚Üí73.8% (12 test) when replacing VGG-16 with ResNet-101 (Table 7).
    - COCO val: mAP@[.5, .95] improves from 21.2%‚Üí27.2% (a 28% relative increase); mAP@.5 from 41.5%‚Üí48.4% (Table 8).
    - With box refinement, global context, and multi-scale testing, COCO test-dev single-model reaches 55.7% @.5 and 34.9% @[.5, .95]; 3-model ensemble hits 59.0%/37.4% (Table 9).
    - PASCAL VOC with the improved system and additional COCO pretraining reaches 85.6% (2007) and 83.8% (2012) mAP (Tables 10‚Äì11).
  - ImageNet DET and LOC:
    - Detection: single-model mAP 58.8% on test; 3-model ensemble 62.1% (Table 12), first place in ILSVRC 2015 detection.
    - Localization: using per-class RPN + R-CNN, top-5 localization error drops to 10.6% single-model (val) and 9.0% ensemble (test) (Table 13), dramatically better than VGG‚Äôs 25.3% (Table 14).

- Do the experiments support the claims?
  - Yes. The paper demonstrates:
    - Optimization relief: deeper residual nets have lower training error than shallower ones (Fig. 4 right), reversing the degradation seen in plain nets (Fig. 4 left; Fig. 1).
    - Accuracy gains with depth: consistent improvements from 34‚Üí50‚Üí101‚Üí152 layers (Tables 3‚Äì4).
    - Generalization to other tasks: detection and localization gains arise solely from better backbones when the detection pipeline is otherwise unchanged (Tables 7‚Äì8 and 12‚Äì14).
  - Ablations and diagnostics:
    - Shortcut type ablation (A/B/C) shows identity shortcuts suffice; projections give small extra gains (Table 3).
    - Response magnitude analysis supports the residual-perturbation hypothesis (Fig. 7).
    - Depth scaling on CIFAR shows overfitting at 1202 layers without stronger regularization (Table 6; Fig. 6 right).

- Conditions and trade-offs:
  - Benefits grow with depth up to 152 layers on ImageNet with the bottleneck design; on small datasets excessive depth can overfit without stronger regularization (Sec. 4.2; Table 6).

## 6. Limitations and Trade-offs
- Assumptions and design constraints:
  - The residual formulation assumes input and output of a block have matching shapes for identity addition; otherwise a learnable projection (`W_s`) or zero-padding is needed (Eqns. 1‚Äì2). This adds mild complexity at stage transitions.
  - BN is integral to stable training in all experiments (Sec. 3.4); behavior without BN is not explored.
- Scope and edge cases:
  - The theoretical reason why residual reformulation eases optimization is argued heuristically and supported empirically (Fig. 7) but not formally proven (Sec. 3.1).
  - Extremely deep models can overfit small datasets (e.g., ResNet-1202 on CIFAR-10) without additional regularization (Sec. 4.2; Table 6).
- Computational considerations:
  - While identity shortcuts add negligible cost, very deep models still incur substantial compute and memory; training uses large-scale resources and multi-scale testing for best results (Sec. 3.4; Tables 1, 9).
  - Bottleneck blocks are critical to keep FLOPs manageable; non-bottleneck very deep residual nets are ‚Äúnot as economical‚Äù (footnote in Sec. ‚ÄúDeeper Bottleneck Architectures‚Äù).
- Open questions:
  - How deep is ‚Äútoo deep‚Äù for a given dataset without overfitting?
  - What is the optimal balance of depth vs. width vs. regularization for different tasks and data regimes?
  - Why exactly do residual connections act as an effective preconditioner from an optimization-theoretic perspective?

## 7. Implications and Future Directions
- Field impact:
  - Residual learning turns depth scaling into a practical tool rather than a training bottleneck. The identity shortcut design is simple, parameter-free, and slots into standard training pipelines, enabling networks with over 100 layers that outperform prior architectures on core benchmarks (Tables 3‚Äì5).
  - The same residual backbones, without task-specific bells and whistles, yield large gains in object detection and segmentation when dropped into established systems (Tables 7‚Äì9, 12‚Äì14), suggesting the approach improves general-purpose feature quality.

- Follow-up research enabled or suggested:
  - Theory: Formalizing why residual parameterization eases optimization; analyzing the spectrum of Jacobians and the role of identity mappings in gradient flow.
  - Architecture: Exploring alternative shortcut patterns, deeper/wider hybrids, and more efficient bottlenecks; studying when to use projection vs. identity; designing regularization specifically for ultra-deep residual networks (e.g., for small datasets like CIFAR-10).
  - Training dynamics: Investigating initialization, warmup schedules (as used briefly for ResNet-110 on CIFAR-10), and the interaction with BN at extreme depths.
  - Task extensions: Applying residual learning to non-vision domains and structured prediction tasks, informed by the strong cross-task gains shown here (Appendix A‚ÄìC).

- Practical applications and use cases:
  - High-accuracy image classification at manageable compute cost (ResNet-152 beats VGG/GoogLeNet with fewer FLOPs than VGG-19; Table 1, Tables 3‚Äì5).
  - Stronger backbones for detection/localization/segmentation without reengineering the detection pipeline (Faster R-CNN results; Tables 7‚Äì12).
  - A robust design template‚Äî`residual block` + `identity shortcut`‚Äîthat practitioners can adopt, scale, and adapt to their data and constraints.

> Key takeaway: replacing direct mappings with residual mappings, realized by identity shortcut additions, is a minimal yet powerful architectural change that makes very deep networks both trainable and more accurate across multiple vision tasks (Sec. 3; Figs. 2‚Äì5; Tables 2‚Äì5, 7‚Äì14).

# Scaling Agents via Continual Pre-training

**ArXiv:** [2509.13310](https://arxiv.org/abs/2509.13310)

## ğŸ¯ Pitch

This paper introduces Agentic Continual Pre-training (Agentic CPT), a novel intermediate training stage designed to endow large language models with intrinsic agentic behaviorsâ€”such as robust tool use, multi-step reasoning, and adaptive decision-makingâ€”before post-training. By synthesizing massive, diverse agentic data (First-order and High-order Action Synthesis) and applying this method to create AgentFounder-30B, the authors demonstrate substantial new state-of-the-art performance on deep research and tool-use benchmarks, surpassing both open-source and commercial competitors. This work redefines how we build agentic AI, closing the performance gap for open-source systems and laying the foundation for more capable and reliable autonomous research assistants.

---

## 1. Executive Summary
This paper introduces Agentic Continual Preâ€‘training (Agentic CPT), an additional training stage inserted between standard preâ€‘training and postâ€‘training to build â€œagentic foundation modelsâ€ that already possess toolâ€‘use and longâ€‘horizon decisionâ€‘making priors. Using this stage plus two scalable dataâ€‘synthesis methodsâ€”Firstâ€‘order Action Synthesis (FAS) and Highâ€‘order Action Synthesis (HAS)â€”the paper trains AgentFounderâ€‘30B, which achieves new stateâ€‘ofâ€‘theâ€‘art results on multiple deepâ€‘research benchmarks while retaining broad toolâ€‘use skills (e.g., 39.9% on BrowseCompâ€‘en and 31.5% Pass@1 on HLE; Tables 1â€“2).

## 2. Context and Motivation
- Problem addressed:
  - Postâ€‘training only (SFT/RL) on generalâ€‘purpose LLMs underperforms on complex â€œagenticâ€ tasks like web research and multiâ€‘step tool use (Section 1). Agentic tasks require models to plan, adapt, and decide under evolving external feedback; general LLMs lack such â€œagentic inductive biases.â€
  - The paper reframes alignment for agents as agentic alignment: matching expertâ€‘like behaviors over entire decision chains (reasoning steps, tool calls, recovery from tool failures), not just final answers (Section 1).
- Why it matters:
  - Practical: Highâ€‘quality â€œdeep researchâ€ assistants must search, browse, compute, and synthesize over long horizons. Commercial systems show strong performance, but openâ€‘source agent models lag (Section 1; performance gaps on BrowseComp in Table 1).
  - Scientific: Postâ€‘training struggles because it must simultaneously (i) teach entirely new behaviors and (ii) align to demonstrations, creating â€œoptimization tensionâ€ and limited exploration of the huge policy space (Section 1).
- Shortcomings of prior approaches:
  - SFT/RL on general models depends on limited, trajectoryâ€‘level supervision, often locking models into imitating specific paths (Section 1).
  - Full trajectory generation with live tools is expensive and slow; openâ€‘source implementations rarely scale toolâ€‘grounded data (Section 2.2.2, â€œScalability Challengesâ€).
- Positioning:
  - The paper inserts an intermediate CPT stage focused on agentic behavior formation itself (Figure 2). Rather than relying solely on postâ€‘training, it builds an agentâ€‘ready base model and then applies SFT/RL. It also proposes toolâ€‘free, offline data synthesis for scale (Sections 2.2â€“2.3).

## 3. Technical Approach
The approach modifies the standard pipeline and introduces scalable synthetic data and a progressive training strategy.

- Pipeline redesign (Figure 2; Section 2.1)
  - Standard preâ€‘training: nextâ€‘token prediction on broad corpora (Equation (1)).
    - Equation (1) uses crossâ€‘entropy over the next token P(xt+1 | x1â€¦xt).
  - New Agentic CPT in two stages:
    - Stage 1: ~200B tokens, context 32K; absorbs broad planning patterns and multiâ€‘step reasoning cues without requiring groundâ€‘truth trajectories (Section 2.1).
    - Stage 2: ~100B tokens, context 128K; trains on longer, higherâ€‘quality agentic sequences to improve longâ€‘horizon planning (Section 2.1).
  - Postâ€‘training: supervised fineâ€‘tuning (SFT) with a mixture of general instruction data and agent trajectories. Three variants (SFTâ€‘A/B/C) are tested (Section 3.1.1).

- Firstâ€‘order Action Synthesis (FAS): zero external supervision or tool calls (Section 2.2)
  - Goal: Create abundant, diverse contexts that induce planning and reasoning, cheaply and at scale.
  - Step 1 â€” Knowledgeâ€‘toâ€‘Question transformation (Section 2.2.1; Figure 3):
    - Build an â€œentityâ€‘anchored openâ€‘world memoryâ€: map entities to dense, timeâ€‘stamped declarative statements harvested from evolving sources (web, search logs, Wikipedia). This is not a rigid knowledge graph; it emphasizes statement density and recency.
    - Sample clusters of entities + statements to synthesize multiâ€‘style questions (fact retrieval, numerical, multiâ€‘hop, synthesis). This converts static text into dynamic problem contexts requiring retrieval and reasoning.
    - Example in Section 2.2.1: from statements about Paris (Louvre 8.7M visitors in 2024; 2023 bedbug crisis; 2025 Paris Air Show orders), generate an obliquely phrased question whose answer is â€œRiyadh Air.â€
  - Step 2 â€” Planning Action Synthesis (Section 2.2.2; Figure 4-left):
    - For each question, generate diverse â€œfirstâ€‘stepâ€ analyses and predicted actions (e.g., which tool to call next) but do not execute tools. Instead of repeating the same question K times, diversify at the question level: produce K different but related questions from the same underlying memory, then synthesize a first step for each. This reduces repetition and broadens actionâ€‘space coverage.
    - Quality control via â€œLLMâ€‘asâ€‘judgeâ€: reject samples whose reasoning/actions are unlikely to reach the needed knowledge. Appendix B.1 shows filtering removes 43.5% and raises retained accuracy to 82% (Figure 9).
  - Step 3 â€” Reasoning Action Synthesis (Section 2.2.3):
    - Twoâ€‘step chainâ€‘ofâ€‘thought without tools:
      1) Decompose the question into subâ€‘questions and produce a speculative answer A1 using the modelâ€™s internal knowledge.
      2) Provide the mapped requisite knowledge from the memory and ask the model to refine A1 into A2, correcting logic and citing clues.
    - Accept only samples where A2 yields the correct final answer (checked by LLMâ€‘asâ€‘judge). This creates highâ€‘quality, logicâ€‘guided chains for later CPT.

- Highâ€‘order Action Synthesis (HAS): reuse real trajectories and create stepâ€‘wise decision spaces (Section 2.3; Figure 5)
  - Motivation: Postâ€‘training produces many discarded or singleâ€‘use trajectories because overall success labels are coarse and stepâ€‘level rewards are uncertain. The insight is to turn each step into a local decision with alternatives.
  - Stepâ€‘level scaling (Section 2.3, (1)):
    - For each real trajectory T = {(S1,R1)â€¦(SK,RK)} and step k with context Ck = (Q,S1,R1,â€¦,Skâˆ’1,Rkâˆ’1), generate N alternative â€œthought+invocationâ€ candidates A_k = {S_k^(1)â€¦S_k^(N)} without executing tools. Keep the original S_k, shuffle the N+1 options, and remember the original optionâ€™s index n_k.
  - Contrastive decisionâ€‘action synthesis (Section 2.3, (2); Figure 5):
    - For each step, explicitly write: â€œI will choose option n_k,â€ then append the real environment response R_k that followed the original action. At the end, append an overall binary outcome text (â€œMy decision is Correct/Incorrectâ€) using the original trajectoryâ€™s success Jâˆˆ{0,1}.
    - This lets the model learn to choose among alternatives under realistic context and feedback without noisy stepâ€‘level rewards or live tool costs. It repurposes subâ€‘optimal trajectories into rich, stable training signals.

- Progressive twoâ€‘stage CPT strategy (Section 2.1; Section 3.4.1)
  - Stage 1 (32K): absorb massive FAS and short HAS sequencesâ€”cheap to generate/learn.
  - Stage 2 (128K): focus on longer, carefully curated HAS to teach longâ€‘horizon planning. Table 4 shows Stage 1+2 beats Stage 1 alone (+4.1 Pass@1 on BrowseCompâ€‘en).

- Implementation at inference time (Appendix A.1)
  - Tools provided to agents: `Search` (Google topâ€‘10 results), `Visit` (page fetch + goalâ€‘conditioned summarization), `Google Scholar`, `Python Interpreter`, and `File Parser`.

Glossary of paperâ€‘specific terms:
- `Agentic alignment` (Section 1): consistency with expertâ€‘like behaviors across multiâ€‘step reasoning and tool interactions in dynamic environments.
- `Deep research agent`: an LLMâ€‘based system that autonomously orchestrates search, browsing, computation, and synthesis to answer complex, knowledgeâ€‘intensive tasks (Section 1).
- `Pass@n`: proportion of questions solved when sampling up to n candidate runs per question (used across Tables and Figures).
- `FAS`/`HAS`: the two proposed synthetic data pipelines described above.
- `MoE` (mixtureâ€‘ofâ€‘experts): a model architecture that routes tokens to subsets of expert subâ€‘networks; Appendix B.4 analyzes expert activations.

## 4. Key Insights and Innovations
- Adding a dedicated agentic CPT stage (Figure 2; Section 2.1)
  - Whatâ€™s new: inserts an intermediate scaling phase that preâ€‘aligns the base model with agent behaviors before SFT/RL.
  - Why it matters: reduces the â€œdualâ€‘burdenâ€ during postâ€‘training (learning capabilities and alignment at once). Evidence: during identical SFT, CPT models converge to lower lossâ€”baseline 0.8656 vs. 0.7953 for the best CPT model (Figure 7).
- Toolâ€‘free, offline, largeâ€‘scale action data (FAS) (Sections 2.2.1â€“2.2.3; Figure 3â€“4)
  - Whatâ€™s new: transforms static web knowledge into diverse question contexts and synthesizes planning and reasoning actions without paying API costs for live tool calls.
  - Why it matters: enables hundreds of billions of tokens for CPT that are grounded in realistic tasks. Filtering methodology in Appendix B.1 demonstrates quality control (accuracy among retained samples: 82%; Figure 9).
- Turning trajectories into decision problems (HAS) (Section 2.3; Figure 5)
  - Whatâ€™s new: expands each step with multiple plausible actions and creates a â€œcontrastive decisionâ€ record that pairs the chosen option with the actual next state and final outcome.
  - Why it matters: converts discarded or subâ€‘optimal trajectories into stable, abundant supervision that teaches decisionâ€‘making, not just path imitation.
- Longâ€‘context, twoâ€‘stage CPT strategy (Section 3.4.1; Table 4)
  - Whatâ€™s new: Stage 1 learns broad patterns cheaply; Stage 2 teaches longâ€‘horizon dependencies with 128K contexts.
  - Why it matters: consistently improves Pass@1/Pass@3 (e.g., +2.9 on GAIA Pass@1, Table 4), indicating better handling of extended workflows.
- Scaling laws for agentic capabilities (Figure 6)
  - Model scale: Average accuracy rises from 20.4% (1B) â†’ 32.7% (4B) â†’ 48.9% (30Bâ€‘A3B), surpassing larger baseline systems (DeepSeekâ€‘V3.1 at 43.0% and Kimiâ€‘K2 at 29.6%; Figure 6a).
  - Data scale: Average Pass@3 improves from 54.2% to 62.2% as CPT tokens scale 0Bâ†’315B with logarithmic returns; Stageâ€‘2 longâ€‘context continues to add gains at 65B and 315B (Figure 6b).

## 5. Experimental Analysis
- Evaluation setup (Section 3.1)
  - Benchmarks:
    - General web search: BrowseCompâ€‘en/zh, GAIA (textâ€‘only subset of 103 problems), Xbenchâ€‘DeepSearch, WebWalkerQA (Section 3.1.3).
    - Scenarioâ€‘targeted: DeepResearch Bench (RACE Overall), SEALâ€‘0 (misleading/conflicting results), Frames (multiâ€‘perspective reasoning), HLE (expertâ€‘level QA), AcademicBrowse (literature navigation) (Section 3.1.3).
  - Baselines: general LLMs with tools (e.g., Qwen3â€‘30B, Claudeâ€‘4â€‘Sonnet), commercial deepâ€‘research agents (OpenAI o3, DeepResearch, etc.), and leading openâ€‘source agents (GLMâ€‘4.5, DeepSeekâ€‘V3.1, WebSailor, etc.) (Section 3.1.2).
  - Tools and hyperâ€‘parameters: five core tools; temperature 0.85, repetition penalty 1.1, topâ€‘p 0.95; max 128 tool calls; 128K context (Section 3.1.4).
- Main results (Tables 1â€“2)
  - General web search (Table 1):
    - AgentFounderâ€‘30B: 39.9 (BrowseCompâ€‘en), 43.3 (BrowseCompâ€‘zh), 72.8 (GAIA), 73.0 (Xbenchâ€‘DeepSearch), 71.9 (WebWalkerQA).
    - Relative position:
      - Beats all openâ€‘source agents on 4/5 benchmarks and is near or above some commercial offerings (e.g., 72.8 on GAIA vs. 70.5 for OpenAIâ€‘o3 in Table 1).
      - On BrowseCompâ€‘zh, performance is strong but not best among openâ€‘source (49.2 for DeepSeekâ€‘V3.1 vs. 43.3); the paper attributes this in part to lower Chinese data coverage and search bias (Section 3.2).
  - Scenarioâ€‘targeted (Table 2):
    - AgentFounderâ€‘30B achieves 31.5% Pass@1 on HLE, 47.9% on DeepResearch Bench (RACE Overall), 89.6% on Frames, 43.9% on SEALâ€‘0, and 75.3% on AcademicBrowse.
    - Notable: first openâ€‘source model above 30% Pass@1 on HLE, and substantially above openâ€‘source peers on AcademicBrowse (75.3 vs. 65.0 for DeepSeekâ€‘V3.1).
- Do experiments support claims?
  - Evidence for â€œpreâ€‘aligned agentic base helps all SFTsâ€ (RQ2): Using the same SFT corpora, AgentFounderâ€‘Base outperforms Qwen3â€‘30Bâ€‘Base across all three SFT variants (Table 3). Example: with SFTâ€‘B, BrowseCompâ€‘en jumps from 28.6 â†’ 39.9 Pass@1; HLE from 27.0 â†’ 31.5.
  - Evidence for twoâ€‘stage CPT (RQ3): Stage 1+2 consistently beats Stage 1 only, including a +4.1 Pass@1 on BrowseCompâ€‘en and +8.0 Pass@3 on BrowseCompâ€‘zh (Table 4).
  - Evidence for FAS and HAS contributions (RQ4): With 50B singleâ€‘stage CPT, FAS alone improves over nonâ€‘CPT across tasks; adding HAS further improves in most metrics, notably BrowseCompâ€‘zh Pass@1 from 37.0 (FAS) â†’ 40.1 (FAS+HAS) (Table 5).
  - Evidence for scaling (RQ5): Figure 6 shows monotonic improvements with both model size and CPT token count; Figure 7 shows strictly better SFT convergence as CPT data increases.
  - Behavioral analyses:
    - Toolâ€‘use patterns adapt to task type (Figure 8): heavyâ€‘tailed usage for BrowseCompâ€‘en/HLE versus conservative usage for WebWalker/GAIA.
    - General toolâ€‘use remains strong on ACEBench: 70.0 vs. 67.2 for the same backbone without CPT (Table 6).
    - Diversity scaling: Pass@n on BrowseCompâ€‘en increases from 31.5 (n=1) to 75.8 (n=16) (Appendix B.2; Figure 10), indicating the model maintains diverse solution strategiesâ€”consistent with HASâ€™s training signal.
    - Difficulty sensitivity on GAIA: largest drop at Levelâ€‘3 (Pass@1 50.0; Pass@3 58.3) versus Levelâ€‘1 (79.5; 87.2), showing remaining gaps on the hardest problems (Appendix B.3; Figure 11).
    - MoE diagnostics: CPT balances expert activations in late layers, reducing â€œdead expertsâ€ (Appendix B.4; Figure 12), which may contribute to training stability cited in Section 3.6.1.
    - Accuracy vs. toolâ€‘call turns shows expected trendâ€”higher accuracy with fewer turns but nonâ€‘trivial success even >40 calls (17.5% average; Appendix B.5; Figure 13).
- Mixed/conditional findings:
  - Chinese browsing (BrowseCompâ€‘zh) lags the best openâ€‘source baseline (49.2 for DeepSeekâ€‘V3.1). The paper links this to trainingâ€‘data language mix and possible search engine bias (Section 3.2).
  - In a 50B singleâ€‘stage setting, adding HAS slightly lowers GAIA Pass@1 vs. FAS alone (72.8 â†’ 69.9) while improving Pass@3 (80.6 â†’ 82.5), suggesting a variance/recall tradeâ€‘off (Table 5).

> Table 1: â€œAgentFounderâ€‘30B â€¦ 39.9% on BrowseCompâ€‘en, 43.3% on BrowseCompâ€‘zh, 72.8% on GAIA, 73.0% on Xbenchâ€‘DeepSearch, 71.9% on WebWalkerQA.â€

> Table 2: â€œAgentFounderâ€‘30B â€¦ 31.5% (HLE Pass@1), 47.9% (DeepResearch RACE Overall), 89.6% (Frames), 43.9% (SEALâ€‘0), 75.3% (AcademicBrowse).â€

Overall, the experimental suite is broad (10 benchmarks), includes ablations (Tables 3â€“5), scaling (Figure 6), convergence evidence (Figure 7), and behavior analyses (Figures 8, 10â€“13), which together make a convincing case that Agentic CPT yields a stronger agentic foundation.

## 6. Limitations and Trade-offs
- Dependence on synthetic supervision:
  - FAS/HAS rely on LLMâ€‘asâ€‘judge filtering and correctness checks (Sections 2.2.2â€“2.2.3; Appendix B.1). Judge errors can pass flawed reasoning into CPT. The final answers are validated, but intermediate step quality is only indirectly controlled.
  - HAS does not execute alternative tool actions; it assumes semantic plausibility without verifying actual outcomes (Section 2.3). This may teach good decision heuristics but not true environmentâ€‘level causality.
- Language/domain skew:
  - Lower BrowseCompâ€‘zh relative to the best openâ€‘source baseline is attributed to less Chinese data and possible search bias (Section 3.2). Agentic CPTâ€™s data mixture may underâ€‘represent certain languages or domains.
- Compute and data scale:
  - CPT uses very large token budgets (e.g., 200B + 100B in Section 2.1 and up to 315B in scaling experiments; Figure 6b) and long contexts (128K in Stage 2). This brings significant compute/memory costs.
- Tool set and environment scope:
  - Experiments use five core tools (Appendix A.1) and a specific browsing stack (Google search, Jina). Different environments or APIs (e.g., nonâ€‘Google ecosystems) may change performance due to retrieval quality.
- Evaluation coverage:
  - GAIA evaluation uses only the textâ€‘only subset (103 questions; Tables 1 and Section 3.2), so multimodal research capabilities are not tested here.
- Reproducibility of some baselines:
  - Several comparator scores come from provider reports or prior work (Tables 1â€“2). Differences in tool backends and prompts may affect crossâ€‘paper comparability.

## 7. Implications and Future Directions
- Field impact:
  - Establishes â€œagentic foundation modelsâ€ as a new target: preâ€‘align agent behavior during CPT so that postâ€‘training can focus on alignment refinements rather than capability acquisition (Figure 2; Section 3.6.1).
  - Demonstrates that largeâ€‘scale, toolâ€‘free synthesis (FAS) and trajectory reuse (HAS) can unlock openâ€‘source agents that approach or surpass commercial systems on several tasks (Tables 1â€“2).
- Immediate followâ€‘ups enabled by this work:
  - Stronger multilingual CPT: expand entityâ€‘anchored memories and trajectory sources in underâ€‘represented languages to address BrowseCompâ€‘zh gaps (Section 3.2).
  - Verified stepâ€‘level supervision: combine HAS with selective live execution or simulator feedback to label alternatives, turning contrastive decisions into grounded causal signals.
  - Better judges and filters: calibrate LLMâ€‘asâ€‘judge with metaâ€‘evaluation or human audits to further increase FAS/HAS data reliability (Appendix B.1 shows a promising 82% retained accuracy).
  - Multiâ€‘modal and multiâ€‘agent CPT: extend FAS/HAS to images/tables/videos (the paper cites a multimodal agent in related work) and to cooperative/competitive agent settings.
  - Safety and robustness: integrate SEALâ€‘style adversarial retrieval during CPT to harden agents against misinformation and tool failures (Section 3.1.3; Table 2).
- Practical applications:
  - Enterprise research assistants (market analysis, patent/literature reviews), scientific tooling (systematic evidence synthesis), investigative journalism support, and education (explanatory research reports).
  - The reported ACEBench gains (Table 6) suggest portability to general toolâ€‘use agents beyond web research.

In short, the paperâ€™s central moveâ€”teaching agentic behavior during continual preâ€‘training with scalable, mostly offline dataâ€”reframes how openâ€‘source communities can train robust research agents. The combination of FAS, HAS, and longâ€‘context twoâ€‘stage CPT materially advances both capability and efficiency, while leaving clear avenues for multilingual, multimodal, and causally grounded extensions.

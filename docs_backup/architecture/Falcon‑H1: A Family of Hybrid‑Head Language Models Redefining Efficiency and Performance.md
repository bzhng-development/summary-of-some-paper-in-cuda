# Falconâ€‘H1: A Family of Hybridâ€‘Head Language Models Redefining Efficiency and Performance

**ArXiv:** [2507.22448](https://arxiv.org/abs/2507.22448)
**Authors:** JingweiÂ Zuo, MaksimÂ Velikanov, IlyasÂ Chahed, YounesÂ Belkada, DhiaÂ EddineÂ Rhayem, GuillaumeÂ Kunsch, HakimÂ Hacid, HamzaÂ Yous, BrahimÂ Farhat, IbrahimÂ Khadraoui, MugariyaÂ Farooq, GiuliaÂ Campesan, RuxandraÂ Cojocaru, YasserÂ Djilali, ShiÂ Hu, IhebÂ Chaabane, PuneeshÂ Khanna, MohamedÂ ElÂ AmineÂ Seddik, NgocÂ DungÂ Huynh, PhucÂ LeÂ Khac, LeenÂ AlQadi, BillelÂ Mokeddem, MohamedÂ Chami, AbdalgaderÂ Abubaker, MikhailÂ Lubinets, KacperÂ Piskorski, SlimÂ Frikha
**Institutions:** Technology Innovation Institute (TII)

## ðŸŽ¯ Pitch

Falcon-H1 revolutionizes large language models with a hybrid architecture that combines minimal Transformer attention and a robust State Space Model, achieving performance often surpassing 70B-class models while maintaining remarkable efficiency at long contexts. This innovation is crucial as it offers environmentally sustainable AI solutions and expands accessibility, particularly for real-world applications requiring extensive contexts and modest computational resources.

---

## 1. Executive Summary (2-3 sentences)
Falcon-H1 introduces a â€œhybridâ€‘headâ€ large language model (LLM) family that mixes a small amount of Transformer attention with a larger State Space Model (SSM, specifically Mambaâ€‘2) inside each layer, run in parallel and then fused. With careful architectural choices, training dynamics (effective LR/WD, ÂµP multipliers), tokenizer design, and data strategy, the 34B model often matches or exceeds 70B-class models, while supporting 256K context and delivering large longâ€‘context throughput gains (up to 4Ã— prefill and 8Ã— generation; Â§5.3, Fig. 16).

## 2. Context and Motivation
- Gap addressed
  - Transformers scale quadratically with sequence length, making very long contexts expensive (Â§1). SSMs such as Mamba offer linear-time sequence mixing and strong long-context memory, but pure-SSM models can lose precision on tasks where attention excels (Â§1â€“Â§2).
  - Existing hybrids typically wire SSM and attention sequentially and keep their channel sizes equal, limiting flexibility (e.g., Jamba, Samba, Zamba; Â§1, Â§2). Mamba design hyperparameters are also less explored than standard Transformers (Â§2.2).
- Why it matters
  - Real deployments increasingly require long-context (RAG systems, large documents, multi-turn dialogue), multilingual coverage, and efficient inference on modest hardware. Improving parameter and training efficiency reduces cost and environmental impact while expanding accessibility (Â§1, Â§7).
- Prior approaches and their limits
  - Pure Transformers: strong accuracy, poor long-context efficiency (Â§1). 
  - Pure SSMs (e.g., RWKV, Mamba): efficient, but can struggle on some precision-demanding tasks and lack mature training recipes (Â§1â€“Â§2.2).
  - Earlier hybrids: mostly sequential wiring that forces equal attention/SSM dimensions, curbing design freedom (Â§2, contrast to Dong et al., 2024).
- Positioning
  - Falcon-H1 adopts a parallel hybrid block that lets attention and SSM see the same input and then concatenates their outputs, so their channel allocations can be tuned independently (Â§2, Fig. 1; Â§2.1, Eqs. 3â€“5). The series revisits SSM hyperparameters, training stability, effective schedule design, tokenizer and data strategy, and distributed training to make hybrids practical at multiple scales (0.5Bâ†’34B; Table 1).

## 3. Technical Approach
This section explains how Falcon-H1 is built and trained, why those choices were made, and how the system is evaluated and deployed.

- Hybrid-head block (how a layer works)
  - Each decoder block has two token mixers in parallel: an attention head group and a Mambaâ€‘2 SSM head group. Both consume the same normalized input; their outputs are concatenated and projected back to the model dimension (Â§2, Fig. 1).
  - The block order is â€œsemiâ€‘parallelâ€: attention and SSM run in parallel, then MLP runs on the residual updated by both (â€œSA_Mâ€ in Â§2.1, Eq. 4). This outperformed fully-parallel and fully-sequential variants (Fig. 2 right).

- Channel allocation (how many channels go to SSM vs. attention vs. MLP)
  - Falcon-H1 discretizes the inner dimensions into 8 â€œchunksâ€ that can be allocated across SSM, attention, and MLP (Â§2.1, Eq. 1â€“2).
  - Exhaustive sweeps on ~1.2B proxies show:
    - Putting more channels into attention hurt performance; the best attention fraction is minimal (1/8 of chunks; Fig. 2 left).
    - The best block order is SA_M with a roughly 2:1:5 ratio across SSM:Attention:MLP (within a flat optimum region that eases size-specific adjustments; Fig. 2 right, Â§2.1).
  - Result: most mixing work is offloaded to SSMs; a small attention slice is retained for precision (Â§2.1).

- Mambaâ€‘2 SSM design (what happens inside the SSM and why itâ€™s stable)
  - Mambaâ€‘2 processes a sequence recurrently with a hidden state `h`:
    - Update equations (Â§2.2, Eq. 6): `h_{t+1} = A_t h_t + B_t dt_t x_t`; `y_t = C_t^T h_t + D x_t`.
    - Equivalent view as a causal â€œattention-likeâ€ matrix over time (Â§2.2, Eq. 7).
    - Inputs are projected and gated with SiLU and a depthwise causal 1D convolution before recurrence (Â§2.2, Eqs. 8â€“9).
  - Key ablations and choices (Â§2.2):
    - State dimension `d_state` vs groups `n_g`: at fixed parameter budget, increasing `d_state` improves accuracy far more than increasing groups; throughput peaks near `d_state=16`, but for long contexts they choose `(n_g, d_state)=(1,256)` (Fig. 3). For 34B with TP=4, `n_g=2` for divisibility (Â§2.2).
    - SSM head dimension `d_head`: larger heads (â‰¥64) give better accuracy and efficiency (Fig. 4a).
    - Depthwise conv kernel: kernel size 4 minimizes loss; both smaller and larger are worse (Fig. 4b).
    - SSD scan chunk size `cs`: throughput plateaus at 128â€“256; they fix `cs=256` (Â§2.2).
    - Cross-document leakage: they reset the SSM hidden state exactly at document boundaries by injecting a large negative bias into `AÌ„_t` so `exp(-80)â‰ˆ0` (Â§2.2 â€œHidden State Resettingâ€), eliminating contamination without extra compute.
  - Training stability: Early runs showed loss spikes originating in the SSM path when width (many heads) pushes the `dt` (â€œtimeâ€‘stepâ€) activation too high. Clipping or attenuating positive `dt` removesspikes; they adopt a softer attenuation as a ÂµP multiplier in the forward pass (Â§3.2.1).

- Positional encoding choice (RoPE base frequency)
  - They raise the RoPE base `b` to 10^11 (very large). Sweeps (Fig. 5a) show that too-small `b` hurts loss, especially when the training sequence length increases; large `b` flattens the loss curve and avoids needing â€œNTK-awareâ€ tricks when extending context (Â§2.3.1). With large `b`, many very-low-frequency dimensions remain unused at train time, making later context scaling simple.

- Depth vs. width at fixed parameters (why the â€œ1.5Bâ€‘Deepâ€ exists)
  - Under a 1.5B budget, they sweep model depth/width (Fig. 5b). Deeper shapes consistently deliver better pretraining loss, albeit with ~25â€“30% lower throughput. This motivates releasing both a â€œbalancedâ€ 1.5B and a deeper 1.5Bâ€‘Deep (Â§2.3.2).

- Tokenizer (how text is split to tokens and why it matters)
  - BPE tokenizers with 32k/65k/131k/262k vocabularies are trained on 121+ languages; vocab size scales with model size (Table 5; Â§2.4.2).
  - Data scaling: more corpus isnâ€™t always better; optimal size depends on vocab (Table 2, Â§2.4.1).
  - Splitting rules: enabling both punctuation and digit splitting improves code/math and multilingual segmentationâ€”even if fertility (compression) worsens (Table 4; Fig. 6â€“7; Â§2.4.1).
  - Inject domain tokens: adding common LaTeX commands to the vocabulary improves math benchmarks during training (Â§2.4.1, Fig. 8).
  - They reserve 1,024 special tokens for downstream customization (Â§2.4.2).

- Data and curriculum (how data is organized through training)
  - Sources include filtered web (FineWeb-derived), curated multilingual corpora for 18 languages, large code (file- and repo-level with HQ splits), math corpora, and synthetic/rewritten data (Â§3.1.1).
  - Deterministic dataloader reads sources sequentially, enabling reproducible runs and flexible on-the-fly mixture changes and multi-epoch reuse (Â§3.1.2 â€œDeterministic data loadingâ€).
  - Antiâ€‘curriculum: mix â€œhardâ€ and â€œeasyâ€ data from the start rather than staging it later; this outperformed curriculum schedules in their setting (Â§3.1.2 â€œData organization and schedulingâ€).
  - Web fraction is surprisingly low in the final mixtures for large models (e.g., 34B ends at ~15% raw web; Table 6), with rewritten data (web/code/math/curated rewrites) dominating (up to ~52% at 34B end-of-training; Table 6).
  - Memorization window: by checkpoint rollback tests and loss monitoring (Fig. 9), they argue high-quality data can be reused multiple times without overfitting at their scale (Â§3.1.2).

- Training dynamics and schedules (how to set LR/WD robustly)
  - Parameter norms empirically scale with `sqrt(Î·/Î»)` across layers (Fig. 10), which they interpret via a toy stochastic dynamics model (Â§3.2.2, Eqs. 10â€“11 & Appendix B).
  - They define two composite controls:
    - Effective learning rate `Î·_eff = sqrt(Î· Î»)` governs noise level and loss reduction at LR decay (Fig. 11 right; Eq. 12).
    - Effective weight decay `Î»_eff = sqrt(Î»/Î·)` governs parameter norms (Fig. 10 right; Eq. 12).
  - Because `Î·_eff` and `Î»_eff` are orthogonal in logâ€‘space (Eq. 13), grid sweeps can vary noise and norms independently (Â§3.2.2). 
  - Effective Power Scheduler (EPS): instead of standard Power Scheduler that keeps WD constant and scales LR as `t^{-1/2}` on stable stages, they propose keeping `Î»_eff` constant and decaying both LR and WD as `t^{-1/4}` so norms stay stable while `Î·_eff` decays (Eq. 15). EPS improved convergence in their tests (Â§3.2.2).

- ÂµP with forward multipliers (how they transfer hyperparameters across sizes)
  - Maximal update parametrization (ÂµP) predicts how LR/WD/initialization/forward multipliers should scale with width for consistent feature learning. Instead of using different LR/WD per size, Falconâ€‘H1 fixes optimizer hyperparameters across sizes and moves ÂµP scaling into explicit forward multipliers placed throughout the model (Table 7; Â§3.2.3).
  - They then â€œtune ÂµP multipliersâ€ at a base shape (L=66, d=1280) via stagewise microâ€‘sweeps (Appendix C) across 35 multipliers covering forward paths, matrix ELR/EWD groups, and vector LR multipliers (Table 8, Fig. 12). Sensitivity analysis shows ELR multipliers have highest impact, followed by forward multipliers (Â§3.2.3).
  - Practical bonuses: same LR/WD for all sizes; stable transfers; fewer optimizer parameter groups.

- Rampup, batch scaling, warmup
  - LR scales with sqrt(batch) when batch changes (Eq. 19), which preserves learning better than no scaling (Â§3.2.4).
  - Batch rampup with LR batchâ€‘scaling may look worse early but wins later, likely because it guides optimization to a better region (Fig. 13).
  - Short LR warmup (~0.1 GT) has a longâ€‘lasting positive impact on loss (Fig. 13, bottom-right).

- Distributed training and inference
  - Training infrastructure (â€œMambatronâ€) supports 5D parallelism: Data, Tensor, Pipeline, Context (long sequences), and a new Mixer Parallelism (MP) (Â§3.3, Table 9).
  - Mixer Parallelism: split the TP world so attention and SSM compute concurrently per layer; interleaving layers across groups balances load (Fig. 14). Interleaved MP improves training throughput by 1.43Ã— over no MP (Table 10).
  - MP for inference: helps when batch and generation lengths are small; benefits reduce or reverse at large batches/long generations (Fig. 15; Â§3.3.2).
  - Context Parallelism: attention uses RingAttention (K/V circulate around a ring), SSM uses chunked state passing; both keep perâ€‘GPU memory O(chunk-length) (Â§3.3.3).

- Postâ€‘training
  - SFT: 6 GT totalâ€”3 GT at 16K, then 3 GT at 128K with LR fixed at Î·/8; data weighted toward high-quality instruction corpora (e.g., Tulu3). No WD during SFT (Â§4.2, Table 11).
  - DPO: standard DPO loss with Î²=5; best stopping point is ~1 epoch rather than the full 2â€‘epoch LR schedule (Â§4.3, Table 12).

## 4. Key Insights and Innovations
- Parallel hybrid mixer with independent channel allocation (fundamental)
  - Most prior hybrids forced equal attention/SSM dimensions or ran them strictly in series. Falconâ€‘H1 concatenates parallel outputs and tunes each mixerâ€™s width independently (Â§2, Fig. 1), allowing a small attention sliver and a large SSM core. Systematic sweeps show the best performance with the minimal attention fraction (1/8) and SA_M block order (Fig. 2). This design drives both accuracy and efficiency, especially at long contexts.

- SSM-specific, longâ€‘contextâ€‘oriented design and stability (fundamental)
  - Carefully chosen Mambaâ€‘2 hyperparametersâ€”large `d_state`, head sizes â‰¥64, conv kernel=4, chunk size=256â€”plus exact hiddenâ€‘state reset at doc boundaries and `dt` attenuation remove instability and preserve longâ€‘range memory (Â§2.2). These are the practical recipes missing from many SSM reports.

- Effective LR/WD decomposition and schedule (conceptual + practical)
  - The identification of `Î·_eff = sqrt(Î·Î»)` as the main â€œnoiseâ€ control and `Î»_eff = sqrt(Î»/Î·)` as the main â€œnormâ€ control is supported by loss and norm measurements (Fig. 10â€“11; Eq. 12). Building on that, the Effective Power Scheduler (Eq. 15) decays LR and WD together to keep parameter norms stable while reducing noise (Â§3.2.2). This provides a simple, transferable way to set LR/WD across scales.

- ÂµP with tuned forward multipliers (practical transfer)
  - Shifting ÂµP scaling into forward multipliers and tuning a minimal, architectureâ€‘aware set (Table 7â€“8) lets all model sizes share the same optimizer hyperparameters. Sensitivity diagnostics (Fig. 12) clarify what matters most (matrix ELR multipliers), enabling robust, computeâ€‘efficient HP transfer (Â§3.2.3, Appendix C).

- Tokenizer provenances that matter for downstream tasks (applied but important)
  - Two decisionsâ€”(1) enabling both digit and punctuation splitting and (2) injecting frequent LaTeX tokensâ€”consistently improved code and math performance, despite modest changes in compression metrics (Fig. 6â€“8; Table 3â€“4; Â§2.4.1). This shifts tokenizer tuning away from proxy metrics and toward downstream outcomes.

- Data mixture that deemphasizes raw web and embraces rewriting (applied)
  - The final largeâ€‘model mixtures use only ~12â€“15% raw web and >50% rewrites (Table 6), with an antiâ€‘curriculum schedule and deterministic loader enabling multiâ€‘epoch reuse without observed memorization issues (Fig. 9; Â§3.1.2). This is a distinctive data philosophy aimed at â€œknowledge density,â€ not just token count.

- Mixer Parallelism for hybrid models (systems innovation)
  - Interleaving attention and SSM across TP groups allows genuine concurrency within a layer. It yields 1.43Ã— training throughput versus a nonâ€‘MP baseline (Table 10) and can speed up inference in low-latency regimes (Fig. 15; Â§3.3.2).

## 5. Experimental Analysis
- Evaluation methodology
  - Frameworks: lmâ€‘evaluationâ€‘harness, evalchemy, evalplus, HELMET (Â§5). For math in evalchemy, they use 16 generation turns and a fixed system prompt, and postâ€‘check with Mathâ€‘Verify (Â§5).
  - Standardization: same Docker environment; â€œthinking modeâ€ disabled for Qwen3 so inference is comparable (Â§5).
  - Benchmarks
    - General: BBH, ARCâ€‘C, HellaSwag, Winogrande, MMLU.
    - Math: GSM8K, MATH (lvl5 or 500), AMCâ€‘23, AIMEâ€‘24/25.
    - Science: GPQA (+Diamond), MMLUâ€‘Pro, MMLUâ€‘STEM.
    - Code: HumanEval(+), MBPP(+), LiveCodeBench, CRUXEval.
    - Multilingual: Multiâ€‘HellaSwag, Multiâ€‘MMLU, MGSM (6 languages).
    - Longâ€‘context: HELMET LongQA, RAG, Recall (8kâ†’131k).
    - Efficiency: vLLM throughput vs Qwen2.5â€‘32B (TP=2; H100 GPUs; Â§5.3).

- Main quantitative results (selected highlights; bold shows standout points)
  - Base models
    - 0.5B (Table 14): 
      > GSM8K 60.20 vs Qwen3â€‘0.6B 50.04; MATHâ€‘lvl5 15.18 vs 9.29; HumanEval+ 31.10 vs 27.44.  
      This 0.5B model â€œleads on every Math, Science, and Code benchmarkâ€ among subâ€‘1B baselines (Â§5.1).
    - 1.5Bâ€‘Deep vs peers (Table 15):
      > MMLU 66.29; MMLUâ€‘Pro 41.07 (vs Qwen3â€‘1.7B 33.81); MBPP 70.90; GSM8K 68.69.  
      Deeper 1.5B consistently outperforms the shallower 1.5B and many 7B-class results on several tasks (Â§5.1).
    - 3B (Table 16):  
      > MATHâ€‘lvl5 25.83 (SOTA at this scale); MGSM 64.00.  
      Despite only 2.5T tokens (vs Qwen3â€‘4Bâ€™s 36T), math and multilingual math are strong.
    - 7B (Table 17):
      > MMLU 77.38 (best); MATHâ€‘lvl5 34.67 (best); MBPP 78.57 (best); MGSM 74.53 (best).  
      Clearly competitive across reasoning, science, code, and multilingual.
    - 34B (Table 18):
      > BBH 69.36 (best vs 32â€“72B and 70B baselines); MATHâ€‘lvl5 40.71 (best); GPQA 42.70 (best); HumanEval 70.12 (best); MGSM 82.40 (topâ€‘tier).  
      On general knowledge (MMLU/HellaSwag), 70B models sometimes edge out, but 34B is often second-best.
  - Instruct models
    - 0.5B (Table 20):
      > GSM8K 68.39 (best); MATHâ€‘500 58.40 (best); HumanEval 51.83 (best); IFEval 72.07 (best).  
      Emphasizes robust reasoning and instruction following at tiny scale.
    - 1.5Bâ€‘Deep (Table 21):
      > GSM8K 82.34; MATHâ€‘500 77.80; HumanEval 73.78; GPQA_Diamond 40.57; IFEval 83.50.  
      Outperforms sameâ€‘size and many larger baselines broadly; 1.5B (shallow) is usually second-best.
    - 3B (Table 22): 
      > MMLU 68.30 (top); GPQA_Diamond 38.72; MBPP 79.63; MGSM 63.90.  
      Very balanced across domains.
    - 7B (Table 23):
      > HumanEval 86.59 (best); GPQA_Diamond 56.90 (best); MMLUâ€‘Pro 51.75 (best); strong across multilingual tasks (e.g., Multiâ€‘MMLU 67.83).  
      Qwen3â€‘8B does better on some math and preference tests (AIME, Alpacaâ€‘Eval).
    - 34B vs 32â€“72B and 70B (Table 24):
      > Science suite leadership: GPQA 41.53 (best), GPQA_Diamond 49.66 (best), MMLUâ€‘STEM 83.57 (best).  
      > MTBench 9.20 (best conversational quality).  
      For Math/Code, 70Bâ€‘class models sometimes lead; for general reasoning and science, Falconâ€‘H1â€‘34B is extremely competitive.
  - Longâ€‘context (HELMET; Table 25)
    - RAG: 
      > 131k tokens â€” 62.21 (best; Qwen3â€‘32B 57.08; Llamaâ€‘3.3â€‘70B 55.38; Qwen2.5â€‘72B 42.33).  
      Strong evidence Falconâ€‘H1â€™s hybrid design and training are wellâ€‘suited to documentâ€‘augmented tasks at extreme lengths.
    - Recall and longQA:
      > At 131k, Recall 56.63 (lower than Qwen3â€‘32B 86.13 and Llamaâ€‘3.3â€‘70B 82.19), and longQA 33.81 (vs Qwen3â€‘32B 53.52).  
      Authors attribute this to longâ€‘context data composition; it flags an avenue for further data work (Â§5.2).
  - Efficiency (Fig. 16; Â§5.3)
    - Prefill throughput: at long inputs (up to 262k), Falconâ€‘H1â€‘34B is up to 4Ã— faster than Qwen2.5â€‘32B.
    - Generation throughput: at long outputs (up to 262k), up to 8Ã— faster.
    - At short contexts, an optimized Transformer can be slightly fasterâ€”SSM kernels are less mature.

- Ablations and robustness
  - Channel allocation and block order sweeps (Fig. 2) substantiate the SA_M choice and minimal attention fraction.
  - SSM ablations (Fig. 3â€“4) justify `d_state`, head dims, conv kernel, and chunk size.
  - Instability investigations (Fig. 13; Â§3.2.1) identify the `dt` activation as the culprit and show attenuation solves spikes.
  - ELR/EWD studies (Fig. 10â€“11) support the composite-control view and motivate the EPS schedule.
  - Tokenizer and data studies (Table 2â€“4; Fig. 6â€“8; Table 6; Fig. 9) demonstrate downstream gains from punctuation/digit splitting, LaTeX tokens, and rewritten/antiâ€‘curriculum data.

- Do the experiments support the claims?
  - Yes, for three central claims:
    - Hybrid-head with tuned channel allocation achieves strong accuracy: across scales, Falconâ€‘H1 holds or leads on many reasoning, science, code, and multilingual tasks (Tables 14â€“18, 20â€“24).
    - Longâ€‘context capability and efficiency: HELMET RAG at 131k is best; detailed systems design (CP/MP) plus SSM yields large throughput gains (Table 25; Fig. 16; Â§3.3).
    - Parameter/training efficiency: 34B rivals or beats 70B models in several categories; 1.5Bâ€‘Deep rivals many 7â€“10B models (summaries in Â§5.1â€“Â§5.2).  
  - Mixed results on a few fronts (e.g., HellaSwag at some scales, HELMET longQA/Recall at 131k) are acknowledged, with plausible causes (data composition, maturity of SSM kernels).

## 6. Limitations and Trade-offs
- Attention is small by design
  - Minimal attention width (1/8 fraction) is optimal in their sweeps (Fig. 2), but edge cases might benefit from more attention (e.g., tasks dominated by global tokenâ€‘token interactions). The flat optimum near 2:1:5 SSM:Attn:MLP allows some flexibility (Â§2.1), but the reported settings skew toward SSM.
- Longâ€‘context QA and recall at extreme lengths
  - While RAG excels at 131k, pure recall/QA is weaker than some Transformers (Table 25). The authors attribute this to longâ€‘context data composition rather than architecture, but it remains a practical tradeâ€‘off to address in future mixtures.
- Efficiency depends on SSM kernels
  - At short contexts, Qwen2.5â€‘32B has marginally higher throughput; SSM kernels are newer and less optimized in inference stacks (Â§5.3). Gains emerge strongly at very long contexts.
- Complexity of training recipe
  - The full recipe includes ELR/EWDâ€‘aware schedules, tuned ÂµP multipliers, SSMâ€‘specific resets and `dt` attenuation, deterministic loading, antiâ€‘curriculum mixing, and MP/CP strategies. Replicating all parts could be nontrivial.
- Data composition and potential biases
  - Heavy reliance on rewritten data (>50% for large models; Table 6) can shape model style and knowledge coverage in ways different from raw web. Memorization analysis (Fig. 9) is suggestive but not exhaustive.
- Multilingual scope
  - Multilingual coverage is strong (18 languages), but the smallest 0.5B is Englishâ€‘only (Â§1, Table 1), and performance varies by language (Appendix D.1â€“D.2). True â€œ100+ languageâ€ scaling is stated as feasible, not demonstrated (Â§1).
- Theory is mostly empirical
  - ELR/EWD motivation includes a toy model (Appendix B), but formal proofs that strictly predict optimal exponents or schedules arenâ€™t provided; results are empirical and could be contextâ€‘dependent (Â§3.2.2).

## 7. Implications and Future Directions
- How it changes the landscape
  - Establishes a practical, strong-performing blueprint for attentionâ€“SSM hybrids: parallel mixing with small attention, carefully tuned SSM hyperparameters, and stability methods. It shows hybrids can be parameterâ€‘ and computeâ€‘efficient without sacrificing accuracy, and can excel in longâ€‘context settings (RAG at 131k) where Transformers struggle.
- What it enables next
  - Research:
    - Dynamic channel allocation per layer or per task, or learned attention/SSM ratios.
    - Extending ELR/EWD analysis into formal optimization theory; exploring EPS variants.
    - Better longâ€‘context data recipes to push longQA/Recall at 256k+.
    - Kernel engineering for SSMs (e.g., fused ops, caching) to remove shortâ€‘context throughput deficits.
    - Applying largeâ€‘`b` RoPE strategy to pure Transformers to test generality (Â§2.3.1).
  - Practical applications:
    - Enterprise RAG systems over very long corpora (legal, scientific, codebases) with strong speed/quality at 32kâ€“256k contexts.
    - Edge or resourceâ€‘constrained deployments: 0.5B and 1.5Bâ€‘Deep deliver multiâ€‘domain competence comparable to older 7â€“10B models (Â§1, Â§5.1â€“Â§5.2).
    - Multilingual assistants covering 18 languages with good math/code capabilities and 256k context.
    - Code reasoning agents benefiting from tokenizer and repoâ€‘level pretraining (file+repo HQ splits; Â§3.1.1).

> Summary of impact: Falconâ€‘H1 is not just a new model family; it is an endâ€‘toâ€‘end methodologyâ€”architecture, tokenizer, data, training dynamics, and systemsâ€”that shows hybrid attentionâ€‘SSM LLMs can be smaller, faster at long contexts, and still competitive with much larger baselines. The openâ€‘weight release across many sizes and the detailed ablations make it a strong foundation for both research and realâ€‘world deployments.

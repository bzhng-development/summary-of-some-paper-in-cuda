# DeepSeek-Coder: When the Large Language Model Meets Programming â€“ The Rise of Code Intelligence

**ArXiv:** [2401.14196](https://arxiv.org/abs/2401.14196)
**Authors:** Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y.Â Wu, Y.â€¯K.Â Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang
**Institutions:** 

## ğŸ¯ Pitch

DeepSeek-Coder revolutionizes open-source code-specialized models by implementing repository-level training and a balanced Fill-in-the-Middle objective, setting new state-of-the-art results. This enables greater project-wide understanding and outperforms many leading closed models, all while providing unrestricted access for wider research and practical applications in IDEs and educational tools.

---

## 1. Executive Summary (2-3 sentences)
DeepSeekâ€‘Coder introduces an openâ€‘source family of code-specialized large language models (`1.3B`, `6.7B`, `33B`) trained from scratch on 2 trillion tokens with projectâ€‘level (repositoryâ€‘level) code organization and a balanced Fillâ€‘inâ€‘theâ€‘Middle (`FIM`) objective. Across code generation, crossâ€‘file completion, code infilling, and programâ€‘aided math reasoning benchmarks, the models set new state-of-the-art results for open models and in several cases surpass closed models such as Codex and GPTâ€‘3.5, while offering a 16K token context window and a permissive license (Abstract; Fig. 1; Sec. 4; Table 3â€“8).

## 2. Context and Motivation
- Problem addressed:
  - Open code LLMs lag behind closed models in performance and capabilities (Introduction).
  - Most prior pretraining treats each source file independently, ignoring crossâ€‘file dependencies needed for real project work (Sec. 2.2).  
- Why this matters:
  - Practical coding involves reading and modifying multiple files that depend on each other; IDE assistance, repository modernization, and automated refactoring all require repository-level understanding.
  - Wider research and commercial access needs strong open models under permissive licenses (Abstract).
- Prior approaches and gaps:
  - Open models like StarCoder and CodeLlama are trained mostly on fileâ€‘level data and typically use FIM, but do not organize data at repository level nor thoroughly analyze FIM tradeâ€‘offs (Introduction; Sec. 2.2; Sec. 3.1.2).
  - Closed systems (e.g., GPTâ€‘3.5/4) achieve strong results but are proprietary, limiting reproducibility and control (Introduction).
- Positioning:
  - DeepSeekâ€‘Coder contributes both model weights and a training recipe emphasizing repositoryâ€‘level construction, balanced FIM, extended context, and extensive evaluation. It aims to narrow the gap with GPTâ€‘4 while remaining fully open (Abstract; Sec. 4).

## 3. Technical Approach
This section explains the endâ€‘toâ€‘end pipeline: data, repository organization, deduplication and quality control, model design, training objectives, long context adaptation, and instruction tuning.

- Data construction (Sec. 2; Fig. 2; Table 1):
  - Composition: 87% source code across 87 languages, 10% English codeâ€‘related text (GitHub Markdown, StackExchange), 3% nonâ€‘code Chinese natural language (Sec. 2).
  - Ruleâ€‘based filtering: reuse StarCoderâ€‘style rules to remove overly long lines, low alphabetic content, XMLâ€‘like files (except XSLT), HTML with low visibleâ€‘text ratio, and sizeâ€‘bounded JSON/YAML (Sec. 2.1).
  - Quality screening and decontamination: compile checks and a quality model plus heuristics; nâ€‘gram filtering removes overlaps with popular benchmarks such as HumanEval/MBPP/GSM8K/MATH (Sec. 2.4).

- Repositoryâ€‘level dependency parsing and packing (Sec. 2.2; Algorithm 1):
  - Goal: preserve crossâ€‘file dependencies by concatenating files in an order where a fileâ€™s prerequisites appear earlier.
  - How it works:
    - Extract simple static import/use/include edges via regular expressions per language (e.g., Python `import`, C# `using`, C `include`).
    - Build an adjacency list and inâ€‘degree map over files in a repo (Algorithm 1, lines 1â€“16).
    - For each disconnected subgraph, perform a modified topological sort: iteratively pick the node with minimum inâ€‘degree (not necessarily zero) to tolerate cycles, decrement inâ€‘degrees of its outgoing neighbors, append to result (lines 18â€“32).
    - Concatenate the ordered file contents into one training sample, prefixing each file with a comment indicating its path to preserve location information (end of Sec. 2.2).
  - Why this choice: static regex-based extraction is simple and languageâ€‘portable; the â€œminimal inâ€‘degreeâ€ variant resolves small cycles without discarding files, ensuring trainable sequences.

- Repositoryâ€‘level deduplication (Sec. 2.3):
  - Instead of fileâ€‘level dedup (common in prior work), treat the concatenated repository string as the unit for nearâ€‘duplicate removal.  
  - Rationale: fileâ€‘level dedup can remove random files and destroy dependency structure; repoâ€‘level keeps project integrity.

- Training objectives (Sec. 3.1):
  - Next Token Prediction (Sec. 3.1.1): standard autoregressive objective on packed sequences.
  - Fillâ€‘inâ€‘theâ€‘Middle (`FIM`) (Sec. 3.1.2):
    - What is FIM: a pretraining task where each document is split into `prefix`, `middle`, `suffix`; the model is fed rearranged text and must generate the missing middle given both sides.
    - Modes:
      - `PSM` = Prefixâ€“Suffixâ€“Middle.
      - `SPM` = Suffixâ€“Prefixâ€“Middle.
      - A variant `MSP` (Masked Span Prediction) masks multiple spans (as in T5) for reconstruction.
    - Implementation:
      - Use three sentinel tokens `<|fim_start|>`, `<|fim_hole|>`, `<|fim_end|>`.
      - Example packed format (PSM): `<|fim_start|> f_pre <|fim_hole|> f_suf <|fim_end|> f_middle <|eos_token|>` (Sec. 3.1.2).
      - Apply at document level before sequence packing at a 50% rate in final models (Sec. 3.1.2).
    - Design choice: Ablation (Fig. 3) shows 100% FIM maximizes singleâ€‘line infilling but hurts normal completion; 50% PSM balances both and outperforms MSP at 50%.

- Tokenizer and architecture (Sec. 3.2â€“3.3; Table 2):
  - BPE tokenizer with 32k vocab (Sec. 3.2).
  - Decoderâ€‘only Transformer with RoPE positional encoding; `33B` model uses `Groupedâ€‘Query Attention (GQA)` with group size 8 to speed inference; `FlashAttention v2` for efficient attention computation (Sec. 3.3).
  - Key sizes (Table 2):  
    - `1.3B`: 24 layers, 2048 hidden, 16 heads.  
    - `6.7B`: 32 layers, 4096 hidden, 32 heads.  
    - `33B`: 62 layers, 7168 hidden, 56 heads, GQA(8).

- Optimization and infrastructure (Sec. 3.4â€“3.5):
  - AdamW (Î²1=0.9, Î²2=0.95). Threeâ€‘stage LR schedule with 2000 warmâ€‘up steps; each stageâ€™s LR scaled by âˆš(1/10) vs the previous; final LR = 10% of initial (Sec. 3.4).
  - Training with HAIâ€‘LLM framework using tensor parallelism, ZeRO data parallelism, and pipeline parallelism on A100/H800 clusters connected by NVLink/NVSwitch and InfiniBand (Sec. 3.5).

- Long context adaptation (Sec. 3.6):
  - Reconfigure RoPE with linear scaling factor 4 and base frequency 100000 (vs 10000).  
  - Extra 1000 steps of training at 16K sequence length; theoretically supports up to 64K tokens, but empirical reliability is best at 16K.

- Instruction tuning (Sec. 3.7):
  - Create `DeepSeekâ€‘Coderâ€‘Instruct` by fineâ€‘tuning base models on highâ€‘quality instructions in Alpaca format.  
  - Special delimiter `<|EOT|>` marks end of each conversational turn; cosine LR schedule with 100 warmâ€‘up steps; LR `1eâ€‘5`; batch of 4M tokens; total 2B tokens.

- Continued pretraining from a general LLM (Sec. 5; Table 9â€“10):
  - `DeepSeekâ€‘Coderâ€‘v1.5 7B`: initialize from DeepSeekâ€‘LLMâ€‘7B and continue pretraining on 2T tokens with a 4K context and only the nextâ€‘token objective.
  - Data mix emphasizes 70% source code, plus natural language and math text to improve broader reasoning (Table 9).

## 4. Key Insights and Innovations
- Repositoryâ€‘level construction and ordering (Sec. 2.2â€“2.3):
  - Novelty: training samples are whole repositories, files ordered by inferred dependencies and deduplicated at repo granularity.
  - Why it matters: preserves crossâ€‘file context that fileâ€‘level corpora discard; improves crossâ€‘file completion (Table 7 shows higher exact match with retrieval vs other 7Bâ€‘scale models; removing repoâ€‘level pretraining reduces EM across Java/TS/C#).

- Balanced `FIM` at 50% `PSM` rate with custom sentinels (Sec. 3.1.2; Fig. 3):
  - Novelty: systematic ablation that clarifies the tradeâ€‘offâ€”100% FIM maximizes infilling but harms general completion; `PSM@50%` outperforms `MSP@50%`.
  - Impact: high singleâ€‘line infill accuracy without sacrificing standard code completion (Table 6 shows mean infilling accuracy 80.7% for `7B` and 81.2% for `33B`, strong vs CodeLlama).

- Repositoryâ€‘aware deduplication (Sec. 2.3):
  - Novelty: deduplicate after concatenating the repo to avoid breaking structure.  
  - Impact: likely reduces overfitting to repeated boilerplate while keeping dependency graph intact; though not isolated as an ablation, it is integral to the crossâ€‘file gains.

- Longâ€‘context training with RoPE scaling to 16K tokens (Sec. 3.6):
  - Novelty: simple yet effective RoPE scaling and short continued training yield reliable 16K context processing for large repositories.
  - Impact: supports projectâ€‘wide tasks and â€œfillâ€‘inâ€‘middleâ€ with long prefixes/suffixes.

- Open models with strong sizeâ€‘efficiency (Abstract; Table 3â€“8):
  - Observation: `DeepSeekâ€‘Coderâ€‘Base 6.7B` matches or beats `CodeLlamaâ€‘Base 34B` on many tasks (e.g., HumanEvalâ€‘X average 44.7% vs 41.0% in Table 3), highlighting data/recipe quality beyond scale.

## 5. Experimental Analysis
- Evaluation setup and metrics (Sec. 4):
  - Benchmarks:
    - Code generation: HumanEval and MBPP; multilingual HumanEvalâ€‘X across Python/C++/Java/PHP/TS/C#/Bash/JS (Sec. 4.1; Table 3).
    - Practical dataâ€‘science tasks: DSâ€‘1000 across seven libraries (Table 4).
    - LeetCode Contest: 180 recent problems (Jul 2023â€“Jan 2024) with 100 tests each; zeroâ€‘shot prompting; Chainâ€‘ofâ€‘Thought (CoT) variant examined (Sec. 4.1; Table 5).
    - FIM code completion: Singleâ€‘Line Infilling for Python/Java/JS, metric = line exact match (Sec. 4.2; Table 6).
    - Crossâ€‘file completion: CrossCodeEval with exact match (EM) and edit similarity (ES), with/without BM25 retrieval (Sec. 4.3; Table 7).
    - Programâ€‘aided math reasoning (`PAL`): GSM8K, MATH, GSMâ€‘Hard, SVAMP, TabMWP, ASDiv, MAWPS solved by alternating natural language and Python (Sec. 4.4; Table 8).
  - Baselines: CodeGeeX2, StarCoder, CodeLlama families; Codex (`codeâ€‘cushmanâ€‘001`); GPTâ€‘3.5â€‘Turbo and GPTâ€‘4â€‘Turbo for instruct comparisons (Sec. 4).
  - Decoding: for HumanEval/MBPP, greedy decoding with matched scripts to ensure fairness (Sec. 4.1).

- Main quantitative results (selected):
  - HumanEvalâ€‘X and MBPP (Table 3):
    > `DeepSeekâ€‘Coderâ€‘Base 33B`: average 50.3% on HumanEvalâ€‘X and 66.0% on MBPP.  
    > Beats `CodeLlamaâ€‘Base 34B` by +9.3 points (50.3 vs 41.0) on HumanEvalâ€‘X average and +10.8 on MBPP (66.0 vs 55.2).  
    > `DeepSeekâ€‘Coderâ€‘Instruct 33B`: HumanEvalâ€‘X average 69.2%, exceeding `GPTâ€‘3.5â€‘Turbo` 64.9%, though still below `GPTâ€‘4` 76.5%.
  - DSâ€‘1000 (Table 4):
    > `DeepSeekâ€‘Coderâ€‘Base 33B` achieves 40.2% average across libraries, outperforming `CodeLlamaâ€‘Base 34B` (34.3%). Gains are broad: e.g., NumPy 49.6% vs 42.7%, PyTorch 36.8% vs 25.0%.
  - LeetCode Contest (Table 5):
    > `DeepSeekâ€‘Coderâ€‘Instruct 33B` 27.8% overall Pass@1 (Easy 57.8, Medium 22.0, Hard 9.1), the only open model outperforming `GPTâ€‘3.5â€‘Turbo` (23.3%). With CoT prompting, 28.9%. Still behind `GPTâ€‘4â€‘Turbo` 40.6% (41.8% with CoT).
    > The paper flags possible data contamination in the earliest contest months but uses recent problems to minimize it (note under Table 5).
  - FIM infilling (Table 6):
    > `DeepSeekâ€‘Coderâ€‘Base 7B` mean 80.7% and `33B` mean 81.2%; both exceed `CodeLlamaâ€‘Base 13B` mean 75.5% and `StarCoder 16B` mean 69.7%.
  - Crossâ€‘file completion (Table 7):
    > Without retrieval, `DeepSeekâ€‘Coderâ€‘Base 6.7B` has the best EM across all four languages among 7Bâ€‘scale peers. With retrieval, EM improves furtherâ€”e.g., Python 16.14% vs 13.06% (StarCoder) and 13.02% (CodeLlama).  
    > Removing repoâ€‘level pretraining reduces EM in Java/TS/C# (e.g., C#: 16.23% â†’ 14.48%), directly evidencing the benefit of repositoryâ€‘level data.
  - Programâ€‘aided math (Table 8):
    > `DeepSeekâ€‘Coderâ€‘Base 33B` average 65.8% vs `CodeLlamaâ€‘Base 34B` 62.0%, with strong results on GSM8K 60.7% and MAWPS 93.3%.
  - Continued pretraining (Table 10):
    > `DeepSeekâ€‘Coderâ€‘Baseâ€‘v1.5 6.9B` improves nonâ€‘code reasoning (MMLU 49.1 vs 36.6; HellaSwag 69.9 vs 53.8) while maintaining similar code scores (HumanEval 43.2 vs 44.7; MBPP 60.4 vs 60.6).  
    > The instruct version gains even more on math/NL (e.g., GSM8K 72.6% vs 62.8%).

- Ablations and training curves:
  - FIM rate ablation (Fig. 3):  
    > 100% FIM maximizes HumanEvalâ€‘FIM but depresses HumanEval/MBPP completion; 50% PSM strikes a balance and beats `MSP@50%`.
  - Repoâ€‘pretraining ablation (Table 7 last row): measurable drops w/o repo pretraining, validating the design.
  - Learning progress (Fig. 7): performance rises smoothly with tokens for all model sizes across multiple metrics, indicating effective scaling to 2T tokens.

- Do the experiments support the claims?
  - Yes for openâ€‘model SOTA: across diverse tasks, `DeepSeekâ€‘Coder` consistently tops open baselines of similar or larger size.  
  - Yes for repositoryâ€‘level benefit: ablations and crossâ€‘file benchmark substantiate it.  
  - Yes for FIM balance: ablation demonstrates the claimed tradeâ€‘off.

## 6. Limitations and Trade-offs
- Static dependency parsing (Sec. 2.2):
  - Uses regex over `import/using/include`. This misses dynamic imports, reflective loading, buildâ€‘generated code, and languageâ€‘specific module systems; dependency order can be incomplete in such projects.
- FIM tradeâ€‘off (Fig. 3):
  - High FIM ratios improve infilling but harm standard completion; the chosen 50% PSM is a compromise, not optimal for either extreme.
- Longâ€‘context reliability (Sec. 3.6):
  - Although RoPE scaling implies up to 64K tokens theoretically, robust behavior is only claimed up to 16K after a short continuation phase.
- Compute and data demands (Sec. 3.4â€“3.5):
  - Training requires multiâ€‘node A100/H800 clusters and 2T tokens; reproducing the full recipe is resourceâ€‘intensive.
- Evaluation caveats:
  - Potential residual data contamination for the LeetCode benchmark is acknowledged (Table 5 note).  
  - Greedy decoding is standardized but may underâ€‘estimate peak performance relative to sampling-based strategies.
- Scope:
  - Focuses on code generation/completion and programâ€‘aided math; does not study advanced tool use (e.g., compilers, static analyzers) during inference or longâ€‘horizon refactoring workflows.

## 7. Implications and Future Directions
- Field impact:
  - Demonstrates that carefully prepared repositoryâ€‘level corpora and balanced FIM can push open models to (and beyond) Codex/GPTâ€‘3.5 levels on many code tasks, reshaping the openâ€‘source baseline.
  - Provides reproducible configurations (sizes from 1.3B to 33B) with permissive licensing, likely accelerating IDE integration, educational tools, and research on crossâ€‘file reasoning.
- Research enabled:
  - Retrievalâ€‘augmented repository assistance: Table 7 shows further gains with BM25; future work could combine repositoryâ€‘aware pretraining with learned retrievers or code graph retrieval.
  - Stronger longâ€‘context behavior: build on the RoPEâ€‘scaled 16K training to establish dependable 32â€“64K code understanding for monorepos and projectâ€‘wide audits.
  - Better dependency modeling: replace regex with languageâ€‘aware parsers or build systems; incorporate build graphs and symbol resolution for more faithful ordering.
  - Objective design: extend FIM beyond single middle span (multiâ€‘span, syntaxâ€‘aware holes), or mix with executionâ€‘guided objectives.
  - Generalâ€‘LLM synergy: results from `v1.5` (Table 10) suggest that continuing from a general LLM improves reasoning without hurting coding; largerâ€‘scale versions could merge strengths of general and codeâ€‘focused pretraining.
- Practical applications:
  - Repositoryâ€‘wide code completion and refactoring assistance in IDEs, where crossâ€‘file context and 16K windows are essential.
  - Dataâ€‘science notebooks: DSâ€‘1000 gains indicate improved library usage (NumPy/Pandas/PyTorch/TensorFlow).
  - Automated patching and fillâ€‘in tasks (FIM) for template expansion, code migration, and snippet completion across languages.

Overall, DeepSeekâ€‘Coder is a methodical advance: it operationalizes repositoryâ€‘level pretraining and a carefully tuned FIM objective to deliver strong, accessible code LLMs. The empirical study is broad and ablationâ€‘backed, and the open release is positioned to catalyze both research and deployment in real software engineering environments.

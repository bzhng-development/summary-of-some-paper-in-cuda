# Camels in a Changing Climate: Enhancing LM Adaptation with TÃœLU 2

**ArXiv:** [2311.10702](https://arxiv.org/abs/2311.10702)

## ğŸ¯ Pitch

TÃœLU 2 delivers an open, reproducible suite of large language modelsâ€”spanning up to 70B parametersâ€”finetuned on a rigorously curated instruction dataset and enhanced with scalable Direct Preference Optimization (DPO). This work not only sets a new open benchmark for instruction-following and coding ability, but also demystifies what matters in adapting LLMs to downstream tasks, providing the open-source community with all the models, data, and recipes needed for rapid and reliable language model alignment.

---

## 1. Executive Summary
TÃœLU 2 is a publicly released suite of instructionâ€‘tuned and preferenceâ€‘optimized large language models (LLMs) based on LLAMAâ€‘2 (and Code LLaMA) that systematizes recent advances in data curation, longâ€‘context training, and human preference learning. It delivers strong openâ€‘weight models up to 70B parameters, demonstrates stable Direct Preference Optimization (DPO) at that scale, and provides a carefully curated training mixture that improves openâ€‘ended generation and coding ability (Tables 1â€“6, Figure 1).

## 2. Context and Motivation
- Problem addressed
  - How to reliably adapt open LLMs to follow user instructions and preferences across many tasks using openly available resources (datasets, code, and checkpoints).
  - Prior open instructionâ€‘tuned models varied in data quality, lacked consistent evaluation, and did not demonstrate that newer preferenceâ€‘learning techniques like `DPO` scale to very large models (70B).
- Importance
  - Practical: Industryâ€‘grade instruction following and safety alignment typically depend on proprietary data and RLHF pipelines; open, reproducible recipes lower the barrier to strong assistants.
  - Scientific: Isolating the effects of data mixtures, finetuning recipes, and preference learning clarifies what actually drives performance across reasoning, knowledge, multilinguality, and openâ€‘ended generation.
- Prior approaches and their gaps
  - Instruction tuning on mixes such as FLAN, Dolly, OpenAssistant, and ShareGPT improved helpfulness but varied in quality and length coverage. Earlier TÃœLU (V1) used smaller context lengths and older base models (LLAMAâ€‘1).
  - RLHF primarily used PPO or proprietary pipelines; DPO is simpler but was not shown to be stable and beneficial at 70B scale.
  - Parameterâ€‘efficient finetuning (`QLoRA`) promised efficiency but its impact on openâ€‘ended generation had not been deeply compared to full finetuning.
- Positioning of this work
  - Provides: (1) an improved instruction mixture `TÃœLUâ€‘V2â€‘mix`; (2) fully finetuned LLAMAâ€‘2 models at 7B/13B/70B; (3) the largest DPOâ€‘trained model to date (70B); (4) `CODE TÃœLU 2` models from Code LLaMA showing strong coding performance; (5) a unified evaluation covering knowledge (MMLU), reasoning (GSM8k/BBH), multilingual QA (TyDiQA), coding (HumanEval/Codexâ€‘Eval), openâ€‘ended generation (AlpacaEval, MTâ€‘Bench), toxicity (ToxiGen), and truthfulness (TruthfulQA) with pinned evaluation details (Appendix A; Tables 1â€“6, 8).

Definitions for uncommon terms:
- Instruction tuning: supervised finetuning on datasets of instructionâ€“response pairs to make a model follow user prompts more reliably.
- RLHF (Reinforcement Learning from Human Feedback): training that uses human preferences to shape model behavior.
- DPO (Direct Preference Optimization): a simple, offline preferenceâ€‘learning method that directly increases the logâ€‘likelihood of â€œchosenâ€ responses over â€œrejectedâ€ ones relative to the base policy; it avoids a learned reward model and PPO complexity (Section 2, â€œRLHF trainingâ€; Appendix B for hyperparameters).
- QLoRA: parameterâ€‘efficient finetuning using 4â€‘bit baseâ€‘model quantization plus lowâ€‘rank adapters, enabling largeâ€‘model training on limited hardware (Section 2, â€œQLoRA trainingâ€; Appendix B).
- AlpacaEval and MTâ€‘Bench: benchmarks that use GPTâ€‘4 as a judge to compare model responses on openâ€‘ended prompts (Section 3, â€œEvaluation toolsâ€).

## 3. Technical Approach
The system is a staged adaptation pipeline with careful data choices and controlled training recipes.

1) Base models and sizes
- Use LLAMAâ€‘2 base models at 7B, 13B, 70B and Code LLaMA at 7B, 13B, 34B (Section 2, â€œImproved base modelsâ€).
- Rationale: LLAMAâ€‘2 is trained on ~2T tokens and outperforms LLAMAâ€‘1. Code LLaMA adds codeâ€‘focused pretraining to boost coding tasks.

2) New instruction mixture: `TÃœLUâ€‘V2â€‘mix`
- Composition after filtering: 326,154 samples (Section 2, â€œV2 data mixtureâ€).
- Sources (new additions marked with â€œ*â€; sample counts below are from Section 2):
  - FLAN v2 (50k) and its Chainâ€‘ofâ€‘Thought subset (50k) to include stepâ€‘byâ€‘step reasoning.
  - OpenAssistantâ€‘1 highâ€‘quality paths (7,708) to emphasize curated human interactions.
  - ShareGPT (114,046) for realâ€‘world conversational patterns.
  - GPT4â€‘Alpaca (20k) and Codeâ€‘Alpaca (20,022) for distilled general and code instructions.
  - *LIMA (1,030) for careful, highâ€‘quality instruction data.
  - *WizardLM Evolâ€‘Instruct V2 (30k) to increase diversity and difficulty.
  - *Openâ€‘Orca (30k) for GPTâ€‘4 explanations augmenting FLANâ€‘style prompts.
  - *Science literature tasks (7,544) spanning QA, factâ€‘checking, summarization, and IE (Appendix C).
  - *Hardcoded identity prompts (140), plus filtering of references to other LLMs (e.g., â€œGPTâ€‘4â€) to keep selfâ€‘descriptions consistent.
- Longâ€‘context training: maximum sequence length increased from 2,048 to 8,192 tokens. Only 20 samples are truncated in V2 vs 63,900 in V1, improving coverage of long ShareGPT/OpenAssistant conversations (Section 2, â€œExtended context lengthâ€; Figure 1 shows the token length histogram).

3) Supervised instruction finetuning (SFT)
- Training setup (Appendix B):
  - Precision bfloat16, 2 epochs, learning rate 2eâ€‘5 (1eâ€‘5 for 70B), warmup 0.03, no weight decay, effective batch size 128, max length 8,192.
- Purpose: align the base models to follow instructions with longâ€‘context support using the V2 mixture.

4) Preference learning with DPO (optional second stage)
- Data: filtered, binarized UltraFeedback preference data (highâ€‘quality GPTâ€‘4â€‘graded comparisons) for 3 epochs (Section 2, â€œRLHF trainingâ€).
- Training hyperparameters (Appendix B): learning rate 5eâ€‘7, bfloat16, batch size 32, beta 0.1, max length 8,192, warmup 0.1.
- Mechanism (intuition): for each prompt with two responses (chosen vs rejected), DPO increases the relative logâ€‘likelihood of the chosen answer while regularizing against the base SFT policy; the `beta` parameter scales this relative preference pressure. It is an offline objectiveâ€”no environment rollouts or learned reward model are needed.

5) QLoRA experiments (alternative to full SFT)
- Setup (Appendix B): 5 epochs, learning rate 1eâ€‘4, max length 4,096, effective batch size 128; LoRA rank 64, alpha 16, dropout 0.1; adapters on attention and feedforward layers.
- Goal: estimate the computeâ€“performance tradeâ€‘offs by replacing full finetuning with parameterâ€‘efficient adaptation.

6) CODE TÃœLU 2 (Code LLaMA + V2 mixture)
- Apply the same SFT recipe to Code LLaMA to assess coding vs generalâ€‘purpose tradeâ€‘offs (Section 3.5; Table 6).

7) Evaluation protocol
- Benchmarks (Appendix A):
  - MMLU (knowledge), GSM8k and BBH with Chainâ€‘ofâ€‘Thought (reasoning), TyDiQA (multilingual QA, Gold Passage), HumanEval/Codexâ€‘Eval (coding), AlpacaEval (openâ€‘ended, GPTâ€‘4 judge), MTâ€‘Bench (openâ€‘ended, GPTâ€‘4 judge), ToxiGen (toxicity rate; lower is better), TruthfulQA (truthfulness/informativeness).
- Important controls:
  - AlpacaEval judged with GPTâ€‘4â€‘0613 to ensure comparability (Section 3, â€œEvaluation toolsâ€).
  - TruthfulQA is omitted for DPOâ€‘trained models due to overlap with UltraFeedback (data contamination note in Section 3).
  - Possible trainingâ€‘data overlap for proprietary models is acknowledged (Section 3).

## 4. Key Insights and Innovations
- Improved, smaller, longâ€‘context instruction mixture (`TÃœLUâ€‘V2â€‘mix`)
  - Innovation: curated mix emphasizing highâ€‘quality distilled data (e.g., LIMA, WizardLM Evolâ€‘Instruct V2, Openâ€‘Orca), longâ€‘context coverage (8,192 tokens), and removal/downsampling of weaker or redundant sources (e.g., Dolly removed; FLAN downsampled) while keeping diverse conversation styles (Section 2).
  - Significance: V2 improves openâ€‘ended generation and coding performance relative to V1 while reducing total size (326k vs 490k samples). Table 2 shows consistent gains in BBH, Codexâ€‘Eval, AlpacaEval, and TruthfulQA across sizes, e.g., 7B AlpacaEval 73.9 vs 64.5 (+9.4), Codexâ€‘Eval 36.9 vs 33.9 (+3.0).

- Stable DPO at 70B scale with large gains in openâ€‘ended generation
  - Innovation: first public demonstration of DPO training that is stable and beneficial at 70B parameters, using a very low learning rate (5eâ€‘7) and 3 epochs on UltraFeedback (Section 2; Table 3).
  - Significance: Table 3 shows large AlpacaEval gains at every scale (+11.2 at 7B, +10.6 at 13B, +8.5 at 70B), and Table 4 shows MTâ€‘Bench improvements (e.g., 70B from 7.49 to 7.89). This establishes DPO as a simple, scalable alternative to PPOâ€‘style RLHF.

- Clear empirical picture for QLoRA vs full finetuning on openâ€‘ended tasks
  - Insight: QLoRA underperforms full finetuning on openâ€‘ended generation (AlpacaEval) by large margins, though the gap narrows with model size (Table 5). Example: 7B AlpacaEval 56.1 (QLoRA) vs 73.9 (full), âˆ’17.8 points; 70B 78.6 vs 86.6, âˆ’8.0 points.
  - Significance: Parameterâ€‘efficiency is attractive, but for openâ€‘ended generation quality, full finetuning remains superiorâ€”an actionable guidance for practitioners.

- Leveraging codeâ€‘specialized pretraining (CODE TÃœLU 2)
  - Innovation: Finetuning Code LLaMA on V2 yields substantial coding gains without proprietary data (Table 6).
  - Significance: At 7B, Codexâ€‘Eval jumps to 68.9 vs 36.9 for the general LLAMAâ€‘based TÃœLU 2; at 13B, 76.2 vs 49.0. However, general openâ€‘ended generation (AlpacaEval) drops notably (e.g., 7B: 58.0 vs 73.9), clarifying domain specialization tradeâ€‘offs.

Incremental vs fundamental:
- Incremental: switching to LLAMAâ€‘2 base models, adding longâ€‘context SFT.
- More fundamental: demonstrating scalable DPO at 70B and disentangling data/recipe choices that move openâ€‘ended generation and coding ability in different directions.

## 5. Experimental Analysis
- Evaluation design
  - Benchmarks span knowledge (MMLU), reasoning (GSM8k/BBH), multilingual QA (TyDiQA GoldP), coding (HumanEval/Codexâ€‘Eval), openâ€‘ended dialogue (AlpacaEval, MTâ€‘Bench), toxicity (ToxiGen), and truthfulness (TruthfulQA) (Appendix A).
  - Models compared include LLAMAâ€‘2â€‘Chat, Xwinâ€‘LM, Zephyrâ€‘Beta, and proprietary GPTâ€‘3.5/4 references (Table 1). MTâ€‘Bench category scores are reported in detail (Table 8).

- Main quantitative results
  - Overall standing among open models
    - Quote (Table 1): â€œTÃœLU 2 70B average 73.8,â€ best among open models on average; top in 3/7 tasks and within ~1% of the best open result on the rest.
  - Against GPTâ€‘3.5 (older vs newer variants)
    - Quote (Table 1): GPTâ€‘3.5â€‘turboâ€‘0301 average 72.3 vs TÃœLU 2 70B 73.8 (TÃœLU 2 slightly higher on average; better on AlpacaEval and ToxiGen).
    - Newer GPTâ€‘3.5â€‘turboâ€‘0613 still leads across several metrics (average 77.6).
  - Effect of DPO
    - Quote (Table 3): â€œAlpacaEval +11.2 (7B), +10.6 (13B), +8.5 (70B).â€ Minimal changes on MMLU/BBH/Codexâ€‘Eval; large multilingual drop on TyDiQA (âˆ’1.9, âˆ’13.5, âˆ’17.8).
    - MTâ€‘Bench: 70B rises from 7.49 to 7.89, the top openâ€‘weight result at reporting time (Table 4).
    - Verbosity: Average AlpacaEval output length increases postâ€‘DPO (Table 4), consistent with known RLHF verbosity bias.
  - V2 mixture vs V1
    - Quote (Table 2): At 7B, average improves from 47.8 (V1) to 54.2 (V2); at 13B, 56.0 to 60.8; at 70B, 71.5 to 72.4. Improvements concentrate on BBH, Codexâ€‘Eval, AlpacaEval, TruthfulQA; GSM8k and TyDiQA decline.
  - QLoRA vs full finetuning
    - Quote (Table 5): Average gap of âˆ’5.3 (7B), âˆ’3.6 (13B), âˆ’2.4 (70B). Largest deficits on AlpacaEval.
  - Code LLaMA finetuning
    - Quote (Table 6): Coding gains are largeâ€”e.g., at 7B, Codexâ€‘Eval 68.9 (CODE TÃœLU 2) vs 36.9 (TÃœLU 2). But AlpacaEval dropsâ€”58.0 vs 73.9 at 7B.
  - MTâ€‘Bench category breakdown (Table 8)
    - DPO at 70B especially boosts Roleplay and Writing (e.g., Roleplay rises from 8.30 to 9.25; Writing 9.15 to 9.25), with mixed changes in Coding/Math.

- Ablations, robustness, and caveats
  - Data mixture ablation is implicit in Table 2 (V1 vs V2) and ShareGPTâ€‘only comparison at 7B; V2 surpasses ShareGPTâ€‘only overall and even on AlpacaEval (Table 2).
  - Multilingual performance degrades with DPO; the training data are largely English (Section 3.3), so TyDiQA becomes outâ€‘ofâ€‘distribution during preference optimization.
  - TruthfulQA contamination: UltraFeedback includes TruthfulQA prompts; results are omitted when comparing DPOâ€‘trained models (Section 3).
  - External evaluators: AlpacaEval and MTâ€‘Bench depend on GPTâ€‘4; the paper locks AlpacaEval to GPTâ€‘4â€‘0613 for fairness (Section 3), but notes GPTâ€‘4 versions are not permanently pinned communityâ€‘wide.

- Do the experiments support the claims?
  - Yes for the key claims:
    - V2 mixture: clear gains on openâ€‘ended generation and coding (Table 2).
    - DPO: consistent, large boosts in GPTâ€‘4â€‘judged openâ€‘ended metrics, stable at 70B (Tables 3â€“4), with known verbosity sideâ€‘effects (Table 4).
    - QLoRA tradeâ€‘offs: strong evidence of openâ€‘ended degradation (Table 5).
    - Code specialization: marked coding gains but general chat tradeâ€‘offs (Table 6).
  - Mixed outcomes:
    - Multilingual QA (TyDiQA) worsens with DPO (Table 3), indicating alignment choices matter for multilingual capabilities.

## 6. Limitations and Trade-offs
- Data distribution and multilinguality
  - SFT and DPO data are predominantly English; multilingual QA performance drops substantially after DPO (TyDiQA âˆ’17.8 at 70B, Table 3). This limits applicability for multilingual assistants unless additional multilingual preference/SFT data are included.
- Openâ€‘ended generation vs other skills
  - DPO strongly boosts openâ€‘ended judged quality but may not improve factual or mathematical reasoning metrics (Table 3, small deltas on MMLU/BBH/GSM8k) and increases verbosity (Table 4).
- Parameterâ€‘efficiency vs quality
  - QLoRA saves compute but underperforms full finetuning on openâ€‘ended generation, especially at smaller scales (Table 5). Teams with constrained hardware face a quality tradeâ€‘off.
- Domain specialization
  - Using Code LLaMA raises coding performance but reduces openâ€‘ended chat quality (Table 6). Specialization can harm general assistance.
- Evaluation and contamination concerns
  - GPTâ€‘4â€‘based judges (AlpacaEval, MTâ€‘Bench) may change over time; the paper fixed GPTâ€‘4â€‘0613 for reported runs but broader comparisons can drift (Section 3).
  - TruthfulQA contamination in UltraFeedback means DPOâ€‘trained modelsâ€™ TruthfulQA results are not reported in crossâ€‘model comparisons (Section 3).
  - For proprietary baselines, trainingâ€‘set overlaps with benchmarks cannot be ruled out (Section 3).
- Compute requirements
  - Largeâ€‘scale training remains substantial; e.g., 70B DPO ran for ~7 days on a 512â€‘core TPUv3 pod (Section 3, â€œTrainingâ€).

## 7. Implications and Future Directions
- Field impact
  - Provides a reproducible, open pipeline showing that simple, offline preference optimization (DPO) scales to 70B and meaningfully improves GPTâ€‘4â€‘judged openâ€‘ended quality (Tables 3â€“4). This lowers the barrier to building strong open assistants without PPOâ€‘style RL.
  - Offers an improved, public instruction mixture and longâ€‘context training recipe, enabling the community to study how distilled data and context length drive performance (Section 2; Figure 1).
  - Clarifies that parameterâ€‘efficient finetuning may not match full SFT on openâ€‘ended tasks, guiding practitionersâ€™ compute/budget decisions (Table 5).

- Suggested followâ€‘ups (some named in the Conclusion)
  - Multilingual alignment: Add multilingual SFT and multilingual preference data to recover TyDiQA and test DPOâ€™s behavior in nonâ€‘English settings (Section 3.3).
  - RLHF method comparisons at scale: Headâ€‘toâ€‘head 70Bâ€‘scale comparisons of DPO vs PPO, rejection sampling (RS/ReST), and offline RL variants, including effects on refusals, verbosity, and factuality (Conclusion).
  - Data ablations: Systematically vary the proportions of distilled vs humanâ€‘written data, CoT density, and conversation length to quantify which ingredients drive which metrics (Sections 2â€“3.2).
  - Length/verbosity control: Incorporate lengthâ€‘aware preference models or penalties to retain DPO gains while avoiding excessive verbosity (Table 4).
  - Domainâ€‘adaptive assistants: Explore mixtures that retain general ability while selectively leveraging domainâ€‘specialized bases (e.g., hybrid LLAMAâ€‘2/Code LLaMA training or multiâ€‘adapter routing; Table 6 tradeâ€‘offs).
  - Larger and newer bases: Apply the recipe to newer base models (e.g., Mistralâ€‘family or successors) and extend beyond 70B with longâ€‘context SFT/DPO.

- Practical applications
  - Open, highâ€‘quality chat assistants for research and industry with strong openâ€‘ended generation (TÃœLU 2+DPO 70B, AlpacaEval 95.1 in Table 4).
  - Codeâ€‘focused copilots based on `CODE TÃœLU 2`, which dramatically improves functional correctness on HumanEval (e.g., 82.5 at 34B; Table 6).
  - Educational and scientific assistants leveraging longâ€‘context capabilities and a scienceâ€‘task subset (Appendix C), within the multilingual limitations noted.

Blockâ€‘quoted highlights
- V2 mixture longâ€‘context coverage gain: 
  > â€œMoving from 2,048 to 8,192 max length means we only truncate 20 (as opposed to 63,900) samples within our V2 mixtureâ€ (Section 2; Figure 1).
- DPO effect on openâ€‘ended quality:
  > â€œTÃœLU 2+DPO 70B â€¦ AlpacaEval 95.1 vs 86.6 without DPO; MTâ€‘Bench 7.89 vs 7.49â€ (Tables 3â€“4).
- QLoRA tradeâ€‘off:
  > â€œ7B AlpacaEval 56.1 (QLoRA) vs 73.9 (full); 70B 78.6 vs 86.6â€ (Table 5).
- Coding specialization:
  > â€œCODE TÃœLU 2 (7B) Codexâ€‘Eval 68.9 vs 36.9 for TÃœLU 2; AlpacaEval drops to 58.0 vs 73.9â€ (Table 6).

Overall, TÃœLU 2 provides a transparent, wellâ€‘controlled demonstration that careful data curation plus longâ€‘context SFT and scalable DPO yields stateâ€‘ofâ€‘theâ€‘art openâ€‘weight instructionâ€‘following models, with clear guidance on when parameterâ€‘efficient methods and domain specialization help or hurt.

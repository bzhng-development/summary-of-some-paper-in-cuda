# "Give Me BF16 or Give Me Death"? Accuracyâ€‘Performance Tradeâ€‘Offs in LLM Quantization

**ArXiv:** [2411.02355](https://arxiv.org/abs/2411.02355)
**Authors:** Eldar Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, Dan Alistarh
**Institutions:** Not listed on arXiv

## ðŸŽ¯ Pitch

This study provides a comprehensive evaluation of low-precision formats for large language model inference, identifying FP8 as nearly lossless in accuracy while unlocking significant speed and cost savings through tailored INT8 and INT4 quantization strategies. The findings offer concrete, evidence-based guidance, enabling organizations to optimize LLM deployments based on serving conditions, enhancing both cost-efficiency and performance without significant accuracy loss. This shifts the quantization dialogue from speculative to data-driven decision-making, opening new pathways for scalable, high-performance AI applications.

---

## 1. Executive Summary
This paper delivers a large-scale, end-to-end examination of how different low-precision formats for large language model (LLM) inference trade off accuracy, speed, and cost. It benchmarks FP8, INT8, and INT4 schemes across the full Llamaâ€‘3.1 family (8B, 70B, 405B) and multiple GPU generations, and ties accuracy findings to measured throughput/latency under realistic serving modes in vLLM. The central practical takeaways are: FP8 (W8A8-FP) is near-lossless in accuracy, wellâ€‘tuned INT8 (W8A8-INT) is much better than commonly believed, and 4â€‘bit weightâ€‘only (W4A16-INT) often gives the best latency/cost in synchronous deployments while 8â€‘bit formats dominate throughput in asynchronous serving.

## 2. Context and Motivation
- Problem addressed
  - LLM quantization reduces numeric precision of model parameters and/or activations to lower memory footprint and improve inference speed. The community lacks a systematic, applesâ€‘toâ€‘apples picture of the accuracy versus performance tradeâ€‘offs for the formats that are actually supported and fast in modern inference stacks.
  - The gap spans two axes:
    - Accuracy across both academic and realâ€‘world, openâ€‘ended tasks at multiple model sizes.
    - Throughput/latency/cost on real hardware (A6000, A100, H100) and serving modes (synchronous and asynchronous continuous batching).

- Why it matters
  - Serving large models is expensive and often bottlenecked by memory bandwidth. Small degradations in accuracy can be acceptable if they yield large gains in cost and responsiveness. Organizations need reliable guidance on which quantization format fits their workload and hardware.

- Prior approaches and shortcomings
  - Many studies focus narrowly on accuracy on academic sets or only one model size, or report results without careful tuning, yielding pessimistic views of INT8 activation quantization (Section 2.2).
  - Some highâ€‘compression formats (e.g., 2â€‘bit vector quantization) are not efficient beyond batch size 1, limiting practical value (Section 2.1).
  - Claims that W8A8â€‘INT suffers large losses (10%+) are shown to hinge on poor hyperparameters and weak calibration choices (Section 2.2, Appendix A.2).

- Positioning of this work
  - Comprehensive and deploymentâ€‘oriented: evaluates formats with production kernels in vLLM (version 0.6.4.post1) across three GPU generations and seven realistic use cases; links accuracy to endâ€‘toâ€‘end performance; covers the entire Llamaâ€‘3.1 range including 405B (Sections 3 and 5).
  - Methodologically careful: strong baselines, tuned hyperparameters, and large evaluation volume (>500k runs).

## 3. Technical Approach
This is an empirical study with targeted algorithmic choices that reflect what is both accurate and fast to serve today.

- Quantization formats studied (Section 3.2)
  - Naming shorthand used throughout figures/tables:
    - `W8A8-FP` (â€œFP8â€): weights and activations quantized to 8â€‘bit floating point where hardware supports it (Hopper/Ada).
    - `W8A8-INT` (â€œINT8â€): weights and activations quantized to 8â€‘bit integers (widely supported, including Ampere).
    - `W4A16-INT` (â€œINT4â€): 4â€‘bit integer weights, 16â€‘bit activations (weightâ€‘only lowâ€‘bit; often the fastest for decodeâ€‘bound workloads).
  - Why these: they have mature, efficient kernels in vLLM and map well to todayâ€™s GPUs. Ultraâ€‘lowâ€‘bit vector formats are excluded because they are inefficient for batches >1 (Section 2.1).

- How each format is implemented (Section 3.2)
  - FP8 (W8A8-FP)
    - Weights: symmetric perâ€‘outputâ€‘channel quantization using roundâ€‘toâ€‘nearest assignment.
    - Activations: dynamic perâ€‘token quantization (no calibration data required).
    - Design rationale: FP8 avoids many of INT8â€™s outlier issues for activations and has native hardware support on Hopper/Ada.
  - INT8 (W8A8-INT)
    - Weights: GPTQ, a postâ€‘training quantization method that uses secondâ€‘order information (via calibration data) to minimize layerâ€‘wise quantization error.
      - Intuition: for each weight group, GPTQ quantizes then applies a small, analytically computed correction that accounts for the local curvature of the loss, reducing the error introduced by rounding.
    - Activations: dynamic perâ€‘token quantization; for the 70B model this is augmented with SmoothQuant.
      - SmoothQuant (activationâ€‘toâ€‘weight scaling) shifts amplitude from hardâ€‘toâ€‘quantize activation channels into the weights using a precomputed scaling from calibration data, reducing activation outliers while preserving function (Section 2.1).
    - Calibration data: random tokens are sufficient at 8B; higherâ€‘quality calibration (Platypus/Lee et al., 2023) is used for 70B/405B (Section 3.2).
  - INT4 (W4A16-INT)
    - Weights: GPTQ with MSEâ€‘optimal clipping, group size 128; activations remain at 16â€‘bit for robustness.
      - MSEâ€‘optimal clipping squares the quantization error and selects a clipping threshold that minimizes mean squared error before rounding.
    - Calibration: random tokens hurt accuracy at 4â€‘bit, so OpenPlatypus data is used (Section 3.2).
    - Why not AWQ as default: headâ€‘toâ€‘head comparisons favor GPTQ on realâ€‘world, openâ€‘ended tasks (Table 1 and Appendix A.2).

- Core quantization primitive and why activations are hard (Section 2.1)
  - Many methods start from roundâ€‘toâ€‘nearest (RTN) with minâ€‘max scaling. Equation (1) shows:
    - Each group of `g` weights `x` is scaled by `s(x) = (max(x)âˆ’min(x))/(2^bâˆ’1)` and shifted by zeroâ€‘point `z(x)=min(x)`, then rounded to the nearest integer.
    - Outliers in activations (values far larger than average) inflate `max(x)âˆ’min(x)`, wasting dynamic range and hurting INT8 activation quantization; SmoothQuant counteracts this by rebalancing magnitudes.

- Models, datasets, and serving stack (Sections 3.1â€“3.2, 5)
  - Models: Llamaâ€‘3.1â€‘Instruct at 8B, 70B, and 405B; reasoningâ€‘tuned `DeepSeekâ€‘R1â€‘Distill` variants from Llama and Qwen families (Section 4.3, Table 4).
  - Benchmarks:
    - Academic: Open LLM Leaderboard V1 and V2 (Tables 2, 3, 10, 11).
    - Realâ€‘world openâ€‘ended: Arenaâ€‘Hardâ€‘Autoâ€‘v0.1 (Table 7), HumanEval and HumanEval+ (Table 3; Appendix Figures 5â€“6), longâ€‘context RULER (Table 3).
    - Text similarity vs. fullâ€‘precision: ROUGE/BERTScore/STS on Arenaâ€‘Hard prompts under greedy decoding (Figure 1).
  - Performance evaluation in vLLM across seven representative tasks (code completion, docstrings, code fixing, RAG, instruction following, multiâ€‘turn chat, summarization) with characteristic prefill/decode lengths (Section 5). Hardware: A6000, A100, H100; synchronous and asynchronous serving; cost computed from Lambda Labs pricing (Table 9).

- Serving concepts (Section 5)
  - Prefill vs. decode: prefill processes all input tokens in parallel (computeâ€‘bound); decode generates tokens one by one (memoryâ€‘bandwidthâ€‘bound).
  - Implication: weightâ€‘only quantization (INT4) chiefly accelerates decode; weight+activation quantization (FP8/INT8) also speeds up prefill.

## 4. Key Insights and Innovations
1. FP8 is effectively lossless on accuracy across sizes
   - Whatâ€™s new: endâ€‘toâ€‘end demonstration that `W8A8-FP` recovers â‰ˆ100% accuracy across both simple and challenging suites.
   - Evidence:
     - Leaderboard V1: 99.31â€“100.12% recovery for 8Bâ€“405B (Table 2).
     - Leaderboard V2: 99.9â€“101.2% recovery (Table 3).
     - Realâ€‘world: Arenaâ€‘Hard and coding tasks match baselines within confidence intervals (Table 3; Table 7).
   - Why it matters: FP8 gives prefill speedups without sacrificing accuracy and is natively supported on modern GPUs.

2. Properly tuned INT8 is much stronger than commonly believed
   - Distinguishing factor: combines GPTQ weights with dynamic activations and, where needed (notably 70B), SmoothQuant with good calibration data.
   - Evidence:
     - Average loss is small (â‰ˆ1â€“3 percentage points) rather than the 10%+ often reported without tuning (Section 2.2).
     - Leaderboard V2 average: 97.3% at 70B, 98.3% at 405B (Table 3).
   - Significance: expands the viable use of INT8 activation quantization to larger models when calibration and scaling are handled carefully.

3. 4â€‘bit weightâ€‘only (W4A16-INT) is surprisingly competitiveâ€”and GPTQ > AWQ on realâ€‘world tasks
   - Whatâ€™s different: GPTQ with MSEâ€‘optimal clipping and higherâ€‘quality calibration, plus evaluation on openâ€‘ended tasks.
   - Evidence:
     - Academic parity: AWQ vs GPTQ nearly tied (Table 1 left).
     - Realâ€‘world advantage: GPTQ beats AWQ by noticeable margins, e.g., on 8B HumanEval 67.1 vs 63.0 pass@1 and MBPP 65.8 vs 62.8 (Table 1 right; Appendix Tables 12â€“13).
   - Why it matters: for latencyâ€‘sensitive serving, INT4 gives the best cost/latency in synchronous mode, and practitioners should prefer GPTQ in these conditions.

4. Deploymentâ€‘level guidance: INT4 for synchronous latency; W8A8 for asynchronous throughput
   - Evidence:
     - Synchronous: INT4 gives 2â€“3Ã— cost reduction at 8B/70B and 5â€“7Ã— at 405B, with lower latency (Table 5). Example: 8B code completion latency drops from 24.5s (BF16) to 9.7s (INT4), Q/$ rises from 183 to 462 (Table 5).
     - Asynchronous: W8A8 formats maximize QPS at higher latencies; INT4 remains competitive and sometimes wins but tends to lose at highâ€‘throughput regimes (Table 6; Figures 2â€“3).
   - Practical value: concrete, dataâ€‘driven prescriptions for production choices based on workload and SLA.

5. Larger quantized models preserve semantics and even phrasing
   - Evidence from text similarity (Figure 1):
     - 70B/405B: ROUGEâ€‘1 â‰ˆ0.7, BERTScore â‰ˆ0.93, STS â‰ˆ0.96 vs fullâ€‘precisionâ€”indicating close word and structure overlap.
     - 8B degrades somewhat in phrasing (ROUGEâ€‘L â‰ˆ0.46â€“0.51) but keeps semantic fidelity (STS â‰ˆ0.94â€“0.95).

## 5. Experimental Analysis
- Evaluation methodology (Sections 3.1â€“3.2; 5; Appendices)
  - Benchmarks:
    - Academic: Open LLM Leaderboards V1 and V2, covering world knowledge, reasoning, math, instruction following (Tables 2â€“3, 10â€“11).
    - Realâ€‘world: Arenaâ€‘Hardâ€‘Autoâ€‘v0.1 (two runs, 95% CI reported in Table 7), HumanEval/HumanEval+ coding (Table 3 and Appendix Figures 5â€“6), RULER for longâ€‘context (Table 3).
    - Reasoning: AIMEâ€™24, MATHâ€‘500, GPQAâ€‘Diamond with pass@1 estimated from 20 samples/query using LightEval (Section 4.3; Table 4).
  - Serving performance:
    - vLLM 0.6.4.post1, three GPU types, seven use cases with taskâ€‘typical prefill/decode lengths (Section 5); synchronous and asynchronous settings; cost via Lambda pricing (Table 9).
  - Baselines: Fullâ€‘precision BF16 for all models; FP8, INT8, and INT4 variants as above.

- Headline accuracy numbers
  - Academic (Tables 2â€“3)
    - V1 (8 tasks): All formats recover â‰ˆ99% of BF16 overall. Sample 8B averages: BF16 74.06 vs FP8 73.55 (99.31%), INT8 74.29 (100.31%), INT4 73.11 (98.72%) in Table 2.
    - V2 (harder tasks): 8B shows more variance (INT4 96.1% recovery), but 70B and 405B remain strong: 70B FP8 100.0%, INT8 97.3%, INT4 97.4%; 405B FP8 99.9%, INT8 98.3%, INT4 98.9% (Table 3).
    - Hardest subtasks: integer activation quantization is the main challengeâ€”e.g., MMLUâ€‘Pro at 405B drops to 97.81% for INT8 (Table 11), while FP8 remains â‰ˆ99%.
  - Realâ€‘world tasks (Table 3; Table 7)
    - Arenaâ€‘Hard: differences are within 95% CIs for most configs. 405B BF16 67.4 vs FP8 66.9 vs INT8 64.6 vs INT4 66.5 (Table 3 and Table 7).
    - Coding: HumanEval pass@1 at 70B is stable or slightly improved with quantization (BF16 79.7 vs FP8 80.0 vs INT4 80.5, Table 3). HumanEval+ similarly stable.
    - Longâ€‘context: RULER at 8B/70B maintains â‰¥98% average score recovery; INT4 is slightly lower at 8B (81.1 vs 82.8, Table 3).
  - Reasoningâ€‘tuned models (Table 4)
    - Across Llamaâ€‘70B and Qwenâ€‘32B/14B/7B/1.5B, FP8/INT8/INT4 recover >99% average (except small models at INT4) on AIMEâ€™24, MATHâ€‘500, and GPQAâ€‘Diamond. Example: Llamaâ€‘70B FP8 averages 76.5 vs BF16 76.2; INT4 averages 75.0 (98.3%).

- GPTQ vs AWQ at INT4 (Table 1; Appendix A.2)
  - Academic: almost tied (e.g., 8B average 49.82 vs 50.05).
  - Realâ€‘world: GPTQ clearly aheadâ€”8B average 52.3 vs 49.4; large gaps on coding (HumanEval 67.1 vs 63.0; MBPP 65.8 vs 62.8).
  - Contributing factors: MSEâ€‘optimal clipping for GPTQ (AWQ run used absâ€‘max), higherâ€‘quality calibration, and inclusion of openâ€‘ended tasks (Section â€œINT4 Quantization Algorithmsâ€ and Table 1).

- Performance and cost findings
  - Synchronous (Table 5)
    - INT4 dominates latency and cost per query.
      - 8B on A6000 (code completion): latency 24.5s (BF16) â†’ 9.7s (INT4); Q/$ 183 â†’ 462; cost reduction (CR) 2.39Ã—.
      - 70B on A100 (docstrings): 2.9s (BF16) â†’ 2.8s (INT4) but Q/$ 343 â†’ 718 (2.09Ã—); in several tasks both latency and cost improve.
      - 405B on H100: code completion Q/$ 1 â†’ 8; CR 5.15Ã—. Similar gains across tasks; INT4 enables using fewer GPUs with acceptable latency (Section 5.1).
  - Asynchronous (Table 6; Figures 2â€“3)
    - W8A8 (FP8/INT8) often yields the highest throughput (QPS) and costâ€‘efficiency at higher batching.
      - 16Ã—H100, 405B summarization: BF16 8.5 QPS (Q/$ 638) â†’ FP8 20.7 (1561) â†’ INT4 24.7 (1856). Here INT4 also shines, but across many tasks FP8/INT8 take the QPS crown.
      - 4Ã—H100, 70B summarization: BF16 1.7 QPS (0.5k Q/$) â†’ FP8 2.6 (0.8k) â†’ INT4 2.2 (0.6k).
    - Latencyâ€“throughput tradeâ€‘off:
      - Figure 2 (8B docstrings, 1Ã—A6000): INT4 has lower interâ€‘token latency at low QPS; W8A8â€‘INT overtakes at higher QPS.
      - Figure 3 (70B code fixing, 2Ã—A100): same crossover behaviorâ€”INT4 for lowâ€‘latency; W8A8 for highâ€‘throughput.

- Do the experiments support the claims?
  - Yes, with breadth and detail:
    - Accuracy: dozens of tasks across three model sizes (Tables 2â€“4); text similarity corroborates qualitative parity (Figure 1).
    - Performance: endâ€‘toâ€‘end vLLM benchmarks across multiple GPUs and seven realistic workloads (Tables 5â€“6; Figures 2â€“3).
    - Robustness: 95% CIs for Arenaâ€‘Hard (Table 7); calibration choices explained; additional reasoningâ€‘specific suite (Table 4).
  - Where results are conditional:
    - INT8 activation quantization requires good calibration and SmoothQuant for some sizes (Section 3.2).
    - INT4 degrades more on the smallest models and on a few hard V2 tasks but remains competitive overall (Tables 3â€“4).

## 6. Limitations and Trade-offs
- What the study assumes or leaves out (Limitations section)
  - KVâ€‘cache, input embeddings, and LM head are not compressed here. Real deployments often quantize or pack KV cache; the accuracy/latency impact of KV quantization remains open.
  - Language coverage: primarily English, instructionâ€‘tuned models; multilingual or domainâ€‘specialized tasks may behave differently.
- Sensitivity to calibration and hyperparameters
  - INT8 accuracy hinges on highâ€‘quality calibration and SmoothQuant choices, particularly at 70B (Section 3.2; Tables 3 and 11). Poor calibration can reproduce prior pessimistic results.
  - INT4 requires careful clipping and data; randomâ€‘token calibration hurts (Section 3.2).
- Hardware and software scope
  - Results are tied to vLLM 0.6.4.post1 kernels and three NVIDIA GPU families. Different runtimes/hardware may shift breakâ€‘even points.
- Cost modeling
  - Cost per query uses Lambda onâ€‘demand rates (Table 9). Reserved/onâ€‘prem pricing could change the relative economics.

## 7. Implications and Future Directions
- How this changes the landscape
  - Moves quantization guidance from â€œfolk wisdomâ€ to evidenceâ€‘based rules:
    - For latencyâ€‘sensitive, singleâ€‘query or smallâ€‘batch serving: prefer `W4A16-INT` (INT4) to cut latency and cost substantially (Table 5; Figures 2â€“3).
    - For highâ€‘throughput, asynchronous serving: prefer `W8A8` (FP8 where supported, INT8 otherwise) to maximize QPS (Table 6).
    - When hardware supports FP8 (H100/Ada), it is the safest nearâ€‘lossless option (Tables 2â€“3).
  - Demonstrates that carefully tuned INT8 activation quantization is viable even for very large models (70Bâ€“405B), countering the narrative that INT8 is inherently unreliable (Tables 3 and 11).

- Followâ€‘up research enabled
  - Quantizing beyond weights/activations:
    - KVâ€‘cache quantization and scheduling that coâ€‘optimize memory bandwidth and cache reuse.
    - Quantization of input embeddings and output heads with guardrails for generation quality.
  - Mixedâ€‘precision policies:
    - Layerâ€‘ or blockâ€‘wise format selection (e.g., INT4 for attention/MLP weights, FP8 activations where prefill dominates) driven by perâ€‘operator sensitivity.
  - Better calibration:
    - Automatic selection of calibration corpora per task/domain; onâ€‘theâ€‘fly calibration that adapts to workload drift.
  - System coâ€‘design:
    - Kernel and scheduler improvements that exploit quantized formats more aggressively in continuous batching and contextâ€‘parallel setups.

- Practical applications
  - Chat assistants and code tools: pick INT4 for lowâ€‘latency completions, FP8/INT8 for bulk batch processing (Tables 5â€“6).
  - Highâ€‘end deployments of very large models (e.g., 405B): use INT4 to reduce GPU count/communication for latencyâ€‘sensitive endpoints; FP8/INT8 to maximize throughput in batch jobs.
  - Costâ€‘aware scaling: the costâ€‘reduction factors (up to â‰ˆ5â€“7Ã— in synchronous mode at 405B, Table 5) make previously impractical deployments feasible.

> In short: with careful algorithmic choices (GPTQ with proper clipping, SmoothQuant with good calibration, dynamic activation quantization) and an awareness of serving mode, FP8/INT8/INT4 quantization can preserve â‰¥97â€“100% of accuracy on large, realistic benchmarks (Tables 2â€“4) while yielding large, measurable gains in latency, throughput, and cost (Tables 5â€“6).

# Infini-gram: Scaling Unbounded nâ€‘gram Language Models to a Trillion Tokens

**ArXiv:** [2401.17377](https://arxiv.org/abs/2401.17377)
**Authors:** Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh Hajishirzi
**Institutions:** University of Washington, Allen Institute for AI

## ğŸ¯ Pitch

The paper presents `âˆ-gram`, an innovative unbounded n-gram language model, and `infini-gram`, a powerful engine capable of handling queries over 5 trillion tokens with millisecond latency. This breakthrough offers a nonparametric alternative to neural LLMs, enhancing interpretability, data attribution, and supporting deeper analyses of text, while achieving up to a 73% reduction in perplexity, making it invaluable for large-scale language modeling applications.

---

## 1. Executive Summary
This paper introduces `âˆ-gram`, an unbounded nâ€‘gram language model, and `infini-gram`, a suffixâ€‘arrayâ€“based engine that serves it at web scale. By indexing 5 trillion tokens and answering count/probability queries with millisecond latency, the work turns classic nâ€‘gram modeling into a practical nonparametric complement to neural LLMs, cutting perplexity by up to 73% in some settings and enabling new analyses of human and machine text.

## 2. Context and Motivation
- Problem addressed
  - Can classic nâ€‘gram language models still help at the trillionâ€‘token scale of modern LLMs, and if so, how can we make them practical and useful? The paper targets two historical bottlenecks (Â§1, Â§2, Â§3):
    1) nâ€‘gram models are usually limited to small `n` (e.g., 5), which discards long context.
    2) Precomputing large nâ€‘gram count tables becomes infeasible as `n` and data scale grow.
- Why this matters
  - Practical: A countâ€‘based, dataâ€‘centric LM is interpretable, auditable, and directly tied to source documentsâ€”useful for attribution, decontamination, and copyrightâ€‘sensitive settings (Â§E; Â§5, Table 2 for SILO).
  - Scientific: It provides a new lens on how much of nextâ€‘token prediction is â€œseenâ€‘before textâ€ vs. neural generalization (Â§4, Figures 3â€“5).
- Prior approaches and shortcomings
  - Large nâ€‘gram tables reached 2T tokens but still capped at 5â€‘grams (Brants et al., 2007). Suffix tree/array approaches existed, but not at trillionâ€‘token scale and often without valid probability distributions (Â§6, Â§F).
  - Interpolating small nâ€‘grams with neural LMs has had mixed results and used far less data (e.g., Khandelwal et al., 2020; Li et al., 2022) (Â§6).
  - Retrievalâ€‘based LMs (e.g., kNNâ€‘LM, RETRO) store vectors per token/chunk, leading to massive storage/compute; scaling to trillions of tokens is challenging (Table 6, Â§F).
- Positioning
  - The paper reframes nâ€‘grams as a nonparametric LM with unbounded context (`âˆ-gram`) and provides `infini-gram`, a storage/serving layer that makes counting and probability queries fast on trillions of tokens. It modernizes nâ€‘grams in both data scale (5T tokens) and context length (unbounded) and shows strong complementary gains with large neural LMs (Â§1, Â§2, Â§3, Â§5).

## 3. Technical Approach
This section explains what `âˆ-gram` is, how `infini-gram` works, and how the two are combined with neural LMs.

- 3.1 The `âˆ-gram` language model (Â§2)
  - Core idea
    - For each prediction step, use the longest suffix of the prefix that appears at least once in the corpus. Define `effective n` as one plus the length of that suffix.
  - Probability definition
    - Plainly: look at all times the chosen context occurred in the corpus; among those, count how often each next token followed; the ratio is the probability of that next token.
    - Notation (from Â§2): Let `w1:i-1` be the prefix and `n = max{n' | cnt(wi-(n'-1):i-1 | D) > 0}`. Then
      `Pâˆ(wi | w1:i-1) = cnt(wi-(n-1):i-1 wi | D) / cnt(wi-(n-1):i-1 | D)`.
  - How it differs from standard backoff
    - Traditional backoff reduces `n` until the numerator (joint count) is nonzero, then uses discounting (e.g., Katz). `âˆ-gram` instead reduces `n` until the denominator (context count) is nonzero and does not require discounting, because the effective `n` depends only on the context, not on the candidate token, making `Pâˆ(Â·|context)` a proper distribution (Â§2).
  - Special notions
    - `effective n`: 1 + longest seen suffix length for the current prefix (Â§2).
    - `sparse estimate`: the `âˆ-gram` distribution places probability 1 on exactly one next token (and 0 on all others). Intuitively, the corpus always continues this context in one way (Â§2). The paper shows these cases are especially reliable (Â§4.1, Figure 3 right).
  - Why perplexity is not reported for pure `âˆ-gram`
    - Zero probabilities cause infinite perplexity; instead, the paper reports perplexity for an interpolation of `âˆ-gram` with a neural LM (Â§2, Â§5).

- 3.2 The `infini-gram` engine (Â§3; Appendix A)
  - What is a `suffix array` (definition, Â§3)
    - An array of pointers to all suffixes of a sequence, sorted lexicographically. It lets you find how many times a query string appears as a substring in `O(L + log N)` time (L = query length; N = corpus length).
  - How the index is built and stored
    - The input corpus is tokenized. A `token array` stores token IDs as bytes; documents are separated by a special 0xFFFF token (Â§3, Figure 2 right).
    - The `suffix array` stores for each suffix its starting byte offset into the token array. With 2 bytes per token ID and 5 bytes per pointer (for corpus sizes considered), total storage is about 7 bytes per token (Â§3).
    - Example scale: indexing 1.4T tokens took ~48 hours on one 128â€‘CPU, 1 TiB RAM node and ~10 TB disk (Â§3).
    - Indexes for Dolma (3T), RedPajama (1.4T), Pile (380B), and C4 (200B) are built; these are additive to reach 5T tokens (Â§3, Â§A.2).
  - How counting and probabilities are computed
    - For a query nâ€‘gram, all its occurrences form a contiguous slice in the suffix array; binary searching for the slice boundaries gives the count. This underlies all `COUNT`, `NGRAMPROB`, and `INFGRAMPROB` queries (Â§3, Â§A.4).
    - For `âˆ-gram`, a â€œbinaryâ€‘lifting + binaryâ€‘searchâ€ finds the longest suffix length with nonzero count in `O(log L)` counting calls (Â§A.4).
  - Latency and complexity
    - Onâ€‘disk, memoryâ€‘mapped access with prefetching, shardâ€‘parallelism, and amortization yields subâ€‘second latency for all queries (Â§A.4â€“A.5).
    - Table 3 reports average latencies (single 8â€‘core CPU):
      > Counting an nâ€‘gram: ~13â€“20 ms; `n` from 1 to 1000 has similar time (Pileâ€‘train vs. RedPajama).  
      > `âˆ-gram` token probability: 90â€“135 ms; full `âˆ-gram` nextâ€‘token distribution: 88â€“180 ms (Table 3).
  - Extra capabilities
    - Document retrieval: given an nâ€‘gram or a CNF expression (AND/OR over multiple nâ€‘grams), retrieve all matching documents (`SEARCHDOC`; Â§A.5, Figure 16).
    - Index additivity/subtractivity enables composing/shrinking corpora without rebuilding (Â§A.2).

- 3.3 Combining with neural LMs (Â§2, Â§5)
  - Simple linear interpolation:
    `P(y | x) = Î» Pâˆ(y | x) + (1 âˆ’ Î») Pneural(y | x)`.
  - Two hyperparameters are used in practice (Â§5): `Î»1` for contexts with `sparse` `âˆ-gram` estimates (high confidence) and `Î»2` otherwise. Values are tuned on validation to minimize perplexity.
  - For timeâ€‘shifted data, a Random Forest selects an instanceâ€‘wise interpolation weight using features such as suffix lengths and frequencies, further improving perplexity (Â§D.2, Table 5).

- 3.4 Datasets and decontamination (Â§4, Â§B)
  - To avoid trivial copyâ€‘through, the training corpora for `âˆ-gram` are decontaminated against evaluation sets using the Big Friendly Filter: remove a document if â‰¥80% of its 13â€‘grams appear in evaluation (Â§4, Â§B).
  - Table 4 shows filtering stats; e.g., 0.6% of Pileâ€‘train documents removed overall, with high removal in GitHub (5.3%).

- 3.5 A running example (Figure 1)
  - A 5â€‘gram LM fails to predict the next token in a snippet, while `âˆ-gram` finds the longest matching suffix (`n = 16` here) and predicts correctly by counting continuations in the corpus.

## 4. Key Insights and Innovations
- Unbounded nâ€‘gram LM with a valid distribution (Â§2)
  - Innovation: `âˆ-gram` starts from arbitrarily long context and backs off until the context (denominator) is seen; unlike standard backoff, it yields a proper probability distribution without discounting.
  - Significance: It preserves as much context as the corpus can support on each instance. This is fundamentally different from fixed smallâ€‘`n` models that necessarily truncate context.
- Trillionâ€‘token suffixâ€‘array engine with millisecond latency (Â§3; Â§A.4â€“A.5)
  - Innovation: A compact (7 bytes/token) onâ€‘disk index with memoryâ€‘mapped, sharded, hinted binary search and amortized computation.
  - Significance: Enables interactive nâ€‘gram/`âˆ-gram` probability and document retrieval over 5T tokens. Prior suffixâ€‘based language modeling did not reach this scale or latency (Table 6; Â§6, Â§F).
- Empirical finding: `âˆ-gram` is highly predictive where the corpus â€œcommitsâ€ (Â§4.1)
  - Novel result: On decontaminated Pile validation data, `âˆ-gram` agrees with ground truth on 47% of tokens overall, and exceeds 75% agreement for tokens with `effective n â‰¥ 16` (Figure 3 middle). When the estimate is `sparse`, overall agreement rises to 75%, and to >80% for `effective n â‰¥ 14` (Figure 3 right).
  - Significance: Largeâ€‘context exactâ€‘match statistics are powerful signals, different from neural probabilities (Figure 4).
- Complementarity with strong neural LMs (Â§5)
  - Novel result: Interpolating `âˆ-gram` with Llamaâ€‘2 70B reduces perplexity from 4.59 to 3.96 on Pile validation (âˆ’18%), and to 3.95 on test (âˆ’19%) when using Pileâ€‘train + RedPajama as the reference data (Table 1).
  - Significance: This contradicts the conventional wisdom that nâ€‘grams no longer help large LMs; the key appears to be both data scale and unbounded context.
- Diagnostic lens on decoding and positional effects (Â§4.2)
  - Novel observation: Greedy decoding exhibits strong, sometimes periodic, fluctuations in `âˆ-gram` agreement as `effective n` grows (e.g., Llamaâ€‘2 7B dips at n = 20, 24, 28, 32 with p < 10â»â¹â¹), unlike nucleus sampling which resembles human text distributions (Figure 5). The paper hypothesizes links to positional embeddings.

## 5. Experimental Analysis
- Evaluation methodology
  - Human text: Pile validation/test. Tokenâ€‘wise `âˆ-gram` agreement is measured by checking whether `Pâˆ(true token | prefix) > 0.5` (a lower bound on argmax accuracy), binned by `effective n` (Â§4.1; Figure 3).
  - Machine text: Continue 50â€‘token prompts from Pileâ€‘val across model sizes (Llamaâ€‘2 7B/13B/70B; GPTâ€‘J 6B; GPTâ€‘Neo 125M/1.3B/2.7B) and decoding schemes (greedy, temperature, nucleus). Analyze agreement vs. `effective n` (Â§4.2; Figure 5, Figure 9).
  - Perplexity: Only for the interpolated model (`neural + âˆ-gram`) since `âˆ-gram` has zeros. Evaluate on Pile validation/test and timeâ€‘shifted Wikipedia (Aprilâ€“Aug 2023) (Â§5; Â§D.2).
  - Decontamination: Big Friendly Filter; statistics in Table 4 (Â§B).
  - Tokenizers: Separate `infini-gram` indexes for GPTâ€‘2/Neo/J, Llamaâ€‘2, and SILO tokenizers (Â§5.1).
- Main quantitative results
  - Predictiveness of `âˆ-gram` on human text (Â§4.1)
    > Overall agreement 47%; >75% when `effective n â‰¥ 16` (Figure 3 middle).  
    > With `sparse` estimates: 75% overall; >80% for `effective n â‰¥ 14` (Figure 3 right).
    - 5â€‘gram baselines have much lower agreement because most tokens require context longer than 5 (Figure 3 left; median `effective n` is 7 and mean is 9.1).
    - Qualitatively, `âˆ-gram` excels at continuing multiâ€‘token words, common phrases, and entity tails but struggles to recall the first token of names (Figure 3 discussion).
  - Complementarity with neural LMs (Â§4.1; Figure 4)
    > When Llamaâ€‘2 assigns very low probability, `âˆ-gram` still has >20% agreement, rising to ~50% on `sparse` cases (Figure 4).
  - Perplexity gains from interpolation (Pile val/test; Table 1)
    > Llamaâ€‘2 70B: 4.59 â†’ 3.96 (âˆ’18%) on val and 4.65 â†’ 3.95 (âˆ’19%) on test with Pileâ€‘train+RedPajama.  
    > Llamaâ€‘2 13B: 5.30 â†’ 4.41 (âˆ’21%) on val and 5.43 â†’ 4.42 (âˆ’23%) on test, outperforming Llamaâ€‘2 70B baseline (Â§5.2, Table 1).  
    > GPTâ€‘2 1.6B: 14.42 â†’ 9.93 (âˆ’33%) on val; 14.61 â†’ 9.93 (âˆ’34%) on test.  
    > GPTâ€‘J 6.7B: 6.25 â†’ 5.75 (âˆ’10%) on val; 6.51 â†’ 5.85 (âˆ’12%) on test.
    - Gains are larger when the neural LMâ€™s pretraining data differs more from the `âˆ-gram` reference (e.g., GPTâ€‘2 vs. GPTâ€‘Neo on Pile; Â§5.2).
  - SILO and comparisons to other retrieval methods (Â§5; Table 2)
    > On Enron Emails (domain outâ€‘ofâ€‘distribution for SILO), SILOâ€‘PD PPL drops 19.56 â†’ 6.31 (âˆ’70%) on val and 20.62 â†’ 4.85 (âˆ’73%) on test with `âˆ-gram`, outperforming kNNâ€‘LM and RICâ€‘LM lines reported in Min et al. (2023a) (Table 2).  
    > On Wikipedia and NIH ExPorters, `âˆ-gram` consistently improves SILO variants, often more than kNNâ€‘LM / RICâ€‘LM.
  - Timeâ€‘shifted Wikipedia (Aprilâ€“Aug 2023; Table 5)
    > Simple interpolation gives 0â€“6% relative gains; a Random Forest gating over suffix features raises this to 3â€“20% (Table 5).
  - Scaling and domain ablations (Â§D.3; Figure 10)
    > Gains grow roughly logâ€‘linearly with reference data size; using only inâ€‘domain slices performs similarly to using the full reference set, suggesting most benefit comes from inâ€‘domain matches.
  - Machine text decoding analysis (Â§4.2; Figure 5)
    > Nucleus sampling most closely matches human `effective n` distribution and yields smoother agreement curves.  
    > Greedy decoding shows strong oscillations in agreement as `effective n` increases; smaller models and Llamaâ€‘2 7B show pronounced periodicity (p < 10â»â¹â¹).
- Efficiency results
  - Query latency and complexity summarized in Table 3 and Â§A.4â€“A.5:
    > COUNT: ~13â€“20 ms; `NGRAMDIST (n=5)`: 31â€“39 ms; `INFGRAMPROB`: 90â€“135 ms; `INFGRAMDIST`: 88â€“180 ms.
  - Storage: â‰ˆ7 bytes/token; e.g., RedPajamaâ€™s 1.4T tokens indexed in ~2 days on a single 128â€‘core CPU node using ~10 TB disk (Â§3).
- Do the experiments support the claims?
  - Yes, convincingly for â€œcomplementarityâ€ and â€œpracticalityâ€: sizable PPL reductions across model families (Table 1â€“2), strong on OOD domains (SILO Enron), and measurable millisecondâ€‘level latency at trillionâ€‘token scale (Table 3).
  - The human/machine agreement analyses (Figures 3â€“5) provide granular evidence that longâ€‘suffix counts carry predictive power not captured by neural probabilities alone (Figure 4).
- Robustness and caveats
  - Careful decontamination reduces trivial copy effects (Table 4), and timeâ€‘shifted tests confirm continued value (Table 5).
  - The modelâ€™s own zeroâ€‘probability nature prevents direct perplexity reporting for `âˆ-gram`; interpretation relies on interpolation choices (Â§2, Â§5).
  - A noted failure mode: naive interpolation may harm openâ€‘ended generation by steering the model into irrelevant continuations; gating/learning when to trust `âˆ-gram` is important (Â§5.2, â€œA note on text generationâ€).

## 6. Limitations and Trade-offs
- Dependence on exact surface forms
  - `âˆ-gram` only â€œknowsâ€ what appears verbatim in the reference data. It cannot generalize semantically beyond exact counts, so it struggles with novel paraphrases, rare entitiesâ€™ first tokens, or reasoning beyond memorized strings (qualitative notes in Â§4.1).
- Zero probabilities
  - As a countâ€‘based model, `âˆ-gram` assigns zero to unseen continuations, precluding standalone perplexity evaluation and risking harmful guidance in generation. Interpolation and gating are necessary (Â§2, Â§5.2).
- Data and storage requirements
  - While compact for its scale (7 bytes/token), a 5Tâ€‘token index is â‰ˆ35 TB. Building and serving such indexes require significant CPU time and fast SSDs, though still far below vectorâ€‘retrieval footprints at comparable scales (Table 6).
- Tokenization and vocabulary constraints
  - The storage layout assumes 2 bytes per token ID (|V| < 65,536). Other tokenizers must be separately indexed; crossâ€‘tokenizer use is not supported (Â§5.1).
- Domain shift and coverage
  - Gains are largest with inâ€‘domain or overlapping distributions (Figure 10). For strongly outâ€‘ofâ€‘domain text with little surface overlap, `âˆ-gram` contributes less.
- Open questions
  - The cause of periodic agreement dips under greedy decoding (Figure 5) is hypothesized to involve positional embeddings, but this remains to be rigorously tested (Â§4.2).
  - Best practices for using `âˆ-gram` in generation (how to gate, when to trust) remain to be developed (Â§5.2).

## 7. Implications and Future Directions
- Shifting the role of data in language modeling
  - This work reâ€‘establishes nonparametric, dataâ€‘centric modeling as a firstâ€‘class complement to large neural LMs, at the actual pretraining scale (5T tokens). The ability to query what the corpus literally containsâ€”fast and preciselyâ€”has farâ€‘reaching implications for attribution, compliance, and debugging (Â§E).
- Practical applications enabled by `infini-gram` (Â§E; Appendix G)
  - Retrieval and attribution at pretraining scale (e.g., â€œwhich documents contain this phrase?â€ via `SEARCHDOC`).
  - Data curation and decontamination loops using additive/subtractive indexing (Â§A.2; Figures 11â€“16).
  - Hallucination mitigation by biasing toward observed continuations in factual contexts; copyrightâ€‘risk mitigation by diverting away from long nâ€‘grams that uniquely occur in copyrighted sources (Â§E).
  - Memorization/plagiarism detection, novelty/creativity measurement, and entity popularity quantification using counts (Â§E).
- Research directions
  - Learned gating for interpolation: Beyond two global `Î»`s, learn instanceâ€‘wise trust policies (Random Forest results in Table 5 are a first step).
  - Hybrid retrieval+`âˆ-gram`: Combine neural semantic retrieval with exact suffixâ€‘based counts, using `âˆ-gram` for local continuation probabilities.
  - Nonparametric speculative decoding: Use `âˆ-gram` as the fast proposer in speculative decoding pipelines (Â§E), similar in spirit to retrievalâ€‘based speculative approaches.
  - Understanding positional representations: Use the `effective n` agreement curves to probe LLM positional encoding behaviors (Figure 5).
  - Multilingual and code domains: Extend indexes and analyses to diverse tokenizers and domains; exploit inâ€‘domain advantage observed in Figure 10.

> Most important takeaways, grounded in the paperâ€™s figures and tables:
> - `âˆ-gram` is surprisingly predictive on real text, especially when it finds long seen suffixes: 47% overall agreement; >75% at `effective n â‰¥ 16` (Figure 3).  
> - It complements large LLMs substantially: Llamaâ€‘2 70B perplexity improves 4.59 â†’ 3.96 (âˆ’18%) on Pileâ€‘val with Pile+RedPajama (Table 1), and SILO gains up to âˆ’73% on OOD domains (Table 2).  
> - The `infini-gram` engine makes trillionâ€‘token, unbounded nâ€‘gram modeling practical with 13â€“180 ms per query (Table 3), 7 bytes/token storage, and additive indexing (Â§3, Â§A.2, Â§A.5).

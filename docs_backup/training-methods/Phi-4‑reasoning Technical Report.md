# Phi-4â€‘reasoning Technical Report

**ArXiv:** [2504.21318](https://arxiv.org/abs/2504.21318)
**Authors:** Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo deÂ Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, CaioÂ CÃ©sarÂ TeodoroÂ Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng
**Institutions:** Microsoft Research

## ğŸ¯ Pitch

The paper presents two innovative 14-billion-parameter reasoning models, `Phi-4-reasoning` and `Phi-4-reasoning-plus`, leveraging teachable prompts and chain-of-thought demonstrations to rival much larger models in tasks such as math, coding, and planning. This breakthrough in data-centric training and compact reinforcement learning offers a cost-effective solution that powers diverse applications on affordable hardware, while setting new benchmarks for model training and evaluation processes.

---

## 1. Executive Summary (2â€“3 sentences)
The report introduces two 14â€‘billionâ€‘parameter reasoning models, `Phi-4-reasoning` (via supervised fineâ€‘tuning) and `Phi-4-reasoning-plus` (SFT + a short phase of reinforcement learning), that use carefully curated â€œteachableâ€ prompts and highâ€‘quality chainâ€‘ofâ€‘thought demonstrations to unlock inferenceâ€‘time reasoning. Across math, science, coding, planning, and spatial tasks, these models approach or surpass much larger openâ€‘weight baselines and show strong costâ€“accuracy tradeâ€‘offs, while also spotlighting evaluation pitfalls for small, stochastic benchmarks (Figures 1, 8â€“12; Table 1).

## 2. Context and Motivation
- Problem addressed
  - Most small open models struggle with complex, multiâ€‘step reasoning that benefits from â€œthinking longerâ€ at inference time. Existing strong reasoning models either are very large, proprietary, or rely on heavy RL; smaller distilled models often lose capability or require expensive inference budgets.
  - This work seeks a scalable, dataâ€‘centric path to teach a 14B model to reason stepâ€‘byâ€‘step with inferenceâ€‘time scaling, and to do so with transparent training recipes and robust evaluation.

- Why it matters
  - Practical impact: Reasoning models that fit on affordable hardware can power math tutoring, scientific QA, planning, code assistance, and agentic workflows without the cost of frontier models.
  - Scientific significance: Shows how careful data selection and outcomeâ€‘based RL interact to induce longer, more effective chainâ€‘ofâ€‘thought (CoT) behavior; highlights reproducible evaluation practices for stochastic reasoning (Section 5.1.2; Figures 2, 9â€“12).

- Prior approaches and gaps
  - Distillation + RL for reasoning has been explored (e.g., DeepSeekâ€‘R1 and distilled variants; Section 1, citations [21, 58, 59, 34, 61]). However:
    - Data selection is often coarse; prompts may be too easy, too hard, or unverified, limiting transfer.
    - Evaluation commonly reports singleâ€‘run scores on tiny benchmarks (e.g., AIME), which are highly variable (Figure 9).
  - This work positions itself as a dataâ€‘centric alternative: select â€œteachableâ€ seeds lying near the base modelâ€™s capability boundary; generate highâ€‘quality demonstrations; then add a compact but effective RL stage with a lengthâ€‘aware reward (Sections 2â€“4).

## 3. Technical Approach
This section explains how the models are constructed and trained, the data pipeline, and how the RL stage works.

- Base and architectural edits (Section 3)
  - Start from `Phi-4` (14B). Modify two unused tokens into `<think>` and `</think>` to bracket the reasoning block.
  - Extend maximum context from 16K to 32K tokens by doubling the RoPE base frequency; this supports longer CoT traces during training and inference.
  - Supervised fineâ€‘tuning (SFT) uses 1.4M curated promptâ€“response pairs totaling 8.3B tokens from math, coding, and safety/RAI domains (Section 3).

- Seed selection and data curation (Sections 2â€“2.2)
  - Build a large seed set (prompts) from filtered web data, licensed sources, and synthetic rewrites. Then aggressively filter to keep only â€œteachableâ€ seeds:
    - â€œTeachableâ€ = at the boundary of the base modelâ€™s capability and requiring multiâ€‘step reasoning.
    - When no gold answers exist, create proxy ground truth using plurality from a strong reference model; measure difficulty as agreement gaps with weaker models (Section 2.1).
    - Use rubricâ€‘based LLM evaluators to estimate required reasoning steps and filter accordingly (Section 2.1).
  - Synthetic rewriting for verifiability: convert hardâ€‘toâ€‘verify problems into formats with concise, checkable final answers, easing future RL (Figure 3).
  - Decontaminate against many benchmarks (AIMEâ€‘2024, MATH, GPQA, LiveCodeBench, OmniMATH, SWEâ€‘Benchâ€‘Verified, and more; Section 2.2); AIMEâ€‘2025 is postâ€‘cutoff and thus clean.

- Supervised training recipe (Section 3; Figure 5; Figure 4)
  - Teacher signals: generate long CoT traces with `o3-mini` (medium/high â€œreasoning effortâ€) and place them inside `<think> ... </think>` followed by a concise final â€œSolutionâ€ (Section 3).
  - System message: a fixed reasoning prompt teaches consistent twoâ€‘part output: Thought in `<think>...</think>`, then a succinct Solution (Section 3.1, â€œRole of system messageâ€).
  - Hyperparameters: AdamW, learning rate 1eâ€‘5 (best among 1eâ€‘6â€“2eâ€‘5; Experiments 1â€“3 in Figure 5), linear warmup 450 steps, weight decay 1eâ€‘4, global batch 32, context 32K, ~16K steps (Section 3).
  - Data mixture â€œadditivityâ€: tune weights for clusters per domain (math, code, safety), then combine recipes; improvements persist across domains (Figure 5, experiments 6â€“12).
  - Training dynamics: accuracy improves steadily on AIMEâ€‘24 and GPQAâ€‘Diamond (Figure 4a). Notably, average response length slightly decreases as SFT proceeds (Figure 4b), suggesting more efficient use of tokens as reasoning quality improves.

- Reinforcement learning phase (Section 4; Figure 7)
  - Algorithm: Group Relative Policy Optimization (`GRPO`), a PPOâ€‘style method where each prompt yields a group of candidate completions; advantages are normalized within the group (Section 4.2).
  - Reward design (Section 4.1; Figure 6):
    - Lengthâ€‘aware correctness reward `Racc_scaled`: encourages concise generations when correct and longer exploration when incorrect. Intuition: do not waste tokens if youâ€™re on track; invest more when youâ€™re not.
      - Correct answer: reward smoothly decays if the output becomes unnecessarily long beyond a threshold (`Lpos_control = 25,600` tokens).
      - Incorrect answer: reward is less negative for longer attempts up to a minimum threshold (`Lneg_control = 3,702` tokens), nudging more thinking before answering incorrectly.
    - Formatting penalties: missing EOS or malformed `<think>` tags receive negative overrides to promote wellâ€‘formed outputs.
    - Repetition penalty `Rrep`: discourages repeated 5â€‘grams above frequency thresholds.
    - Final reward: `Rfinal = (8/13)*Racc_scaled + (1/13)*Rrep`, combining accuracy dominance with light repetition control.
  - RL data and compute:
    - Focused exclusively on math with verifiable answers; 72,401 seeds available, subsampled 64 per iteration. Best checkpoint obtained after only ~90 steps (~6.4K problems Ã— 8 samples each), using 32 H100s, LR 5eâ€‘8, KL 0.001, entropy 0.001, max length 32K; outputs clipped at 31K to save 1K for prompts (Section 4.2).
  - RL dynamics and effects (Figure 7):
    - AIMEâ€‘24 accuracy increases by >10% within the first 90 steps (Figure 7a).
    - Accuracy correlates positively with response length (Figure 7c); reward correlates weakly with accuracy (Figure 7b).
    - Incorrect generations grow faster in length than correct ones (Figure 7d), matching the intended â€œthink more when youâ€™re wrongâ€ design.
    - As more samples hit the 31K clip limit, total reward plateaus (Figure 7e), hinting at benefits from even larger context windows (64K+).

- Evaluation methodology and inferenceâ€‘time scaling (Sections 5â€“5.1.4; Figures 1â€“2, 8â€“12, 17)
  - Standardized pipelines: reuse MathArena for HMMT and Eureka ML Insights for most tasks to ensure consistent prompts, judges, and extraction (Section 5).
  - Stochasticity handling: run many repetitions for small benchmarks (e.g., 50 independent runs for AIMEâ€‘2025; Figure 9) and analyze distributions rather than singleâ€‘run scores.
  - Testâ€‘time compute scaling: majorityâ€‘ofâ€‘N or bestâ€‘ofâ€‘N improves accuracy markedly (Figures 2, 12, 17), revealing headroom if one can afford parallel sampling.

## 4. Key Insights and Innovations
- Dataâ€‘centric â€œteachable seedâ€ curation with verifiable outputs
  - Whatâ€™s new: using agreement gaps with weaker models and rubricâ€‘based step assessments to select prompts â€œnear the boundaryâ€ of base capability, then rewriting problems into easily verifiable forms (Sections 2â€“2.1; Figure 3).
  - Why it matters: SFT learns transferrable reasoning strategies rather than shallow pattern matching; measurable accuracy gains across math, code, planning, and spatial tasks (Figures 1, 8; Table 1).

- Structured reasoning format with `<think>` tags plus a stable system message
  - Whatâ€™s new: repurposed tokens for explicit Thought/Solution structure and a single, consistent reasoning system message to teach formatting robustness (Section 3.1).
  - Why it matters: Rapid adoption of CoT structure early in SFT and stable formatting at inference (Figure 4). The model learns to be concise in the Solution while exploring in `<think>`.

- Lengthâ€‘aware, outcomeâ€‘based RL that operationalizes â€œthink more when youâ€™re wrongâ€
  - Whatâ€™s new: a reward `Racc_scaled` that penalizes overly long correct answers yet encourages longer exploration for (likely) incorrect ones; lightweight repetition and formatting penalties (Section 4.1; Figure 6).
  - Why it matters: With only ~6.4K problems and 90 RL steps, AIME accuracy jumps by >10% (Figure 7a), and generations become longer primarily when needed (Figure 7d).

- Evaluation that moves beyond singleâ€‘run reporting for tiny benchmarks
  - Whatâ€™s new: distributional analyses over 50 runs on AIMEâ€‘2025 (Figure 9), perâ€‘year breakdowns (Figure 10), and bestâ€‘ofâ€‘N/worstâ€‘ofâ€‘N diagnostics (Figures 12, 17).
  - Why it matters: Demonstrates that singleâ€‘run AIME scores can differ by 5â€“10 points; Phiâ€‘4â€‘reasoningâ€‘plusâ€™ accuracy distribution largely overlaps with `o3-mini-high` and is almost disjoint from `R1-Distillâ€‘70B` (Figure 9), providing a more reliable comparative picture.

## 5. Experimental Analysis
- Evaluation setup (Sections 5, A; Table 3â€“4)
  - Benchmarks for reasoning: AIMEâ€‘2025 (30 items, postâ€‘training), AIMEâ€‘83â€“24 (949 items), HMMTâ€‘Febâ€‘2025 (30), OmniMATH (4,428), GPQA Diamond (198), LiveCodeBench 8/24â€“1/25, Codeforces (contests 1505â€“1536), TSP and 3SAT (new), BAâ€‘Calendar (2,000), Maze (10Ã—10) and SpatialMap (1,500 each). See Table 4 for sources.
  - Generalâ€‘purpose: FlenQA (lengthâ€‘controlled QA), IFEval (instruction following), ArenaHard (chat preference), HumanEvalPlus (code), MMLUâ€‘Pro, Kitab (RAGâ€‘style retrieval with constraints), Toxigen (toxicity detection), and internal PhiBench (Table 2).
  - Metrics: pass@1 accuracy (averaged over multiple runs), Elo for Codeforces, precision/recall for Kitab, length/accuracy tradeâ€‘offs (Figures 11, 14â€“16).
  - Baselines: `DeepSeek-R1`, `R1-Distill-Llamaâ€‘70B`, `o1`, `o1-mini`, `o3-miniâ€‘high`, `Claudeâ€‘3.7â€‘Sonnetâ€‘Thinking`, `Geminiâ€‘2.5â€‘Pro/Flashâ€‘Thinking`, and `Phiâ€‘4` (Table 1; Figure 8). Temperatures and token limits in Table 3.

- Main quantitative results
  - Math and science (Table 1; Figures 1, 8, 10)
    - AIMEâ€‘2025 (50 independent runs): 
      > `Phi-4-reasoning-plus` 78.0%; `Phi-4-reasoning` 63.1%; `DeepSeek-R1` 70.4%; `R1-Distill-70B` 51.5%; `o3-mini-high` 82.5%; `o1` 71.4%.
    - AIMEâ€‘83â€“24:
      > `Phi-4-reasoning-plus` 89.4%; `Phi-4-reasoning` 83.1%; `DeepSeek-R1` 86.0%; `o3-mini-high` 93.0%.
    - OmniMATH:
      > `Phi-4-reasoning-plus` 81.9%; `Phi-4-reasoning` 76.6%; `DeepSeek-R1` 85.0%; `o3-mini-high` 74.6%.
    - GPQAâ€‘Diamond:
      > `Phi-4-reasoning-plus` 69.3%; `Phi-4-reasoning` 67.1%; `R1-Distill-70B` 66.2%; `DeepSeek-R1` 73.0%; `o1` 76.7%; `o3-mini-high` 77.7%.
    - Perâ€‘year AIME analysis shows large variance by year and a common dip in 1994 and 2025 (Figure 10).
  - Algorithmic, planning, and spatial (Figure 8)
    - BAâ€‘Calendar planning:
      > `Phi-4-reasoning` 67.7%; `Phi-4-reasoning-plus` 65.6%; `DeepSeek-R1` 79.2%; `o1` 86.1%; `Claude` 88.5%.
    - TSP:
      > `Phi-4-reasoning-plus` 42.6% vs. `Phi-4-reasoning` 37.5%; `o3-mini-high` 56.4%; `DeepSeek-R1` 46.7%.
    - Maze and SpatialMap:
      > On Maze, both Phiâ€‘4â€‘reasoning models score ~55â€“55.1â€“53.4% (vs. `o1` ~79.7%); on SpatialMap, both are ~73â€“74% with `o1` ~83.6% and `o3-mini-high` ~77.4%.
  - Coding (Table 1)
    - LiveCodeBench (8/24â€“1/25):
      > `Phi-4-reasoning` 53.8%; `Phi-4-reasoning-plus` 53.1%; `R1-Distill-70B` 57.5%; `DeepSeek-R1` 65.9%; `o1` 63.4%.
      - Note: RL focused on math and did not include coding seeds (Section 4), which explains smaller gains here.
    - Codeforces Elo (10 attempts per problem):
      > `Phi-4-reasoning` 1736; `Phi-4-reasoning-plus` 1723; `R1-Distill-70B` 1633; `DeepSeek-R1` 2029 (Table 1).
  - Generalâ€‘purpose (Table 2; Figure 13)
    - FlenQA (3Kâ€‘token subset): 
      > `Phi-4-reasoning` 97.7%; `Phi-4-reasoning-plus` 97.9%; `GPT-4o` 90.8%.
      - Accuracy degrades less with longer contexts, and is insensitive to where key information appears (Figure 13).
    - IFEval (Strict): 
      > `Phi-4-reasoning-plus` 84.9% vs. `Phi-4` 62.3% and `GPT-4o` 81.8%.
    - ArenaHard: 
      > `Phi-4-reasoning-plus` 79.0% vs. `Phi-4` 68.1%.
    - HumanEvalPlus: 
      > `Phi-4-reasoning` 92.9% vs. `Phi-4` 83.5%.
    - MMLUâ€‘Pro:
      > `Phi-4-reasoning-plus` 76.0% vs. `Phi-4` 71.5%.
    - Kitab (RAG subset):
      > With context, precision ~93â€“94% and recall ~75%â€”on par with `o3-mini` on this split; without context, precision rises with reasoning but recall can drop (Table 2).
    - Toxigen discriminative:
      > `Phi-4-reasoning` improves â€œtoxicâ€ detection (86.7%) but slightly lowers â€œneutralâ€ (84.7%); `Phi-4-reasoning-plus` flips the tradeâ€‘off (77.3% toxic, 90.5% neutral). Aggregate trends and perâ€‘group patterns shown in Figure 18.

- Robustness checks and ablations
  - SFT ablations: learning rate search; effect of synthetic math data (Figure 5, experiments 4â€“5); system message stability (Section 3.1).
  - Teacher strength and context length: `o3-mini` highâ€‘effort produces stronger but longer traces; extending context to 32K enables training on longer CoT (Section 3.2).
  - Variance analysis: 50â€‘run KDE on AIMEâ€‘2025 (Figure 9) shows wide ranges for all models; bestâ€‘ofâ€‘N can substantially outperform averageâ€‘ofâ€‘N (Figures 12, 17).
  - Token efficiency: `Phi-4-reasoning-plus` uses ~1.5Ã— more tokens than `Phi-4-reasoning` on average; tokenâ€‘accuracy tradeâ€‘offs visualized per benchmark (Figure 11).

- Do the experiments support the claims?
  - Yes, with caveats. On uncontaminated AIMEâ€‘2025, `Phi-4-reasoning-plus` is competitive with much larger or proprietary models and clearly surpasses `R1-Distillâ€‘70B` (Table 1, Figure 9). Gains generalize across OmniMATH and several generalâ€‘purpose tasks (Tables 1â€“2). However, coding lags `DeepSeek-R1` and `o1`, and planning/spatial tasks show room for improvement. The paper also convincingly demonstrates evaluation variance and the benefits of parallel testâ€‘time compute (Figures 2, 9, 12, 17).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - RL data is mathâ€‘only; improvements in coding, planning, and spatial tasks come mostly from SFT and are smaller than in math (Sections 4, 5.1.3; Figure 8).
  - Teacherâ€‘generated CoT (from `o3-mini`) provides highâ€‘quality signals but may bias reasoning styles; reliance on a strong, proprietary teacher could limit fully open replication (Section 3.2).

- Computational and data constraints
  - Context window is 32K; RL training clips outputs at ~31K tokens, which caps â€œthinking depthâ€ and flattens reward at high steps (Figure 7e). The report suggests 64K support would help (Section 4.2).
  - `Phi-4-reasoning-plus` consumes ~1.5Ã— more tokens than `Phi-4-reasoning` on average; gains are largest in math but not universal (Figure 11).

- Evaluation challenges
  - Tiny, hard benchmarks like AIME (30 items) are highly stochastic; singleâ€‘run comparisons are unreliable (Figure 9). The report addresses this by running 50 seeds and showing distributions, but the broader community often does not.

- Safety and alignment
  - Automated RAI metrics show minor regressions relative to `Phi-4` (Section 5.3), and existing LLM judges may mis-handle long, nonâ€‘linear CoT traces (Section 5.3). Toxigen reveals tradeâ€‘offs between detecting toxicity and avoiding erasure (Table 2; Figure 18).

## 7. Implications and Future Directions
- How this work shifts the field
  - Demonstrates that a carefully curated SFT corpus plus a compact, outcomeâ€‘based RL stage can produce a 14B reasoning model that competes with much larger openâ€‘weight models on complex math and generalizes beyond math (Figures 1, 8; Tables 1â€“2).
  - Establishes best practices for evaluation under stochasticity: multiâ€‘run distributions, bestâ€‘/worstâ€‘ofâ€‘N, and accuracyâ€“token tradeâ€‘offs (Figures 2, 9â€“12, 17).

- Followâ€‘up research enabled/suggested
  - RL beyond math: extend GRPO with verifiable rewards for planning, spatial reasoning, and coding (Section 5.1.3).
  - Longer contexts: support 64K+ with interpolation or RoPE variants to reduce clipping and further raise RL ceilings (Section 4.2).
  - Smarter decoding: exploit the gap between averageâ€‘ofâ€‘N and bestâ€‘ofâ€‘N with better verifiers, selection policies, or confidence estimators to reduce parallel sampling costs (Figures 2, 12, 17).
  - Safer CoT evaluation: develop judges/verifiers designed for long, branching thoughts to reduce false positives/negatives in safety assessments (Section 5.3).

- Practical applications
  - Education and assessment (AIME/OmniMATHâ€‘level problem solving), scientific QA (GPQA), calendar/constraint planning (BAâ€‘Calendar), and longâ€‘context reasoning tasks (FlenQA).
  - Costâ€‘sensitive deployments where 14Bâ€‘scale models with strong reasoning are preferred over massive frontier models, with the option to dial inferenceâ€‘time computeâ€“accuracy tradeâ€‘offs (Figures 1, 11).

> Bottom line: With rigorous data selection, explicit thought formatting, and a short, lengthâ€‘aware RL phase, `Phi-4-reasoning` and `Phi-4-reasoning-plus` deliver competitive reasoning at 14B scale and set higher standards for how reasoning models should be trained and evaluated.

# TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning

**ArXiv:** [2509.25760](https://arxiv.org/abs/2509.25760)

## ðŸŽ¯ Pitch

TruthRL introduces a novel reinforcement learning framework for large language models that directly optimizes for truthfulness by rewarding correct answers, treating honest abstentions neutrally, and explicitly penalizing hallucinations. This approach moves beyond traditional accuracy-driven training, enabling models to recognize the limits of their knowledge and avoid confidently producing false informationâ€”a crucial capability for high-stakes applications where mistakes can have severe consequences. Extensive evaluations demonstrate that TruthRL substantially reduces hallucinations and boosts reliability across tasks, establishing a new paradigm for developing trustworthy AI systems.

---

## 1. Executive Summary
TruthRL is a reinforcementâ€‘learning framework that trains language models to be truthful by rewarding three distinct outcomes: correct answers, explicit abstentions (e.g., â€œI donâ€™t knowâ€), and penalizing hallucinations (confidently wrong answers). Across four knowledgeâ€‘intensive benchmarks and multiple backbones, it lowers hallucinations by large margins and raises a composite truthfulness score, showing that directly optimizing for uncertaintyâ€‘aware behavior outperforms accuracyâ€‘only training.

## 2. Context and Motivation
- Problem addressed
  - Large language models (LLMs) often â€œhallucinateâ€â€”produce fluent but factually wrong statementsâ€”especially when questions fall outside what they know or when retrieved evidence is noisy. The paper defines a truthful model as one that both answers correctly when it can and abstains when it cannot (Introduction; Section 2.1).
- Why this matters
  - In highâ€‘stakes domains (law, medicine), a wrong but confident answer is more harmful than an admission of uncertainty. The paper explicitly argues that â€œaccuracy alone does not guarantee truthfulnessâ€ because accuracyâ€‘centric training can incentivize guessing (Introduction).
- Shortcomings of prior approaches
  - Accuracyâ€‘driven supervised fineâ€‘tuning (SFT) and vanilla RL reward â€œansweringâ€ behavior and suppress abstentions, which can amplify hallucinations (Sections 2.2â€“2.3; Figure 2).
  - Retrievalâ€‘augmented generation (RAG) helps but retrieval can be noisy or misleading (Introduction).
  - Methods that teach abstention (e.g., Râ€‘Tuning) require nontrivial dataset construction and often become overly conservative, reducing correct coverage (Introduction; Section 3.1; Table 1).
- Positioning
  - The paper reframes the objective from â€œmaximize accuracyâ€ to â€œmaximize truthfulness,â€ i.e., reward correct answers and calibrated abstentions while penalizing hallucinations (Section 2.1). It proposes an RL algorithm (TruthRL) that operationalizes this with a simple ternary reward (+1 correct, 0 uncertain, âˆ’1 incorrect) trained via GRPO, a groupâ€‘relative policy optimization scheme (Section 3.2).

## 3. Technical Approach
This section explains all moving parts and why they were chosen.

- Problem formulation (Section 2.1)
  - Define three perâ€‘question outcomes:
    - `Acc` (accuracy): fraction of questions answered correctly.
    - `Unc` (uncertainty rate): fraction answered with abstention (e.g., â€œI donâ€™t knowâ€).
    - `Hall` (hallucination rate): fraction answered incorrectly.
  - Define a `truthfulness score` as a weighted sum: Truthfulness = `w1Â·Acc + w2Â·Unc âˆ’ w3Â·Hall`. Experiments use `w1=1, w2=0, w3=1` (Table 1 description), which evaluates truthfulness as accuracy minus hallucination, while treating abstention neutrally.
- Training objective and algorithm (Section 3.2)
  - The model is optimized with GRPO. In plain language, GRPO samples a small â€œgroupâ€ of responses per prompt from the current policy, evaluates each with a reward, and updates the policy to increase the probability of responses that score above the groupâ€™s mean (with PPOâ€‘style clipping and a KL penalty to prevent drift).
  - Notation (for intuition rather than derivations):
    - For each prompt `x`, sample `G` responses `{y_i}`. Compute rewards `{r_i}`.
    - Each responseâ€™s advantage `Ã‚_i` scales how much to push up or down its probability, defined as zâ€‘score within the group: `(r_i âˆ’ mean(r))/std(r)`. This â€œrelative withinâ€‘groupâ€ comparison is the key mechanism that lets small reward differences matter.
  - Reward design (Section 3.2)
    - `Binary reward`: +1 if correct, âˆ’1 otherwise. This conflates abstentions with wrong answers (both âˆ’1). In GRPOâ€™s groupâ€‘relative update, abstentions get no advantage over hallucinations.
    - `Ternary reward` (TruthRL): +1 correct, 0 uncertain, âˆ’1 incorrect. Now, if a group contains an abstention and an incorrect answer, the abstentionâ€™s 0 is above the âˆ’1 average, so it gets positive advantage while the hallucination gets negative advantage.
    - Concrete example (Section 3.2): in a group with an abstention (`r=0`) and a hallucination (`r=âˆ’1`), the abstention outranks the hallucination under the ternary scheme, but not under the binary scheme.
- Why ternary over alternatives
  - It incentivizes two desired behaviors simultaneously: (1) prefer correct answers over anything, and (2) when not confident, prefer abstention over guessing.
- Baselines that can express uncertainty (Section 3.1)
  - `Knowledge boundary probing`: sample 256 responses per training question; if none are correct, label it `outâ€‘ofâ€‘knowledge (OOK)` and set the target to â€œI donâ€™t know.â€ This creates training data for abstentionâ€‘aware SFT baselines.
  - `Râ€‘Tuning`: standard SFT where OOK questions are paired with â€œI donâ€™t knowâ€ ground truth (Section 3.1).
  - `RFT` (rejection sampling fineâ€‘tuning): choose the modelâ€‘generated trace that ends in â€œI donâ€™t knowâ€ for OOK, and the correct trace for nonâ€‘OOK (Section 3.1).
- Optional reward variants (Section 3.2; Section 4.4; Section 4.6)
  - `Knowledgeâ€‘enhanced`: reward abstention positively (+1) when a question is OOK (identified by knowledgeâ€‘boundary probing) and penalize nonâ€‘abstention on OOK items.
  - `Reasoningâ€‘enhanced`: add a separate reward for reasoning quality (judged by an LLM; Section 4.6), combined multiplicatively/additively/conditionally with the outcome reward.
- Verifier (Section 4.5; Table 5)
  - A large LLM judge (e.g., Llama3.3â€‘70Bâ€‘Instruct) evaluates answer correctness for rewards. Replacing it with a ruleâ€‘based string match collapses learning into overâ€‘abstention and negative truthfulness (Table 5), showing the importance of semantic judging.

## 4. Key Insights and Innovations
- Ternary outcome reward that separates abstention from error (fundamental)
  - Novelty: Previous RL setups typically use binary correct/incorrect rewards, implicitly treating abstentions as errors. The ternary reward gives abstention zero (or positive in knowledgeâ€‘enhanced mode) rather than âˆ’1, making â€œI donâ€™t knowâ€ preferable to guessing (Section 3.2).
  - Significance: It directly optimizes truthfulness rather than raw accuracy, leading to large hallucination reductions across settings (Table 1).
- Groupâ€‘relative optimization amplifies the abstention signal (mechanistic innovation within GRPO)
  - Because advantages are computed within sampled groups, any abstentionâ€™s 0 reward can become relatively â€œbetterâ€ than a âˆ’1 hallucination in the same group (Section 3.2). This small design detail explains why the model learns to abstain when uncertain without overâ€‘penalizing coverage.
- Simple reward outperforms more elaborate knowledgeâ€‘ or reasoningâ€‘augmented schemes (empirical insight)
  - `Table 3` shows the ternary reward yields the best average truthfulness and lowest hallucinations across benchmarks, outperforming binary reward and knowledgeâ€‘enhanced variants. `Table 8` shows that adding reasoning rewards does not improve outcome truthfulness and can trade off accuracy vs. reasoning score.
- Online RL beats offline/semiâ€‘online preference optimization for this objective (empirical insight)
  - `Table 4` compares offline `DPO`, semiâ€‘online iterative DPO, and online TruthRL. Iterative DPO improves through early iterations but regresses later, while online TruthRL achieves the best truthfulness and lowest hallucination consistently.

## 5. Experimental Analysis
- Evaluation setup (Section 4.1; Appendix Aâ€“B)
  - Datasets: CRAG (Comprehensive RAG benchmark), NaturalQuestions (NQ), HotpotQA, MuSiQue.
  - Training: models trained on CRAG; evaluated on all four datasets.
  - Retrieval vs. nonâ€‘retrieval: both evaluated. Retrieval provides up to 50 documents (CRAG) or Wikipedia (others) with E5 retriever (Appendix A).
  - Backbones: `Llama3.1â€‘8Bâ€‘Instruct`, `Qwen2.5â€‘7Bâ€‘Instruct`, plus scale studies from 3B to 32B (Table 7).
  - Metrics: Truthfulness score (Acc âˆ’ Hall with `w1=1, w2=0, w3=1`), hallucination rate, accuracy; uncertainty rate appears in breakdowns/figures.
  - Verifier for correctness: default LLM judge `Llama3.3â€‘70Bâ€‘Instruct`, with robustness checks across other judges (Table 6).
  - Baselines: Prompting, SFT, RFT, Râ€‘Tuning; TruthRL with binary reward (â€œTruthRLBinaryâ€) equates to â€œvanilla RLâ€; offline/semiâ€‘online DPO variants (Table 4).
- Before training: evidence that SFT/RL suppress abstention (Section 2.3; Figure 2)
  - Figure 2 shows that the base modelâ€™s majority@k improves accuracy and abstention while reducing hallucination as more samples are aggregated. After SFT or vanilla RL, uncertainty collapses to near zero and hallucinations increase at larger k, revealing accuracyâ€‘only trainingâ€™s antiâ€‘abstention bias.
- Main results (Table 1)
  - With retrieval, `Llama3.1â€‘8B` on CRAG:
    - Prompting: T=5.3, H=43.5, A=48.8.
    - SFT: T=1.4, H=49.3, A=50.7.
    - Râ€‘Tuning: T=15.2, H=33.1, A=48.4.
    - TruthRLBinary: T=20.8, H=39.5, A=60.3.
    - TruthRL: T=37.2, H=19.4, A=56.6.
    - Quote:
      > Table 1 (CRAG, with retrieval, Llama3.1â€‘8Bâ€‘Inst): TruthRL achieves Truthfulness 37.2 with Hallucination 19.4 and Accuracy 56.6, outperforming all baselines.
  - With retrieval, `Qwen2.5â€‘7B` on CRAG:
    - Prompting: T=10.6, H=38.4, A=49.0.
    - RFT: T=22.6, H=31.4, A=54.0.
    - TruthRL: T=33.1, H=17.3, A=50.4.
    - Quote:
      > Table 1: TruthRL reduces hallucinations from 31.4 (RFT) to 17.3 and increases truthfulness from 22.6 to 33.1 on CRAG with retrieval.
  - Without retrieval (harder): `Llama3.1â€‘8B`
    - Prompting: T=âˆ’4.4, H=44.5, A=40.1.
    - SFT: T=âˆ’42.1, H=71.1, A=28.9 (hallucination explodes).
    - TruthRLBinary: T=âˆ’14.5, H=57.2, A=42.8.
    - TruthRL: T=22.4, H=16.3, A=38.7.
    - Quote:
      > Table 1 (no retrieval): TruthRL still improves truthfulness to 22.4 while cutting hallucinations to 16.3, whereas SFT raises hallucinations to 71.1.
  - Crossâ€‘dataset averages (Table 1):
    - For `Llama3.1â€‘8B`, with retrieval, average T=25.6 and H=18.8 for TruthRL; prompting yields T=âˆ’16.4, H=54.1; TruthRLBinary yields T=4.5, H=47.7.
- Behavior decomposition and hardâ€‘question analysis (Figure 3)
  - On all CRAG questions, TruthRL has the lowest hallucination and the highest uncertainty among methods while keeping strong accuracy (Figure 3a).
  - On a difficult subset where almost no method is correct, hallucinations for SFT and TruthRLBinary approach 100%, while TruthRL hallucinates only 15.5% and abstains 84.5% (Figure 3b).
  - Quote:
    > Figure 3b: On hard items, TruthRL: H=15.5%, Unc=84.5%, vs. SFT/TruthRLBinary: nearâ€‘universal hallucination.
- Hallucinationâ€‘baiting questions (Table 2)
  - Multipleâ€‘choice style comparisons are known to induce guessing. TruthRL attains T=52.4 with H=16.5 and highest abstention among methods tested, while others have Hâ‰ˆ39â€“49.
  - Quote:
    > Table 2: TruthRLâ€™s hallucination rate is 16.5% on baiting questions, substantially lower than SFT (48.5%) or Râ€‘Tuning (43.7%).
- Reward ablations (Table 3; Figure 4)
  - Binary reward excels at accuracy but keeps high hallucinations (e.g., CRAG with retrieval: T=20.8, H=39.5).
  - Ternary reward achieves CRAG T=37.2 with H=19.4â€”the best truthfulness/lowest hallucination.
  - Knowledgeâ€‘enhanced variants help abstention but tend to reduce accuracy or underperform ternary overall.
  - Learning curves (Figure 4) show ternary steadily reduces hallucination and maintains uncertainty; binary drives uncertainty to ~0.
- Online vs. offline RL (Table 4)
  - DPO: low truthfulness (average T=âˆ’10.1, H=51.1 across datasets).
  - Iterative DPO improves up to Iter 3 (avg T=12.6, H=31.7) but regresses at Iter 4.
  - TruthRL (online) achieves avg T=25.6 with H=18.8, the best across all regimes.
- Confidence calibration (Figure 5)
  - TruthRL increases the fraction of highâ€‘confidence correct answers and reduces overconfident hallucinations compared to prompting.
- Verifier quality (Table 5)
  - Ruleâ€‘based judge leads to overâ€‘abstention and negative truthfulness (T=âˆ’3.6), while LLM judge enables usable reward signals (T=37.2).
- Judge robustness (Table 6)
  - Using three different highâ€‘capacity judges, TruthRL consistently gives the lowest hallucination and highest truthfulness, indicating it does not â€œoverfitâ€ a specific judge.
- Scale trends (Table 7)
  - Gains are consistent from 3B to 32B. Improvements are relatively larger for smaller models (e.g., Llama3.2â€‘3B: Prompting T=1.9/H=45.1 â†’ TruthRL T=27.4/H=21.5).
- Reasoning rewards (Table 8)
  - Outcomeâ€‘only TruthRL already lifts a separate reasoningâ€‘quality score (50.2 â†’ 56.6). Adding reasoning reward via simple heuristics fails to improve outcome truthfulness and can trade off metrics (e.g., additive increases reasoning to 59.1 but slightly lowers truthfulness to 36.1).

Do the experiments support the claims?
- Yes, because:
  - Hallucination reductions and truthfulness improvements are shown across datasets, model families, scales, and under multiple evaluators (Tables 1, 6, 7).
  - Mechanismâ€‘aligned behavior changes (more abstentions when necessary; fewer overconfident errors) appear in decomposition and confidence analyses (Figures 3 and 5).
  - Ablations isolate the reward structure as causal (Table 3; Figure 4) and show online RL is critical (Table 4).
- Caveats:
  - Truthfulness score sets `w2=0`, so abstention is neutral in evaluation; the benefit of abstentions is argued qualitatively and via reduced hallucination, but not rewarded numerically in the main score (Section 4.1 setup).

## 6. Limitations and Trade-offs
- Dependence on LLMâ€‘asâ€‘aâ€‘judge for rewards and evaluation (Section 4.5; Tables 5â€“6)
  - Assumption: the judge accurately recognizes semantic correctness and abstentions. Judge bias or inconsistency could skew training/evaluation. The paper partially mitigates this by crossâ€‘judge checks (Table 6), but dependence remains.
- Metric choice treats abstention neutrally (Section 4.1)
  - With `w2=0`, the main truthfulness metric does not directly reward abstention; effects appear via reduced hallucination and behavioral analyses. In deployments that value abstention, alternative weights or separate metrics would be needed.
- Training cost and complexity (Appendix A)
  - Online RL with GRPO requires rollouts, a verifier LLM, and long context windows (16kâ€“32k), trained on 8Ã—H100 GPUs with vLLM/DeepSpeed infrastructure. This may be costly relative to SFT or offline DPO.
- OOK detection for knowledgeâ€‘enhanced variants and SFT baselines (Section 3.1)
  - Identifying outâ€‘ofâ€‘knowledge questions via sampling 256 generations is approximate. Mislabeling could misguide knowledgeâ€‘enhanced rewards or Râ€‘Tuning data.
- Scope of benchmarks and retrieval noise
  - Training is on CRAG and tests include three other QA datasets; results are strong but focused on knowledgeâ€‘intensive QA. Other domains (dialogue safety, coding, planning) are not evaluated here.
- Risk of style gaming
  - Because abstention is recognized by naturalâ€‘language phrases (e.g., â€œI donâ€™t knowâ€), models could learn template phrases without perfectly calibrated internal uncertainty. The paperâ€™s behavioral analyses suggest genuine improvements (Figures 3 and 5) but do not fully rule out surfaceâ€‘form gaming.

## 7. Implications and Future Directions
- Fieldâ€‘level shift: from accuracyâ€‘centric to truthfulnessâ€‘centric training
  - The work demonstrates that a minimal change in reward structure can steer models to recognize knowledge boundaries and avoid harmful guesses, a key property for highâ€‘stakes use. This reframes postâ€‘training objectives for reliable LLMs.
- Practical applications
  - Highâ€‘risk domains (clinical decision support, legal research), enterprise assistants, and RAG systems where sources can be noisy. TruthRLâ€™s behaviorâ€”answer when warranted, otherwise abstainâ€”fits workflows that escalate to humans or trigger retrieval/reâ€‘query.
- Followâ€‘up research
  - Reward design:
    - Multiâ€‘objective formulations that explicitly value abstention (`w2>0`) and calibrate penalties by risk.
    - Better reasoningâ€‘aware rewards beyond simple heuristics (Table 8 indicates nontrivial tradeâ€‘offs).
  - Verifier/judge reliability:
    - Ensemble judges or calibrated verifiers to reduce bias.
    - Programmatic semantic matching or weak supervision to complement LLM judges and lower cost (avoiding the collapse seen in Table 5).
  - Broader domains and tasks:
    - Apply ternaryâ€‘reward RL to code generation, tool use, and longâ€‘form synthesis where selective abstention could trigger tools or retrieval.
  - Deployment policies:
    - Combine TruthRL with dynamic RAG: abstention triggers search or human review; correct answers bypass escalation.
  - Theoretical analysis:
    - Formal guarantees about abstention calibration under groupâ€‘relative updates; conditions where ternary rewards minimize expected risk vs. binary rewards.

Overall, TruthRLâ€™s main contribution is conceptual simplicity paired with strong empirical validation: using an outcomeâ€‘based ternary reward under GRPO teaches LLMs to convert many wouldâ€‘be hallucinations into either correct answers (when information is sufficient) or honest abstentions (when it is not), with consistent gains across models, datasets, and judges (Figures 3â€“5; Tables 1â€“7).

# Scaling Pre-training to One Hundred Billion Data for Vision Language Models

**ArXiv:** [2502.07617](https://arxiv.org/abs/2502.07617)

## ğŸ¯ Pitch

This paper empirically explores the impact of scaling vision-language model pre-training to an unprecedented 100 billion imageâ€“text pairs with the new WebLI-100B dataset. While performance on traditional Western-centric benchmarks plateaus at this scale, the study reveals substantial new gains in cultural diversity, support for low-resource languages, and demographic fairnessâ€”making a compelling case that massive, diverse data is crucial for building truly inclusive multimodal AI systems.

---

## 1. Executive Summary (2-3 sentences)
This paper investigates what happens when preâ€‘training data for visionâ€‘language models (VLMs) is scaled by an order of magnitudeâ€”from 10 billion to 100 billion imageâ€“text pairsâ€”using a new dataset, `WebLIâ€‘100B`. The central finding is counterintuitive but important: at this scale, â€œstandardâ€ Westernâ€‘centric benchmarks (e.g., ImageNet zeroâ€‘shot, COCO retrieval) largely saturate, but inclusivityâ€‘related capabilitiesâ€”cultural diversity, multilinguality (especially lowâ€‘resource languages), and performance parity across subgroupsâ€”improve markedly (Sections 1, 4; Figure 1; Tables 2â€“5).

## 2. Context and Motivation
- Problem/gap addressed
  - The largest reported webâ€‘scale imageâ€“text datasets have plateaued around 10 billion pairs. It is unclear whether pushing to 100 billion unique examples yields meaningful benefits, and if so, where those benefits appear (Section 1).
  - Prior work on scaling laws mostly focused on accuracy improvements in established Westernâ€‘centric benchmarks; less is known about inclusivityâ€‘related outcomes such as cultural diversity and lowâ€‘resource multilingual performance (Sections 1â€“2).

- Why it matters
  - Realâ€‘world impact: Inclusive multimodal systems need breadthâ€”coverage of lowâ€‘frequency, longâ€‘tail cultural and linguistic concepts. These are underâ€‘represented on the web and are often pruned by common quality filters (Section 1; Section 5.1).
  - Theoretical significance: Classic powerâ€‘law scaling suggests diminishing returns but continued gains with more data; this paper examines whether the â€œreturnsâ€ shift from headline benchmarks to inclusivity metrics at massive data scale (Section 1; scaling laws fit in Tables 2â€“3).

- Prior approaches and shortcomings
  - Datasets like Conceptual Captions, LAIONâ€‘5B, and WebLI (~10B) enabled strong VLMs (CLIP, ALIGN, SigLIP), typically with quality filtering (often Englishâ€‘centric) to improve benchmark performance (Section 2).
  - These filters can disproportionately remove longâ€‘tail, culturally diverse examples (Section 2; Section 5.1), and many benchmarks reflect Western images/languages, masking inclusivity gaps (Section 2).

- This paperâ€™s position
  - Builds `WebLIâ€‘100B`, a 100â€‘billionâ€‘pair multilingual, minimally filtered web dataset (only safety/PII filters), and studies the effect of scaling on both traditional and inclusivityâ€‘oriented evaluations under computeâ€‘matched training (Sections 3.1â€“3.3).
  - Goes beyond accuracy: also measures multilingual retrieval across 36 languages (Crossmodalâ€‘3600), cultural diversity (Dollar Street, GeoDE, GLDv2), fairness (bias and disparity), and transfer to a generative VLM (`PaliGemma`) (Section 3.3).

## 3. Technical Approach
This is an empirical scaling study with a carefully controlled preâ€‘training and evaluation pipeline.

- Data construction and splits (Section 3.1)
  - `WebLIâ€‘100B`: 100B imageâ€“text pairs scraped from the web, using image altâ€‘text and page titles as text. Only essential filters applied: remove harmful images and PII; nearâ€‘duplicates removed against >90 common evaluation tasks to prevent leakage.
  - `1B` and `10B` subsets: random 1% and 10% samples of the 100B set.
  - Qualityâ€‘filtered sets (Section 5.1): From raw web data, create three 5Bâ€‘pair English datasets:
    - â€œCLIPâ€‘filteredâ€ using `CLIPâ€‘L/14` as a filter (keep high imageâ€“text alignment).
    - â€œClassifierâ€‘filteredâ€ using a custom VLM classifier trained to detect aligned pairs.
    - â€œBaseline (en)â€ as an unfiltered English subset for comparison.
  - Languageâ€‘rebalanced sets (Section 3.1; Section 5.2): Upsample selected lowâ€‘resource languages used in Crossmodalâ€‘3600â€”Bengali, Filipino, Hindi, Hebrew, MÄori, Swahili, Teluguâ€”so that each comprises 1% of training batches (collectively 7%); the remaining 93% sampling follows the original distribution. This isolates the effect of targeted language balancing.

- Models and training (Section 3.2)
  - Contrastive preâ€‘training using `SigLIP` (Sigmoid loss variant of CLIP) with `ViTâ€‘B/16`, `ViTâ€‘L/16`, and `ViTâ€‘H/14` backbones for both image and text encoders.
  - Training details:
    - Batch size 32k; inverse squareâ€‘root learningâ€‘rate schedule with 200M warmup and cooldown examples.
    - LR = 0.001; weight decay = 1eâ€‘4.
    - Images at 224Ã—224 resolution; text tokenized with multilingual `mt5` tokenizer, max 64 tokens.
  - Computeâ€‘matched regime: every model is trained until it has â€œseenâ€ 100B examples totalâ€”e.g., 1Bâ€‘data models see 100 epochs; 10Bâ€‘data models see 10 epochs; 100Bâ€‘data models see 1 epoch (Section 3.2). Checkpoints evaluated after seeing 3, 7, 10, 17, 26, 33, 49, 66, and 100B examples.

- Evaluations (Section 3.3)
  - Westernâ€‘centric tasks:
    - Zeroâ€‘shot classification: ImageNet, CIFARâ€‘100, Oxfordâ€‘IIIT Pet.
    - 10â€‘shot classification (few labeled examples per class; a lightweight classifier is trained on frozen features): Birds (CUB), Caltechâ€‘101, Cars196, Colorectal Histology, DTD.
    - Imageâ€“text retrieval on COCO Captions and Flickr30k in both directions; metric: `Recall@1` (what fraction of queries retrieve the correct item at rank 1).
  - Cultural diversity:
    - Zeroâ€‘shot classification on Dollar Street (mapped to ImageNet labels), GeoDE, GLDv2.
    - 10â€‘shot geolocalization on Dollar Street and GeoDE (predict country/region from image with few labeled examples).
  - Multilinguality:
    - Crossmodalâ€‘3600: zeroâ€‘shot retrieval in 36 languages; report languageâ€‘wise results and aggregates for lowâ€‘ vs highâ€‘resource languages.
  - Fairness (Section 3.3; Section 4.4):
    - Representation bias (RB): how often the model prefers â€œMaleâ€ over â€œFemaleâ€ for random images (firstâ€‘order bias).
    - Association bias (AB): how often gender correlates with occupation words (e.g., â€œnurseâ€ vs â€œdoctorâ€) using FairFace images (secondâ€‘order bias); reported as preference probabilities (Figure 2).
    - Performance disparity: max accuracy gap across socioeconomic (Dollar Street income buckets) and geographic (GeoDE regions) subgroups (Table 5).
  - Transfer to generative models (Section 3.3; Section 4.5):
    - Initialize `PaliGemma`â€™s vision tower with each contrastivelyâ€‘trained encoder; run 50M preâ€‘training examples (stageâ€‘1, 224Ã—224) under two settings: frozen vs unfrozen vision tower; then fineâ€‘tune on a broad suite of captioning, VQA (incl. OCR, counting), multilingual, and remote sensing tasks (Table 6; Appendix C).

- Scalingâ€‘law fitting (Sections 4.1, 4.2; Tables 2â€“3)
  - For each benchmark and model size, fit a power law `f(x) = Î± x^{-c} + Îµ` where `x` is data size and `f(x)` is error. Report the exponent `c` and the asymptotic â€œlimitâ€ (`Îµ`) to assess whether additional compute would change the observed trends.

## 4. Key Insights and Innovations
- A1. At 100B scale, traditional benchmarks saturate, but inclusivity improves
  - Whatâ€™s new: The paper separates â€œWhere do we see gains?â€ into two buckets and shows they diverge at 100B.
  - Evidence:
    - Westernâ€‘centric: â€œScaling from 10B to 100B shows limited benefitsâ€ (Table 2). Example: `ViTâ€‘L` ImageNet zeroâ€‘shot error drops modestly 29.7% â†’ 28.5% (âˆ’1.2 pts); many COCO/Flickr retrieval numbers stagnate or even worsen slightly at `Recall@1` (higher error) for larger backbones.
    - Inclusivity: Cultural and multilingual metrics improve more at 100B (Figure 1 right; Table 3; Figure 3). Example: Dollar Street 10â€‘shot (`ViTâ€‘L`) error 64.1% â†’ 58.3% (âˆ’5.8 pts), i.e., accuracy +5.8 points (Table 3); lowâ€‘resource language retrieval improves more than highâ€‘resource (Figure 3).
  - Why it matters: It reframes the objective of extreme scalingâ€”less about squeezing extra points on COCO/ImageNet, more about reaching the long tail of cultural/linguistic phenomena.

- A2. Common quality filters trade off diversity for benchmark performance
  - Whatâ€™s new: A controlled comparison of CLIPâ€‘style filtering vs no filtering at fixed dataset size (5B) shows clear tradeâ€‘offs (Section 5.1; Figure 4; Appendix D).
  - Evidence:
    - Westernâ€‘centric averages improve with CLIP filter (e.g., at 30B seen examples, â€œAverage Westernâ€‘centricâ€ error: 23.21% baseline vs 22.14% CLIP; Appendix D).
    - Cultural diversity degrades (e.g., at 30B seen examples, â€œAverage Cultural Diversityâ€ error: 49.49% baseline vs 54.96% CLIP; Appendix D).
  - Why significant: Many pipelines default to CLIPâ€‘filtering; the study warns this can erase rare cultural contexts even when starting from 100B raw examples.

- A3. Language rebalancing helps lowâ€‘resource languages with minimal collateral damage
  - Whatâ€™s new: Simple upsampling of seven lowâ€‘resource languages to 1% each substantially improves their zeroâ€‘shot retrieval while only slightly affecting highâ€‘resource languages (Section 5.2; Figure 5; Table 11).
  - Evidence (for `ViTâ€‘L`, 100B seen examples):
    - Lowâ€‘resource average error: 75.01% â†’ 70.10% (âˆ’4.91 pts).
    - Highâ€‘resource average error: 45.43% â†’ 45.75% (+0.32 pts).
    - Cultural diversity average improves slightly (44.01% â†’ 43.29%), while Westernâ€‘centric average degrades slightly (26.87% â†’ 27.55%) (Table 11).
  - Why significant: It shows a lightweight, dataâ€‘mixing knob that directly targets inclusivity without requiring more total data.

- A4. Data scale reduces performance disparity across groups, but not intrinsic gender biases
  - Evidence (Table 5): With 100B seen examples, regional disparity on GeoDE shrinks (e.g., `ViTâ€‘L`: 3.2 â†’ 2.8), and Dollar Street income disparity is stable or slightly improved for some backbones.
  - However, representation bias remains high (Table 4): models prefer â€œMaleâ€ over â€œFemaleâ€ ~85% of the time, and association bias heatmaps (Figure 2) show persistent genderâ€“occupation stereotypes; scaling alone does not fix these.

## 5. Experimental Analysis
- Evaluation design recap (Section 3.3)
  - Broad coverage: Zeroâ€‘shot and fewâ€‘shot classification, bidirectional retrieval, multilingual retrieval, cultural geolocalization, fairness (bias and subgroup disparity), and transfer to generative tasks.
  - Computeâ€‘matched training ensures observed differences are attributable to data scale and mix, not more optimizer steps or larger batches.

- Main quantitative results and comparisons
  - Westernâ€‘centric saturation (Table 2; Section 4.1)
    > â€œIncreasing the dataset size from 10B to 100Bâ€¦ does not improve performance substantially,â€ supported by Wilcoxonâ€™s test with p = 0.9 and scalingâ€‘law limits that are statistically indistinguishable (p = 0.09).
    - Examples (error â†“ is better):
      - ImageNet zeroâ€‘shot: `ViTâ€‘B`: 39.35 â†’ 39.04 (âˆ’0.31); `ViTâ€‘L`: 29.70 â†’ 28.49 (âˆ’1.21); `ViTâ€‘H`: 25.60 â†’ 24.90 (âˆ’0.70).
      - COCO I2T Recall@1 error (lower is better): `ViTâ€‘L`: 47.18 â†’ 45.28 (small gain), but Flickr I2T: 15.50 â†’ 16.60 (worse).
    - Scaling exponents vary (âˆ’0.1 to âˆ’1.3 across rows) with nearâ€‘identical asymptotic limits for 10B vs 100B, reinforcing saturation.

  - Cultural diversity gains (Table 3; Section 4.2)
    > â€œScaling â€¦ yields substantial gains on Dollar Street 10â€‘shotâ€¦ `ViTâ€‘L` and `ViTâ€‘H` see absolute improvements of 5.8% and 5.4%,â€ with p = 0.002 (Wilcoxon).
    - Dollar Street 10â€‘shot error: `ViTâ€‘L`: 64.1 â†’ 58.3; `ViTâ€‘H`: 59.1 â†’ 53.7.
    - GeoDE 10â€‘shot region error: `ViTâ€‘H`: 47.6 â†’ 44.7 (âˆ’2.9); country: 50.2 â†’ 47.6 (âˆ’2.6).
    - GLDv2 zeroâ€‘shot error improves substantially with larger models at 100B vs 10B (e.g., `ViTâ€‘H`: 40.1 â†’ 38.8).

  - Multilinguality (Figure 3; Appendix B; Section 4.3)
    > â€œLowâ€‘resource languages benefit more from the 100B scale than the highâ€‘resource ones,â€ and the gap widens with model size.
    - Examples (`ViTâ€‘L`, Imageâ€‘toâ€‘Text error, lower is better): Telugu 76.67 â†’ 69.69 (âˆ’6.98), Bengali 66.36 â†’ 63.75 (âˆ’2.61), Hebrew 39.44 â†’ 35.72 (âˆ’3.72).
    - Highâ€‘resource languages see smaller changes; sometimes flat or mixed across directions (Appendix B, Table 8).

  - Fairness (Section 4.4; Tables 4â€“5; Figure 2)
    - Representation bias: 
      > â€œModelsâ€¦ have a significantly higher preference to associateâ€¦ â€˜Maleâ€™ over â€˜Female.â€™ In fact, this occurs nearly 85% of the time. Training on 100B examples does not mitigate this effect.â€  
      Table 4 shows: `ViTâ€‘L` RB 88.2% (1B) â†’ 85.5% (100B); `ViTâ€‘H` 86.8% â†’ 86.6%.
    - Association bias: heatmaps (Figure 2) display persistent genderâ€“occupation associations across all scales; no systematic reduction from 10B â†’ 100B.
    - Disparity (max subgroup accuracy gap): improves overall with scale on GeoDE (e.g., `ViTâ€‘L` 3.2 â†’ 2.8; Table 5), and is stable/slightly improved on Dollar Street for some backbones (e.g., `ViTâ€‘B` 32.5 â†’ 29.0).

  - Transfer to generative models (`PaliGemma`; Section 4.5; Table 6)
    > â€œWe do not observe consistent performance gains across downstream tasks as we scale the preâ€‘training dataset.â€
    - Aggregated (unfrozen vision): Semantics 77.1 â†’ 77.2; OCR 69.5 â†’ 70.0; Multilinguality 66.9 â†’ 67.0; Remote Sensing 92.0 â†’ 91.8. Similar small shifts with frozen vision. Gains are within noise.

  - Qualitative attention maps (Section 5.3; Table 1; Appendix A)
    > â€œModels trained on larger data tend to have more focused attention on semantically relevant regions,â€ e.g., sharper focus on igloo dome structure and bison rather than background.

  - Quality filtering ablation (Section 5.1; Figure 4; Appendix D)
    - Westernâ€‘centric averages: improve under CLIP filtering (e.g., at 30B seen, 23.21% â†’ 22.14% error).
    - Cultural diversity: worsens under CLIP filtering (e.g., at 30B seen, 49.49% â†’ 54.96% error).
    - Fairness averages: marginal changes; sometimes worse under filtering (Appendix D).

  - Language rebalancing ablation (Section 5.2; Figure 5; Table 11)
    - Lowâ€‘resource average error drops by ~5 points at 100B seen; highâ€‘resource slightly up (~0.3 points); cultural metrics slightly better; Western tasks slightly worse.

- Convincingness and robustness
  - Strong: computeâ€‘matched training, multiple backbones (B/L/H), multiple â€œseenâ€‘examplesâ€ checkpoints, significance tests (Wilcoxon), and scalingâ€‘law fits all support the central claim that inclusivity gains dominate at 100B while traditional benchmarks saturate (Sections 4.1â€“4.3).
  - Balanced: transfer experiments and bias metrics show where scaling alone is insufficient (Sections 4.4â€“4.5).
  - Coverage: extensive appendices detail perâ€‘language results and filter/rebalance ablations (Appendices Bâ€“F).

- Conditions and tradeâ€‘offs
  - Improvements concentrate in diverse/longâ€‘tail tasks; small or negative changes on familiar benchmarks and for highâ€‘resource languages (Tables 2â€“3; Figure 3).
  - Filtering trades Westernâ€‘centric gains for diversity losses (Figure 4).
  - Rebalancing trades highâ€‘resource performance slightly for lowâ€‘resource improvement (Figure 5; Table 11).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Raw web data is assumed beneficial for longâ€‘tail coverage; only essential safety and PII filters were applied (Section 3.1). This keeps diversity but retains noise/misalignment that may depress classic benchmarks.
  - Evaluation of â€œinclusivityâ€ is via specific proxies (Dollar Street, GeoDE, GLDv2, Crossmodalâ€‘3600). Inclusivity is broader than these metrics (Section 6, Limitations).

- What is not addressed
  - Public release of `WebLIâ€‘100B` is not indicated; reproducibility at this scale is challenging.
  - Bias mitigation beyond data scale/mix (e.g., debiasing objectives, counterfactual augmentation) is not explored; gender biases persist (Section 4.4).
  - Only 224Ã—224 resolution and SigLIP are studied; other architectures/losses or higher resolutions might interact differently with 100B scale.

- Computational and data constraints
  - Training to 100B â€œseen examplesâ€ is extremely computeâ€‘intensive; while computeâ€‘matched comparisons are fair, many labs cannot reproduce them.
  - Data labeling is weak (altâ€‘text/page titles), which is noisy; filters that help benchmarks tend to erase longâ€‘tail data (Section 5.1).

- Open questions
  - Can we design â€œdiversityâ€‘preservingâ€ filters that retain longâ€‘tail cultural/linguistic content while improving alignment?
  - How would multiâ€‘stage curricula (e.g., start diverse, then refine with filtered data) affect both inclusivity and benchmark performance?
  - What architectural or objective choices (e.g., generative preâ€‘training at scale) better exploit 100B raw data for both inclusivity and standard benchmarks?

## 7. Implications and Future Directions
- How this changes the landscape
  - Reorients the rationale for extreme data scaling: at 100B scale, the most salient returns are inclusivityâ€‘centricâ€”cultural diversity, lowâ€‘resource multilinguality, and reduced subgroup disparityâ€”rather than headline benchmark gains (Sections 4, 7).
  - Calls into question heavy reliance on Westernâ€‘centric leaderboards to judge progress at massive scale.

- Followâ€‘up research enabled/suggested
  - Diversityâ€‘preserving data curation:
    - New filter objectives that explicitly protect longâ€‘tail cultural/linguistic content (Section 6 Discussion).
    - Languageâ€‘aware or regionâ€‘aware sampling schedules beyond uniform upsampling.
  - Objective and architecture innovations:
    - Combine contrastive and generative objectives at 100B scale; investigate whether generative preâ€‘training can translate inclusivity gains into broader task improvements (Section 4.5).
    - Study resolution scaling and tokenization choices for multilingual altâ€‘text at scale.
  - Better benchmarks:
    - Develop broader, more representative test suites for culture, language, and fairness, since current gains may be underestimated by Westernâ€‘centric metrics (Section 6, Limitations).
  - Bias mitigation beyond data volume:
    - Integrate debiasing techniques (balanced batches, adversarial losses, counterfactual prompts) since gender representation/association biases persisted even at 100B (Section 4.4).

- Practical applications
  - Building inclusive multimodal assistants and retrieval systems for global users; better recognition/geolocalization of nonâ€‘Western concepts and landmarks; improved zeroâ€‘shot performance in lowâ€‘resource languages (Sections 4.2â€“4.3).
  - Caution for industry pipelines: CLIPâ€‘style filtering improves nearâ€‘term benchmarks but can harm inclusivity (Section 5.1). For global products, prefer larger, lessâ€‘filtered corpora and languageâ€‘balanced sampling.

> Overall conclusion (Section 7): scaling noisy, raw web data to 100B examples is â€œvital for building truly inclusive multimodal systems,â€ even if it delivers only â€œtiny improvementsâ€ on established Westernâ€‘centric benchmarks.

# EXAONE 3.5: Series of Large Language Models for Realâ€‘world Use Cases

**ArXiv:** [2412.04862](https://arxiv.org/abs/2412.04862)
**Authors:** LG AI Research, Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun
**Institutions:** LG AI Research

## ðŸŽ¯ Pitch

EXAONE 3.5 presents a groundbreaking suite of instruction-tuned language models capable of bilingual, long-context reasoning with exceptional efficiency, achieving top results on real-world benchmarks while utilizing significantly less computational power. By introducing novel Korean RAG benchmarks, EXAONE 3.5 not only enhances language model capabilities but catalyzes further research in bilingual retrieval and application, marking a crucial step in deploying powerful AI on resource-constrained devices.

---

## 1. Executive Summary
EXAONE 3.5 is a family of instruction-tuned large language models (LLMs) in three sizes (`32B`, `7.8B`, `2.4B`) that target strong realâ€‘world instruction following and longâ€‘context reasoning up to 32K tokens. The work matters because it delivers bilingual (Korean/English) models that achieve top performance on several realâ€‘world and longâ€‘context benchmarks while using notably less preâ€‘training compute than comparable open models (Table 3), and it contributes new Korean longâ€‘context RAG benchmarks.

## 2. Context and Motivation
- Problem and gap
  - Many users need LLMs that 1) follow instructions robustly in everyday tasks, 2) reason with long inputs typical in RAG pipelines, and 3) run at a range of cost and device constraints. Prior EXAONE 3.0 (7.8B) delivered good bilingual capability, but context length was limited (4K), and there was clear demand for both larger/faster models and much smaller onâ€‘device models (Section 1).
- Importance
  - Realâ€‘world usage increasingly centers on RAG and long documents (contracts, reports, web search results). If the model cannot reliably retrieve and reason over 10Kâ€“30K tokens, downstream applications suffer. Cost and deployability (e.g., small models) are also decisive for academic and industrial adoption.
- Prior approaches and gaps
  - Recent open models (e.g., Qwen 2.5, Gemma 2, Llama 3.x) reach high general benchmarks but often trade off cost or bilingual strength, and many do not support 32K context across sizes. Longâ€‘context training often risks catastrophic forgetting of prior capabilities or relies on much higher preâ€‘training budgets.
- Positioning
  - EXAONE 3.5 positions itself as a costâ€‘efficient, bilingual, longâ€‘context instructionâ€‘tuned family (2.4B/7.8B/32B) with:
    - 32K token context across all sizes (Table 1).
    - A twoâ€‘stage preâ€‘training process with replay to prevent forgetting (Section 2.2.1).
    - Careful testâ€‘set decontamination (Section 2.2.2; Figure 4; Table 10).
    - Supervised fineâ€‘tuning and staged preference optimization for alignment (Sections 2.3.1â€“2.3.2; Figures 1â€“2).
    - New longâ€‘context Korean RAG benchmarks (Section 3.4; Appendix D.2).

## 3. Technical Approach
This is an empirical systems paper: it defines three model configurations, a preâ€‘ and postâ€‘training pipeline, and an evaluation suite.

- Architecture (Table 1)
  - All models are decoderâ€‘only Transformers with:
    - `pre-normalization` residual blocks and `SwiGLU` nonâ€‘linearity (a gated activation that improves optimization stability).
    - `GQA` (Grouped Query Attention): queries are grouped to share key/value projections, reducing memory/computation while retaining performance.
    - Rotary position embeddings with large `RoPE theta = 1,000,000`, which keeps positional encodings wellâ€‘behaved for long contexts.
    - `BBPE` tokenizer (byteâ€‘level BPE) with a 102,400 vocabulary designed to split coverage roughly 50% Korean / 50% English.
    - 32K maximum sequence length for all sizes.
    - Sizeâ€‘specific choices (Table 1), e.g., `32B` uses 64 layers, model width 5,120, 40 attention heads with 8 K/V heads; `2.4B` ties input/output embeddings to save parameters (â€œTied word embedding: Trueâ€).

- Twoâ€‘stage preâ€‘training (Section 2.2; Table 2)
  - Stage 1 trains on a large multiâ€‘domain webâ€‘scale corpus (9T tokens for `7.8B`, 6.5T for `2.4B` and `32B`).
  - Stage 2 â€œlongâ€‘context fineâ€‘tuningâ€ (Section 2.2.1) extends the effective context to 32K using the positional interpolation method [7]. To avoid catastrophic forgetting, a replay strategy reuses part of Stageâ€‘1 data (Section 2.2.1). A key procedural change: in Stage 2, long documents are trained in their intact form rather than chunked, directly exercising longâ€‘range dependencies.

- Decontamination (Section 2.2.2; Figure 4; Table 10)
  - Goal: remove any training examples that leak benchmark test content. The pipeline:
    1) Normalize test items (keep alphanumeric only).
    2) Build a substring pool from all unique 50â€‘character sliding windows.
    3) For a candidate training example, sample N=10 substrings and flag as contaminated if any match occurs.
  - Table 10 shows real overlap examples for MMLU and KMMLU that were removed.

- Compute efficiency (Table 3)
  - Compute is approximated by â€œmodel size Ã— training tokens.â€ EXAONE `32B` (6.5T tokens) requires 1.0Ã— relative compute; Qwen 2.5 `32B` at 18T requires 2.77Ã—; Gemma 2 `27B` at 13T requires 1.69Ã—. The argument is that EXAONE 3.5 matches or beats longâ€‘context/realâ€‘world performance using lower training budgets.

- Postâ€‘training alignment (Section 2.3; Figures 1â€“2)
  - Supervised fineâ€‘tuning (SFT) (Section 2.3.1; Figure 1):
    - Build a â€œknowledge taxonomyâ€ from 8M web pages (e.g., â€œMath â†’ Algebra â†’ Arithmetic sequence,â€ â€œArts â†’ Music â†’ Jazzâ€) and generate instructions grounded in those topics.
    - Use an â€œinstruction evolutionâ€ step (a method inspired by [58]) to systematically increase difficulty and variety, producing diverse instructionâ€‘response pairs.
  - Preference optimization (Section 2.3.2; Figure 2):
    - Use direct alignment algorithms (DAAs) such as `DPO` and `SimPO`. Build preference data by sampling N responses from multiple models for each prompt `x`, then ranking them with a reward model to choose best `y_w` and worst `y_l`.
    - Validate preference pairs with a second reward model and keep only pairs where both reward models agree above a threshold. 
    - Train in stages `M0 â†’ M1 â†’ M2` to mitigate overâ€‘optimization (a known risk where models overfit the reward model; [38]).

- Longâ€‘context focus in evaluation and training
  - Needleâ€‘inâ€‘aâ€‘Haystack (NIAH; Section 3.4.1; Figure 3) is used up to 32K tokens in English and Korean. The â€œneedleâ€ is a specific sentence inserted at random depth; the task is to retrieve it verbatim. EXAONE shows nearâ€‘perfect accuracy at all depths/lengths.
  - RAGâ€‘oriented benchmarks: `LongBench`, extended `LongRAG` with explicit unanswerable cases (Appendix D.2.3), and new Korean datasets `Koâ€‘LongRAG` and `Koâ€‘WebRAG` (Appendix D.2.4â€“D.2.5). For `LongRAG`, the prompts explicitly instruct models to answer â€œUnanswerableâ€ when evidence is missing.

- Evaluation protocol details (Section 3.1; Appendix D)
  - Mix of automatic metrics (exact match, accuracy, F1, ROUGE) and `LLMâ€‘asâ€‘aâ€‘judge` (GPTâ€‘4oâ€‘2024â€‘08â€‘06 or GPTâ€‘4â€‘1106) for openâ€‘ended outputs (Tables 4, 6â€“8; Appendix D.2â€“D.3).
  - For generalâ€‘domain tasks, use zeroâ€‘shot prompts and, where specified, zeroâ€‘shot chainâ€‘ofâ€‘thought (`CoT`) prompts with answer parsing (Appendix D.3). Greedy decoding, max generation length 2,048.

Definitions of less common terms:
- `GQA` (Grouped Query Attention): an attention variant where multiple query heads share a smaller set of key/value heads, reducing memory and latency.
- `RoPE theta`: a scaling factor in rotary position embeddings; larger values help preserve positional distinctions at long sequence lengths.
- `DAA`/`DPO`/`SimPO`: families of preference optimization that train the model to prefer humanâ€‘preferred responses without explicit reinforcement learning.
- `LLMâ€‘asâ€‘aâ€‘judge`: using a strong model (e.g., GPTâ€‘4o) to grade another modelâ€™s outputs when no exact ground truth exists.
- `NIAH`: synthetic longâ€‘context test where a â€œneedleâ€ sentence must be found in a very long â€œhaystack.â€

## 4. Key Insights and Innovations
- Costâ€‘efficient longâ€‘context training with replay
  - Whatâ€™s new: A twoâ€‘stage preâ€‘training scheme that switches from chunked data (Stage 1) to full, intact documents (Stage 2) with a replay buffer to prevent catastrophic forgetting (Section 2.2.1). 
  - Why it matters: Enables all model sizes to support 32K tokens while maintaining prior capabilities. Evidence: nearâ€‘perfect NIAH retrieval across lengths and depths in both languages (Figure 3).

- Strong realâ€‘world instruction following under tight compute
  - Whatâ€™s new: A focus on â€œrealâ€‘worldâ€ instruction datasets and evaluation, plus staged preference optimization with double rewardâ€‘model agreement to curate highâ€‘quality preference pairs (Section 2.3.2; Figure 2).
  - Why it matters: On MTâ€‘Bench, Arenaâ€‘Hard, AlpacaEval, IFEval, etc., EXAONE outperforms similarâ€‘size open baselines (Table 6). This supports the claim that alignment and data design choices, not just scale, drive practical gains.

- Bilingual longâ€‘context RAG evaluation resources
  - Whatâ€™s new: Extension of `LongRAG` with unanswerables and creation of `Koâ€‘LongRAG` and `Koâ€‘WebRAG` (Section 3.4.2; Appendix D.2.3â€“D.2.5).
  - Why it matters: These datasets stress both retrieval and generation under long contexts, including Korean web search scenarios. EXAONE leads on these tasks (Table 7), demonstrating a key realâ€‘world capability.

- Small model that punches above its weight
  - Whatâ€™s new: A `2.4B` model trained for 32K context with strong instruction following and RAG performance (Tables 5â€“7).
  - Why it matters: It tops or matches larger models (â‰¤9B) in realâ€‘world and longâ€‘context averages and is competitive in general benchmarks (Table 5), enabling onâ€‘device or resourceâ€‘constrained deployments.

Incremental vs. fundamental:
- Incremental: Architecture choices (SwiGLU, GQA, RoPE) are established techniques. 
- More fundamental for this work: The training recipe to reliably scale longâ€‘context across sizes with replay; the alignment pipeline with staged DAAs and dual rewardâ€‘model filtering; and the creation of new Korean longâ€‘context RAG evaluations.

## 5. Experimental Analysis
- Evaluation setup (Section 3; Table 4; Appendix D)
  - Categories:
    - Realâ€‘world instruction following: MTâ€‘Bench, LiveBench (2024â€‘08â€‘31), Arenaâ€‘Hardâ€‘v0.1, AlpacaEval 2.0 LC, IFEval, KoMTâ€‘Bench, LogicKor; metrics include LLMâ€‘asâ€‘judge win rates/scores and instruction strict accuracy.
    - Long context: NIAH (EN/KR), LongBench, extended LongRAG, Koâ€‘LongRAG, Koâ€‘WebRAG; metrics include F1/ROUGE, LLMâ€‘asâ€‘judge scores, and accuracy.
    - General domain: GSM8K (CoT), MATH (CoT), HumanEval, MBPP, GPQA (CoT), ARCâ€‘C, BBH (CoT), MMLU (CoT), KMMLU (CoT); zeroâ€‘shot prompts with standardized parsing (Appendix D.3).
  - Baselines: recent open models across sizes (Appendix D.1), including Qwen 2.5, Llama 3.1/3.2, Gemma 2, Phiâ€‘3, Yi 1.5.

- Headline results (Tables 5â€“8; Figure 3)
  - Realâ€‘world use cases (macro average; Table 5 and Table 6)
    - > â€œ`EXAONE 3.5 32B` average 74.3 vs `Qwen 2.5 32B` 69.8â€ (Table 5).
    - Perâ€‘benchmark (32B; Table 6): â€œArenaâ€‘Hard 78.6 vs 67.0; AlpacaEval 60.6 vs 41.0; IFEval 81.7 vs 78.7.â€ MTâ€‘Bench is tied (8.51 vs 8.49). LiveBench is lower (43.0 vs 50.6).
    - `7.8B` and `2.4B` models also lead their peer groups on the macro average (Table 6).
  - Longâ€‘context (macro average; Table 7)
    - > â€œ`EXAONE 3.5 32B` average 71.1 vs `Qwen 2.5 32B` 66.9,â€ with strong Korean RAG scores: Koâ€‘LongRAG 85.3 and Koâ€‘WebRAG 82.3.
    - `7.8B` average 66.6 vs Qwen 2.5 7B at 56.1; `2.4B` average 63.4 vs Qwen 2.5 3B at 40.7.
    - NIAH: nearâ€‘perfect retrieval across depths and lengths in EN/KR (Figure 3).
  - General domain (macro average; Table 8)
    - Mixed: `Qwen 2.5 32B` leads (78.7) over `EXAONE 32B` (74.8), especially on MATH (+6.0), MBPP (+7.1), BBH (+7.4).
    - `EXAONE 2.4B` leads its size class with 63.3 vs Qwen 2.5 3B at 62.1 and Llama 3.2 3B at 54.9.
  - Compute vs. performance (Table 3)
    - > â€œQwen 2.5 32B needs 2.77Ã— the compute of EXAONE 3.5 32B,â€ yet EXAONE matches or surpasses it on longâ€‘context and realâ€‘world averages (Tables 5, 7).

- Safety/harmlessness (Table 9)
  - On a 10,000â€‘item Korean trustworthiness benchmark: 
    - > â€œOverall accuracy: 87.1% (32B), 85.6% (7.8B), 72.2% (2.4B).â€ 
    - High in â€œHateâ€ and â€œIllegalâ€ subcategories for larger models; smaller model lags.

- Do the experiments support the claims?
  - The longâ€‘context and realâ€‘world instructionâ€‘following advantages are well supported by:
    - Nearâ€‘perfect NIAH (Figure 3).
    - Dominance on Koâ€‘LongRAG/Koâ€‘WebRAG (Table 7).
    - Strong wins on Arenaâ€‘Hard/AlpacaEval/IFEval (Table 6).
  - Generalâ€‘domain superiority is not claimed; indeed, Qwen 2.5 32B leads there (Table 8), which is consistent and increases credibility.
  - Robustness checks:
    - Decontamination is described rigorously (Section 2.2.2; Figure 4; Table 10).
    - LongRAG is extended with unanswerable cases and explicit instructions to answer â€œUnanswerableâ€ (Appendix D.2.3), a good stress test for RAG reliability.
  - Caveats:
    - Several benchmarks use `LLMâ€‘asâ€‘aâ€‘judge` (Table 4) with GPTâ€‘4o/4â€‘1106. The paper acknowledges separability issues with earlier judges and switches to GPTâ€‘4o (footnote in Table 4). This improves reliability but still introduces potential bias and variance.

- Missing ablations
  - No ablation on replay vs. noâ€‘replay for longâ€‘context, or on the effect of dual rewardâ€‘model filtering in preference optimization.
  - No cost breakdown showing training time/throughput from GQA or memory savings from embedding tying.

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Heavy use of web data and synthetic preferences; while decontamination is strong, residual leakage is always possible (Section 2.2.2).
  - `LLMâ€‘asâ€‘aâ€‘judge` evaluation may favor certain response styles or language fluency; although prompts and judges are specified (Table 4; Figures 5 and 8), interâ€‘judge reliability is an open concern.

- Scenarios not directly addressed
  - Multimodal inputs are out of scope (textâ€‘only).
  - Ultraâ€‘long contexts beyond 32K are not evaluated, nor are memoryâ€‘compression methods for >32K contexts.

- Computational and data constraints
  - While compute is lower than some peers (Table 3), training still requires multiâ€‘trillion tokens and major GPU resources (Table 2). The license is researchâ€‘only (Appendix B), limiting commercial deployment without separate agreement.

- Performance tradeâ€‘offs
  - Generalâ€‘domain reasoning/coding lags Qwen 2.5 32B on several tasks (Table 8).
  - Smaller model (`2.4B`) performs very well given size but still shows safety gaps relative to larger models (Table 9).

- Open questions
  - How much of the realâ€‘world/longâ€‘context gain comes from data recipes versus model architecture? Ablations could quantify this.
  - How sensitive are results to the judge choice and prompt phrasing in LLMâ€‘asâ€‘aâ€‘judge settings?

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that carefully engineered data/finetuning pipelines can deliver strong longâ€‘context and instruction following at lower training budgets (Table 3) and across sizes, including small models suitable for edge/onâ€‘device scenarios (Table 5).
  - Provides Korean longâ€‘context RAG benchmarks that will likely catalyze research in bilingual retrieval and generation (Appendix D.2.4â€“D.2.5).

- Practical applications
  - Highâ€‘reliability RAG for enterprise search and knowledge bases (Koâ€‘LongRAG/Koâ€‘WebRAG results in Table 7).
  - Longâ€‘document analytics: contracts, scientific articles, and reports (32K context; Figure 3).
  - Bilingual assistants for Korean/English markets (vocab and results across KO/EN in Tables 6â€“7).

- Followâ€‘up research
  - Ablations on longâ€‘context replay, RoPE scaling, and instruction evolution to isolate contribution of each technique.
  - Robust evaluation beyond LLMâ€‘asâ€‘aâ€‘judge: human studies, pairwise calibration, and adversarial testing for longâ€‘context hallucinations.
  - Safety advancement: improve smaller modelsâ€™ trustworthiness (Table 9) and analyze bias across more languages/domains.
  - Methods to extend context further with compression or memory modules while retaining the replayâ€‘based stability shown here.
  - Public release and standardization of Koâ€‘LongRAG/Koâ€‘WebRAG leaderboards to benchmark bilingual RAG more broadly.

> Bottom line: EXAONE 3.5 contributes a wellâ€‘engineered training/evaluation pipeline that turns modest compute into strong realâ€‘world and longâ€‘context performance (Tables 5 and 7), introduces valuable Korean RAG benchmarks, and shows that a 2.4B model can meaningfully compete in these scenarios. Generalâ€‘domain reasoning is competitive but not bestâ€‘inâ€‘class (Table 8), highlighting a clear tradeâ€‘off and an opportunity for future optimization.

# A SURVEY OF SELF-EVOLVING AGENTS: ON PATH TO ARTIFICIAL SUPER INTELLIGENCE

**ArXiv:** [2507.21046](https://arxiv.org/abs/2507.21046)

## ğŸ¯ Pitch

This survey delivers the first comprehensive, systematic framework for understanding and building 'self-evolving agents'â€”AI systems that autonomously adapt and improve through continual updates to their models, memories, prompts, tools, and architectures. By organizing the field along the dimensions of what, when, how, and where agents evolve, and pairing this with concrete evaluation criteria and benchmarks, the paper charts a roadmap for overcoming the static limitations of current agents and propelling AI toward greater adaptability, robustness, and the emergence of Artificial Super Intelligence. This matters because real-world deployments demand agents capable of evolving with their environments; such continual self-improvement is foundational for creating AI systems that go beyond static performance to achieve human-level versatility and resilience.

---

## 1. Executive Summary (2-3 sentences)
This survey builds the first structured, end-to-end framework for understanding and designing â€œselfâ€‘evolving agentsâ€â€”AI agents that can improve themselves during and between tasks by updating models, prompts/memories, tools, and architectures. It formalizes the problem (Sec. 2.1; Eqs. 1â€“3), organizes the field along four axesâ€”what to evolve, when to evolve, how to evolve, and where to evolve (Figs. 2â€“3)â€”and pairs these with concrete evaluation goals, metrics, and benchmarks tailored to continual adaptation (Sec. 7; Table 5; Figure 9; Table 7).

## 2. Context and Motivation
- Problem addressed
  - Modern large language model (LLM) agents are â€œstaticâ€: they typically do not change their internal parameters, prompts, tools, or workflows once deployed. The survey targets the gap between these static systems and agents that can â€œselfâ€‘evolveâ€â€”learn from interactions and feedback to adapt to new tasks and environments in real time and over time (Abstract; Sec. 1).
- Why it matters
  - Practical: Deployed agents operate in open-ended, dynamic settings (coding assistants, web/GUI agents, medical and education agents). Static behavior limits robustness, adaptability, and real-world performance.
  - Conceptual: The path from LLMs to Artificial Super Intelligence (ASI) likely requires agents that autonomously learn and restructure themselves (Fig. 1 conceptual trajectory).
- Prior approaches and their gaps (Sec. 2.2; Table 1)
  - Curriculum learning: orders training data by difficulty but â€œupdates only model parametersâ€ and uses static datasets (Table 1).
  - Lifelong/continual learning: adds knowledge sequentially but again focuses on model parameters and â€œacquires knowledge passivelyâ€ (Sec. 2.2).
  - Model editing/unlearning: efficiently modifies or removes specific knowledge but cannot evolve non-parametric components (memory, tools, workflows) and lacks autonomous exploration (Sec. 2.2).
- Positioning of this survey
  - It expands the unit of evolution from parameters to the full agent systemâ€”`model`, `context` (prompts and memory), `toolset`, and `architecture`â€”and adds timing (intra-test vs inter-test) and mechanism (reward-based, imitation, population-based) dimensions (Figs. 2â€“3; Secs. 3â€“5). It also systematizes evaluation for adaptive agents (Sec. 7).

## 3. Technical Approach
This paperâ€™s â€œmethodâ€ is a formal framework and taxonomy rather than a new algorithm.

1) Formal problem definition (Sec. 2.1)
- Environment as POMDP (partially observable Markov decision processâ€”a standard framework for sequential decision-making under uncertainty):
  - â€œE = (G, S, A, T, R, Î©, O, Î³)â€ where `G`=goals, `S`=states, `A`=actions (including language and tool calls), `T`=transition, `R`=feedback (scalar or textual), `Î©`=observations, `O`=observation function, `Î³`=discount (Sec. 2.1).
- Agent system components: Î  = (Î“, {Ïˆi}, {Ci}, {Wi})
  - `Î“` = architecture/workflow graph; at each node `Ni`:
    - `Ïˆi` = underlying LLM/MLLM,
    - `Ci` = context (prompt `Pi` and memory `Mi`),
    - `Wi` = tools/APIs available (Sec. 2.1).
  - The node policy `Ï€Î¸i(Â·|o)` outputs action distributions; `Î¸i = (Ïˆi, Ci)` (Sec. 2.1).
- Selfâ€‘evolving strategy:
  - > â€œf(Î , Ï„, r) = Î â€² = (Î“â€², {Ïˆâ€²i}, {Câ€²i}, {Wâ€²i})â€ (Eq. 1) â€” given trajectory `Ï„` and feedback `r`, the agent updates its components.
- Objective:
  - > â€œGiven tasks (T0, â€¦, Tn) and initial Î 0, evolve Î j+1 = f(Î j, Ï„j, rj) to maximize âˆ‘j U(Î j, Tj)â€ (Eqs. 2â€“3).
  - `U` is a utility function derived from feedback and metrics.

2) Taxonomy: What, When, How, Where (Figs. 2â€“3)
- What to evolve (Sec. 3; Table 2)
  - Models: update policies/weights via self-generated data, online feedback, or RL (Sec. 3.1). Example methods: SCA, SELF, SCoRe, PAG, TextGrad (Table 2).
  - Context: Memory evolution (add/merge/delete/update long-term experiences; e.g., Mem0, SAGE) and Prompt optimization (search, gradient-like edits, evolutionary methods; e.g., APE, PromptBreeder, DSPy, TextGrad) (Secs. 3.2.1â€“3.2.2).
  - Tools: Creation (e.g., Voyager, Alita, SkillWeaver), Mastery/refinement from feedback (LearnAct, DRAFT), and Selection/management at scale (ToolGen, AgentSquare) (Sec. 3.3).
  - Architecture: Single-agent node/workflow optimization (TextGrad; EvoFlow; AgentSquare) and multi-agent workflow generation and co-evolution (AFlow, ADAS, ReMA, GiGPO) (Sec. 3.4).
- When to evolve (Sec. 4; Fig. 5)
  - Intra-test-time selfâ€‘evolution: adaptation while solving the current task instanceâ€”without (ICL) or with (SFT/RL) weight updates; examples: Reflexion, AdaPlanner, LADDER (Sec. 4.1).
  - Inter-test-time selfâ€‘evolution: learning between tasks from past trajectories or datasets via ICL/SFT/RL; examples: STaR, Quietâ€‘STaR, SiriuS, RAGEN (Sec. 4.2).
- How to evolve (Sec. 5; Figs. 6â€“7; Table 4)
  - Reward-based: feedback as language critiques, internal confidence, external rewards, or implicit rewards (Sec. 5.1).
  - Imitation/demonstration: self-generated demonstrations (STaR variants), cross-agent demos (SiriuS), hybrid strategies (Sec. 5.2).
  - Population-based/evolutionary: evolve code, prompts, teams, or policies via selection/mutation/self-play (DGM, SPIN, EvoMAC) (Sec. 5.3).
  - Cross-cutting dimensions: online vs offline learning, on-policy vs off-policy, and reward granularity (process vs outcome vs hybrid) (Sec. 5.4; Fig. 7; Table 4).
- Where to evolve (Sec. 6; Fig. 8)
  - General-purpose assistants: memory mechanisms, modelâ€“agent coâ€‘evolution, and curriculum-driven training (Sec. 6.1).
  - Specialized domains: coding, GUI, finance, medical, education, others (Sec. 6.2). Representative systems listed for each.

Design choices and why they matter
- Broadening the evolvable surface: Moving beyond model weights to include prompts/memory, tools, and architectures acknowledges that many agent failures arise from insufficient context, capabilities, or workflows, not just parametric deficiencies (Sec. 3; Table 2).
- Two-phase timing (intra vs inter test time): Separating immediate adaptation from retrospective consolidation clarifies method selection and constraints (Fig. 5; Secs. 4.1â€“4.2).
- Multiple evolution mechanisms: Reward-based, imitation, and population-based approaches capture complementary strengths (sample efficiency vs exploration vs structural innovation) (Sec. 5; Table 4).
- Evaluation tied to adaptivity and safety: The paper aligns metrics and benchmarks with the longitudinal nature of selfâ€‘evolution (Sec. 7; Fig. 9; Table 5; Table 7).

## 4. Key Insights and Innovations
- A unifying formalism for selfâ€‘evolution (Sec. 2.1; Eqs. 1â€“3)
  - Whatâ€™s new: A precise mapping from agent experience (`Ï„`, `r`) to updates over architecture, model, context, and tools (`Î  â†’ Î â€²`), plus a cumulative-utility objective.
  - Why it matters: It turns â€œagent improvementâ€ into a well-posed optimization problem that can be analyzed, compared, and implemented.
- The â€œWhatâ€“Whenâ€“Howâ€“Whereâ€ decomposition (Figs. 2â€“3; Secs. 3â€“6)
  - Whatâ€™s new: A comprehensive taxonomy that simultaneously covers components (what), timing (when), mechanisms (how), and application domains (where).
  - Significance: Prior work surveyed narrower facets (e.g., tool use or model training). Here the interdependencies (e.g., prompt evolution inside auto-generated workflows) become explicit, guiding system design and research roadmaps.
- Cross-cutting evolutionary dimensions (Sec. 5.4; Fig. 7; Table 4)
  - Whatâ€™s new: A comparative lensâ€”offline/online, on/off-policy, reward granularityâ€”that cuts across reward-based, imitation, and evolutionary methods.
  - Significance: It exposes key trade-offs (stability vs sample efficiency; dense vs sparse rewards) that practitioners must manage when building real systems.
- Evaluation framework tailored to adaptive agents (Sec. 7; Fig. 9; Table 5; Table 7)
  - Whatâ€™s new: Goalâ€‘driven metrics such as Adaptivity over iterations, Retention (Forgetting/BWT), Generalization across domains, Efficiency, and Safety; paired with static/shortâ€‘horizon/longâ€‘horizon paradigms and concrete benchmarks.
  - Significance: Moves beyond oneâ€‘shot accuracy to longitudinal performance, enabling rigorous assessment of selfâ€‘evolution.

## 5. Experimental Analysis
Because this is a survey, it does not run new experiments. Instead, it specifies how to evaluate selfâ€‘evolving agents and compiles the landscape of benchmarks and methods.

- Evaluation methodology and metrics (Sec. 7)
  - Goals (Table 5; Fig. 9):
    - Adaptivity: â€œSuccess rate by iteration stepsâ€ and â€œAdaptation speedâ€ track improvement during interaction.
    - Retention: â€œForgetting (FGT)â€ and â€œBackward Transfer (BWT)â€ quantify whether learning new tasks degrades or improves prior tasks (Sec. 7.1; equations given).
    - Generalization: Aggregate and out-of-domain performance across task suites.
    - Efficiency: Token/time cost, steps, tool productivity.
    - Safety: Safety/Harm scores, Completion under Policy (CuP), Risk ratio, Refusal and Leakage rates.
  - Evaluation paradigms (Sec. 7.2; Fig. 9; Table 6):
    - Static assessment: one-shot capability on fixed sets (e.g., AgentBench, OSWorld; Table 7).
    - Short-horizon: improvement across attempts/episodes; includes built-in dynamic tasks like MemoryAgentBenchâ€™s â€œTest-Time Learningâ€ (Sec. 7.2.2).
    - Long-horizon/lifelong: sequences of diverse tasks (e.g., LTMBenchmark, LifelongAgentBench) with retention metrics and dynamic benchmarks (Sec. 7.2.3).
- Benchmarks covered (Table 7; Sec. 7.2)
  - External task-solving: WebArena (812 tasks), SWE-bench (2,294 issues), OSWorld (369 tasks), GAIA (466 tasks), TheAgentCompany (175 tasks).
  - Component skills: Planning (PlanBench ~26,250 tasks), Tool use (ToolBench 126,486 examples; Tâ€‘Eval 23,305), Memory (StoryBench; MemoryAgentBench 2,200), Multi-agent collaboration (MultiAgentBench, SwarmBench).
  - Safety benchmarks: Agentâ€‘SafetyBench (20,000), STâ€‘WebAgentBench (235) (Table 7).
- Evidence strength
  - The survey aggregates methods and benchmarks (Tables 2â€“4, 7) and specifies metrics (Table 5), but it does not perform meta-analysis or standardized cross-method comparisons. Claims are therefore qualitative (e.g., â€œautomatically discovered workflows could outperform human-designed onesâ€ in the workflow-optimization narrative; Sec. 3.4.2), with references to source papers for quantitative details.
- Ablations/robustness/failure modes
  - Not directly provided by the survey; instead, Secs. 7 and 8 highlight evaluation gaps (e.g., catastrophic forgetting, dynamic safety, and cost of dynamic reasoning) and call for long-horizon assessments and safety stress tests.

## 6. Limitations and Trade-offs
Grounded in the surveyâ€™s analysis and â€œFuture Directionâ€ section (Sec. 8):

- Assumptions and scope
  - The framework presumes access to feedback signals `r` (scalar or textual) and well-defined task utilities `U` (Sec. 2.1). In many real applications, reliable, low-noise feedback may be costly or delayed (Sec. 5.1 and 7.1 discuss signal design and sparsity).
- Unaddressed scenarios / edge cases
  - Non-stationary or adversarial environments where feedback is strategically misleading are not deeply treated; safetyâ€‘critical adversarial threats for agents remain an open challenge (Sec. 8.3; Benchmarks in Sec. 7 focus on controlled settings).
  - Multi-agent ecosystems with evolving norms and incentives need dynamic evaluation and role adaptation beyond current static benchmarks (Sec. 8.4).
- Computational and data costs
  - â€œThe cost of dynamic reasoningâ€ and test-time scaling can be substantial (Sec. 8.2; cites infrastructure concerns in [308]); population-based methods can be resourceâ€‘intensive (Table 4 â€œScalabilityâ€ row).
  - Online RL for web/GUI agents needs simulators or live environments; reward models and world models introduce additional training loops (Secs. 5.1, 6.1).
- Methodological weaknesses and open questions
  - Generalization vs specialization tension (Sec. 8.2): agents optimized for a narrow domain often struggle to transfer.
  - Continual learning risks catastrophic forgetting; balancing stability vs plasticity remains unsettled in LLM agents (Sec. 8.2; Retention metrics in Sec. 7.1 address evaluation, not solution).
  - Safety/control: agents still â€œstruggle to accurately differentiate between necessary and irrelevant sensitive informationâ€ and handle goals involving unethical methods (Sec. 8.3).
  - Knowledge transfer across agents and emergence of robust world models are not well-understood (Sec. 8.2, â€œKnowledge Transferabilityâ€).

## 7. Implications and Future Directions
- How this changes the field
  - The survey reframes agent improvement as a fullâ€‘stack, continual process across model, context, tools, and architecture, anchored in a rigorous formalism (Sec. 2.1) and a multi-axis taxonomy (Figs. 2â€“3). This provides a common language to design, compare, and evaluate selfâ€‘evolving systems and aligns research with longâ€‘horizon, safetyâ€‘aware goals.
- Research directions (Sec. 8)
  - Personalized agents at scale: move from heavy post-training to selfâ€‘generated preference data and robust â€œcoldâ€‘startâ€ personalization with reliable long-term memory (Sec. 8.1).
  - Generalization: test-time scaling policies, meta-learning for cross-domain adaptation, and mechanisms to prevent catastrophic forgetting while maintaining adaptability (Sec. 8.2).
  - Safe and controllable agents: richer constitutions, better risk detection in ambiguous contexts, privacy-aware memory, and diverse real-world safety datasets (Sec. 8.3; Table 7 lists initial safety benchmarks).
  - Multi-agent ecosystems: dynamic role allocation, knowledge sharing, and workflow/knowledge coâ€‘evolution with live, longitudinal benchmarks (Sec. 8.4).
- Practical applications
  - General domain assistants with persistent memory and curricula (Mobile-Agentâ€‘E, WebRL; Sec. 6.1).
  - Specialized domains:
    - Coding: self-improving and multi-agent code generation and repair (SICA, EvoMAC; Sec. 6.2).
    - GUI/Web: end-to-end computer use and navigation with reflection and RL (OSWorld agents, WebVoyager; Sec. 6.2).
    - Finance: self-improving trading agents with domain knowledge evolution (QuantAgent; Sec. 6.2).
    - Medical and Education: consultation/tutoring agents that accumulate cases and refine strategies over time (Agent Hospital, MedAgentSim, PACE; Sec. 6.2).

Quoted anchors for key definitions and objectives:
- > â€œf(Î , Ï„, r) = Î â€² â€¦â€ (Eq. 1; Sec. 2.1)
- > â€œÎ j+1 = f(Î j, Ï„j, rj)â€ (Eq. 2; Sec. 2.1)
- > â€œmax_f âˆ‘ U(Î j, Tj)â€ (Eq. 3; Sec. 2.1)
- Comparative scope:
  - > â€œSelfâ€‘evolving Agents âœ“ evolve Context and Toolset; Curriculum/Lifelong/Editing do notâ€ (Table 1).
- Taxonomic overviews:
  - > â€œWhatâ€“Whenâ€“Howâ€“Whereâ€ structure and examples (Figs. 2â€“3).
- Evaluation blueprint:
  - > â€œAdaptivity, Retention (FGT/BWT), Generalization, Efficiency, Safetyâ€ (Table 5; Fig. 9), and benchmark catalog (Table 7).

Overall, this survey supplies the conceptual scaffoldingâ€”and the evaluation toolkitâ€”needed to push from static LLM tools to adaptive, safe, and scalable agentic systems capable of selfâ€‘evolution on the road toward stronger general intelligence.

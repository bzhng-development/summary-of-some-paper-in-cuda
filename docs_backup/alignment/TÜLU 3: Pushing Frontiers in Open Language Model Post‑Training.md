# TÃœLUÂ 3: Pushing Frontiers in Open Language Model Postâ€‘Training

**ArXiv:** [2411.15124](https://arxiv.org/abs/2411.15124)
**Authors:** NathanÂ Lambert, JacobÂ Morrison, ValentinaÂ Pyatkin, ShengyiÂ Huang, HamishÂ Ivison, FaezeÂ Brahman, LesterÂ JamesÂ V.Â Miranda, AlisaÂ Liu, NouhaÂ Dziri, ShaneÂ Lyu, YulingÂ Gu, SaumyaÂ Malik, VictoriaÂ Graf, JenaÂ D.Â Hwang, JiangjiangÂ Yang, RonanÂ LeÂ Bras, OyvindÂ Tafjord, ChrisÂ Wilhelm, LucaÂ Soldaini, NoahÂ A.Â Smith, YizhongÂ Wang, PradeepÂ Dasigi, HannanehÂ Hajishirzi
**Institutions:** Allen Institute for AI

## ğŸ¯ Pitch

TÃ¼lu 3 redefines transparency in language model development by releasing an open post-training recipe, incorporating novel RL with Verifiable Rewards (RLVR) to enhance reproducibility and performance, surpassing some closed competitors. This innovation empowers researchers and practitioners with high-quality, trustworthy models and methodologies, paving the way for significant advancements in applications like safe chat systems, coding, and math reasoning without reliance on opaque, proprietary systems.

---

## 1. Executive Summary
TÃ¼lu 3 is a fully open postâ€‘training recipe and model family (8B, 70B, 405B parameters) that combines supervised finetuning (SFT), preference optimization (DPO), and a new reinforcementâ€‘learning stage called RL with Verifiable Rewards (RLVR). It closes a major transparency and performance gap by releasing models, code, datasets, training and evaluation infrastructure, and it achieves stateâ€‘ofâ€‘theâ€‘art results among open modelsâ€”reaching or surpassing several strong closed baselines (e.g., GPTâ€‘4oâ€‘mini, Claude 3.5â€‘Haiku) on a broad skill suite (Table 2).

## 2. Context and Motivation
- Problem addressed
  - Modern language models require â€œpostâ€‘trainingâ€ (instruction tuning, preference learning, RLHFâ€‘style optimization) to behave helpfully and follow instructions. However, the most successful recipes are opaque: data, code, and procedures are not openly released, which hinders reproducibility and progress (Section 1).
  - Open efforts have existed (e.g., TÃ¼lu 2, Zephyrâ€‘Î²), but they typically rely on simpler pipelines, smaller or lowerâ€‘quality datasets, and limited evaluation rigor. They trail closed systems on core capabilities like math, precise instruction following, and safety (Section 2; Table 6 and Table 5 baselines).

- Why it matters
  - Practical impact: highâ€‘quality, reproducible postâ€‘training enables labs and practitioners to adapt base models for real applicationsâ€”coding, math reasoning, safe chatâ€”without relying on closed APIs.
  - Scientific impact: full release of datasets, decontamination tooling, training code, and an evaluation framework allows systematic comparison and ablation across methods and scales (Table 1; Section 7).

- Prior approaches and gaps
  - Typical recipe: SFT â†’ RLHF (or DPO variants). Gaps: limited data transparency; uncertain contamination with test sets; narrow evaluations; few ablations on algorithmic and infrastructure decisions (Section 2).
  - Specific weaknesses the paper targets: weak math and instructionâ€‘following performance in open recipes; limited scaling of preference data; little clarity on trainingâ€‘time pitfalls (e.g., loss aggregation) (Sections 3â€“6).

- Positioning
  - TÃ¼lu 3 offers a complete, open pipeline spanning:
    - Curated and synthetic data targeting core skills with aggressive decontamination (Sections 3.1â€“3.2; Table 7; Table 8).
    - Multiâ€‘stage training with extensive ablations (SFT in Section 4; DPO in Section 5).
    - A novel, generalist RL stageâ€”RLVRâ€”that uses task verifiers instead of a learned reward model (Section 6).
    - A standardized development vs. unseen evaluation suite and toolkit (Section 7; Table 24).

## 3. Technical Approach
Highâ€‘level recipe (Figure 1): curate prompts â†’ SFT on promptâ€“completion pairs â†’ DPO on preference pairs â†’ RLVR on verifiable tasks â†’ evaluate on development and unseen suites while decontaminating training data against them.

1) Data curation and decontamination (Section 3)
- Core skills targeted: knowledge recall, reasoning, math, coding, precise instruction following (IF), general chat, and safety (Table 3).
- Sources:
  - Public datasets with clear licenses (e.g., WildChat, OpenAssistant, FLAN v2, NuminaMathâ€‘TIR, OpenMathInstruct2, Evolâ€‘CodeAlpaca, Aya, SciRIFF, TableGPT) and AI2â€‘generated personaâ€‘driven synthetic datasets (precise IF, math, coding) (Table 7; Sections 3.1.1â€“3.1.2).
  - Safety and nonâ€‘compliance prompts gathered and synthesized (CoCoNot, WildGuardMix, WildJailbreak) (Section 3.1.2).
- Personaâ€‘driven synthesis: use ~250K personas from Persona Hub to generate diverse instructions for math, coding, and verifiable IF; completions produced by strong models (GPTâ€‘4o, Claudeâ€‘3.5â€‘Sonnet) (Section 3.1.2; Figures 30â€“36).
- Decontamination: 8â€‘gram overlap at the prompt level; remove datasets with >2% overlap with an evaluation; remove overlapping instances when necessary (Section 3.2). Released decontaminated versions for several public sets (Table 8).

2) Supervised Finetuning (SFT) (Section 4)
- Data: ~0.94M curated promptâ€“completion pairs (Table 7; Figure 2 for length distribution).
- Training setup: Llamaâ€‘3.1 base models (8B, 70B); 2 epochs; effective batch 128; max length 4096; LR 5eâ€‘6 (8B) and 2eâ€‘6 (70B) (Table 11). Compute: 8B on 32 H100 GPUs for ~6h; 70B on 64 H100s for ~50h (Section 4.3).
- Critical engineering fixâ€”loss aggregation: default â€œmean lossâ€ across padded tokens interacts badly with gradient accumulation/distributed training. They switch to â€œsum loss,â€ reâ€‘tuning LR, which yields better stability and performance (Section 4.3.2; Figures 5â€“6).
- Chat template choice matters: removing a trailing newline (their final â€œTÃ¼lu 3â€ template) avoids later inconsistencies while staying competitive with alternatives (Section 4.3.1; Table 13).

3) Preference Tuning (DPO) with scalable onâ€‘policy data (Section 5)
- Data creation pipeline (Figure 7):
  - Stage 1: select prompts (some reused from SFT; some subsampled but unused; some new IFâ€‘augmented prompts).
  - Stage 2: generate 4 responses per prompt from a model pool (22 models, including the onâ€‘policy `TÃ¼lu 3 SFT` model and various external models; Appendix Table 38).
  - Stage 3: judge each response on helpfulness, instructionâ€‘following, honesty, truthfulness using `GPTâ€‘4oâ€‘2024â€‘08â€‘06`; binarize to chosen vs rejected for DPO (Section 5.2.1).
- Final preference mix sizes: 8B uses 271K pairs; 70B uses 334K (Table 15).
- Algorithm: lengthâ€‘normalized DPO (divide logâ€‘likelihood by response length to reduce length bias), which consistently outperforms vanilla DPO and SimPO in their setting (Section 5.4.1; Table 18). Final hyperparameters: LR 5eâ€‘7 (8B) / 2eâ€‘7 (70B); Î²=5; 1 epoch; effective batch 128; max length 2048 (Table 20).
- Infrastructure optimizations for 70B: cache reference logâ€‘probs; run chosen/rejected forwards separately to cut peak GPU memoryâ€”yields nearâ€‘identical losses with much lower memory (Section 5.4.2; Figure 17).

4) Reinforcement Learning with Verifiable Rewards (RLVR) (Section 6)
- Idea: use a deterministic verifier (`v(x,y)`) that gives a fixed positive reward `Î±` if the generated answer is correct/constraintâ€‘satisfying, else 0. Optimize the standard KLâ€‘regularized PPO objective with this reward, avoiding rewardâ€‘model pitfalls (Eq. 7â€“8; Section 6).
- Verifiable tasks and training prompts (Table 22):
  - GSM8K train (gradeâ€‘school math); extract final numeric answer with 8â€‘shot CoT prompt in the input.
  - MATH train (competition math); 3â€‘shot CoT; flexible answer extraction (â€œflexâ€) during evaluation.
  - IFEval constraints (precise IF); verifier functions for each constraint type.
- PPO setup highlights (Section 6.2):
  - Initialize the value function from a general reward model trained on UltraFeedback (Table 36), which performs best vs alternative initializations (Figure 21).
  - Disable dropout in policy and reference models to keep logâ€‘probs consistent across rollout and learning phases.
  - Penalty when responses do not end with EOS; advantage normalization; shuffle prompts across epochs.
  - Asynchronous RL infrastructure: inference on dedicated GPUs via vLLM PagedAttention; learners run concurrently; scale to 405B with ZeROâ€‘3 + Ray orchestration (Section 6.3). Typical runtimes: 8B RLVR ~65h on 8 H100s; 70B ~60h on 48 GPUs (Section 6.3).
- Hyperparameters (Table 21): effective batch sizes up to 640 (70B); KL Î² sweeps; response length up to 2048; reward `Î±=10`.

5) Evaluation framework (Section 7)
- OLMES toolkit for reproducible runs and consistent prompting (Section 7.1).
- Two suites (Table 24):
  - Development: MMLU (zeroâ€‘shot CoT with â€œsummarize reasoningâ€ prompt), PopQA, TruthfulQA (MC2), BBH (3â€‘shot CoT), DROP (3â€‘shot), GSM8K (8â€‘shot CoT), MATH (4â€‘shot CoT; â€œflexâ€ extraction), HumanEval/+, IFEval (promptâ€‘level accuracy), AlpacaEval 2 (lengthâ€‘controlled winâ€‘rate), safety suite (six datasets scored by WildGuard or refusal metrics; Section 7.2.1; Table 25â€“26).
  - Unseen: MMLUâ€‘Pro, GPQA, AGIEvalâ€‘English, DeepMind Mathematics (zeroâ€‘shot â€œconcise reasoningâ€ prompt + SymPy equivalence), BigCodeBenchâ€‘Hard, new IFEvalâ€‘OOD (52 outâ€‘ofâ€‘distribution constraints; Appendix F.3), new HREF (11 IF subtasks with human references; Section 7.3.2; Table 48).

## 4. Key Insights and Innovations
- RL with Verifiable Rewards (RLVR) for general postâ€‘training (Section 6)
  - Novelty: replaces a learned reward model with explicit verifiers across multiple domains (math and constraint following), integrating into a general training pipeline beyond mathâ€‘only RL (contrast with VinePPO, STaR/Quietâ€‘STaR; Section 9.2).
  - Significance: targeted and reliable improvements on verifiable tasks without rewardâ€‘model brittlenessâ€”8B gains on GSM8K (+3.3 points over DPO to 87.6) and IFEval (+1.3 to 82.4) while improving MATH (+1.7 to 43.7) (Table 23).

- Scalable, onâ€‘policy preference data generation (Section 5.2)
  - Novelty: large, mixed onâ€‘policy/offâ€‘policy preference sets created with a unified pipeline (Figure 7), at scale (>270K pairs per model; Table 15).
  - Significance: clear empirical gains from more unique prompts (Figure 8), from including onâ€‘policy generations (Figure 11), and from targeted IF preference sets (Figure 14). This shows how to move beyond UltraFeedback while keeping costs manageable.

- Aggressive decontamination + development/unseen eval split (Sections 3.2 and 7)
  - Novelty: systematic 8â€‘gram promptâ€‘level matching with dataset removals where overlap exceeds 2%, plus decontaminated releases (Table 8), and explicit unseen evaluation suite (Table 24).
  - Significance: reduces overfitting risk and allows measurement of generalization. For instance, TÃ¼lu 3 improves on unseen DeepMind Math and AGIEval relative to its development gains (Table 31).

- Practical training/infrastructure guidance (Sections 4.3.2, 5.4.2, 6.3)
  - Lossâ€‘aggregation fix (sumâ€‘loss) prevents subtle weighting bugs (Section 4.3.2).
  - GPU memory reductions in DPO via cached reference logâ€‘probs and split forwards (Figure 17).
  - Asynchronous RL layout with vLLM + ZeROâ€‘3 + Ray scales to 405B (Section 6.3; Section 8.1).

These go beyond incremental tuningâ€”RLVR and the data scaling pipeline are conceptual advances; the decontamination/evaluation rigor and infrastructure lessons are broadly reusable.

## 5. Experimental Analysis
- Evaluation methodology
  - Core development suite and unseen suite (Table 24). Chainâ€‘ofâ€‘thought prompting is used selectively (e.g., MMLU zeroâ€‘shot â€œsummarize reasoningâ€ prompt; Section 7.2) with robust answer extraction (e.g., MATH â€œflexâ€; Section 7.2).

- Main results (8B and 70B; Table 2)
  > TÃ¼lu 3â€‘70B average (development suite): `76.2`, beating Llamaâ€‘3.1â€‘70Bâ€‘Instruct (`74.1`) and Qwenâ€‘2.5â€‘72Bâ€‘Instruct (`72.8`), and approaching Claudeâ€‘3.5â€‘Haiku (`75.3`) and GPTâ€‘4oâ€‘mini (`69.6`).  
  > TÃ¼lu 3â€‘8B average: `65.1`, above Llamaâ€‘3.1â€‘8Bâ€‘Instruct (`62.9`) and near Qwenâ€‘2.5â€‘7Bâ€‘Instruct (`66.5`).

  Highlights by skill (70B; Table 2):
  - Math: MATH `63.0` vs `56.4` (Llamaâ€‘3.1â€‘70Bâ€‘Inst); GSM8K `93.5` (parity with Llamaâ€‘3.1â€‘70Bâ€‘Inst `93.7`).
  - Instruction following: IFEval `83.2` vs `88.0` (Llamaâ€‘3.1â€‘70Bâ€‘Inst), but higher AlpacaEvalâ€‘2 winâ€‘rate `49.8` vs `33.4`.
  - Safety: strong overall safety (`88.3` vs `76.5`, Table 2; detailed breakdowns in Table 26).

- Stageâ€‘wise gains (70B; Table 5) and (8B; Table 6)
  - SFT â†’ DPO â†’ RLVR tracks incremental improvements, especially in targeted domains. For 8B, GSM8K rises from `76.2` (SFT) â†’ `84.3` (DPO) â†’ `87.6` (RLVR), and IFEval from `72.8` â†’ `81.1` â†’ `82.4` (Table 6).
  - At 70B, DPO boosts MATH (`53.7`â†’`62.3`) and GSM8K stays saturated (`93.5`), RLVR yields modest further MATH/IF gains (`63.0`, `83.2`) (Table 23).

- 405B scaling (Table 4; Section 8.1)
  > TÃ¼lu 3â€‘405B (RLVR) achieves average with safety `80.7`, outperforming Llamaâ€‘3.1â€‘405Bâ€‘Instruct (`79.0`) and Nousâ€‘Hermesâ€‘3â€‘405B (`73.5`), and competitive with DeepSeekâ€‘V3 (`75.9`) and GPTâ€‘4o (11â€‘24) (`81.6`).

- Unseenâ€‘suite generalization (Table 31)
  > The final 8B and 70B models improve the unseen averages relative to SFT and DPO stages. For 70B unseen, averages are `44.4` (DPO) and `44.4` (RLVR), beating SFT `41.0`.

  - HREF (instruction following with human references): at 70B, TÃ¼lu 3 scores `42.3`, below Llamaâ€‘3.1â€‘70Bâ€‘Instruct `45.6` but above Hermesâ€‘3â€‘70B `36.8` (Table 33; subtask breakdown Table 48). Shows instructionâ€‘following is multiâ€‘faceted; distributional differences matter.
  - IFEvalâ€‘OOD: all models drop vs IFEval, indicating overâ€‘specialization to known constraints. TÃ¼lu 3â€‘70B gets `27.8` vs Llamaâ€‘3.1â€‘70Bâ€‘Instruct `34.5` (Table 33).

- Ablations and diagnostics
  - SFT data ablations show:
    - Removing WildChat hurts AlpacaEval and degrades many skills (Table 10).
    - Removing safety data barely affects nonâ€‘safety metrics but reduces safety average substantially (Table 10).
    - Persona datasets materially help the skills they target: MATH/GSM8K/HumanEval(+)/IFEval drop when they are removed (Table 10).
  - Preference data ablations show:
    - More unique prompts â†’ consistent gains (Figure 8); duplicating prompts without new content does not (Figure 9).
    - Onâ€‘policy data helps over offâ€‘policy alone (Figure 11).
    - GPTâ€‘4o is a slightly better judge, but several judges are similar (Table 17).
    - IFâ€‘targeted preference sets (`Persona IF` + `IFâ€‘augmented`) improve IFEval, with modest average tradeâ€‘offs; best mix balances both (Figure 14; Table 16).
  - RLVR ablations (Figures 19â€“22):
    - Verifiable rewards on GSM8K/MATH/IFEval yield higher training rewards and test gains.
    - Initialization of the value function from a general RM is best (Figure 21).
    - Adding RM scores to verifiable rewards is noisier and worse (Figure 22).
    - Larger KL divergence can reduce overall averagesâ€”â€œoverâ€‘optimizationâ€ tradeâ€‘off (Figures 19â€“22; Appendix B.4 shows pathological outputs for constraints).

- Safety evaluation
  - Strong refusal on harmful content while maintaining compliance on benign prompts (Tables 25â€“26; categories in Tables 39â€“40). E.g., 70B SFT reaches `94.4` safety average.

Overall, the experiments convincingly back the claims: staged training improves broad capabilities; RLVR is an effective, simple RL addition; data choices and infrastructure details measurably matter.

## 6. Limitations and Trade-offs
- Scope of verifiable RL
  - RLVR relies on tasks with reliable verifiers (math final answers, explicit constraints). It does not directly cover openâ€‘ended dialog quality, nuanced safety, or multiâ€‘step toolâ€‘use without additional verifiers (Section 6.1). Overâ€‘optimization can appear when KL is too low/high (Figures 19â€“22; Appendix B.4).

- Evaluation coverage and distribution shift
  - Despite an unseen suite, some generalization gaps remain: e.g., IFEvalâ€‘OOD drops across models (Table 33), and DeepMind Math formatting differences can interact with CoT behavior (Section 7.4.1).

- Judge and metric dependence
  - Preference data depends on LLMâ€‘asâ€‘aâ€‘judge (mainly GPTâ€‘4o). Although alternatives yield similar results (Table 17), this can encode judge biases. Some evaluations (AlpacaEval 2, HREF subsets) rely on LLM judges or embedding similarity (Sections 7.2, 7.3.2).

- Compute and engineering complexity
  - Training large models with DPO and RLVR requires substantial compute and careful engineering (Sections 4.3, 5.4.2, 6.3). The 70B and 405B runs needed dozens to hundreds of H100 GPUs with distributed inference/training orchestration and careful reliability handling (Section 8.1).

- Data constraints and contamination risk
  - Although decontamination is systematic (Section 3.2), paraphraseâ€‘level contamination is difficult to detect perfectly; the method chooses precision over catching every paraphrase (embeddingâ€‘based checks were less reliable for this purpose).

- Mixed or conditional results
  - RLVR at 70B showed small average improvements and unusually low KL (Table 23; Figure 23), suggesting tuning sensitivity. Gains in one dimension (e.g., IFEval) can trade off with averages if KL is not balanced (Figure 21).

## 7. Implications and Future Directions
- Field impact
  - TÃ¼lu 3 sets a new reproducibility bar for postâ€‘training: full data, code, decontamination tooling, and evaluation regime (Table 1). The release enables applesâ€‘toâ€‘apples comparisons and faster research progress on methods and data.

- Research directions
  - Richer verifiers and RL: extend RLVR to code execution feedback, toolâ€‘use, or multiâ€‘step verifiable workflows (Section 9.2; Section 8.3). Explore valueâ€‘modelâ€‘free variants or adaptive KL schedules to mitigate overâ€‘optimization.
  - Preference data science: continue scaling unique, onâ€‘policy prompts; study judge diversity; explore curriculum mixing by skill and difficulty (Figures 8â€“16).
  - Longâ€‘context and multiâ€‘turn postâ€‘training: TÃ¼lu 3 focuses on shorter, mostly singleâ€‘turn interactions; extending to long documents and sustained dialog is a natural next step (Section 8.3).
  - Multilingual postâ€‘training: beyond Aya inclusion, build full multilingual recipes with balanced data and crossâ€‘lingual alignment (Section 8.3).
  - Safety beyond refusal: evaluate and train for calibrated helping, contextâ€‘aware compliance/nonâ€‘compliance, and robustness to jailbreaks with minimal overâ€‘refusal (Section 7.2.1; Tables 25â€“26).

- Practical applications
  - Open, highâ€‘quality assistants for research labs and startups; domainâ€‘adapted assistants for math, coding, and instructionâ€‘following tasks; safer chat systems with transparent refusal/compliance behavior; foundations for agentic systems that can be trained with verifiable subâ€‘goals.

In short, TÃ¼lu 3 is both a strong set of models and a methodologically complete blueprint. Its staged recipe, scalable data practices, RLVR innovation, and rigorous evaluation toolkit should shape how the community builds, measures, and iterates on open postâ€‘training.

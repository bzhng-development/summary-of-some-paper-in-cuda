# Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens

**ArXiv:** [2401.17377](https://arxiv.org/abs/2401.17377)

## ğŸ¯ Pitch

This paper introduces the âˆ-gram language modelâ€”a modern, unbounded n-gram approachâ€”and the infini-gram engine, enabling state-of-the-art n-gram modeling over trillions of tokens via an efficient suffix array architecture. By allowing for arbitrarily large contexts and making n-gram statistics instantly queryable at massive scale, Infini-gram demonstrates that classical n-gram LMs are not only still relevant but can significantly improve neural language models, reducing perplexity by up to 73%. This work redefines the role of n-gram models in the era of neural LLMs by empowering transparent text analysis, robust data inspection, and practical hybrid modeling at unprecedented scale.

---

## 1. Executive Summary
This paper introduces an â€œunboundedâ€ nâ€‘gram language model (`âˆâ€‘gram`) and a highâ€‘throughput engine (`infiniâ€‘gram`) that serves it over trillions of tokens using suffix arrays. It shows that (a) longâ€‘context nâ€‘gram statistics are surprisingly predictive by themselves and (b) when linearly interpolated with neural LMs, they reduce perplexity substantiallyâ€”up to 73% relative improvement on certain domains (Table 2) and consistent gains even for large models like `Llamaâ€‘2â€‘70B` (Table 1).

## 2. Context and Motivation
- Problem addressed
  - Can classical nâ€‘gram language models still matter at modern data scales, and if so, how can we make them both accurate and practical? (Â§1)
  - Two longâ€‘standing obstacles:
    1) Data scale: prior nâ€‘gram tables were built for small n (typically n â‰¤ 5) and relatively smaller corpora, because count tables grow explosively with n and corpus size (Introduction; Â§2).  
    2) Context length: small n discards most of the prompt, which hurts nextâ€‘token prediction (Figure 1; Â§4.1).

- Importance
  - Practical: fast, easily interpretable counts enable analysis of corpora (contamination checks, PII filtering, retrieval) and can augment neural LMs (Â§E).
  - Scientific: reveals where modern neural LMs agree/disagree with corpus statistics; surfaces training irregularities (e.g., positionalâ€‘embedding artifacts shown by periodic agreement drops under greedy decoding; Â§4.2, Figure 5).

- Prior approaches and their gaps
  - Classical nâ€‘gram LMs: usually n â‰¤ 5; huge count tables; smoothing/backoff to handle sparsity; limited longâ€‘context modeling (Brants et al., 2007; Â§2).
  - Nonparametric neural augmentations (e.g., kNNâ€‘LM, RETRO): powerful but storage/computationâ€‘heavy (Table 6; Â§F). Often store vectors for billions of items (hundreds of TB).
  - Suffix trees/arrays for unbounded n were explored but at limited scales or without a proper probability model (Â§6; Â§F).

- Positioning of this work
  - Scales unbounded nâ€‘gram modeling to 5T tokens across multiple corpora (Dolma, RedPajama, The Pile, C4)â€”largest of its kind (Â§3; A.1â€“A.2).
  - Introduces a valid probability formulation for `âˆâ€‘gram` that uses as much context as exists in the data and backs off only when necessary (Â§2), and an engine that answers nâ€‘gram/âˆâ€‘gram queries in tens to hundreds of milliseconds from disk (Table 3; Â§A.5).

## 3. Technical Approach
There are two main pieces: the language model (`âˆâ€‘gram`) and the serving engine (`infiniâ€‘gram`).

- Background: nâ€‘gram LM (brief refresher)
  - An nâ€‘gram LM estimates `P(w_i | w_{iâˆ’(nâˆ’1):iâˆ’1})` by counts: `cnt(context + token) / cnt(context)`. Classic models smooth or back off to shorter contexts when counts are zero (Â§2). Small n loses longâ€‘range information (Figure 1).

- The `âˆâ€‘gram` LM (Section Â§2)
  - Key idea: use the longest suffix of the entire prompt that appears at least once in the training data (â€œeffective nâ€). If a longer suffix has zero count (i.e., never appears), back off by one token, repeatedly, until the denominator (`cnt(context)`) is positive.
  - Probability definition
    - For position i with preceding tokens `w1:iâˆ’1`, choose `n` equal to 1 + length of the longest suffix of `w1:iâˆ’1` that appears in the data.  
    - Then compute `Pâˆ(w_i | w1:iâˆ’1) = cnt(suffix + w_i) / cnt(suffix)` where the suffix length is `nâˆ’1`.  
    - Because `n` depends only on the context (not on the candidate `w_i`), `Pâˆ(Â· | context)` forms a valid distribution without extra discounting (Â§2).
  - â€œSparseâ€ estimates
    - If only one next token was ever seen after the chosen suffix, the estimate is â€œsparseâ€: probability 1 for that token and 0 for all others. Sparse cases turn out to be highly predictive (Â§4.1; right panel of Figure 3).

- Interpolating with neural LMs (Section Â§2; Â§5)
  - Combined model: `P(y | x) = Î» Â· Pâˆ(y | x) + (1 âˆ’ Î») Â· P_neural(y | x)`.  
  - Practical variant: use two weights, `Î»1` for sparse âˆâ€‘gram events and `Î»2` for nonâ€‘sparse ones, tuned on validation to minimize perplexity (Â§5).

- The `infiniâ€‘gram` engine (Section Â§3; Appendix A)
  - Representation
    - Tokenize the corpus and store it as a byte array (â€œtoken arrayâ€), using 2 bytes per token ID (assumes vocabulary size < 65,536). Insert a document separator token `\xff\xff`.  
    - Build a suffix array over the token array. A suffix array stores the starting positions of all suffixes, sorted lexicographically. It supports substring counting by binary search (Figure 2).
  - Space and build cost
    - Each pointer in the suffix array needs about 5 bytes for shards of size 2Bâ€“500B tokens; the token array uses 2 bytes per token. Total index size â‰ˆ 7 bytes/token (Â§3).  
    - For RedPajama (1.4T tokens), building took ~48 hours on a single 128â€‘core CPU node with 1 TiB RAM and ~10 TB disk (Â§3; Â§A.3).
  - Counting and probability queries
    - Counting `cnt(x1...xn)`: in the suffix array, all occurrences form one contiguous segment; binary search finds the segmentâ€™s bounds, so runtime is O(log N) random accesses (Â§3; Â§A.4).  
    - nâ€‘gram LM probability: compute `cnt(context + token) / cnt(context)` using two counts.  
    - `âˆâ€‘gram` probability: find the longest matching suffix length by a binaryâ€‘lifting + binaryâ€‘search procedure (O(log L) counts), then compute the ratio (Â§A.4 â€œSpeeding up âˆâ€‘gram computationâ€).
  - Optimizations for low latency (Appendix Â§A.4)
    - Parallel shard processing; hinted search; memory prefetching; reusing previous search segments across adjacent nâ€™s; amortized processing across consecutive tokens (effective n can increase by at most 1).
  - Latency (Table 3; Â§A.5)
    - Count an nâ€‘gram (any n up to 1000 tested): ~13â€“20 ms.  
    - nâ€‘gram LM nextâ€‘token distribution: ~31â€“39 ms/token.  
    - `âˆâ€‘gram` probability: ~90â€“135 ms; full `âˆâ€‘gram` distribution: ~88â€“180 ms/token.  
    - All while keeping indexes on disk; no GPU required.
  - Extra features
    - Indexes are additive/subtractive across datasets and shards (Â§A.2â€“A.3).  
    - Documentâ€‘level retrieval, including CNF queries over multiple nâ€‘grams (`SEARCHDOC`; Â§A.5 and Figures 11â€“16).

- Decontamination of reference data (Appendix Â§B)
  - To avoid evaluating on training duplicates, they remove documents in the âˆâ€‘gram reference sets that have high overlap with The Pileâ€™s validation/test sets using the â€œBig Friendly Filterâ€ with 13â€‘gram overlap at an 80% threshold (Table 4; Â§4).

## 4. Key Insights and Innovations
- Unbounded nâ€‘gram with a valid distribution (fundamental)
  - Starting backoff from â€œinfiniteâ€ context and backing off only when the denominator is zero produces a proper probability distribution without Katzâ€‘style discounting (Â§2). This is conceptually simple yet crucial for using long contexts reliably.

- Suffixâ€‘array engine that makes trillionâ€‘token, unboundedâ€‘n modeling practical (systems innovation)
  - 7 bytes per token, onâ€‘disk operation, millisecondâ€‘level latencies, with a reproducible singleâ€‘node build for 1.4T tokens in ~2 days (Â§3; Table 3; Â§A). Previous unbounded approaches (suffix trees) were too large or were not true probability models (Â§6; Â§F).

- Empirical finding: longâ€‘context corpus statistics are strong predictors and complement neural LMs (scientific insight)
  - For human text, `âˆâ€‘gram` matches the true next token 47% of the time overall, exceeding small nâ€‘gram models (Figure 3; Â§4.1).  
  - When `âˆâ€‘gram` is sparse, accuracy jumps to 75% overall and >80% for effective n â‰¥ 14 (Figure 3, right).  
  - Interpolating with `âˆâ€‘gram` reduces perplexity across model families and sizes, including large `Llamaâ€‘2â€‘70B` (Table 1) and `SILO` models (Table 2).

- Diagnostic lens on decoding and model internals (analytical capability)
  - Agreement profiles vs. suffix length reveal that nucleus sampling best resembles human text; greedy decoding shows strong fluctuations and even periodic accuracy drops (e.g., at effective n = 20, 24, 28, 32 for `Llamaâ€‘2â€‘7B`, p < 1eâˆ’99; Figure 5; Â§4.2), pointing to potential positionalâ€‘embedding issues.

## 5. Experimental Analysis
- Reference data and tokenizers
  - Built `infiniâ€‘gram` indexes for Dolma (3T), RedPajama (1.4T), The Pile (â‰ˆ380B), C4 (â‰ˆ200B), and often evaluate with Pileâ€‘train as the reference data for analyses (Â§3; Â§4).  
  - Separate indexes for different tokenizers: GPTâ€‘2/Neo/J share one; Llamaâ€‘2 another; SILO another (Â§5.1 â€œTokenizersâ€).

- Humanâ€‘written text analysis (Section Â§4.1; Figure 3; Figure 4)
  - Setup: Sample 50 docs per Pileâ€‘val domain (â‰ˆ50k tokens/domain), truncate to 1024 tokens; compute `âˆâ€‘gram` probability for the true next token and call it â€œaccurateâ€ if >0.5 (lower bound on argmax accuracy) (Â§4.1).  
  - Findings:
    - Overall `âˆâ€‘gram` accuracy = 47%. Accuracy increases with effective n; for n â‰¥ 16, >75% (Figure 3, middle).  
    - Small 5â€‘gram LM has much lower accuracy (left of Figure 3), reflecting insufficient context (the Abstract quantifies 29%).  
    - Sparse `âˆâ€‘gram` covers >50% tokens and yields 75% overall accuracy; for effective n â‰¥ 14, >80% (Figure 3, right).  
    - Complementarity with neural LMs: when `Llamaâ€‘2` assigns very low probability to the true token, `âˆâ€‘gram` still agrees >20%; if sparse only, â‰ˆ50% (Figure 4). This motivates interpolation.

- Machineâ€‘generated text analysis (Section Â§4.2; Figure 5; Figure 9)
  - Setup: Prompt neural LMs with first 50 tokens of each Pileâ€‘val doc; generate until original length/EOS using greedy, temperature, or nucleus sampling. Models include Llamaâ€‘2 70B/13B/7B and GPTâ€‘J/Neo variants (Â§4.2).  
  - Findings:
    - Decoding matters: nucleus sampling yields an effectiveâ€‘n distribution most similar to human text; greedy decoding uses longer effective contexts and shows large agreement oscillations with suffix length (Figure 5 top; Â§4.2).  
    - Model size: larger models show slightly higher effective n and higher agreement (Figure 5 bottom). GPTâ€‘Neo/J agree more with `âˆâ€‘gram` likely because they trained on The Pile (which is also the `âˆâ€‘gram` reference set there).  
    - Notable irregularity: periodic agreement drops for `Llamaâ€‘2â€‘7B` under greedy decoding (effective n = 20,24,28,32; p < 1eâˆ’99), hypothesized to relate to positional embeddings (Â§4.2).

- Perplexity improvements by interpolation (Section Â§5; Table 1; Table 2; Table 5; Figure 10)
  - Across GPTâ€‘2, GPTâ€‘Neo/J, and Llamaâ€‘2 on Pileâ€‘val/test (Table 1):
    - Consistent gains, decreasing with model size within a family, but still sizeable for large models.  
    - With Pileâ€‘train as reference data, `Llamaâ€‘2â€‘70B` improves from 4.59 to 4.21 on validation and 4.65 to 4.20 on test (â‰ˆ11â€“12% relative improvement).  
    - With Pileâ€‘train + RedPajama (1.8T tokens), `Llamaâ€‘2â€‘70B` drops below perplexity 4.0 (to ~3.96â€“3.95), and `Llamaâ€‘2â€‘13B + âˆâ€‘gram` beats `Llamaâ€‘2â€‘70B` (Table 1 bottom block).  
    - GPTâ€‘2 family benefits more than GPTâ€‘Neo/J, consistent with GPTâ€‘Neo/J training on The Pile (closer to the `âˆâ€‘gram` reference), highlighting the value of complementary data distributions (Â§5.2).
  - On SILO (permissively licensed data; 1.3B) across Wikipedia, Enron Emails, NIH ExPorters (Table 2):
    - Very large relative gains, especially inâ€‘domain or related domains. Example: on Enron Emails test, `SILOâ€‘PD` improves from 20.62 to 4.85 (73% improvement).  
    - Outperforms alternative nonparametric augmentations kNNâ€‘LM and RICâ€‘LM that use much smaller reference sets (45Mâ€“1.2B tokens) (Table 2 notes).
  - Timeâ€‘shifted evaluation (new Wikipedia pages after Pile/RPJ cutoff; Table 5):
    - Simple global interpolation improves in 4/5 months by 3â€“6% (sometimes 0%).  
    - Instanceâ€‘wise Î» via a Random Forest (features: suffix lengths and counts) yields stronger gains, 3â€“20% (Table 5, right column), supporting generalization beyond the referenceâ€‘data time window (Â§D.2).
  - Scaling ablation (Figure 10; Â§D.3):
    - Gains increase roughly logâ€‘linearly as the reference datastore grows (downsampling study).  
    - Using only inâ€‘domain reference data is about as strong as using the full Pile, indicating most benefit comes from inâ€‘domain textâ€”even after decontamination.

- Engine performance (Table 3; Â§A.5)
  - Subâ€‘200 ms per token for `âˆâ€‘gram` distributions on trillionâ€‘token corpora; counting any nâ€‘gram in ~20 ms with latency largely independent of n (tested up to n = 1000).

- Note on generation quality (Section â€œA note on text generationâ€ within Â§5.2)
  - Despite better perplexity, naive interpolation can harm openâ€‘ended generation because `âˆâ€‘gram` occasionally suggests offâ€‘topic tokens and can derail the output. This combined model is not yet a dropâ€‘in decoder replacement.

Overall, the experiments are broad (multiple model families, sizes, domains, and timeâ€‘shifted data) and include careful decontamination (Table 4), making the case that unbounded corpus statistics are both predictive and complementary. The diagnostics (Figures 3â€“5) also add qualitative insight beyond mere perplexity numbers.

## 6. Limitations and Trade-offs
- Zero probabilities and sparsity
  - `âˆâ€‘gram` assigns zero to unseen continuations; its standalone perplexity is undefined on typical test sets. The paper always evaluates perplexity for the interpolated model (Â§2; Â§5). This design requires tuning Î» (and Î»1/Î»2 for sparse/nonâ€‘sparse).

- Generation vs. scoring
  - While perplexity improves, naive decoding with `âˆâ€‘gram` interpolation can degrade openâ€‘ended generation (â€œodd mistakesâ€ that cause digressions; Â§5.2 note). More sophisticated integration is needed for generation.

- Data and tokenizer assumptions
  - The efficient token storage assumes vocabulary size < 65,536 (2 bytes/token). Separate indexes are needed per tokenizer (GPTâ€‘2/Neo/J vs. Llamaâ€‘2 vs. SILO; Â§5.1). Crossâ€‘tokenizer use is not covered.

- Build resources and storage
  - Building trillionâ€‘token suffix arrays needs large RAM during construction (e.g., 1 TiB for 1.4T tokens) and sizable disk (â‰ˆ7 bytes/token; 35 TB for 5T tokens; Â§3; A.1â€“A.3). Although serving is CPUâ€‘only and onâ€‘disk, organizations still need significant storage.

- Domain/language coverage
  - Most analyses center on English webâ€‘style corpora (The Pile subsets, RedPajama). Generalization to lowâ€‘resource languages or very different tokenization regimes is not studied.

- Diagnostic claims about model internals
  - The observed periodic fluctuations under greedy decoding (Figure 5) are hypothesized to relate to positional embeddings, but a causal link is not proven (Â§4.2).

- Decontamination nuances
  - Even with BFF (13â€‘gram, 80% threshold), defining and achieving perfect decontamination is hard (Â§4; Â§B). The paper follows accepted practice but acknowledges ambiguity (quotes vs. contamination).

## 7. Implications and Future Directions
- How this changes the landscape
  - Demonstrates that simple, interpretable, nonparametric signals from massive corpora can materially improve strong neural LMs and diagnose their behavior. This reâ€‘opens nâ€‘gram LMs as practical tools at modern scales.
  - Provides infrastructure (web UI, API, Python package) to query trillions of tokens for counts, probabilities, and document retrieval (Â§Abstract; Figures 11â€“16).

- Followâ€‘up research enabled/suggested
  - Better integration for decoding: learn contextâ€‘aware Î» or gating policies; fuse `âˆâ€‘gram` with neural decoders in ways robust to offâ€‘topic suggestions (Â§5.2 note).  
  - Investigate positionalâ€‘embedding or trainingâ€‘data effects underlying the agreement oscillations under greedy decoding (Figure 5; Â§4.2).  
  - Retrievalâ€‘augmented modeling at pretraining scale using exact nâ€‘gram retrieval instead of or alongside vector search (discussion Â§E; Table 6 comparisons).
  - Adaptive, instanceâ€‘wise interpolation (e.g., the Random Forest success on timeâ€‘shifted data; Table 5) and learning to predict â€œsparseâ€ cases that deserve high Î».

- Practical applications
  - Corpus understanding and curation: membership checks, contamination detection, removal of toxic/PII nâ€‘grams, and attribution via document lookup (`SEARCHDOC`; Â§E; Â§A.5; Figures 11â€“16).  
  - Reducing hallucinations by preferring corpusâ€‘attested continuations in factual settings (Â§E).  
  - Auditing memorization/plagiarism: measure long nâ€‘gram overlaps between generated text and training corpora (Â§E).  
  - Scaling retrieval for nonparametric LMs: `infiniâ€‘gram` provides a compact, exact index at webâ€‘scale (Table 6) that can complement or replace vectorâ€‘only stores in some pipelines.

> Bottom line: By making unbounded nâ€‘gram statistics cheap to query at trillionâ€‘token scale and showing they both predict next tokens and substantially lower perplexity when combined with neural LMs (Tables 1â€“2, Figure 4), this work elevates nâ€‘grams from a historical baseline to a modern, scalable component for analysis and modeling.

# DEMOCRATIZING OPEN AND COMPLIANT LLMS FOR GLOBAL LANGUAGE ENVIRONMENTS APERTUS V1 TECHNICAL REPORT

**ArXiv:** [2509.14233](https://arxiv.org/abs/2509.14233)

## ğŸ¯ Pitch

Apertus sets a new standard for transparency and accountability in large language models by releasing a fully open, end-to-end LLM suiteâ€”at both 8B and 70B scalesâ€”that combines rigorous data compliance (including retroactive robots.txt enforcement and PII/toxicity filtering), robust memorization mitigation, and truly global multilingual coverage (1,811 languages). By making not only weights but also all training data recipes, filtering code, and evaluation tools open and auditable, Apertus empowers researchers and developers worldwide to build LLMs that are both lawful and inclusiveâ€”addressing regulatory demands and opening access to high-quality AI for diverse language communities.

---

## 1. Executive Summary (2â€“3 sentences)
Apertus is a fully open suite of large language models (LLMs) released at 8B and 70B parameters that tackles two persistent gaps in the open-model ecosystem: lawful, auditable training data and strong multilingual coverage. It combines a compliance-first data pipeline (retroactive robots.txt optâ€‘outs, PII and toxicity filtering), a memorizationâ€‘mitigating pretraining objective (â€œGoldfish lossâ€), and broad multilingual pretraining/postâ€‘training (1,811 languages in pretraining; 149 in postâ€‘training), while releasing all artifactsâ€”weights, data preparation code, checkpoints, and evaluation suitesâ€”for full reproducibility (Section 1; Section 3; Section 4).

## 2. Context and Motivation
- Problem/gap addressed
  - â€œOpenâ€ models often mean open weights only, without transparent data pipelines; many include material that content owners forbid for AI training, and few models prioritize nonâ€‘English users (Section 1).
  - LLMs can memorize and regurgitate training text, carrying privacy and copyright risks; most demonstrations of mitigation are smallâ€‘scale (Section 1; Section 5.4).
- Why this matters
  - Regulatory compliance (e.g., EU AI Actâ€“style provisions) requires traceable, lawful data use and provable risk mitigation (Section 1, â€œData Complianceâ€).
  - Many communities operate in lowâ€‘ or midâ€‘resource languages; models that underperform outside English exclude these users (Section 1, â€œMultilingualityâ€).
- Shortcomings of prior approaches
  - Openâ€‘weight releases typically do not publish data recipes or legal filtering, making audits impossible (Section 1).
  - Memorization defenses are often postâ€‘hoc (e.g., safety tuning, constrained decoding) and reversible via fineâ€‘tuning or prompt attacks (Appendix F, â€œLimitations of postâ€‘hocâ€¦â€).
  - Multilingual efforts exist but usually cover far fewer languages and/or devote a small fraction of tokens to nonâ€‘English (Section 1; footnote 2).
- Positioning
  - Apertus frames itself as a â€œfully openâ€ alternative: it releases weights plus scripts, checkpoints, and evaluation harnesses; it enforces retroactive consent and targeted filtering; and it expands multilingual coverage and postâ€‘training alignment to 149 languages (Sections 1, 3, 4).
  - The 70B model is trained on 15T tokens at production scale while remaining fully auditable, which is rare among fully open efforts (Section 1, â€œScaleâ€; Figure 11; Section 6).

## 3. Technical Approach
This section explains how Apertus is built and trained, from architecture to data, training objectives, and alignment.

- Model architecture (Section 2.1; Table 1)
  - Dense decoderâ€‘only Transformers at two scales: `Apertusâ€‘8B` (32 layers) and `Apertusâ€‘70B` (80 layers), both using groupedâ€‘query attention (GQA) for inference efficiency and rotary positional embeddings (RoPE) with NTKâ€‘aware scaling for longâ€‘context extension.
  - Two stabilizing components:
    - `QKâ€‘Norm`: normalizes queries and keys in attention to control logit magnitude spikes (Section 2.1).
    - `xIELU` activation: a modified activation with trainable positive/negative branches; on the positive side it behaves like a smooth squareâ€‘root growth, on the negative side like a corrected ELU (Equation in Section 2.1). This reduces outliers while retaining expressivity.
  - Input/output embeddings are untied; documents are bracketed with begin/end tokens and attention across document boundaries is masked (Section 2.1).

- Tokenizer selection (Section 2.2; Figure 1)
  - Uses the `Mistralâ€‘Nemo v3 tekken` byteâ€‘BPE (131k vocab), chosen via four intrinsic metrics on FLORES+ (55 languages): fertility, compression ratio, vocabulary utilization, and a crossâ€‘language fairness measure (Gini coefficient). It offers competitive compression and lower inequity across languages than the alternatives compared (Figure 1).

- Training recipe (Section 2.3; Table 2)
  - Objective: `Goldfish loss` (define). Instead of training on every token, a small, deterministic subset of tokens per sequence (2%) is used to compute the loss, with a hash computed over the preceding 50 tokens to ensure the mask is reproducible but inputâ€‘dependent (Algorithm 1; Section 2.3; Appendix F). Intuition: by breaking the tight coupling between every contextâ€‘token pair and its next token, the model is less likely to recall long exact spans verbatim while still learning general patterns.
  - Optimizer: `AdEMAMix` (define). An Adamâ€‘style method that keeps an extra longâ€‘term exponential moving average (â€œslow momentumâ€) to better leverage old gradients during long training; warmâ€‘ups are used for the additional terms (Section 2.3; Appendix C).
  - Learning rate schedule: `Warmupâ€‘Stableâ€‘Decay (WSD)` with a 1â€‘sqrt cooldown tail, enabling continued training without reâ€‘warming and safer lateâ€‘stage convergence (Section 2.3).
  - Batch size doubled midway without changing LR, using the WSD plateau (Table 2; Figure 3), to improve hardware efficiency late in training.

- Data pipeline with compliance (Section 3)
  - Retroactive robots.txt (â€œwith hindsightâ€): crawl permissions as of Janâ€‘2025 are applied to all historical snapshots; if a site blocks major AI bots, its content is removed from 2013â€“2024 data (Appendix B; Table B.1 shows token reductions â‰ˆ8% in English, â‰ˆ4% multilingual; Tables B.2â€“B.3 list blocked bots and volumes).
  - PII removal via regex for emails, IPs, and IBAN; multilingual toxicity filtering with XLMâ€‘R encoders + languageâ€‘specific MLPs, removing the top 5% toxic documents per language for nine languages (Section 3.1.2â€“3.1.3; Figure 4 shows score distributions and thresholds).
  - Pretraining mixture:
    - `FineWebâ€‘2` across 1,811 languages as the base multilingual source; English highâ€‘quality slices from `FineWebâ€‘HQ` and `FineWebâ€‘Edu`; code from `StarCoderData`; math from `FineMath` and `MegaMath`; and parallel corpora (`EuroParl`, `ParaDocs`) for translation (Section 3.2; Figure 5).
    - Curriculum in five stages to gradually raise quality and increase math/code proportions (Section 3.3; Table 6). Stage choices were validated via cooldown experiments on smaller checkpoints (Table 7).
  - Longâ€‘context extension to 65,536 tokens:
    - RoPE base Î¸ increased across 8kâ†’16kâ†’32kâ†’64k phases; context parallelism used for memory scaling; data mixture enriched with â€œFineWebâ€‘Longâ€ (documents >4k) and `Institutional Books 1.0` (postâ€‘1900, OCRâ€‘cleaned) (Section 2.5; Section 3.4; Table 5; Table 8).

- Postâ€‘training for instruction following and alignment (Section 4)
  - Supervised fineâ€‘tuning (SFT): â‰ˆ4.18M examples across general instructions, math, code, and multilingual/conversational data (149 languages), after license filtering and decontamination (Section 4.1; Table 12). Romanshâ€”six idiomsâ€”receives dedicated coverage (Appendix J.1).
  - Preference alignment via `QRPO` (define). A directâ€‘alignment algorithm that optimizes absolute rewards using quantile ranks of completions sampled from a reference model (Section 4.3). Rewards come from (1) a pretrained reward model (`Skyworkâ€‘Rewardâ€‘V2`, Section 4.3.1) for standard topics and (2) an LLMâ€‘asâ€‘judge that scores adherence to the â€œSwiss AI Charterâ€ for ideologically sensitive prompts (Section 4.3.2; Appendix O).
    - Lengthâ€‘normalized QRPO is used (divide the KL regularizer by completion length), improving stability (Section 4.3).

- Infrastructure & engineering (Section 6)
  - Trained on up to 4,096 NVIDIA GH200 GPUs at CSCS with a vCluster setup enabling containerâ€‘first ML workloads and robust node vetting (Sections 6.1â€“6.3).
  - Throughput and stability were improved through systems fixes (driver/kernel patches, storage and NCCL/libfabric alignment), checkpointing strategy (Young/Dalyâ€‘guided), and distributed training tweaks (Figure 12; Section 6.3).
  - Estimated 6.74Ã—10^24 FLOPs to train the 70B on 15T tokens; â‰ˆ6M GPUâ€‘hours consumed (Section 6.2; Appendix E code for FLOPs).

## 4. Key Insights and Innovations
1) Compliance you can audit endâ€‘toâ€‘end (Section 3; Appendix B)
- Whatâ€™s new: retroactive application of robots.txt optâ€‘outs (â€œwith hindsightâ€) to all historical snapshotsâ€”data from sites blocking AI crawlers in Janâ€‘2025 is removed across the entire 2013â€“2024 period (Appendix B).
- Why it matters: enables legal and ethical reuse for downstream models and provides a clear audit trail; token impact is quantified (Table B.1).
- Distinct from prior work: most openâ€‘weight releases neither document nor implement such retroactive consent enforcement.

2) Memorization mitigation at scale with `Goldfish loss` (Sections 2.3, 5.4; Appendix F)
- Whatâ€™s new: a 70B model trained on 15T tokens with a loss that masks ~2% of tokens deterministically based on recent context, suppressing verbatim recall even after up to 128 exposures (Figure 8; Table 25).
- Why it matters: reduces copyright/privacy risks without sacrificing performance (Appendix F, Table F.5 shows downstream parity; Section 5.4 shows low Rougeâ€‘L/LCCS memory signals).
- Caveat: failure modes on ubiquitous texts that exist as many nearâ€‘duplicates (e.g., Shakespeare, US Constitution) due to hash fragility to formatting/tokenization changes (Section 5.4.2; Figure 9).

3) Multilingual breadth and targeted postâ€‘training (Sections 3.2, 4.1)
- Whatâ€™s new: pretraining spans 1,811 languages; postâ€‘training covers 149 languages, including Swiss Romansh idioms (Appendix J.1) and extensive conversational data (Table 12).
- Why it matters: Apertus performs strongly on multilingual cultural/knowledge benchmarks relative to fully open peers (Tables 15, 20) and achieves better Romanshâ†”German translation than `Llamaâ€‘3.3â€‘70Bâ€‘Instruct` (Table 24).

4) QRPO + Constitutional judging for sensitive topics (Section 4.3.2; Appendix O)
- Whatâ€™s new: instead of a single global reward model, the alignment uses an LLM judge prompted with the â€œSwiss AI Charterâ€â€”11 articles distilled from Swiss constitutional and civic values. A public survey shows high approval of these principles (Table 13).
- Why it matters: makes valueâ€‘laden alignment explicit, inspectable, and adaptable to a cultural context; integrates naturally with QRPOâ€™s absoluteâ€‘reward training.

5) Transparent, reproducible scaling recipe (Sections 2.4â€“2.6; 6)
- Whatâ€™s new: ablations show `xIELU + AdEMAMix + QKâ€‘Norm + WSD + Goldfish` lowers loss and gradient volatility; a reâ€‘run over OLMo2 data achieves similar loss with 30â€“46% fewer tokens in the first 20k steps (Table 4). Full training logs and checkpoints are released.
- Why it matters: provides a tested, efficient training blueprint for future fully open models.

## 5. Experimental Analysis
- Evaluation setup
  - Pretraining evaluation uses the lmâ€‘evaluationâ€‘harness in probabilistic mode (logâ€‘likelihood) for sensitivity during early training (Section 5.1), covering general understanding (ARC, HellaSwag, WinoGrande, XNLI, PIQA/XCOPA) and factual knowledge (MMLU, Globalâ€‘MMLU, INCLUDE v1/v2, CulturalBench, BLEnD, SwitzerlandQA) (Tables 14â€“15).
  - Postâ€‘training evaluation uses open generation with the same harness, spanning knowledge (MMLU, Globalâ€‘MMLU, TruthfulQA), instruction following (IFEval, Multiâ€‘IFEval), reasoning (BBH, DROP, ACPBench, GPQA, MLogiQA, MGSM), coding (HumanEval, MBPP), math (GSM8K, GSM8Kâ€‘Platinum, Hendrycksâ€™ Math, MathQA), cultural knowledge, and longâ€‘context (RULER) (Section 5.2; Tables 17â€“21, 23).
  - Memorization measured by Rougeâ€‘L and normalized longest common contiguous substring (LCCS) on injected Gutenberg probes across exposure frequencies and offsets; Typeâ€“Token Ratio (TTR) used as a degeneracy and filtering signal (Section 5.4; Figures 8â€“10; Table 25).
  - Safety assessed with BBQ (bias), HarmBench (harmful behavior elicitation), RealToxicityPrompts (subsamped with Llamaâ€‘Guardâ€‘3 classifier), and ToxiGen (implicit toxicity detection) (Section 5.5; Table 26); multilingual safety examined with LinguaSafe (Tables 27â€“28).

- Main quantitative results
  - Pretraining capability (Tables 14â€“15; Figure 7)
    > `Apertusâ€‘70B` achieves 67.5% macro on general language understanding (Table 14), leading fully open models and matching or surpassing several openâ€‘weight peers at comparable scale on some tasks (e.g., XCOPA 45.3%).  
    > On factual knowledge, `Apertusâ€‘70B` scores 58.9% macro; it is strong on INCLUDE (57.0% v1; 38.5% CulturalBench) and SwitzerlandQA (60.2%), outperforming fully open baselines like EuroLLMâ€‘9B (58.1% SwitzerlandQA) and OLMo2â€‘7B (52.5%) (Table 15).
  - Postâ€‘training (Tables 17â€“21)
    - Knowledge & commonsense: `Apertusâ€‘70Bâ€‘Instruct` achieves 63.4% macro across knowledge tasks, with 69.6% on MMLU and 78.1% on HellaSwag; this trails top openâ€‘weight models (`Llamaâ€‘3.3â€‘70Bâ€‘Instruct` 68.4% macro, MMLU 87.5%; Table 17) but is competitive with fully open baselines.
    - Coding & math: Results are mixed. `Apertusâ€‘70Bâ€‘Instruct` scores 73.0% pass@10 on HumanEval and 77.6% on GSM8K, but its Hendrycksâ€™ Math score (30.8%) lags models that likely used RL with verifiers (Table 18).
    - Reasoning & instruction following: `Apertusâ€‘70Bâ€‘Instruct` reaches 61.8% macro across BBH/DROP/ACP/IFEval, solid but behind the best openâ€‘weight systems (`Qwen3â€‘32B` 80.8% macro) (Table 19).
    - Cultural knowledge: Both Apertusâ€‘Instruct models are strong among fully open models, with `Apertusâ€‘70Bâ€‘Instruct` scoring 61.5% macro; SwitzerlandQA 67.2% (Table 20).
    - Heldâ€‘out tests: `Apertusâ€‘70Bâ€‘Instruct` achieves 51.4% macro across AGIeval, ARCâ€‘Challenge Chat/Multilingual, GPQA, GSM8Kâ€‘Platinum, and MLogiQA; `OLMoâ€‘2â€‘32Bâ€‘Instruct` is higher at 58.3% (Table 21).
  - Long context (Table 23)
    > `Apertusâ€‘70Bâ€‘Instruct` scores 94.8/89.9/85.7/81.9 on RULER at 4k/8k/16k/32k contexts; evaluation at 64k was runtimeâ€‘limited. Scores are competitive but below `Llamaâ€‘3.3â€‘70Bâ€‘Instruct` (95.2/94.7/94.8/93.7).
  - Lowâ€‘resource translation (Table 24)
    > On WMT24++ Romanshâ†”German, `Apertusâ€‘70Bâ€‘Instruct` beats `Llamaâ€‘3.3â€‘70Bâ€‘Instruct` in all six Romansh variants in both directions (e.g., Rumantsch Grischun DEâ†’RM: 27.8 vs 21.6 BLEU).
  - Memorization (Section 5.4; Figures 8â€“10; Table 25)
    - Across 1â€“128 exposures and 50â€“5,000â€‘token prefixes, Rougeâ€‘L stays â‰ˆ0.17â€“0.19 (baseline level), showing no scalable verbatim recall under greedy or nucleus sampling; TTR remains high under nucleus sampling (â‰ˆ0.50), confirming mitigation is not an artifact of degeneration (Table 25; Figure 8).
    - Failure mode: nearâ€‘duplicate canonical texts across the web can escape masking alignment and show higher recall (Figure 9); lowâ€‘diversity templates (tables, lists) yield high Rougeâ€‘L without privacy/copyright risk (Figure 10).
  - Safety (Table 26; Tables 27â€“28)
    - RealToxicityPrompts (Llamaâ€‘Guardâ€‘3 subsample): very low average toxicity score (0.2), competitive with openâ€‘weight models.
    - HarmBench: higher harm rates than the very best models, especially under human jailbreaks (e.g., 36.2 for `Apertusâ€‘70Bâ€‘Instruct` vs 10.1 for `Qwen2.5â€‘72Bâ€‘Instruct`), indicating room for stronger guardrails.
    - BBQ/ToxiGen: midâ€‘tier performance; multilingual safety (LinguaSafe) shows nonâ€‘trivial harm scores, highlighting inherent difficulty across languages.

- Ablations and robustness (Section 2.4; Table 3; Figure 2)
  - On a 1.5B/3B setting, each design element improves stability or loss: `AdEMAMix` and `xIELU` provide the largest singleâ€‘changes; the combined recipe matches baseline loss with 30â€“40% fewer tokens (Figure 2; Table 3).
  - Replicating OLMo2â€™s early training with identical data, the Apertus recipe achieves similar loss with 30â€“46% fewer tokens (Table 4).

- Do results support the claims?
  - Yes on the core claims: demonstrable compliance pipeline; memorization mitigation at 70B scale; strong multilingual outcomes and Romansh translation; full transparency of artifacts.
  - Performance is competitive but not stateâ€‘ofâ€‘theâ€‘art on math/reasoning/coding compared to top openâ€‘weight models that apply heavier reinforcement learning and verifier pipelines (Tables 18â€“19), which the paper explicitly lists as future work (Section 7).

## 6. Limitations and Tradeâ€‘offs
- Compliance scope and coverage
  - Robots.txt retroactivity enforces consent, but legality can involve more than crawler directives (licenses, database rights); toxicity filtering covers only nine languages during pretraining (Section 3.1.3).
  - Postâ€‘training license filtering and decontamination measurably reduce benchmark scores in some settings (e.g., MMLU CoT: 0.513â†’0.253 when licenseâ€‘filtering Tulu3; Table 10), illustrating a real complianceâ€‘vsâ€‘capability tradeâ€‘off.
- Memorization defense boundaries
  - Goldfish loss can miss nearâ€‘duplicates because hash decisions differ with minor formatting/tokenization changes (Section 5.4.2). Highâ€‘frequency canonical texts remain a risk area.
- Capability tradeâ€‘offs
  - Math and coding lag behind leaders that applied RL with verifiers (Table 18); instructionâ€‘following and reasoning are solid but not bestâ€‘inâ€‘class (Tables 17â€“19).
- Computational cost and practicality
  - â‰ˆ6M GPUâ€‘hours; â‰ˆ5 GWh estimated energy on 4,096 GH200s over ~90 days for a full run (Section 6.2). While Alps is hydroâ€‘powered, not every lab can replicate this.
- Safety guardrails
  - HarmBench shows notable vulnerability to jailbreaks (Table 26); multilingual safety remains unconquered (Tables 27â€“28). Paper acknowledges that jailbreak resistance cannot be guaranteed for open weights and should be handled in deployment (Section 5.5.1).

## 7. Implications and Future Directions
- Field impact
  - Sets a new bar for â€œfully openâ€ releases: transparent, auditable data/process; memorization risk quantified and reduced at 70B scale; and serious multilingual commitment. This makes Apertus a practical baseline for regulatoryâ€‘grade AI development and multilingual research.
- Enabled research
  - Data governance science: with released scripts and filters, the community can measure how specific data slices and compliance steps affect capability, fairness, and memorization (Section 7, â€œDataâ€‘toâ€‘performance mappingâ€).
  - Memorization & privacy: the Goldfish framework plus FMâ€‘Probes (Appendix F; Section 3.2.4) invites deeper tests, especially on nearâ€‘duplicate robustness and alternative masking designs.
  - Alignment methods: QRPO with constitutional judges opens a pathway to culturally scoped alignment objectives and humanâ€‘inâ€‘theâ€‘loop evaluations (Section 4.3.2; Appendix O).
- Practical applications
  - Publicâ€‘sector and enterprise deployments requiring data provenance and auditability; multilingual assistants in government, education, and healthcare; localization for underâ€‘served languages (Romansh results, Table 24).
- Nearâ€‘term directions proposed in the paper (Section 7)
  - Scaling (larger or longerâ€‘context models), distillation for deployment efficiency, RL with verifiers for math/code (`RLVR`), adaptive testâ€‘time compute, multimodal extensions with the same compliance standards, broader preference elicitation in Swiss/multilingual populations, and field evaluations with professionals.

> â€œApertusâ€‘70B is trained on 15T tokens with retroactive robots.txt optâ€‘outs, toxicity/PII filtering, and Goldfish loss; performance is strong on multilingual knowledge and competitive overall, while memorization remains at baseline across exposures.â€ (Sections 1â€“5; Tables 14â€“15, 20; Figures 8â€“10)

> â€œAll artifactsâ€”training code, data preparation, checkpoints, evaluation harnessâ€”are released for audit and extension.â€ (Section 1, â€œTransparencyâ€)

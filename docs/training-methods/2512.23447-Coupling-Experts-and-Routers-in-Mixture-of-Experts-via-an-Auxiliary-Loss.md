# Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss

**ArXiv:** [2512.23447](https://arxiv.org/abs/2512.23447)

## ğŸ¯ Pitch

The paper introduces expert-router coupling (ERC) loss, a lightweight auxiliary loss that uses perturbed router embeddings as proxy tokens to probe all experts and enforce margin-style constraints so router embeddings and expert activations become mutually consistent. This tightly aligns routing decisions with expert capabilities, improving downstream accuracy of MoE LLMs (3Bâ€“15B) while adding only ~0.2â€“0.8% training overhead and no inference cost, making MoEs both more effective and practically efficient.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces `expert-router coupling (ERC) loss`, a lightweight auxiliary loss for Mixture-of-Experts (MoE) language models that explicitly aligns the routerâ€™s decisions with what each expert can actually do. The key idea is to treat each router embedding as a â€œproxy tokenâ€ for the tokens routed to that expert, probe all experts with these proxies, and enforce a pair of margin-like constraints so experts and router embeddings become mutually consistent (Figure 1, Eq. (1)â€“(3)). Empirically, adding `L_ERC` improves downstream accuracy for 3Bâ€“15B MoE-LLMs while adding only ~0.2â€“0.8% training overhead and no inference overhead (Figure 3, Table 1, Appendix B.2).

## 2. Context and Motivation
- **Problem / gap addressed:**  
  In standard MoEs, the `router` (a linear classifier/gating network) decides which expert MLPs process each token, but there is **no explicit constraint** ensuring that:
  1) the routerâ€™s expert â€œpreferencesâ€ reflect true expert capabilities, and  
  2) experts specialize in the tokens they are actually assigned.  
  The paper argues this weak coupling causes â€œmisroutedâ€ tokens and gradients that interfere with expert specialization (Introduction).

- **Why this matters:**  
  MoE is a core architecture for scaling LLM parameters efficiently by activating only `K` experts per token (Introduction, Background Â§2). If routing is mismatched to capabilities, MoEâ€™s conditional computation can underperform despite large parameter counts.

- **Prior approaches and limitations (as framed here):**
  - **Denser-activation coupling methods** (e.g., approaches that incorporate all expertsâ€™ activations in routing) improve coupling but are computationally/memory expensive because they require evaluating many experts per token (Introduction; Â§2 background discussion).
  - **Autonomy-of-Experts (`AoE`)** computes routing scores using per-expert intermediate activation norms, effectively â€œencoding routing into expert parametersâ€ (Background Â§2, Figure 2).  
    - Strength: routing is tied to expert responses via norms.  
    - Weakness: overhead scales with the number of tokens and worsens with larger number of experts `n` or smaller `K` (Background Â§2), making scaling difficult (Experiments Â§4.2, Â§4.3; Appendix B.1/B.2).
  - **Training-time dense activation supervision** (Pham et al. [28] as described) uses expertsâ€™ outputs to supervise router logits but requires fully dense activation during training, contradicting MoE sparsity (Background Â§2). The paper does not use it as a baseline.

- **Positioning of this paper:**  
  The paper targets the same coupling goal as AoE-like methods (router decisions reflect expert capability) but aims to do so with an auxiliary loss whose cost is **independent of the number of tokens** (Method Â§3 design principles; Â§3.3).

## 3. Technical Approach
### 3.1 Reader orientation (approachable technical breakdown)
- The system is a standard sparse MoE Transformer where each MoE layer has multiple expert MLPs and a router that picks the top-`K` experts per token.
- The solution adds a training-only auxiliary loss, `L_ERC`, that probes experts using perturbed router embeddings as proxy inputs and pushes router embeddings and experts to become consistent, without requiring sending every training token through every expert.

### 3.2 Big-picture architecture (diagram in words)
- **Inputs:** token representations `x âˆˆ R^d` flowing through a Transformer.  
- **Standard MoE path:**  
  `x â†’ router logits (x R^T) â†’ softmax weights w â†’ top-K selection â†’ selected experts E_i(x) â†’ weighted sum output`.
- **ERC auxiliary path (training only):**  
  router matrix `R` â†’ make perturbed proxy tokens `RÌƒ[i]` â†’ feed each `RÌƒ[i]` into each expertâ€™s input projection (`W_g`) â†’ compute activation norm matrix `M[i,j]` â†’ compute `L_ERC` enforcing two cross-expert inequalities (Figure 1; Method Â§3.1).
- **Total training loss:** main LM loss + load balancing loss + (weight 1) `L_ERC` (Experiments Â§4.1).

### 3.3 Roadmap for the deep dive
- First, define the baseline MoE layer and router computation (Background Â§2).
- Then explain the â€œrouter-as-cluster-centersâ€ interpretation that motivates proxy tokens (Method Â§3.1).
- Next, walk through ERCâ€™s three steps: proxy generation via bounded noise, building the `M` matrix, and the two coupling constraints / hinge penalties (Figure 1, Eq. (1)â€“(3); Â§3.2).
- Finally, cover efficiency: why this is `O(n^2)` per layer per step (not `O(TÂ·n)`), and what overhead the paper measures (Method Â§3.3; Appendix B).

### 3.4 Detailed, sentence-based technical breakdown
This is an **algorithmic + empirical auxiliary-loss contribution**: it adds a specific training objective to standard MoE layers so routing decisions become consistent with measured expert responsiveness, while keeping sparse activation during normal forward passes.

#### Baseline MoE layer (what ERC is added to)
- An MoE layer contains `n` expert MLPs and a router with weight matrix `R âˆˆ R^{nÃ—d}` (Background Â§2).
- For a token representation `x âˆˆ R^d`, the router produces expert weights:
  - `w = softmax(x R^T) âˆˆ R^n` (Background Â§2).
- Only the top-`K` experts are selected per token (Background Â§2). The final MoE output is the weighted sum of selected expertsâ€™ outputs:
  - `âˆ‘_{k âˆˆ Top-K(w)} w[k] E_k(x)` (Background Â§2).
- Each expert `i` is an MLP of SwiGLU form parameterized by three matrices:
  - `W_g^i âˆˆ R^{dÃ—D}`, `W_p^i âˆˆ R^{dÃ—D}`, `W_o^i âˆˆ R^{DÃ—d}` (Background Â§2),
  - `E_i(x) = (SiLU(x W_g^i) âŠ™ (x W_p^i)) W_o^i` (Background Â§2),
  where `âŠ™` is elementwise multiplication.

#### Core motivation: router embeddings as cluster centers (and why that helps)
- The paper interprets routing as a **clustering-like process** where each router row `R[i]` behaves like a cluster center in `R^d` (Method Â§3.1).
- Under this view, tokens routed to expert `i` form a set `X_i`, and `R[i]` is treated as a representative embedding (proxy) for that token cluster (Figure 1; Â§3.1).
- The advantage of this view is computational: instead of feeding *every token* to *every expert* to see which experts respond, the method probes experts using only `n` proxy inputs (one per expert), producing `n^2` expert-proxy responses total (Method Â§3.1).

#### ERC Step (1): Create proxy tokens with bounded multiplicative noise
- For each expert `i`, ERC constructs a perturbed proxy token:
  - `RÌƒ[i] = R[i] âŠ™ Î´_i` (Method Â§3.1),
  where `Î´_i` is random multiplicative noise.
- Noise distribution: `Î´_i âˆ¼ U(1 âˆ’ Îµ_i, 1 + Îµ_i)^d` (Method Â§3.2), meaning each dimension is uniformly perturbed within a per-expert bound.
- Why perturb at all: using only the clean `R[i]` risks overfitting the coupling to that single point rather than generalizing to the actual cluster `X_i` (Appendix C.2).
- Why noise must be bounded: to keep `RÌƒ[i]` inside the â€œregionâ€ of tokens associated with `R[i]` (Method Â§3.2).
- How the paper bounds `Îµ_i`:  
  Let `j` be the nearest other router center to `R[i]` in Euclidean distance. The paper sets:
  - `Îµ_i â‰¤ ||R[i] âˆ’ R[j]|| / (2 ||R[i]||)` (Eq. (4); Method Â§3.2),  
  and uses the maximum value (the RHS), recomputed dynamically â€œat each layer and every training stepâ€ (Â§3.2).  
  Appendix A derives this as a conservative bound ensuring the perturbed point stays closer to `R[i]` than the nearest other center.

> Important implementation detail: the perturbed `RÌƒ` is used **only** for computing `L_ERC`; routing still uses the clean `R` to compute logits, as in vanilla MoE (Method Â§3.1).

#### ERC Step (2): Probe all experts with all proxy tokens and build an activation matrix `M`
- ERC feeds each proxy token `RÌƒ[i]` through each expertâ€™s input projection `W_g^j` and measures the `L2` norm of the resulting intermediate activation (Method Â§3.1; Figure 1).
- Concretely, the paper defines:
  - `M[i, j] = || RÌƒ[i] Â· W_g^j ||` (Method Â§3.1),
  producing `M âˆˆ R^{nÃ—n}`.
- Intuition: intermediate activation norms are treated as a signal of â€œhow well the expertâ€™s capabilities align with the inputâ€ (motivated by prior work cited as [9, 20, 23] in the paper; described in Â§3.1 and Background Â§2 for AoE).

#### ERC Step (3): Enforce two coupling constraints via a hinge-style auxiliary loss
ERC imposes *two* constraints for every pair `i â‰  j`, controlled by a scalar `Î± âˆˆ [0,1]` (Method Â§3.1):

1) **Row-wise (â€œexpert specializationâ€) constraint**  
   - `M[i, j] < Î± M[i, i]` for all `j â‰  i` (Eq. (1)).  
   - Meaning in plain language: when you feed expert `i`â€™s proxy token, expert `i` should respond more strongly than any other expert.

2) **Column-wise (â€œprecise token routing / faithful router embeddingâ€) constraint**  
   - `M[j, i] < Î± M[i, i]` for all `j â‰  i` (Eq. (2)).  
   - Meaning in plain language: expert `i` should respond more strongly to *its own* proxy than to other expertsâ€™ proxies, so `R[i]` actually represents expert `i`â€™s capabilities.

- These are implemented as a hinge penalty (Eq. (3)):
  - `L_ERC = (1/n^2) âˆ‘_{i=1}^n âˆ‘_{jâ‰ i} [ max(M[i,j] âˆ’ Î±M[i,i], 0) + max(M[j,i] âˆ’ Î±M[i,i], 0) ]` (Method Â§3.1; Figure 1).
- Role of `Î±`:  
  - Smaller `Î±` makes the inequalities stricter, enforcing stronger separation between diagonal and off-diagonal entries in `M` (Method Â§3.1; Â§4.4).
  - The paper frames this as explicit control of the specialization level.

##### A tiny worked micro-example (illustrative, matching Eq. (1)â€“(3))
Assume `n = 3` experts, and you computed:

- Diagonal responses: `M[1,1]=10`, `M[2,2]=8`, `M[3,3]=9`
- Off-diagonal responses include `M[1,2]=9`, `M[2,1]=7`, etc.
- Choose `Î± = 0.8`. Then for `i=1`, `Î±M[1,1]=8`.
  - The term `max(M[1,2] âˆ’ Î±M[1,1], 0) = max(9 âˆ’ 8, 0)=1` penalizes that expert 2 responds too much to proxy 1 (violating Eq. (1)).
  - The term `max(M[2,1] âˆ’ Î±M[1,1], 0) = max(7 âˆ’ 8, 0)=0` does not penalize Eq. (2) for that pair.
Minimizing `L_ERC` pushes the matrix toward having large diagonals and sufficiently smaller off-diagonals (relative to each diagonal).

#### Why `n^2` is the key efficiency lever (and what is actually computed)
- ERC computes `n^2` intermediate activations per MoE layer per step, each roughly a `1Ã—d` by `dÃ—D` matrix multiply plus a norm (Method Â§3.3; Appendix B.1).
- This is explicitly designed to be independent of:
  - batch size / number of tokens `T` (often millions), and
  - sparsity selection count `K` (Method Â§3, Â§3.3).

#### Efficiency analysis (as given in the paper)
- **Standard MoE compute:** For `T` tokens and `K` selected experts, total cost is stated as `6 T K D d` FLOPs (Method Â§3.3; Appendix B.1).
- **ERC extra compute:** adds `2 n^2 D d` FLOPs (Method Â§3.3; Appendix B.1).
- **AoE overhead (contrast baseline):** extra `2 T (n âˆ’ K) d r` FLOPs, where `r` is AoE factorization rank (Method Â§3.3; Appendix B.1).
- **Measured overhead:** ERC adds only `0.2â€“0.8%` overhead in experiments (Method Â§3.3). Appendix B.2 gives a concrete large-scale example where throughput drops from `62.03B tokens/day` (baseline) to `61.52B tokens/day` (ERC), i.e. `0.82%`.

#### Implementation reference in the paper
- Figure 7 provides PyTorch-style pseudocode covering:
  - computing `Îµ_i` from pairwise distances `cdist(R, R)`,
  - sampling bounded multiplicative noise to form `RÌƒ`,
  - computing `M` via an `einsum` over expert `W_g` and `RÌƒ`,
  - computing row/column hinge penalties and masking the diagonal.

## 4. Key Insights and Innovations
- **(1) Proxy-token probing of expert capabilities via router embeddings (`R[i]`)**  
  - Novelty: Instead of probing experts with *all tokens* (token-dependent cost), the method probes with only `n` learned router embeddings treated as cluster centers (Method Â§3.1; Figure 1).  
  - Significance: This makes coupling supervision scale as `O(n^2)` per layer per step, independent of batch token count.

- **(2) Two-sided coupling constraints (row-wise + column-wise) rather than a single direction**
  - Many coupling ideas focus on â€œexpert responds strongly to assigned tokens.â€ ERC adds the symmetric requirement â€œrouter embedding faithfully represents the expert,â€ operationalized by the second inequality (Eq. (2)).  
  - Significance: The paper argues these jointly reduce both mis-specialization and mis-routing (Method Â§3.1).

- **(3) Bounded multiplicative noise as a mechanism to generalize from a single center to a token cluster**
  - Innovation: `RÌƒ[i] = R[i] âŠ™ Î´_i` with `Îµ_i` computed from nearest-center distance and `||R[i]||` (Eq. (4); Method Â§3.2; Appendix A).  
  - Significance: The ablation removing `Î´` â€œgreatly degrades performanceâ€ (Appendix C.2; Figure 6 C.2), supporting that the loss must generalize beyond exact `R[i]`.

- **(4) Specialization control and tracking through `Î±` and the induced noise bound `Îµ`**
  - The paper positions `Î±` as a knob controlling specialization strength (Method Â§3.1; Â§4.4) and claims `Îµ` correlates with specialization during training (Figure 5(a)).  
  - This is presented as a practical research tool: you can both *control* specialization (via `Î±`) and *monitor* it quantitatively (via `Îµ`) (Â§4.4).

## 5. Experimental Analysis
### Evaluation methodology
- **Baselines compared:**  
  - `vanilla MoE` (standard router + sparse expert activation)  
  - `MoE + L_ERC` (same MoE plus ERC auxiliary loss)  
  - `AoE` (Autonomy-of-Experts; Background Â§2; Figure 2) for the 3B setting (Experiments Â§4.1).
- **Model/training setup (3B experiment):** (Experiments Â§4.1)
  - Architecture base: implementation based on `OLMoE` [25].
  - Transformer layers: `12` layers.
  - Model dimension: `d = 1536`.
  - Expert hidden dimension: `D = 768`.
  - Attention heads: `16`.
  - Experts per MoE layer: `n = 64`.
  - Experts selected per token: `K = 8`.
  - Activated parameters: `500M` (with total model size 3B).
  - AoE factorization rank: `r = 512` (chosen to keep parameter count consistent).
  - Training data: `500B` tokens from `dolmap-v1.5-sample`.
  - Batch size: `3 million` tokens.
  - Optimizer: `AdamW` with `(Î²1, Î²2) = (0.9, 0.95)`, weight decay `0.1`.
  - Learning rate: `4e-4` cosine schedule decaying to `4e-5`.
  - Load balancing loss: applied in all experiments with weight `0.01`.
  - ERC settings: ERC loss weight fixed at `1`; `Î± = 1` by default if not specified.
- **Downstream evaluation tasks (3B):** (Experiments Â§4.1)
  - ARC-Challenge, CommonsenseQA, COPA, BoolQ, HellaSwag, OpenbookQA, SciQ, Social IQa, WinoGrande, and MMLU.
  - Reported metric: accuracy (Figure 3(a), Figure 8).

### Main quantitative results (what is explicitly reported)
- **3B performance trend:**  
  - Figure 3(a) shows `MoE + L_ERC` achieves stable average accuracy gains over vanilla MoE across training (300Bâ€“500B tokens shown), and â€œnarrows the gapâ€ to AoE (Experiments Â§4.2).  
  - Task-specific curves are in Figure 8, but the paper excerpt does not provide a table of exact 3B accuracies at a specific checkpoint, so only qualitative comparisons can be stated without guessing.

- **Load balancing compatibility:** (Experiments Â§4.2; Figure 3(b))
  - Difference in load balancing loss between `MoE + L_ERC` and vanilla MoE is on the order of `10^-5`, while the overall load balancing loss magnitude is around `10^-2`.
  - AoE vs vanilla MoE load balancing loss difference is about `4 Ã— 10^-4`.

- **Efficiency comparison (3B setting):** (Experiments Â§4.2)
  - The paper states MoE with and without ERC have â€œnearly identical throughput and memory costs.â€
  - AoE requires `1.6Ã—` more training hours and `1.3Ã—` higher memory usage, limiting scaling.

- **Scaling result at 15B parameters:** (Experiments Â§4.3; Table 1)
  - Configuration changes: increase experts to `n = 256` (keep `K = 8`) and double model depth, yielding `15B` total parameters with ~`700M` activated (Experiments Â§4.3).
  - AoE is omitted at 15B because it â€œfailed to train due to overly costly.â€
  - Table 1 reports consistent improvements from adding `L_ERC`:

  > **Table 1:** `MoE` â†’ `MoE + L_ERC`  
  > - MMLU: `63.2` â†’ `64.6` (+`1.4`)  
  > - C-Eval: `67.5` â†’ `69.0` (+`1.5`)  
  > - MMLU-Pro: `31.0` â†’ `31.9` (+`0.9`)  
  > - AGI-Eval: `42.0` â†’ `44.2` (+`2.2`)  
  > - BBH: `44.3` â†’ `45.6` (+`1.3`)  
  > - MATH: `25.7` â†’ `26.1` (+`0.4`)  
  > - GSM8K: `45.2` â†’ `45.8` (+`0.6`)  
  > - TriviaQA: `47.2` â†’ `49.1` (+`1.9`)

  - The paper also notes no loss spikes or abnormal gradients during this large-scale training (Â§4.3).

### Ablations and diagnostic experiments (what they support)
- **Which activation to use for `M[i,j]`:** Appendix C.1 tests five options; `||RÌƒ W_g||` is best and becomes the default. Using final outputs is comparable but higher cost (Appendix C.1).
- **Importance of noise `Î´`:** Removing `Î´` degrades performance substantially (Appendix C.2; Figure 6 C.2).
- **Router-only orthogonalization is insufficient:** Router orthogonalization yields limited gains compared to ERC (Appendix C.3; Figure 6 C.3), and baseline routers are already near-orthogonal in that setup (average absolute cosine similarity `0.15`, corresponding to ~81Â°â€“99Â° angles as computed in Appendix C.3).
- **Effect of loosening constraints (`Î± > 1`):** Increasing `Î±` to `2` gives only limited improvement; `Î± = 3` yields almost no improvement over vanilla MoE (Appendix C.4; Figure 6 C.4).
- **Ruling out trivial norm manipulation:** Appendix C.5 argues scaling norms reduces some terms but increases others; Table 3 shows similar average norms with/without ERC while ERC loss values drop to `0.00` under `+L_ERC` but remain nonzero for baseline (Table 3).

### Do experiments support the claims?
- **Coupling improves downstream performance:** Supported qualitatively for 3B (Figure 3(a), Figure 8) and quantitatively for 15B (Table 1).
- **Negligible overhead:** Supported both by theory (Method Â§3.3; Appendix B.1) and measured throughput drops in Appendix B.2 (e.g., 0.82% in one 15B distributed setting).
- **Specialization increases:** Supported by parameter visualization via t-SNE on `W_g` rows (Figure 4) and the `Î±`/`Îµ` tracking experiments (Figure 5). Note that these are indirect proxies for specialization (see Limitations).

## 6. Limitations and Trade-offs
- **Assumption about router embedding norms:**  
  The clustering interpretation relies on router rows `R[i]` having â€œcomparable normsâ€ so that inner products correspond to cluster assignments; the paper states it verifies this holds in their experiments (Method Â§3.1, footnote 2), but this is an assumption that may not universally hold.

- **`O(n^2)` scaling with number of experts:**  
  ERC cost is independent of token count `T`, but it grows quadratically with `n` via `n^2` proxyâ€“expert evaluations (Method Â§3.1; Â§3.3). For very large `n`, this could become non-negligible (the paperâ€™s measurements cover `n=64` and `n=256`).

- **Specialization vs collaboration trade-off:**  
  The paper shows that â€œextreme specialization is not advisableâ€ and performance can degrade with overly strict `Î±` (Figure 5(b); Â§4.4). This creates a hyperparameter tuning burden: optimal `Î±` depends on `(n, K)` and other architectural choices like shared experts (Â§4.4).

- **Indirect specialization measurements:**  
  The specialization claims use:
  - t-SNE clustering of `W_g` rows (Figure 4), and
  - the derived noise bound `Îµ` as a metric (Figure 5(a)).  
  These are informative but indirect; they do not directly measure, for example, token-to-expert semantic partitioning quality beyond downstream accuracy.

- **Scope of comparisons:**  
  At 15B, AoE is not compared because it failed to train due to cost (Â§4.3). This means the â€œnarrows the gap to AoEâ€ claim is demonstrated at 3B but not validated at higher scale in this paper.

- **What ERC couples (and what it does not):**  
  ERC couples router embeddings to experts via `W_g` intermediate activations by default (Appendix C.1). This does not explicitly constrain later expert computations (`W_p`, `W_o`) unless you choose alternative `M` definitions, and the paper notes output-based `M` is costlier.

## 7. Implications and Future Directions
- **How this changes the landscape (within the paperâ€™s scope):**
  - ERC provides a practical way to strengthen expertâ€“router alignment without abandoning sparse activation, addressing a key weakness of vanilla MoE training (Introduction; Method Â§3).
  - It also reframes router embeddings `R[i]` as interpretable â€œcluster centersâ€ that can be actively used to probe and regulate specialization (Â§3.1; Â§4.4).

- **Research directions suggested by the paper:**
  - Develop better quantitative metrics of what counts as â€œlargeâ€ or â€œsmallâ€ `n` and `K`, and automate selection of optimal specialization levels (discussed as open problems in Â§4.4).
  - Investigate interactions with architectural choices such as `shared experts` (not evaluated here but discussed in Â§4.4).

- **Practical applications / downstream use cases:**
  - Any sparse MoE-LLM pre-training setting where routing mismatch may limit performance, especially when token batch sizes are huge (millions), making token-dependent coupling methods expensive (Method Â§3; Â§3.3).

- **Repro/Integration Guidance (based on the provided paper):**
  - Start from a standard MoE with router matrix `R âˆˆ R^{nÃ—d}` and SwiGLU experts (Background Â§2).
  - Add ERC as a training-only auxiliary loss computed per MoE layer:
    1) compute bounded `Îµ_i` from nearest-center distances and `||R[i]||` (Eq. (4); Â§3.2; Figure 7),
    2) sample multiplicative noise `Î´_i` and form `RÌƒ[i]` (Method Â§3.2),
    3) compute `M[i,j] = ||RÌƒ[i] W_g^j||` (Method Â§3.1),
    4) compute `L_ERC` via Eq. (3) and add it to the training objective (Method Â§3.1).
  - Hyperparameter guidance given by the paper:
    - Use `Î± = 1` as a safe default that â€œeliminates expert decoupling and should provide some gainsâ€ (Â§4.4).
    - Then search lower `Î±` if you have larger `n` (e.g., their 15B setting finds an optimum around `Î± = 0.5` for `n=256, K=8`, while `Î± = 1` is optimal for `n=64, K=8` in their 3B setup; Â§4.4).
  - Do not use `Î± > 1` if your goal is coupling; the paper shows it loosens constraints and degenerates toward vanilla MoE behavior (Appendix C.4; Figure 6 C.4).
  - If you care about monitoring specialization, track the induced `Îµ` values over training, which the paper argues correlate with specialization when ERC is active (Figure 5(a); Â§4.4).

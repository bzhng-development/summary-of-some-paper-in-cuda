# Depth Anything 3: Recovering the Visual Space from Any Views

**ArXiv:** [2511.10647](https://arxiv.org/abs/2511.10647)

## üéØ Pitch

Depth Anything 3 (DA3) is a minimal, unified any‚Äëview geometry model that uses a single plain transformer backbone to predict per‚Äëview depth maps and per‚Äëpixel ray maps‚Äîenabling direct, spatially consistent 3D reconstruction from any number of images with or without known poses. By reducing targets to a depth‚Äìray representation and leveraging teacher‚Äìstudent training on large synthetic and real data, DA3 achieves state‚Äëof‚Äëthe‚Äëart pose and reconstruction accuracy, simplifying deployment across monocular, multi‚Äëview, and rendering tasks and accelerating downstream 3D vision applications (robotics, AR/VR, and novel‚Äëview synthesis).

---

## 1. Executive Summary (2-3 sentences)

Depth Anything 3 (DA3) is a unified ‚Äúany-view‚Äù geometry model that takes an arbitrary number of images (optionally with known camera poses) and predicts *spatially consistent* per-view depth plus a per-pixel `ray map`, enabling direct reconstruction of a shared 3D scene representation (Figure 1; Section 3.1). The core significance is that DA3 aims to replace specialized monocular depth / SfM / MVS-style networks with a *minimal* transformer-based predictor‚Äîusing a plain pretrained ViT backbone and a single depth‚Äìray target‚Äîwhile achieving strong pose and reconstruction accuracy on a newly introduced visual geometry benchmark (Tables 2‚Äì3; Section 6).

## 2. Context and Motivation

- **Problem / gap.**
  - Many 3D vision tasks differ mainly by *how many views* you have (single image vs. multiple images/video) and whether camera poses are known, yet models are typically designed per-task (Introduction, paragraphs 1‚Äì2).
  - ‚ÄúUnified‚Äù models exist, but DA3 highlights several practical issues:
    - They often use *bespoke, complex architectures*.
    - They often require *joint multi-task optimization from scratch*, which makes it hard to fully leverage large-scale pretrained backbones (Introduction, paragraph 1).

- **Why it matters.**
  - Recovering 3D structure supports robotics, mixed reality, and general spatial understanding (Introduction, paragraph 1).
  - A single model that works for `Nv = 1` (monocular) up through many views would simplify deployment and evaluation across settings (Introduction; Figure 1).

- **Prior approaches and shortcomings (as positioned here).**
  - Traditional SfM/MVS pipelines decompose the problem into matching ‚Üí pose estimation ‚Üí bundle adjustment ‚Üí dense stereo, but can be brittle under low texture/specularities/large viewpoint changes (Section 2, ‚ÄúMulti-view visual geometry estimation‚Äù).
  - Transformer-based feed-forward geometry models (e.g., DUSt3R-style point maps; VGGT-style multi-output models) improve scalability and simplify pipelines, but can struggle with (i) arbitrary numbers of views and (ii) redundancy/entanglement in prediction targets (Section 2; Section 3.1 ‚ÄúMinimal prediction targets‚Äù).

- **How DA3 positions itself.**
  - DA3 explicitly pursues **minimal modeling**:
    1) *A single plain transformer backbone* (e.g., ‚Äúvanilla DINOv2‚Äù) is sufficient (Introduction; Figure 2; Section 3.2).
    2) A *single depth‚Äìray representation* is a minimal but sufficient prediction target, reducing the need for multi-head, multi-task outputs (Section 3.1; Table 6).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- DA3 is a **single transformer-based dense predictor** that ingests one or many RGB images (and optionally camera poses) and outputs per-image `depth maps` and `ray maps` that are consistent across views (Figure 1; Section 3.1).
- It solves ‚Äúrecover the visual space from any views‚Äù by turning multi-view geometry into a **pixel-aligned prediction problem**, then fusing predicted geometry into point clouds / reconstructions (Figure 1; Section 6.1).

### 3.2 Big-picture architecture (diagram in words)

- **Inputs:** `Nv` images `{I_i}`; optionally camera parameters per view (intrinsics/extrinsics encoded as `(t, q, f)` in Section 3.1 / 3.2).
- **Backbone:** a pretrained ViT (e.g., `DINOv2`) that processes image patch tokens (Section 3.2; Figure 2).
- **Cross-view reasoning:** `input-adaptive cross-view self-attention` by rearranging tokens in selected layers so attention can mix information across views (Section 3.2).
- **Heads:**
  - `Dual-DPT head` outputs per-view `depth` and per-view `ray maps` (Figure 3; Section 3.2).
  - Optional lightweight `camera head` predicts `(t, q, f)` from per-view camera tokens (Section 3.1, last paragraph; Section 3.2).
- **Outputs:** `depth maps` + `ray maps` ‚Üí 3D points via `P = t + D * d` (Section 3.1), enabling point clouds / 3DGS applications (Section 5).

### 3.3 Roadmap for the deep dive

- I will explain:
  1) The **geometry formulation** and why DA3 uses `depth + ray` instead of explicit rotation matrices (Section 3.1; Eq. (1)‚Äì(2); Table 6).
  2) The **model architecture**, focusing on how a plain pretrained ViT is adapted to variable view counts (Section 3.2; Figure 2).
  3) The **prediction heads** (Dual-DPT and optional camera components) and what they output (Figure 3; Section 3.1‚Äì3.2).
  4) The **training pipeline**, including the teacher‚Äìstudent pseudo-label alignment and the loss terms (Section 3.3‚Äì3.4; Section 4; Eq. (3), (7), (8)).
  5) The **evaluation/benchmark pipeline** and what the reported metrics actually measure (Section 6; Tables 2‚Äì5).

### 3.4 Detailed, sentence-based technical breakdown

**Framing.** This is primarily an **empirical model + system design paper**: it proposes a minimal transformer-based architecture plus a depth‚Äìray prediction target, then demonstrates effectiveness via large-scale training, ablations, and new benchmarks (Introduction; Sections 3‚Äì7).

#### 3.4.1 Formulation: from images to 3D via depth and rays

- **Inputs and standard projection.**
  - The input is a set of images `I = {I_i}_{i=1..Nv}` with `I_i ‚àà R^{H√óW√ó3}` (Section 3.1).
  - Each view has depth `D_i ‚àà R^{H√óW}`, intrinsics `K_i`, and extrinsics `(R_i | t_i)` (Section 3.1).
  - A pixel `p = (u, v, 1)^T` can be mapped to a 3D point using depth and camera parameters (Section 3.1, first equations).

- **Why ‚Äúdepth‚Äìray‚Äù instead of direct pose.**
  - Predicting a valid rotation matrix `R` is hard because it must satisfy an *orthogonality constraint* (Section 3.1, ‚ÄúDepth-ray representation‚Äù).
  - DA3 avoids directly predicting `R` by predicting a **per-pixel ray map** that implicitly captures camera pose and projection.

- **Depth‚Äìray representation (core idea).**
  - For each pixel, DA3 defines a ray `r ‚àà R^6` as `(t, d)` where:
    - `t ‚àà R^3` is the ray origin (camera center in world coordinates),
    - `d ‚àà R^3` is the ray direction in world coordinates (Section 3.1).
  - The per-image ray map is `M ‚àà R^{H√óW√ó6}` storing `(t, d)` for all pixels (Section 3.1).
  - Importantly, DA3 **does not normalize** `d`, so its magnitude preserves projection scale (Section 3.1).
  - A 3D point is then computed by element-wise combination:
    - `P = t + D(u, v) ¬∑ d` (Section 3.1).
  - Micro-walkthrough (single pixel):
    1) Take pixel coordinates `p=(u,v,1)`.
    2) The model predicts depth `D(u,v)` and ray direction `d(u,v)` and origin `t(u,v)` via `M(u,v,:)`.
    3) Compute point `P(u,v) = t(u,v) + D(u,v) * d(u,v)`.
    4) Repeating over pixels yields a point set that can be fused into a point cloud (Section 3.1; Section 6.1).

- **Recovering camera parameters from a ray map (optional inference step).**
  - Camera center `t_c` is estimated by averaging ray origins across pixels (Eq. (1), Section 3.1).
  - Rotation and intrinsics are recovered by solving for a homography `H = K R` using a least-squares Direct Linear Transform objective:
    - `H* = argmin_{||H||=1} Œ£ || H p_{h,w} √ó M(h,w,3:) ||` (Eq. (2), Section 3.1),
    - followed by `RQ` decomposition of `H*` to get `K` and `R` (Section 3.1).
  - The paper notes this ray‚Üícamera recovery is computationally costly at inference, motivating a lightweight explicit camera head (Section 3.1, ‚ÄúMinimal prediction targets‚Äù).

- **Minimal prediction targets claim (supported by ablations).**
  - DA3 argues that `depth + ray` is a minimal sufficient set compared to alternatives like point maps or redundant multi-head targets (Section 3.1; Table 6 shows the comparative effect).

#### 3.4.2 Architecture: a single pretrained transformer that scales with view count

- **Backbone: ‚Äúsingle transformer‚Äù with no architectural specialization.**
  - DA3 uses a pretrained Vision Transformer backbone (example given: `DINOv2`) as the feature extractor (Section 3.2; Figure 2).
  - The paper emphasizes there are no architectural changes to the transformer blocks themselves; cross-view behavior comes from token handling (Figure 2 caption; Section 3.2).

- **Handling arbitrary numbers of views via input-adaptive attention.**
  - DA3 splits the `L` transformer blocks into:
    - `L_s` layers of within-image self-attention, then
    - `L_g` layers that alternate **cross-view** and within-view attention (Section 3.2).
  - The mechanism is implemented by **rearranging tokens** so the same self-attention operation can operate either per-view or across all views (Section 3.2).
  - The default configuration is `L_s : L_g = 2 : 1` with `L = L_s + L_g`, chosen as the best performance/efficiency trade-off in ablations (Section 3.2; Table 7 discussion points to attention strategy differences, including ‚ÄúFull Alt.‚Äù vs partial).

- **Camera conditioning injection (optional).**
  - Each view gets a `camera token c_i` prepended to its patch tokens (Section 3.2).
  - If camera parameters are available, a lightweight MLP `E_c` encodes `(f_i, q_i, t_i)` into `c_i = E_c(f_i, q_i, t_i)` (Section 3.2).
  - If camera parameters are not available, DA3 uses a shared learned token `c_l` as a placeholder (Section 3.2).
  - These camera tokens participate in attention, letting the model seamlessly handle posed/unposed settings (Figure 2; Section 3.2).

#### 3.4.3 Output heads: Dual-DPT for depth+ray and optional camera head

- **Dual-DPT head (for pixel-aligned depth and rays).**
  - DA3 predicts, for each input view, a depth map and a ray map aligned to the input image (Section 3.1; Figure 2).
  - The `Dual-DPT` head processes backbone features with:
    - shared **reassembly** modules, then
    - two separate sets of **fusion layers**, one for the depth branch and one for the ray branch,
    - followed by separate output layers (Figure 3; Section 3.2).
  - The design goal is that both predictions use the *same processed features* but differ in final fusion, encouraging interaction without duplicating intermediate representations (Section 3.2; Figure 3 caption).

- **Optional camera head `D_C`.**
  - DA3 can predict camera pose explicitly as `v = (t, q, f)` using a small transformer operating only on camera tokens (Section 3.1, last paragraph).
  - Because it processes one token per view, overhead is described as negligible; Table 6 discussion mentions ~0.1% of main backbone compute (Section 7.2.1 text following Table 6).

#### 3.4.4 Training: teacher‚Äìstudent supervision + joint losses for depth, rays, points, and gradients

- **Why teacher‚Äìstudent is used.**
  - Real-world depth sources can be noisy/incomplete (Figure 4; Section 3.3; Section 4.2).
  - DA3 trains a monocular ‚Äúteacher‚Äù on synthetic data to generate dense pseudo-depth for real datasets, then aligns it to available (sparse/noisy) metric depth to preserve geometric correctness (Section 3.3; Section 4.2; Eq. (8)).

- **Training objective for DA3 (student any-view model).**
  - The model `F_Œ∏` maps inputs to `{DÃÇ, RÃÇ, ƒâ}` (depth, ray map, optional camera output) (Section 3.3).
  - Before computing losses, all ground-truth signals are normalized by a common scale: the mean `‚Ñì2` norm of valid reprojected point maps `P` (Section 3.3). (The paper motivates this as stabilizing magnitudes across modalities.)
  - The overall loss is a weighted sum (Section 3.3):
    - `L = L_D(DÃÇ, D) + L_M(RÃÇ, M) + L_P(DÃÇ ‚äô d + t, P) + Œ≤ L_C(ƒâ, v) + Œ± L_grad(DÃÇ, D)`
    - with weights `Œ± = 1` and `Œ≤ = 1` (Section 3.3).
  - The gradient loss is explicitly (Eq. (3), Section 3.3):
    - `L_grad = ||‚àáx DÃÇ ‚àí ‚àáx D||_1 + ||‚àáy DÃÇ ‚àí ‚àáy D||_1`,
    - intended to preserve sharp edges while keeping planar regions smooth (Section 3.3).
  - Depth loss `L_D` uses an `‚Ñì1` term modulated by per-pixel confidence `D_c,p` and includes a `-Œª_c log D_c,p` regularizer (Section 3.3). (The value of `Œª_c` is not provided in the excerpted text.)

- **Training schedule and compute (DA3).**
  - Hardware/steps: trained on **128 H100 GPUs** for **200k steps** (Section 3.4).
  - LR schedule: **8k-step warm-up**, peak LR **`2 √ó 10^-4`** (Section 3.4).
  - Resolution strategy:
    - Base resolution `504√ó504` (chosen to be divisible by many factors for aspect ratios) (Section 3.4),
    - Training samples multiple resolutions: `504√ó504`, `504√ó378`, `504√ó336`, `504√ó280`, `336√ó504`, `896√ó504`, `756√ó504`, `672√ó504` (Section 3.4).
  - View count: for `504√ó504`, `Nv` is sampled uniformly from `[2, 18]` (Section 3.4).
  - Batch sizing: dynamically adjusted to keep token count per step roughly constant (Section 3.4).
  - Supervision transition: switches from ground-truth depth to teacher labels at **120k steps** (Section 3.4).
  - Pose conditioning is randomly activated with probability **0.2** during training (Section 3.4).
  - **Optimizer / ViT internal hyperparameters.**
    - The excerpt does **not** specify DA3‚Äôs optimizer type (e.g., AdamW) or standard ViT configuration details (layers, heads, hidden size) for the any-view model; it reports model scales via parameter counts in Tables 2‚Äì3 and speed in Table 8.

#### 3.4.5 Teacher model construction and pseudo-label alignment

- **Teacher architecture and target.**
  - The teacher is a monocular relative depth model using a `DINOv2` ViT backbone plus a DPT decoder, explicitly described as aligned with DA3‚Äôs ‚Äúplain transformer + DPT‚Äù philosophy (Section 4.1).
  - The teacher predicts **scale‚Äìshift-invariant depth** (not disparity), and predicts **exponential depth** to increase near-range discrimination (Section 4.1, ‚ÄúDepth representation‚Äù).

- **Teacher losses (geometric detail).**
  - Teacher objective combines:
    - gradient loss,
    - a global‚Äìlocal loss (`L_gl`) referenced from prior work,
    - a distance-weighted surface-normal loss defined via Eqs. (4)‚Äì(6),
    - plus sky-mask and object-mask losses (MSE) (Section 4.1; Eq. (7)).
  - The combined teacher loss is (Eq. (7), Section 4.1):
    - `L_T = Œ± L_grad + L_gl + L_N + L_sky + L_obj` with `Œ± = 0.5`.

- **Data scaling for the teacher.**
  - The teacher is trained **exclusively on synthetic data**, and the paper lists an expanded synthetic corpus spanning indoor/outdoor/object-centric/in-the-wild-like scenes (Section 4.1, ‚ÄúData scaling‚Äù; also Table 1 lists many training datasets).

- **Aligning teacher pseudo-depth to real metric depth.**
  - Let `D~` be teacher relative depth and `D` be sparse/noisy depth with mask `m_p`.
  - DA3 fits scale `s` and shift `t` via a RANSAC least-squares procedure (Eq. (8), Section 4.2):
    - `(≈ù, tÃÇ) = argmin_{s>0,t} Œ£ m_p (s D~_p + t ‚àí D_p)^2`,
    - then uses `D_{T‚ÜíM} = ≈ù D~ + tÃÇ`.
  - The inlier threshold is defined as the mean absolute deviation from the residual median (Section 4.2).

## 4. Key Insights and Innovations

- **(1) `Depth + ray map` as a minimal sufficient prediction target for any-view geometry.**
  - Novelty relative to point-map-only or redundant multi-output approaches: DA3 encodes pose implicitly via dense rays, avoiding direct rotation matrix constraints while still enabling consistent 3D point computation `P = t + D¬∑d` (Section 3.1).
  - Significance: Ablations show `depth + ray` outperforms `depth + cam` and `depth + pcd + cam` across datasets (Table 6; Section 7.2.1), suggesting rays provide a strong ‚Äúsingle representation‚Äù for both structure and camera motion.

- **(2) A single plain pretrained transformer backbone is sufficient (no bespoke multi-stage architecture required).**
  - DA3 emphasizes ‚Äúvanilla DINOv2‚Äù and token-rearrangement-based cross-view reasoning, rather than stacking multiple transformers or introducing heavy specialized modules (Figure 2; Section 3.2).
  - The ablation comparing the proposed single-transformer design vs. a ‚ÄúVGGT style‚Äù stacked-transformer design reports large degradation for the stacked variant in their setting (Table 7, rows a vs b; Section 7.2.2).

- **(3) Input-adaptive cross-view attention via token rearrangement supports arbitrary view counts.**
  - The mechanism is conceptually simple: choose which layers do within-view vs cross-view attention by changing how tokens are grouped (Section 3.2).
  - Significance: It allows DA3 to naturally reduce to monocular depth estimation when `Nv=1` without extra overhead (Section 3.2).

- **(4) Dual-DPT head for aligned depth and ray prediction.**
  - The ‚Äúshared reassembly + branch-specific fusion‚Äù design is intended to improve alignment/interaction between depth and ray predictions without duplicating full decoders (Figure 3; Section 3.2).
  - Ablation shows performance drops when replacing Dual-DPT with two separate DPT heads (Table 7, row d; Section 7.2.3).

- **(5) Teacher-only-on-synthetic + robust scale/shift alignment as a practical data unification strategy.**
  - The teacher produces dense relative depth even where real depth is sparse/noisy (Figure 4; Section 3.3).
  - The robust alignment (Eq. (8)) is the mechanism that attempts to preserve metric consistency while improving detail (Section 4.2), and the qualitative example suggests improved fine structures with teacher supervision (Figure 8).

## 5. Experimental Analysis

### 5.1 Evaluation methodology (datasets, metrics, setup)

- **Visual Geometry Benchmark (new).**
  - Datasets: HiRoom (synthetic), ETH3D, DTU, 7Scenes, ScanNet++ (Section 6.3; Table 1 lists counts).
  - Pose evaluation:
    - Uses AUC over thresholds derived from Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA) (Section 6.2).
    - Reports AUC at thresholds **3** and **30** (Section 6.2; Table 2 columns `Auc3` and `Auc30`).
  - Geometry/reconstruction evaluation:
    - Reconstruct point clouds by fusing predicted poses + predicted depths, aligning to ground truth via `evo` alignment plus a RANSAC robustness scheme (Section 6.1).
    - TSDF fusion is used for reconstruction (Section 6.1).
    - Metrics: F1-score at a dataset-specific distance threshold `d`; DTU reports Chamfer Distance (mm) (Section 6.2; Table 3).
- **Feed-forward novel view synthesis benchmark (new).**
  - Uses DL3DV (140 scenes), Tanks and Temples (6), MegaDepth (19) (Section 6.3; Table 5).
  - Protocol: 12 context views selected; evaluate on target views sampled every 8; image resolution `270√ó480` (Section 6.1; Table 5 caption).
  - Metrics: PSNR / SSIM / LPIPS (Section 6.3; Table 5).

### 5.2 Main quantitative results

- **Pose accuracy (Table 2).**
  - DA3-Giant achieves best reported AUCs across all listed datasets for `Auc3` and for most `Auc30` entries.
  - Concrete examples from Table 2:
    - HiRoom: DA3-Giant `Auc3=80.3`, `Auc30=95.9` vs VGGT `49.1/88.0` and Pi3 `67.0/94.8`.
    - ETH3D: DA3-Giant `48.4/91.2` vs VGGT `26.3/80.8` and Pi3 `35.2/87.3`.
    - ScanNet++: DA3-Giant `85.0/98.1` vs VGGT `62.6/95.1` and Pi3 `50.7/92.1`.
  - The text notes the only exception is `Auc30` on DTU (Section 7.1); Table 2 shows VGGT has `99.8` on DTU `Auc30` while DA3-Giant has `99.4`.

- **Reconstruction / geometry accuracy (Table 3).**
  - Pose-free (`w/o p.`) performance: DA3-Giant is best across HiRoom, ETH3D, DTU (CD), 7Scenes, ScanNet++ in the table.
    - HiRoom w/o pose: DA3-Giant `F1=85.1` vs VGGT `56.7`, Pi3 `75.8`.
    - ETH3D w/o pose: DA3-Giant `F1=79.0` vs VGGT `57.2`, Pi3 `72.7`.
    - DTU w/o pose (CD‚Üì): DA3-Giant `1.85 mm` vs VGGT `2.05 mm`, Pi3 `3.28 mm`.
  - With ground-truth pose (`w/ p.`), DA3-Giant remains very strong, e.g. HiRoom `F1=95.6`, ETH3D `F1=87.1`, ScanNet++ `F1=79.3` (Table 3).

- **Monocular depth (Table 4).**
  - DA3 reports Œ¥1 on KITTI/NYU/SINTEL/ETH3D/DIODE and a rank summary.
  - From Table 4:
    - KITTI Œ¥1: DA3 `95.3` vs DA2 `94.6`.
    - ETH3D Œ¥1: DA3 `98.6` vs DA2 `86.5` (large gap in the table).
    - Teacher is strongest overall in that table (e.g., KITTI `97.2`, ETH3D `99.8`) (Table 4).

- **Feed-forward NVS with 3DGS (Table 5).**
  - DA3 as backbone (‚ÄúDAv3 (Ours)‚Äù) achieves the best numbers across all three datasets in Table 5:
    - DL3DV: `PSNR=21.33`, `SSIM=0.711`, `LPIPS=0.241` (best among listed).
    - Tanks and Temples: `18.10 / 0.578 / 0.311`.
    - MegaDepth: `17.89 / 0.561 / 0.351`.
  - The text emphasizes out-of-domain drops (DL3DV vs Tanks/MegaDepth) as a sensitivity to trajectory/pose distributions (Section 7.1).

### 5.3 Do the experiments support the claims?

- **Support for ‚Äúdepth+ray is sufficient / minimal.‚Äù**
  - Table 6 directly tests output-target combinations and shows `depth + ray` substantially improves pose (`Auc3`) and geometry metrics over `depth + cam` and `depth + pcd + cam` across datasets (Section 7.2.1; Table 6).
  - Adding an auxiliary camera head (`depth + ray + cam`) does not consistently improve over `depth + ray` in Table 6, which supports the ‚Äúray sufficiency‚Äù argument (Section 7.2.1 text).

- **Support for ‚Äúsingle transformer suffices.‚Äù**
  - Table 7 compares the proposed architecture to a ‚ÄúVGGT style‚Äù stacked-transformer approach under similar parameter sizing (Section 7.2.2). The stacked variant is dramatically worse in that ablation table (Table 7 row b vs row a).

- **Support for teacher-student value.**
  - Table 7 row (e) ‚Äúw/o Teacher‚Äù shows mixed changes (e.g., improved DTU CD but worse elsewhere), while Figure 8 qualitatively shows more detailed depth with teacher supervision (Section 7.2.3; Figure 8).

### 5.4 Ablations, failure cases, robustness checks

- **Ablations included:**
  - Output target combinations (Table 6).
  - Attention scheduling (‚ÄúFull Alt.‚Äù vs partial) and architecture variants (Table 7 a‚Äìc).
  - Dual-DPT head vs separate heads (Table 7 d).
  - Teacher supervision on/off (Table 7 e; Figure 8).
  - Pose conditioning on/off (Table 7 f‚Äìg; note these rows are evaluated with ground-truth pose fusion, marked `*` in Table 7).
- **Robustness / scaling:**
  - Table 2 and Table 3 report multiple DA3 sizes (Small/Base/Large/Giant) with parameter counts, giving a partial view of scaling trends.
  - Table 8 reports max number of images and FPS at `504√ó336` (Table 8).
- **Failure cases:**
  - The provided excerpt does not include explicit failure-case categories; it mainly provides positive qualitative comparisons (Figures 6‚Äì7, 9‚Äì11).

## 6. Limitations and Trade-offs

- **Compute and accessibility.**
  - Training DA3-Giant is expensive: **~10 days on 128√óH100 GPUs** (Section 7.2).
  - Even ablations are reported as **~4 days on 32√óH100** with ViT-L and max 10 views (Section 7.2), which limits reproducibility for many groups.

- **Ray-map-to-camera recovery cost vs convenience.**
  - Recovering `(K,R,t)` from rays involves solving a DLT least-squares problem (Eq. (2)) and `RQ` decomposition (Section 3.1), and the paper explicitly says this is ‚Äúcomputationally costly‚Äù at inference (Section 3.1).
  - DA3 therefore adds a camera head for practical use (Section 3.1), but that means the system is no longer purely ‚Äúdepth+ray only‚Äù in deployed form (even if the camera head is lightweight).

- **Dependence on teacher pseudo-labeling and alignment assumptions.**
  - The approach assumes teacher-relative depth can be robustly aligned to noisy metric depths with a global scale+shift per image (Eq. (8), Section 4.2). If real depth is extremely sparse/biased, this alignment could be unreliable (the paper does not enumerate such cases in the excerpt).
  - Teacher is trained exclusively on synthetic data (Section 4.1), which is a deliberate choice to avoid poor real depth (Figure 4) but could introduce synthetic-to-real generalization gaps; DA3 mitigates this by aligning to real measurements (Section 4.2), though the residual domain gap is not fully characterized here.

- **Metric depth vs relative depth scope.**
  - The main DA3 any-view formulation predicts depth and rays with normalization steps and optional pose conditioning; metric depth is treated as a separate model variant (‚ÄúDA3-metric‚Äù) with its own training protocol and canonical focal length transformation (Section 4.4; Section 7.4).

- **Benchmark pipeline choices may affect conclusions.**
  - Reconstruction uses `evo`-based pose alignment plus RANSAC selection, then TSDF fusion (Section 6.1). This couples final geometry scores to (i) alignment heuristics and (ii) fusion settings (e.g., dataset-specific TSDF voxel sizes in Section 6.3).
  - DTU evaluation uses background removal (RMBG 2.0) and a specific fusion strategy (Section 6.3), which may advantage methods that behave well under those preprocessing steps.

- **Missing low-level implementation specifics (from the provided excerpt).**
  - For DA3 training, the optimizer type, weight decay, exact batch sizes per resolution/view-count, and full ViT block hyperparameters (layers/heads/hidden size per variant) are not specified in the shown sections (Section 3.4 provides steps/LR/hardware/resolutions but not all optimizer details).

## 7. Implications and Future Directions

- **How this changes the field (as evidenced in this paper).**
  - DA3 suggests that **unified visual geometry** can be effectively approached as *dense per-view prediction* (depth + rays) using a **single pretrained ViT**, rather than designing separate SfM/MVS/monocular architectures (Introduction; Section 3; Tables 2‚Äì3).
  - The new benchmark explicitly scores both pose and reconstructed geometry, encouraging holistic evaluation rather than task-isolated metrics (Section 6; Tables 2‚Äì3).

- **Follow-up research directions suggested by the work.**
  - The conclusion points to extending to **dynamic scenes**, adding **language/interaction cues**, and scaling pretraining further toward actionable world models (Section 8).

- **Practical applications / downstream use cases demonstrated here.**
  - **Feed-forward novel view synthesis (FF-NVS)**: DA3 is fine-tuned with an added DPT head to predict pixel-aligned 3D Gaussian parameters for 3DGS rendering (Section 5.1), and a pose-adaptive variant supports posed or unposed inputs (Section 5.2). Quantitative gains on the NVS benchmark suggest geometry strength transfers to rendering quality (Table 5; Section 7.1).
  - **Monocular depth estimation**: even though DA3 is designed for any-view geometry, it reports strong Œ¥1 results on monocular benchmarks (Table 4).

- **Repro/Integration Guidance (grounded in what the paper describes).**
  - Prefer DA3‚Äôs **pose-free** mode when camera poses are unavailable; DA3 still predicts rays and (optionally) camera pose via its camera head (Sections 3.1‚Äì3.2; Tables 2‚Äì3 `w/o p.`).
  - Prefer **pose conditioning** when accurate camera parameters are available: pose conditioning improves reconstruction metrics in the ablation that compares ‚Äúw/ Pose Cond.‚Äù vs ‚Äúw/o Pose Cond.‚Äù (Table 7 f‚Äìg, noting these are evaluated with ground-truth pose fusion).
  - For NVS/3DGS pipelines, the paper‚Äôs minimal recipe is: **freeze** the DA3 backbone and train only the added GS-DPT head for stability (Section 5.2), and train with varying resolutions/view counts (Section 5.2).



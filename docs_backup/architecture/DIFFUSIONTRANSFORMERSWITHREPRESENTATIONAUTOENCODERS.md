# DIFFUSION TRANSFORMERS WITH REPRESENTATION AUTOENCODERS

**ArXiv:** [2510.11690](https://arxiv.org/abs/2510.11690)

## üéØ Pitch

This paper introduces Representation Autoencoders (RAEs), which swap out traditional VAEs for frozen, pretrained representation encoders (like DINOv2 or MAE) plus lightweight decoders, enabling Diffusion Transformers to operate directly within high-dimensional, semantically-rich latent spaces. With principled architectural changes‚Äîincluding a novel, wide diffusion head‚Äîthe approach achieves state-of-the-art image generation on ImageNet, dramatically improving both sample quality and training efficiency. By bridging semantic understanding and generative modeling, RAEs redefine the foundation for latent diffusion, setting a powerful new default for diffusion transformer training and unlocking faster, higher-fidelity generative models.

---

## 1. Executive Summary
This paper replaces the usual variational autoencoder (VAE) used in latent diffusion with a frozen, pretrained representation encoder (e.g., DINOv2, SigLIP2, MAE) plus a lightweight decoder, forming a Representation Autoencoder (`RAE`). It then ‚Äútames‚Äù Diffusion Transformers (`DiT`) to work directly in these higher‚Äëdimensional, semantically rich latents through three principled changes and a new shallow‚Äëbut‚Äëwide diffusion head (`DiTDH`), achieving state‚Äëof‚Äëthe‚Äëart ImageNet FID with much faster convergence (e.g., 1.51 FID at 256√ó256 without guidance; 1.13 with guidance at 256√ó256 and 512√ó512; Tables 8 and 7).

## 2. Context and Motivation
- Problem addressed
  - Diffusion Transformers almost universally diffuse in VAE latents. These VAEs compress images into low‚Äëdimensional codes optimized only for reconstruction, which:
    - limit information capacity and semantic structure, hurting generation quality and generalization (Intro; Sec. 1).
    - rely on legacy, compute‚Äëheavy convolutional backbones (Fig. 2).
  - Two common beliefs block progress:
    1) semantic encoders (e.g., DINO/CLIP/MAE) are poor for reconstruction because they discard low‚Äëlevel detail; and
    2) diffusion is unstable/inefficient in high‚Äëdimensional latent spaces (Sec. 1).

- Why it matters
  - Latent diffusion dominates modern image/video generation because it trades pixel complexity for compact latent spaces. If the latent space can be made more semantically meaningful without extra compute, both sample quality and training efficiency can improve (Intro; Fig. 1).

- Prior approaches and their shortfalls
  - Improve VAE latents indirectly by aligning with external encoders during DiT training (REPA, REG, DDT; Sec. 2). These add auxiliary losses/stages and tuning complexity, and still inherit compressed, weak latents.
  - Enhanced VAEs (e.g., MAE‚Äëstyle tokenizers) still compress latents and are trained for reconstruction, not semantics (Sec. 2).

- This paper‚Äôs positioning
  - It directly adopts frozen representation encoders as the latent space (no compression), demonstrates they reconstruct competitively or better than SD‚ÄëVAE (Table 1a), and develops theory and practice for training DiTs stably and efficiently on these higher‚Äëdimensional, semantic tokens (Sec. 3‚Äì5).

## 3. Technical Approach
The method has two parts: building the `RAE`, then adapting diffusion training and architecture to operate effectively in high‚Äëdimensional representation features.

1) Representation Autoencoder (RAE) (Sec. 3; Appx. C)
- What it is
  - Encoder `E`: a frozen, pretrained vision representation model (DINOv2, SigLIP2, MAE). It splits an image into patches and outputs one token per patch (N tokens). Each token has `d` channels (e.g., 768 for DINOv2‚ÄëB).
  - Decoder `D`: a lightweight Vision Transformer that maps tokens back to pixels. It is trained; the encoder is not.
- How it works
  - Given an image x, compute tokens z = E(x). Train D to reconstruct xÃÇ = D(z) using a composite loss (Sec. 3; Appx. C.2):
    - L1 (per‚Äëpixel difference),
    - LPIPS (perceptual similarity),
    - adversarial loss with a frozen DINO‚ÄëS/8 discriminator (stabilizes details).
- Training choices and efficiency
  - No encoder compression: the number of tokens equals (H√óW)/pe¬≤; for 256√ó256 with pe=16, N=256 tokens, matching standard DiT sequence length.
  - Decoder losses and hyperparameters are detailed in Appx. C; the discriminator setup follows StyleGAN‚ÄëT, with some stabilizing tweaks (Appx. C.2).
  - Compute: Fig. 2 shows the SD‚ÄëVAE encoder/decoder needs ~135/310 GFLOPs per 256√ó256 image vs ~22/106 GFLOPs for RAE‚Äôs encoder/decoder‚Äîsubstantial savings.

2) Training diffusion in RAE latent space (Sec. 4)
- Base training objective and backbone
  - Use flow matching with linear interpolation: xt = (1‚àít)x + tŒµ, where Œµ is Gaussian noise; train to predict the ‚Äúvelocity‚Äù v(xt,t)=Œµ‚àíx (Sec. 4; Appx. J).
  - Backbone: LightningDiT (a DiT variant), sequence length = 256 (patch size 1 for latents), so DiT compute is comparable to VAE baselines (Sec. 4; Appx. D).
- Why standard DiT fails out of the box on RAE
  - Table 2: training DiT directly on RAE latents yields very poor FID (e.g., DiT‚ÄëS fails catastrophically; DiT‚ÄëXL far worse than with SD‚ÄëVAE).
- Three fixes, with mechanisms:
  a) Match model width to token dimensionality (Sec. 4.1)
     - Observation via ‚Äúsingle‚Äëimage overfitting‚Äù: the diffusion model cannot even overfit unless its width d ‚â• latent dimension n (Fig. 3, left; Table 3). Increasing depth does not help if d < n (Fig. 3, right).
     - Intuition: because training adds Gaussian noise (spreading support over the full space), the target is effectively full‚Äërank; capacity must scale with data dimensionality (Sec. 4.1).
     - Formalization: Theorem 1 (Sec. 4.1; Appx. B.1) lower‚Äëbounds the loss when d<n; in a toy case with a single image, the lower bound equals (n‚àíd)/n, matching the empirical curve in Fig. 3.
     - Practical rule: pick a DiT width at least the latent channel dimension (e.g., ‚â•768 for DINOv2‚ÄëB).
  b) Dimension‚Äëdependent noise schedule shift (Sec. 4.2)
     - Prior ‚Äúresolution‚Äëbased‚Äù schedule shifts only adjust for more spatial tokens; here the per‚Äëtoken channel dimension is also large (e.g., 768), so effective data dimension = (#tokens)√ó(channels).
     - Use the shift from Esser et al. (2024): for base dim n and target dim m, rescale t as tm = Œ±t / (1+(Œ±‚àí1)t), with Œ±=‚àö(m/n). Using n=4096 as base (as in prior work) and m as RAE‚Äôs effective dimension drastically improves FID from 23.08 to 4.81 (Table 4).
  c) Noise‚Äëaugmented decoder training (Sec. 4.3)
     - Mismatch: The RAE decoder is trained on a discrete set of clean latents {E(x)}, but diffusion generates slightly noisy latents. To make D generalize, inject Gaussian noise into latents during decoder training, i.e., train D on zÃÉ = z + n, n~N(0,œÉ¬≤) with randomness in œÉ (Sec. 4.3).
     - Effect: better generation (gFID 4.81‚Üí4.28) at a small cost in reconstruction fidelity (rFID 0.49‚Üí0.57), Table 5. Ablations over œÑ (noise scale) and encoders in Appx. G.2 show consistent gains.

3) Scaling width efficiently: a shallow‚Äëbut‚Äëwide diffusion head (`DiTDH`) (Sec. 5)
- Motivation: simply making the whole DiT wider is expensive (quadratic cost in width).
- Design (Fig. 5): keep a standard DiT `M` (normal width) but add a lightweight, wide transformer head `H` that takes both the noisy input xt and features zt=M(xt|t,y) to predict the velocity vt.
  - This increases denoising width where needed, avoiding quadratic blow‚Äëup.
- What works best: a 2‚Äëlayer, very wide head (e.g., width 2048) outperforms deeper or narrower heads at similar compute (Table 16). Wider heads benefit larger RAE encoders more (Table 17).
- Empirical impact: with RAE latents, DiTDH converges faster and to better FID than DiT at the same or lower FLOPs (Fig. 6a‚Äìc; Table 6).

4) Efficient high‚Äëresolution synthesis via the decoder (Sec. 6.1)
- To go from 256‚Üí512 without 4√ó tokens, keep the same latent tokens and only upsample in the decoder by using a larger patch size `pd=2¬∑pe`. This ‚Äúdecoder upsampling‚Äù attains competitive 512‚ÄëFID (1.61 vs 1.13 trained directly at 512) while being ~4√ó cheaper (Table 9).

## 4. Key Insights and Innovations
- Turn frozen semantic encoders into practical autoencoders for generation (RAE)
  - Novelty: Defies the belief that semantic encoders cannot reconstruct faithfully. With a modest ViT decoder and standard reconstruction+adversarial losses, RAEs match or beat SD‚ÄëVAE reconstruction while being faster (Table 1a,b; Fig. 2).
  - Evidence:
    - Reconstruction FID (rFID) on ImageNet val set: MAE‚ÄëB/16 achieves 0.16 vs SD‚ÄëVAE‚Äôs 0.62; DINOv2‚ÄëB 0.49 (Table 1a).
    - Efficiency: RAE encoder/decoder ~22/106 GFLOPs vs SD‚ÄëVAE ~135/310 (Fig. 2).
    - Representation quality: linear probing top‚Äë1 accuracy 84.5% (DINOv2‚ÄëB), 79.1% (SigLIP2‚ÄëB), 68.0% (MAE‚ÄëB) vs 8.0% for SD‚ÄëVAE (Table 1d).
- A principled capacity rule for diffusion on high‚Äëdimensional latents (Sec. 4.1)
  - Insight: because the training interpolation injects Gaussian noise, the target becomes full‚Äërank; a DiT with width d<n (latent channels) cannot fit, even on one image.
  - Theorem 1 (Appx. B.1) quantifies a loss lower bound when d<n; the empirical single‚Äëimage experiments match the bound exactly (Fig. 3; Table 3).
- Dimension‚Äëaware noise schedule shift (Sec. 4.2)
  - Extends ‚Äúresolution‚Äëaware‚Äù shifts to ‚Äúeffective‚Äëdimension‚Äëaware‚Äù shifts to handle many channels per token. This single change cuts gFID from 23.08 to 4.81 (Table 4).
- Noise‚Äëaugmented decoder to close the training‚Äìsampling gap (Sec. 4.3)
  - Simple but important: training the decoder on a smoothed latent distribution makes it robust to the slightly noisy latents produced by diffusion, improving gFID consistently (Table 5; Appx. G.2).
- `DiTDH`: decoupling denoising width from backbone width (Sec. 5)
  - Fundamental capability: scale width where it matters (denoising head) without quadratic cost. Yields large FID reductions and better compute‚Äìperformance scaling (Fig. 6a‚Äìc; Tables 6, 16, 17).

## 5. Experimental Analysis
- Evaluation setup
  - Dataset: ImageNet‚Äë1K at 256√ó256 and 512√ó512 (Sec. D).
  - Metrics: FID‚Äë50k, Inception Score, precision/recall (Appx. K).
  - Sampling: ODE Euler sampler, typically 50 steps; Class‚Äëconditional generation.
  - Important protocol detail: class‚Äëbalanced sampling (50 samples per class) vs uniform random over labels affects FID by ~0.1; this paper re‚Äëevaluates several baselines with balanced sampling for fairness (Sec. 5.1; Appx. E; Table 14).

- Main results (state of the art)
  - ImageNet 256√ó256
    - Without guidance: DiTDH‚ÄëXL + DINOv2‚ÄëB achieves 1.51 FID after 800 epochs (Table 8).
    - With AutoGuidance: 1.13 FID (Table 8). Also strong IS and recall.
  - ImageNet 512√ó512
    - With guidance: 1.13 FID after 400 epochs (Table 7), surpassing prior diffusion best (EDM‚Äë2 at 1.25).
  - Convergence and compute
    - Fig. 6b: DiTDH‚ÄëXL surpasses REPA‚ÄëXL, MDTv2‚ÄëXL and SiT‚ÄëXL at far less compute; reaches best FID with over 40√ó less training FLOPs than some baselines.
    - Fig. 4: Even before adding the head, DiT trained on RAE latents converges much faster than VAE‚Äëbased SiT or REPA (up to 47√ó and 16√ó speedups to comparable FID).

- Ablations and diagnostics
  - Why standard DiT fails initially on RAE:
    - Table 2 shows large FID gaps; single‚Äëimage overfit experiments (Fig. 3; Table 3) and Theorem 1 explain the need for width ‚â• latent dimension.
  - Noise schedule shift:
    - Table 4: 23.08 ‚Üí 4.81 FID by dimension‚Äëaware shift (crucial).
  - Noise‚Äëaugmented decoder:
    - Table 5: gFID improves (4.81‚Üí4.28) while rFID drops slightly (0.49‚Üí0.57)‚Äîa deliberate trade‚Äëoff; robustness across encoders and œÑ scales in Appx. G.2 (Table 15).
  - `DiTDH` design:
    - Fig. 6a: better FLOP‚ÄìFID scaling than DiT at all sizes.
    - Table 6: DiTDH outperforms DiT consistently across RAE sizes (e.g., DINOv2‚ÄëL gFID 2.73 vs 6.09).
    - Tables 16‚Äì17: the head should be wide and shallow; benefit grows with larger encoders.
  - What does not help:
    - DiTDH on SD‚ÄëVAE latents performs worse than DiT (Table 10)‚Äîthe head mainly helps in high‚Äëdimensional RAE spaces.
    - High dimensionality alone is not enough: pixel diffusion at the same dimensionality (768 per token) performs far worse than RAE (Table 11).
  - High‚Äëresolution without extra tokens:
    - Decoder upsampling nearly matches direct 512 training (gFID 1.61 vs 1.13) at ~4√ó less compute (Table 9).

- Guidance method
  - AutoGuidance (a learned‚Äëmodel‚Äëguides‚Äëmodel scheme) is the default because it outperforms classifier‚Äëfree guidance with interval here and is easier to tune (Appx. I).

- Do the experiments support the claims?
  - Yes: Multiple lines of evidence‚Äîreconstruction quality (Table 1), training dynamics (Fig. 3, Theorem 1), schedule/decoder ablations (Tables 4‚Äì5), wide head scaling (Fig. 6)‚Äîconverge to a coherent story, culminating in SOTA FID at both 256 and 512 resolutions (Tables 8 and 7), with qualitative samples (Fig. 7; Appx. M).

## 6. Limitations and Trade-offs
- Reliance on strong pretrained encoders
  - RAE inherits strengths/weaknesses of the chosen representation encoder. Performance varies by encoder (Table 15a); DINOv2‚ÄëB generally works best for generation, although MAE has the best reconstruction rFID.
- Width requirement
  - Practical constraint: the diffusion model‚Äôs width must match or exceed the latent channel dimension (Sec. 4.1; Fig. 3; Theorem 1). This forces non‚Äëtrivial width, though DiTDH mitigates compute.
- Decoder robustness vs reconstruction fidelity
  - Noise‚Äëaugmented training helps generation but slightly harms reconstruction (Table 5). For use cases demanding exact reconstruction, this trade‚Äëoff must be tuned.
- Evaluation scope
  - Experiments focus on class‚Äëconditional ImageNet; no text‚Äëto‚Äëimage or cross‚Äëdomain tests. Generalization to other domains (medical, satellite) is untested.
- Guidance dependence and evaluation subtlety
  - Best 256/512 numbers use AutoGuidance; gains depend on a small guide model (Appx. I). Also, FID is sensitive to label sampling strategy (random vs balanced, Appx. E).
- High‚Äëresolution pathway
  - Decoder upsampling is efficient but trails direct 512 training (gFID 1.61 vs 1.13; Table 9). There may be a ceiling to what upsampling alone can recover.

## 7. Implications and Future Directions
- Shift in default practice for DiTs
  - The results argue for replacing VAEs with RAEs when training diffusion transformers: better semantics, faster convergence, and SOTA quality without extra token lengths (Fig. 1; Tables 7‚Äì8). This reframes the autoencoder not as a compressor, but as a semantic representation foundation.
- Broader research avenues
  - Unified representation‚Äìgeneration pipelines: leveraging foundation encoders (self‚Äësupervised or multimodal) as the latent space for generative training across images, video, and 3D.
  - Theory of capacity vs. dimensionality in diffusion: Theorem 1 and Fig. 3 suggest general design rules; extending this analysis to other noise processes or schedules could further systematize architecture choices.
  - Decoder training curricula: explore richer noise models, consistency regularization, or partial denoising tasks to further close the train‚Äìsample gap without losing reconstruction fidelity.
  - Multi‚Äëmodal generation: RAEs based on language‚Äësupervised encoders (e.g., SigLIP2) could ease conditioning on text or other modalities without bespoke alignment losses.
  - Efficient scaling: the `DiTDH` pattern (shallow‚Äëbut‚Äëwide denoising heads) may generalize to other high‚Äëdimensional latent diffusion settings, enabling width scaling at modest cost.

> Headline result (Table 8): ‚ÄúDiTDH‚ÄëXL (RAE with DINOv2‚ÄëB) reaches 1.51 FID at 256√ó256 without guidance and 1.13 with AutoGuidance;‚Äù  
> Headline result (Table 7): ‚ÄúAt 512√ó512 with guidance, 1.13 FID‚Äîsurpassing the previous best diffusion result.‚Äù

Overall, this work demonstrates that semantically rich, high‚Äëdimensional features are not an obstacle to diffusion training; with the right capacity rule, dimension‚Äëaware noise scheduling, a robust decoder, and a strategically wide denoising head, they are a distinct advantage.

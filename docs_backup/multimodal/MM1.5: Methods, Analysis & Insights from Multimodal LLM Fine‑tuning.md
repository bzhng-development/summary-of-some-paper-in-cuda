# MM1.5: Methods, Analysis & Insights from Multimodal LLM Fineâ€‘tuning

**ArXiv:** [2409.20566](https://arxiv.org/abs/2409.20566)
**Authors:** Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, ZhenÂ Yang, Aleksei Timofeev, Mingze Xu, Hongâ€‘You Chen, Jeanâ€‘Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, Yinfei Yang
**Institutions:** Apple AI Research

## ğŸ¯ Pitch

The MM1.5 family of multimodal large language models revolutionizes image processing by enhancing text-heavy visual understanding, precise visual grounding, and multi-image reasoning through an innovative data-centric training approach. By refining high-resolution handling and strategic data combination, MM1.5 outperforms peers without new architectures, making it pivotal for document AI and grounded assistant applications, especially for resource-constrained environments.

---

## 1. Executive Summary
`MM1.5` is a family of multimodal large language models (MLLMs) that substantially improves three hard capabilities for images: understanding text-heavy visuals (OCR-like tasks), visual referring and grounding (point/box-based localization), and reasoning across multiple images. The workâ€™s significance is not a new architecture but a dataâ€‘centric training recipeâ€”spanning pre-training, highâ€‘resolution continual pre-training, and supervised fineâ€‘tuning (SFT)â€”plus a robust highâ€‘resolution image handling method, all validated by extensive ablations and strong results from 1B to 30B parameters (Figures 1â€“2; Sections 3â€“4; Tables 4â€“9).

## 2. Context and Motivation
- Problem/gap
  - Many open and proprietary MLLMs handle general visionâ€“language tasks but struggle with:
    - Reading small text in images at diverse aspect ratios and resolutions (â€œtextâ€‘rich image understandingâ€).
    - Precise visual grounding and referring (identifying and reasoning over specific regions via coordinates).
    - Multi-image reasoning and in-context learning with interleaved imageâ€“text inputs.
  - The â€œhowâ€ of building such capabilitiesâ€”especially data composition across the full training lifecycleâ€”has been under-explored. Prior open models often focus mainly on SFT with limited study of pre-training data or high-resolution strategies (Section 2).
- Importance
  - Text-rich understanding powers document analysis, charts, UI comprehension, receipts, and infographics.
  - Referring/grounding is crucial for controllable agents, robotics, and UI automation.
  - Multi-image reasoning supports tasks like change detection, comparison, and episodic reasoning.
- Prior approaches and their limits
  - Closed models (GPT-4V/4o, Gemini 1.5, Claude 3.5) show strong multimodal competence but rely on proprietary training; GPTâ€‘4o often needs â€œsetâ€‘ofâ€‘markâ€ prompting to reference regions instead of native grounded outputs (Section 2).
  - Open models (LLaVA, InternVL2, Qwen2â€‘VL, Cambrianâ€‘1) narrow the gap but often:
    - Underperform on fine-grained grounding (Section 2).
    - Use static image tiles or low token budgets, limiting small text recognition (Sections 2, 3.5).
    - Provide limited analysis of data category mixing across pre-training/continualâ€‘pre-training/SFT.
- Positioning
  - `MM1.5` keeps the MM1 architecture to isolate and study training recipes and highâ€‘resolution processing. It offers detailed, reproducible ablations on:
    - SFT mixtures by capability (general, textâ€‘rich, science/math/code, refer&ground, multi-image, textâ€‘only).
    - Highâ€‘resolution continual pre-training data and resolution.
    - Pre-training data ratios and improved text-only corpora.
    - Dynamic image splitting to reach up to 4 MP effective resolution with minimal padding (Sections 3.2â€“3.5; Figure 11; Tables 1â€“3).

## 3. Technical Approach
Step-by-step overview of how the system is built and trained.

- Overall architecture (Figure 1; Section 3.1)
  - Vision encoder: CLIP-like image encoder.
  - Language backbone: decoder-only LLM.
  - Connector: `Câ€‘Abstractor` projects visual tokens to the LLM (Section 3.1).
  - Capabilities:
    - Accepts multiple images interleaved with text.
    - Emits grounded outputs natively: bounding boxes embedded in text (e.g., â€œ<x1,y1,x2,y2>â€) and interprets point/box inputs (â€œcoordinate tokensâ€).
    - Handles high resolution via dynamic image splitting.

- Three-stage training pipeline (Figure 2; Section 4)
  1) Pre-training (same data sources as MM1 for image portions; updated text-only set)
     - Data composition changed from MM1â€™s 45:45:10 to `50:10:40` for imageâ€‘caption : interleaved imageâ€‘text : textâ€‘only (Section 3.4; Figure 10; Section 4).
       - Imageâ€‘caption: 2B pairs.
       - Interleaved documents: 600M docs (1B images).
       - Textâ€‘only: 2T tokens from a higherâ€‘quality â€œHQâ€‘Textâ€ collection (Section 3.4).
     - Motivation: Improve language and knowledge-heavy benchmarks after SFT; large down-weighting of interleaved data yielded better downstream performance even if pre-training few-shot metrics did not always predict this (Section 3.4).
     - Training: 200k steps, sequence length 4096 (Section 4).
  2) Highâ€‘resolution continual pre-training
     - Target: strengthen text-rich (OCR-like) understanding before SFT.
     - Data: 45M document-centric images from PDFA, IDL, RenderedText, DocStructâ€‘4M (Section 3.3).
     - Resolution: best setup is 1344Ã—1344 with splitting (Figure 9a).
     - Synthetic captions: public sets (LLaVAâ€‘Recapâ€‘3M, ShareGPT4Vâ€‘PT) did not clearly help beyond the OCR mixture at this stage; a separate selfâ€‘trained captioner generating 7M high-quality captions showed promise (Appendix A.1; Figure 13), but OCR-only remained the default (Section 3.3).
  3) Supervised fine-tuning (SFT)
     - Carefully balanced multi-capability mixture (Figure 4; Section 3.2). Final macro-ratios:
       - `wsingle=0.8`, `wmulti=0.1`, `wtext=0.1` (Section 3.2.2).
       - Within single-image 80%: roughly 37.2% textâ€‘rich, 22.5% refer&ground, 11.3% general, 5.6% math, 2.3% code, 1.1% science (Section 4).
     - Key mixing choices from ablations:
       - When blending with general data, best Î± (target:general) for science=0.1, math=0.5, code=0.2 (Figure 6aâ€“c).
       - For refer&ground, Î±=2.0 trades a small drop in base scores for a large grounding gain (Figure 6d).
       - Multi-image `wmulti=0.1` boosts multi-image metrics while limiting regressions on single-image averages; text-only `wtext=0.1` changes little but reserves capacity for images (Figure 7).

- Dynamic highâ€‘resolution image splitting (â€œAnyResâ€) (Section 3.5; Figure 11; Tables 1â€“3)
  - Problem: Fixed 2Ã—2 tiling wastes tokens on small or elongated images (padding) and misses detail on long documents.
  - Method (Equation 1):
    - Predefine allowed grid shapes by `nmin â‰¤ nhÂ·nw â‰¤ nmax`.
    - For a given input image size (h,w) and encoder resolution r:
      - If some grid can cover the image without downscaling below r, choose the grid that minimizes padding.
      - Otherwise choose the grid that minimizes resolution loss due to downscaling.
  - â€œGlobalâ€“Localâ€ format: besides the sub-images, also pass a downscaled â€œoverviewâ€ image; the paper places the overview after the tiles so it can attend to all tiles (Table 3, row 4 vs. row 1).
  - Sub-image position indicators:
    - `index`: tuples `(k,i,j)` describing image k, row i, column j.
    - `seps`: special tokens between tiles to recover 2D layout.
    - Averages show small differences; indicators help some DocVQA/InfoVQA and grounding, but are not strictly necessary (Table 3).
  - Training vs. inference: train with `(nmin,nmax)=(4,9)` but can infer at higher grids like `(4,16)` for more effective resolution (Table 2).

- Model scales and MoE (Section 4)
  - Dense: 1B, 3B, 7B, 30B.
  - MoE: 1Bâ€‘MoE and 3Bâ€‘MoE with 64 experts (topâ€‘2 routing; Section 4).
  - Same image encoder and connector; only LLM FFN layers become experts.

- Specializations
  - `MM1.5-Video` (Section 5):
    - Training-free mode: treat a video as 24 uniformly sampled frames, each encoded into 144 visual tokens; dynamic splitting disabled to fit token budgets.
    - SFT mode: fine-tune with ShareGPTVideo (556k), VideoChat2 (225k), ActivityNetâ€‘QA (31.5k).
  - `MM1.5-UI` (Section 6):
    - Fine-tune general models on the Ferretâ€‘UI mixture (801k samples) for mobile UI tasks that require OCR + grounding + commonsense about GUI widgets.

## 4. Key Insights and Innovations
- A. Data-centric SFT design that trades and balances capabilities (Sections 3.2.1â€“3.2.2; Figures 5â€“8)
  - Novelty: Treat SFT not as a single â€œbig bag of dataâ€ but as capability-aligned categories with explicit ratios.
  - Why it matters: Enables small models (1B/3B) to achieve balanced, strong performance. Example: adding refer&ground data improves grounding â€œa lotâ€ while slightly hurting other averages; the paper picks Î±=2.0 to optimize overall (Figure 6d). The final â€œAll Mixtureâ€ yields the best cross-category average (Figure 8, rightmost bar).

- B. Highâ€‘resolution continual preâ€‘training with documentâ€‘style data is crucial (Section 3.3; Figure 9a)
  - Novelty: An additional stage between pre-training and SFT at high resolution (1344Ã—1344) on 45M OCR-rich images.
  - Why it matters: It raises textâ€‘rich and knowledge performance beyond what SFT alone can deliver. Using 1344Ã—1344 beats 756Ã—756 and 378Ã—378; training at 378Ã—378 can even underperform â€œno continual pre-trainingâ€ (Figure 9a).

- C. Dynamic image splitting with globalâ€“local and flexible grids (Section 3.5; Tables 1â€“3; Figure 11)
  - Novelty: A computeâ€‘aware grid selection that minimizes padding or resolution loss, plus an overview frame and optional position indicators.
  - Why it matters: Improves text-rich benchmarks substantially and is especially effective for nonâ€‘square documents and infographics; e.g., on 3B, raising `nmax` from 4 to 16 improves DocVQA by +3.1 and InfoVQA by +6.9 points (Table 2, rows 1â†’3). Training for the larger grid is better than only changing grid at inference (rows 2 vs. 5).

- D. Rethinking preâ€‘training mix: less interleaved images, more highâ€‘quality text (Section 3.4; Figure 10)
  - Novelty: Move from 45:45:10 to `50:10:40` and upgrade text-only corpus (HQâ€‘Text).
  - Why it matters: After SFT, performance jumps across textâ€‘rich (+0.85), knowledge (+0.99), and refer&ground (+~1.4). There is a small multi-image drop (âˆ’0.05) due to less interleaved data, a trade-off the paper explicitly accepts (Figure 10).

- E. Capabilityâ€‘focused specializations with minimal additional machinery (Sections 5â€“6; Tables 10â€“12)
  - MM1.5 is reused â€œas isâ€ for videos (trainingâ€‘free) or slightly fineâ€‘tuned for video/UI:
    - Trainingâ€‘free VideoQA already beats many 7B trainingâ€‘free baselines at 3B size (Table 10).
    - UI variant sets new SOTA on multiple Ferretâ€‘UI elementary tasks, even at 1B (Table 12).

## 5. Experimental Analysis
- Evaluation Design (Sections 3.1, A.4; Tables 4â€“9; Figure 4)
  - Benchmarks grouped by capability:
    - General: MME, SEEDâ€‘IMG, POPE, LLaVAâ€‘Bench (Wild), MMâ€‘Vet, RealWorldQA.
    - Textâ€‘rich: WTQ, TabFact, OCRBench, ChartQA, TextVQA, DocVQA, InfoVQA.
    - Knowledge: AI2D, ScienceQA, MathVista, MMMU.
    - Referring & Grounding: RefCOCO family, Flickr30k Entities, LVISâ€‘Ref, Ferretâ€‘Bench.
    - Multi-image: QBench2, Mantis, NLVR2, BLINK, MVBench, MuirBench.
    - Inâ€‘context learning: VLâ€‘ICL (6 subtasks).
  - Metrics follow each benchmarkâ€™s standard (Table 14). â€œCategory Average Scoreâ€ is the unweighted average across metrics within a category; â€œMMBase scoreâ€ averages general + textâ€‘rich + knowledge (Section 3.1).

- Main quantitative results
  - Small scales lead among peers (Table 4):
    - At 1B, `MM1.5â€‘1B` tops peers across many benchmarks. Example: TextVQA 72.5 vs LLaVAâ€‘0.5B  â€” (not reported), vs InternVL2â€‘2B 73.4; DocVQA 81.0 vs InternVL2â€‘2B 86.9; RefCOCO avg 81.4 with native grounding (Table 7).
    - At 3B, `MM1.5â€‘3B` is competitive with or better than MiniCPMâ€‘V2â€‘3B, InternVL2â€‘2B, and Phiâ€‘3â€‘Visionâ€‘4B on many axes (Table 4). For textâ€‘rich specifically (Table 6):
      > â€œDocVQA 87.7, InfoVQA 58.5, ChartQA 74.2, TextVQA 76.5â€  
      These are strong for a 3B model and close to or exceeding larger peers.
  - Scaling and MoE help (Tables 4â€“9):
    - Dense 1Bâ†’30B steadily improves (e.g., AI2D: 59.3â†’77.2; Table 5).
    - `3Bâ€‘MoE` often surpasses dense `7B` on knowledge, general, grounding, and multiâ€‘image (Table 4), showing MoEâ€™s parameterâ€‘efficient scaling.
  - Referring & Grounding excellence (Table 7):
    - `MM1.5â€‘3B`: RefCOCO avg 85.6, Flickr30k 85.9, LVISâ€‘Ref avg 67.9â€”on par with or better than larger grounding-specialized models (e.g., Ferretâ€‘7B). GPTâ€‘4o relies on prompting tricks, whereas MM1.5 generates pointers natively.
  - Multiâ€‘image and ICL (Tables 8â€“9):
    - VLâ€‘ICL: `MM1.5â€‘30B` achieves a 77.6 average vs GPTâ€‘4Vâ€™s 65.8 on this suite (Table 8), indicating strong multimodal inâ€‘context learning.
    - Multiâ€‘image: `MM1.5â€‘30B` yields 79.3 on QBench2, 64.6 Mantis, 90.6 NLVR2, 54.0 MVBench, 58.2 MuirBench (Table 9).
  - Textâ€‘rich improvements at 30B (Table 6):
    - `MM1.5â€‘30B`: DocVQA 91.4, InfoVQA 67.3, ChartQA 83.6, WTQ 54.1, TabFact 84.0.
    - These gains align with the continual preâ€‘training and dynamic splitting ablations.

- Ablations that justify design choices
  - SFT category impact (Figure 5): textâ€‘rich data boosts both textâ€‘rich and knowledge averages; science data boosts knowledge; refer&ground adds the capability but slightly hurts other averagesâ€”hence Î±=2.0 chosen later (Figure 6d).
  - Ratio selection (Figures 6â€“7): Î±science=0.1, Î±math=0.5, Î±code=0.2; `wmulti=0.1`, `wtext=0.1`.
  - Continual pre-training (Figure 9): highâ€‘res 1344Ã—1344 gives best MMBase; OCRâ€‘only at highâ€‘res outperforms synthetic caption alternatives at this stage.
  - Pre-training mixture (Figure 10): replacing text-only with HQâ€‘Text and shifting to 50:10:40 improves textâ€‘rich (+0.85), knowledge (+0.99), and grounding (~+1.4) averages; slight multiâ€‘image dip (âˆ’0.05).
  - Dynamic vs static splitting (Section 3.5.1; Table 1; Table 2; Appendix A.6 Tables 15â€“18):
    - More subâ€‘images and higher encoder resolution both help textâ€‘rich; e.g., 10 subâ€‘images at 672Â² with 144 tokens/subâ€‘image is best among tested (Table 1, row 7).
    - Increasing `nmax` benefits DocVQA and InfoVQA; training with larger grids beats inferenceâ€‘only upgrades (Table 2).
    - Despite better performance, dynamic splitting is not necessarily more expensive on average: in a 100k sample, tiles increased only from 500k (static) to 539k (dynamic) (Section 3.5.1).
  - Video (Tables 10â€“11):
    - Trainingâ€‘free `MM1.5â€‘Videoâ€‘3B` achieves strong multipleâ€‘choice scores (e.g., NExTQA 72.8; IntentQA 72.7; Table 10).
    - With SFT, `MM1.5â€‘Videoâ€‘1B` surpasses LLaVAOneVisionâ€‘0.5B by large margins on EgoSchema (+24.2 points) and NExTQA (+14.6) (Table 10). The 7B video model is near or at SOTA on several benchmarks.
  - UI (Table 12):
    - `MM1.5â€‘UIâ€‘1B` outperforms Ferretâ€‘UIâ€‘13B on core elementary tasks by a wide gapâ€”e.g., iOS Referring 90.0 vs 80.5; iOS Grounding 86.5 vs 79.4â€”highlighting the transfer of MM1.5â€™s general recipe to UI.

- Do the experiments support the claims?
  - Yes, because:
    - Each claimed design choice has an ablation (SFT mixture, continual pre-training resolution/data, pre-training ratios, dynamic splitting variants).
    - The final models win or are competitive across capability groups, especially when compared at similar parameter budgets (Tables 4â€“9).
  - Caveats:
    - Some test sets overlap with training sources (marked â€  in Tables 4â€“9 and noted under Figure 4), so generalization must be assessed with that in mind.
    - Openâ€‘ended video scores use LLMâ€‘judged metrics for some datasets (Section 5.1), which, while common practice, can introduce evaluation variance.

## 6. Limitations and Trade-offs
- Data mixture choices are tuned to specific goals
  - Reducing interleaved pre-training improves language-heavy tasks postâ€‘SFT but slightly lowers multi-image averages (Figure 10). Projects prioritizing multiâ€‘image might prefer higher interleaved ratios.
- Resolution vs token budget
  - Dynamic splitting lifts OCR/textâ€‘rich performance but increases token counts for long documents; inference at higher grids (e.g., 4â†’16) boosts accuracy but increases latency/memory (Table 2).
- Continual pre-training dependencies
  - Gains rely on 45M curated OCR-style images at high resolution (Section 3.3). Such data volumes may be costly to acquire/host.
- Video modeling simplifications
  - Per-frame encoding without dynamic splitting and using only 24 frames may miss tiny text or long-range temporal dependencies; still, results are strong, but long videos with sparse cues remain challenging (Section 5.2).
- Grounding sensitivity
  - Inference-time changes to the minimum grid disrupt localâ†’global coordinate conversion and can harm grounding (Table 2, row 7).
- Plateau on UI scaling
  - UI performance improvements from 7Bâ†’30B are modest, suggesting data diversity or resolution, not just parameters, limit further gains (Section 6.2).

## 7. Implications and Future Directions
- How this work changes the landscape
  - Shifts emphasis from â€œwhich modelâ€ to â€œwhich data recipe and resolution strategyâ€ to unlock new capabilities, particularly for small, onâ€‘device models (1Bâ€“3B) that now compete across textâ€‘rich, grounding, and multi-image tasks (Tables 4â€“9).
  - Provides a concrete, endâ€‘toâ€‘end playbookâ€”pre-training mix, highâ€‘res continual pre-training, and SFT ratiosâ€”that others can adapt (Sections 3â€“4).
- Followâ€‘up research enabled
  - Unified training of image, video, and UI within a single set of weights leveraging the same dynamic splitting and data-balancing principles (Sections 5â€“6, Conclusion).
  - Deeper study of synthetic caption quality/style/length and their interaction with OCRâ€‘heavy continual pre-training (Appendix A.1).
  - Smarter, learnable grid selection or tokenâ€‘budget allocation per image to automate the resolutionâ€“compute trade-off.
  - Robust evaluation protocols that reduce reliance on LLMâ€‘judged scoring for video and ensure minimal trainâ€‘test overlap.
- Practical applications
  - Document AI: invoices, contracts, scientific figures, charts (Table 6).
  - Grounded assistants and agents: UI automation, regionâ€‘aware instruction following (Table 7; Section 6).
  - Multi-image analytics: surveillance change detection, medical imaging series, retail catalog comparisons (Table 9).
  - Edge deployment: 1B/3B dense and MoE variants make on-device multimodal assistants more realistic (Table 4).

Overall, `MM1.5` demonstrates that careful data curation, highâ€‘resolution processing, and dynamic token allocationâ€”not just larger modelsâ€”are key to mastering OCRâ€‘heavy, grounded, and multiâ€‘image reasoning tasks. The paperâ€™s ablations and consistent gains across scales make its recipe a practical blueprint for building capable generalist and specialized MLLMs.

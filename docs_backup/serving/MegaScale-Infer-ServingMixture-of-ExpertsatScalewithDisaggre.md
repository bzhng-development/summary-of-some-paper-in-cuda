# MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism

**ArXiv:** [2504.02263](https://arxiv.org/abs/2504.02263)

## ğŸ¯ Pitch

MegaScale-Infer pioneers a novel system for serving large-scale Mixture-of-Experts (MoE) language models by decoupling attention and feed-forward (expert) modules within each Transformer layer, allowing independent scaling and specialized hardware for each. This architectural disaggregation is paired with a smart, micro-batched 'ping-pong' pipeline and an optimized M2N communication library, transforming MoE token sparsity from a throughput bottleneck into an efficiency advantage. As a result, MegaScale-Infer significantly boosts GPU utilization and cost-effectiveness, achieving up to 1.9Ã— greater per-GPU throughput and slashing inference costs by 1.5â€“2Ã— compared to state-of-the-art systemsâ€”a crucial advancement for scalable, economical AI deployment.

---

## 1. Executive Summary
MegaScale-Infer introduces a new way to serve Mixture-of-Experts (MoE) large language models by disaggregating the attention and feed-forward (expert) parts of every Transformer layer onto different GPU pools, then stitching them together with a micro-batched â€œpingâ€‘pongâ€ pipeline and a specialized M2N communication library. This architecture turns MoEâ€™s token sparsity from a utilization problem into an advantage, delivering up to 1.90Ã— higher perâ€‘GPU decoding throughput than strong baselines on homogeneous clusters and up to 1.86Ã— higher throughput per unit cost under heterogeneous deployment (Figures 8 and 9; Abstract).

## 2. Context and Motivation
- Problem addressed
  - During decoding (token-by-token generation), attention is memoryâ€‘bound because it must read the keyâ€“value (KV) cache of all prior tokens, while the feedâ€‘forward network (FFN) becomes computeâ€‘efficient only at large batch sizes (Â§2.1).
  - In MoE, each token activates only K of E experts (e.g., topâ€‘2 of 8). This sparsity reduces the number of tokens per expert, making each expertâ€™s batch small and underutilizing GPUs (Â§2.3, Figure 1b).

- Why it matters
  - Serving costs and throughput: Low GPU utilization during decoding dominates production inference cost. The paper quantifies that for an A100 GPU (312 TFLOPs, 2 TB/s), a dense FFN needs batch size b â‰¥ F/B = 156 to saturate compute. With MoE topâ€‘2-ofâ€‘8 and batch 156, each expert sees 39 tokensâ€”only ~25% of ideal utilization (util â‰ˆ topk/E = 2/8; Â§2.3).

- Prior approaches and their limits
  - Standard parallelism: tensor parallelism (TP), pipeline parallelism (PP), and expert parallelism (EP) (Â§2.2). TP adds communication; EP suits MoE but still suffers from small per-expert batches in decoding.
  - Long-context disaggregation (e.g., Infiniteâ€‘LLM) focuses on dense attention/KV memory pressure in long-context settings and does not address MoEâ€™s token routing complexity (Â§2.4).
  - Prefill/decoding disaggregation (e.g., DistServe, Splitwise) removes interference between phases but leaves MoE sparsity during decoding unaddressed (Â§3).

- Positioning
  - MegaScale-Infer goes beyond phase disaggregation and separates attention and experts within each layer (Â§3, Figure 3). It pairs this with (1) microâ€‘batched pingâ€‘pong pipelining to keep both sides busy (Figure 4), (2) module-specific parallelism (replicate attention; EP for experts), (3) a search-guided deployment plan backed by a performance model (Algorithm 1), and (4) a high-performance M2N communication library to make token dispatch practical at scale (Â§Â§4â€“5).

## 3. Technical Approach
At a high level, the system splits each Transformer layerâ€™s computation into two GPU poolsâ€”attention nodes and expert nodesâ€”and moves microâ€‘batches back and forth between them in a carefully tuned pipeline (Figures 3â€“4).

- Key terms used once for clarity
  - KV cache: intermediate attention states (keys and values) saved per past token to speed up decoding (Â§2.1).
  - MoE: a layer that replaces the dense FFN with many experts (independent FFNs) and a gating network that routes each token embedding to the topâ€‘K experts (Â§2.2, Figure 2a).
  - Expert Parallelism (EP): different experts live on different devices; tokens are dispatched to the devices that host their selected experts (Figure 2b).
  - M2N communication: token dispatch between M attention senders and N expert receivers (and the reverse direction for aggregation) (Â§4).

Step-by-step design

1) Disaggregated expert parallelism (DEP)
- Architecture (Figure 3; Â§3)
  - Attention nodes: replicate attention parameters and store KV caches; use intra-node TP to exploit NVLink.
  - Expert nodes: each node stores one expertâ€™s parameters; all expert nodes together form the EP group; intra-node TP is used as needed.
  - Requests are batched globally, then split into microâ€‘batches that â€œpingâ€‘pongâ€ between attention and expert nodes in every MoE layer.

- Why disaggregate?
  - Aggregating requests from multiple attention replicas increases tokens seen by each expert across the instance, turning expert compute from memoryâ€‘bound to computeâ€‘bound (Â§2.4; Figure 1c).
  - It enables heterogeneous deploymentâ€”attention on GPUs with strong memory bandwidth/capacity; experts on GPUs with costâ€‘effective compute (Â§4.3; Table 3).

2) Pingâ€‘pong pipeline parallelism (PPP)
- Problem: If attention and experts are separated, each side would idle while waiting for the other or for network transfers (Â§4.1).
- Solution: Split the global batch into m microâ€‘batches and run them in a wavefront so that when attention is computing on microâ€‘batch i, experts are computing on iâ€‘1 and communication for i or iâ€‘1 is overlapped (Figure 4).
- When does it work? Three conditions derived and enforced by the deployment planner (Â§4.1):
  - Balance compute: Ta â‰ˆ Te (Eq. 1), where Ta and Te are perâ€‘microâ€‘batch compute times on attention and expert nodes.
  - Communication shorter than compute: Tc < Tf, with Tf = max{Ta, Te} (Eq. 2).
  - Enough microâ€‘batches to cover two crossings per layer: m Ã— Tf â‰¥ 2 Ã— (Tf + Tc), i.e., m â‰¥ 2 Ã— (1 + Tc/Tf) (Eq. 3). In fast networks (Tc < Tf/2), m â‰¥ 3 is sufficient (Â§4.1).
- Latency model: For L MoE layers, the iteration latency per microâ€‘batch is bounded by (Ta + Te + 2Tc) + mTf(L âˆ’ 1) â‰¤ Titer â‰¤ mTfL; total latency of the global batch is Ttotal = (Ta + Te + 2Tc) + Tf(mL âˆ’ 1) (Eq. 4â€“5).

3) Deployment plan search with a calibrated performance model
- Search space and constraints (Â§4.2; Algorithm 1, Table 1)
  - Variables: attention TP size (tpa), expert TP size (tpe), number of attention nodes (na), number of microâ€‘batches (m), global batch size (B).
  - Constraints: service-level objective on timeâ€‘betweenâ€‘tokens (Titer â‰¤ SLO; Eq. 7), and memory capacity for attention GPUs to hold KV cache and attention parameters (4 m b_a s h L / g + 2Pa < tpa Ca; Eq. 8).
- Simulation and balancing
  - Attention compute time is modeled as Ta â‰ˆ k1 ba + k2, experts as Te â‰ˆ k3 be + k4, where ba and be are perâ€‘microâ€‘batch token counts per attention/expert node and kâ€™s come from profiling (Â§4.2; Table 2 lists GEMM shapes considered).
  - Balance condition (Eq. 1) translates to choosing na so that na = (k1 E) / (k3 K) (since ba m na = be m E/K = B; Â§4.2).
  - Communication time per microâ€‘batch Tc is the max of Aâ†’E and Eâ†’A transfers, each estimated from measured link utilization curves as Tc = max{ bahK/tpa / (WaÃ—Util(â€¦)), beh/tpe / (WeÃ—Util(â€¦)) } (Eq. 6).
  - The planner enumerates feasible (tpa, tpe), chooses na to meet Ta â‰ˆ Te, tries m âˆˆ {3,4,â€¦}, and binaryâ€‘searches the largest B that respects SLO; it outputs the plan with best throughput per unit cost (Algorithm 1).

4) Highâ€‘performance M2N communication library (Â§5; Figures 5â€“7)
- Motivation from measurement: standard NCCL incurs extra copies through a CPU proxy, processes peer operations in small groups, and involves GPU synchronizationâ€”leading to higher median and tail latency, especially as the number of receivers grows (Figure 5).
- Design choices (CPUâ€‘orchestrated RDMA; no superfluous copies or GPU synchronizations)
  - Preâ€‘registered GPU buffers; synchronization via CUDA events to ensure producer kernels finished (Figures 6â€“7).
  - Block the CUDA stream with cuStreamWaitValue32 while host issues RDMA Write with immediate to all receivers and polls their completion queues (CQs); then unblock the stream by writing a shared flag (Â§5).
  - Receivers poll CQs and perform a GPU-visible flush with GDRCopy to ensure data visibility, then unblock (Â§5).
  - Traffic optimizations: prioritize ACK packets on separate highâ€‘priority queues and fineâ€‘tune congestion control, which stabilizes tail latency under unbalanced traffic (Â§5).
- Rationale vs DeepEP (Â§5): CPUâ€‘driven data plane avoids consuming GPU SM resources and cache contention; at the typical perâ€‘peer sizes here (hundreds of KB), a single CPU thread can saturate the link. For much smaller messages and many QPs, GPUâ€‘driven approaches may win.

5) Implementation extras (Â§6)
- Fused kernels: (i) fuse intraâ€‘node TP allâ€‘gather with subsequent GEMM using Flux; (ii) fuse gating, topâ€‘K selection, token counting/weighting, and scatter into one pass to reduce memory traffic (Â§6).
- Expert load balancing: replicate hot experts on device proportionally to observed popularity from recent traffic to minimize max perâ€‘node cost with a greedy approximation (Â§6).
- Code: a PyTorch extension with ~4900 C/C++ and ~5000 Python LoC; relies on GPUDirect and GDRCopy (Â§6).

6) Heterogeneous deployment (Â§4.3; Table 3)
- Insight: attention is memoryâ€‘bound and KVâ€‘heavy; experts are computeâ€‘bound. Table 3 shows perâ€‘cost memory bandwidth/capacity favor H20, while perâ€‘cost TFLOPs favor L40S. The planner enumerates hardware pairings and often chooses H20 for attention and L40S for experts.

## 4. Key Insights and Innovations
- Disaggregated expert parallelism (DEP) within each layer
  - Novelty: goes beyond phase disaggregation by splitting attention and experts and scaling them independently (Â§3, Figure 3).
  - Why it matters: raises tokens per expert by consolidating demand from multiple attention replicas, recovering FFN compute efficiency lost to MoE sparsity (Figure 1c).

- Pingâ€‘pong pipeline with principled conditions
  - Novelty: a microâ€‘batch pipeline across attention and experts with explicit conditions to fully hide communication (Eqs. 1â€“3; Figure 4).
  - Significance: prevents idle time on either side and makes perâ€‘layer crossâ€‘cluster communication practical without hurting latency beyond SLO (Â§4.1).

- Performanceâ€‘modelâ€‘guided deployment search
  - Novelty: a compact, profileâ€‘calibrated model ties together compute balance, network utilization vs. message size, memory limits, and SLO to choose tpa/tpe/na/m/B (Algorithm 1; Eqs. 4â€“8).
  - Significance: ensures plans fill the pipeline (m), balance module times (na), and keep M2N under compute (Tc < Tf), maximizing throughput per cost (Â§4.2).

- A purposeâ€‘built M2N communication layer
  - Novelty: CPUâ€‘driven RDMA Writes with immediate, no GPUâ€‘toâ€‘CPU copies, no NCCL group overhead, explicit stream blocking/unblocking, plus prioritized ACK and tuned congestion control (Â§5; Figures 6â€“7).
  - Significance: reduces median and tail latency and raises throughput substantially vs. NCCL in the tokenâ€‘dispatch regime (Figures 11â€“12).

- Heterogeneous hardware coâ€‘design
  - Novelty: uses perâ€‘cost metrics (Table 3) to place attention on bandwidthâ€‘rich H20 and experts on computeâ€‘efficient L40S (Â§4.3).
  - Significance: amplifies gains into throughput per dollar and per watt improvements (Figures 9â€“10).

These are fundamental architectural shifts (DEP, PPP, M2N) combined with a principled planner; not just kernelâ€‘level tweaks.

## 5. Experimental Analysis
- Setup (Â§7.1)
  - Clusters: (i) 8 nodes with 8Ã—80GB Ampere GPUs each (NVLink intraâ€‘node; 200 Gbps NICs), (ii) heterogeneous cluster with H20 (900 GB/s NVLink; 4Ã—400 Gbps) and L40S (PCIe; 2Ã—400 Gbps).
  - Models (Table 4): Mixtralâ€‘8Ã—22B (E=8, K=2), DBRX (E=16, K=4), Scaledâ€‘MoE 317B (E=32, K=4).
  - Workload: production traces; median prompt 571 tokens; median output 159 tokens; bfloat16 for weights/activations/KV.
  - Baselines: vLLM and TensorRTâ€‘LLM with standard optimizations; all methods evaluated with prefill and decoding temporally separated to avoid interference (Â§7.1).

- Primary metric and SLO
  - Decoding throughput per GPU or per cost (heterogeneous), with Timeâ€‘Betweenâ€‘Tokens (TBT) SLO = 150 ms (Â§7.1).

- Main results on homogeneous Ampere (Figure 8)
  - Decoding throughput per GPU:
    - DEP+PPP+M2N (MegaScale-Infer) vs baselines: 
      > â€œachieves 2.56Ã— and 1.28Ã— higher perâ€‘GPU decoding throughput than vLLM and TensorRTâ€‘LLMâ€ across Mixtral and DBRX (Figure 8a).
      - On the largest model (Scaledâ€‘MoE 317B), where interâ€‘node comms dominate for baselines, improvements reach
      > â€œ7.11Ã— vs vLLM and 1.90Ã— vs TensorRTâ€‘LLMâ€ (Figure 8a).
  - Latency:
    - Despite perâ€‘layer crossâ€‘node transfers, TBT is comparable to baselines (Figure 8b), because communication is overlapped by PPP and accelerated by M2N.
  - Endâ€‘toâ€‘end throughput (prefill + decoding):
    - Gains are smaller since prefill is computeâ€‘bound and not helped by DEP; still up to 1.18Ã— better (Figure 8c).

- Heterogeneous results (H20 attention + L40S experts; Figure 9)
  - Decoding throughput per cost:
    - Compared to vLLM (H20) and TensorRTâ€‘LLM (H20):
      > â€œup to 3.24Ã— and 1.86Ã—â€ improvement, respectively (Figure 9a).
  - Latency:
    - TBT comparable to baselines; slightly better than L40Sâ€‘only deployments (Figure 9b).
  - Endâ€‘toâ€‘end throughput per cost:
    - Up to 1.66Ã— improvement (Figure 9c).
  - Power efficiency:
    - Throughput per watt improved by 1.80Ã— (decoding) and 1.72Ã— (endâ€‘toâ€‘end) due to matching bandwidthâ€‘perâ€‘watt (H20) and computeâ€‘perâ€‘watt (L40S) to workload (Â§7.2; Figure 10).

- M2N microâ€‘benchmarks (Figures 11â€“12)
  - Varying message sizes (2 KB to 8 MB), M=N=8:
    - Median latency reduced by up to 80.8%; P99 reduced by up to 96.2%; throughput up to 9.9Ã— higher vs NCCL (Figure 11).
    - At a representative size (â‰ˆ256 KB per peer), improvements are
      > â€œ68.2% lower median latency, 92.9% lower P99, and 4.2Ã— higher throughputâ€ (Figure 11).
  - Scaling M and N (4 to 32) at 256 KB:
    - Tail latency reduced by 54.7%â€“96.9%; throughput improved by 3.3Ã—â€“5.8Ã— (Figure 12).
  - Takeaway: the tokenâ€‘dispatch regime is where M2Nâ€™s design decisions pay off.

- Ablations (Figures 13â€“15)
  - Effect of disaggregation and M2N:
    - Disaggregating attention and experts alone (using NCCL) yields up to 4.66Ã— speedup over colocated baselines; adding M2N gives up to an additional 1.53Ã— (Figure 13).
  - Microâ€‘batches (m):
    - Going from m=1 (no pipeline) to m=2 approximately halves idle time (â‰ˆ1.9Ã— throughput). Raising to m=3 enables full overlap of comms and compute, adding 1.10Ã—â€“1.38Ã— more (Figure 14). Larger m brings diminishing returns in highâ€‘bandwidth settings.
  - Balancing attention replicas (DP) for DBRX:
    - Throughput scales linearly and latency stays flat as DP grows from 1â†’4 (attention bottleneck). At DP=8, Taâ‰ˆTe and throughput peaks. Beyond that, experts become the bottleneck and latency rises while normalized throughput falls (Figure 15).

- Deployment at scale
  - In production on ~10,000 GPUs and, under heterogeneous deployment, reduces cost by 1.5â€“2.0Ã— for the same traffic (Â§8).
  - Real traffic analysis finds both expert and attention imbalances; the system uses expert replication and batch composition to balance runtime (Â§8; Figure 16).

- Do the experiments support the claims?
  - Yes, across three MoE models, two clusters, two strong baselines, and with ablations that isolate each componentâ€™s effect. The separation of prefill and decoding for all systems ensures a fair applesâ€‘toâ€‘apples decoding comparison (Â§7.1). The M2N advantages are validated via focused microâ€‘benchmarks (Figures 11â€“12).

## 6. Limitations and Trade-offs
- Additional perâ€‘layer communication
  - DEP introduces two crossâ€‘pool transfers per MoE layer (Aâ†’E, Eâ†’A). PPP hides most of it, but the SLO constraint (Eq. 7) and pipelineâ€‘fill condition (Eq. 3) can force choices (e.g., mâ‰¥3 or 4) that slightly increase perâ€‘token latency (Figure 8b).
- Network and system assumptions
  - The design assumes RDMAâ€‘capable networking and benefits from multiple highâ€‘speed NICs per node (e.g., 200â€“400 Gbps; Â§7.1). Commodity Ethernet without RDMA or with limited bandwidth would reduce Tcâ€‘hiding effectiveness.
- CPU involvement in the data plane
  - The M2N library uses CPUâ€‘orchestrated RDMA. This is ideal for hundredsâ€‘ofâ€‘KB messages and modest QP counts (Â§5), but for very small messages and many QPs, GPUâ€‘driven approaches (e.g., DeepEP) may achieve higher peak packet rates at the cost of GPU SM time (Â§5).
- Planning model portability
  - The Ta/Te linear models and network Util(Â·) curves are learned from profiling (Â§4.2). Porting to new hardware or drivers requires reâ€‘profiling to maintain accuracy.
- Memory pressure on attention GPUs
  - Attention nodes hold KV caches; Eq. (8) shows memory grows with microâ€‘batches m, sequence length s, hidden size h, and layers L. Extremely long contexts or very large L may limit feasible m or batch size B.
- Expert load balancing via replication
  - Replicating hot experts (for load balance) consumes additional memory capacity and requires periodic replanning (Â§6, Â§8), which adds operational complexity.

## 7. Implications and Future Directions
- How this changes the field
  - Establishes that disaggregating attention and experts is a firstâ€‘order architectural lever for MoE inferenceâ€”comparable in impact to prefill/decoding disaggregationâ€”unlocking high utilization despite MoE sparsity (Figures 1c, 8, 9).
  - Demonstrates that a targeted communication layer (M2N) can materially change systemâ€‘level outcomes for token routing (Figures 11â€“12).

- Followâ€‘up research enabled
  - Adaptive runtime planning: dynamically adjust m, na, and even hardware assignment as load and sequence-length distributions shift.
  - Unified CPU/GPU communication strategies: combine CPUâ€‘based M2N and GPUâ€‘driven paths (Ã  la DeepEP) and switch based on message size/QP count (Â§5).
  - Crossâ€‘phase coâ€‘design: extend DEP to optimize prefill as well (e.g., selective attention offload or attentionâ€‘specific accelerators) without hurting latency (Â§7.2).
  - Smarter gating/dispatch: incorporate preâ€‘gating or token clustering to reduce communication volume (related to Â§6 and [47] in References).
  - Wider heterogeneity: include future bandwidthâ€‘rich/computeâ€‘lean accelerators for attention and computeâ€‘dense cards for experts; integrate perâ€‘watt planning (Figures 9â€“10; Table 3).

- Practical applications
  - Costâ€‘optimized deployment of production MoE models (e.g., Mixtral, DBRXâ€‘style architectures) in cloud and onâ€‘prem clusters, with substantial savings under heterogeneous fleets (Â§7.2, Â§8).
  - Multiâ€‘tenant serving platforms: DEP lets operators independently scale attention (memoryâ€‘bound) and expert (computeâ€‘bound) pools and apply targeted loadâ€‘balancing policies (Figure 16; Â§8).

In short, MegaScale-Infer provides a principled, endâ€‘toâ€‘end recipeâ€”architecture, pipeline, planner, and communicationâ€”to make largeâ€‘scale MoE decoding efficient and economical. The combination of DEP, PPP, and M2N is broadly applicable to other conditionalâ€‘compute architectures that exhibit similar sparsityâ€‘induced underutilization.

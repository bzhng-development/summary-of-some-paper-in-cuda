# Log-Linear Attention

**ArXiv:** [2506.04761](https://arxiv.org/abs/2506.04761)

## ğŸ¯ Pitch

Log-Linear Attention introduces a novel attention mechanism that achieves a 'sweet spot' between the efficiency of linear/state-space models and the expressive recall of softmax attention. By using a hierarchical Fenwick-tree-based memory that grows logarithmically with sequence length, it enables O(T log T) training and O(log T) memory/time for decodingâ€”significantly improving recall and long-context modeling while preserving parallelism and hardware efficiency. This breakthrough directly addresses a central limitation in linear-time models and unlocks powerful, scalable sequence modeling for applications requiring long-range context, such as document understanding and code generation.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces Log-Linear Attention, a family of attention layers that keep the matmul-rich parallelism of linear/state-space models while expanding the modelâ€™s memory from a fixed size to O(log T) hidden states for a sequence of length T. Using a Fenwick-tree (binary indexed tree) partition of the past and a hierarchical-mask matrix, it achieves O(T log T) training time, O(log T) decoding time/space, and improves long-context recall over strong linear-time baselines such as Mamba-2 and Gated DeltaNet (Table 1, Â§3â€“Â§4).

## 2. Context and Motivation
- Problem addressed
  - Efficient attention variants (linear attention and state-space models) run in linear time with constant memory during decoding, but they compress all history into a single fixed-size state. This limits recall of arbitrary items deep in the context and degrades performance on long sequences (Â§1; Â§2 â€œRelationship between masking structure and efficient algorithmsâ€; Fig. 6).
- Why it matters
  - Long-context modeling is key for tasks like long-document QA, code, and streaming. Full softmax attention is accurate but has quadratic compute and linear memory in sequence length, which becomes a training and inference bottleneck even with optimized kernels (FlashAttention) (Â§1; Table 1).
- Prior approaches and shortcomings
  - Softmax attention: accurate and parallelizable but O(T^2) compute and O(T) memory (Â§2).
  - Linear attention and SSMs (e.g., RetNet, Mamba-2, DeltaNet): O(T) compute and O(1) decoding memory by using a single hidden state, which can hurt associative recall and long-range use of context (Â§1â€“Â§2; [2], Fig. 6, LongBench results in Table 4).
  - Long convolutions: subquadratic compute but still linear memory during inference and limited parallel training efficiency (Â§2, â€œLong convolution modelsâ€).
- Positioning of this work
  - A â€œmiddle groundâ€ between linear attention and softmax: keep efficient parallel training, but grow the state size logarithmically with sequence length using a hierarchical (H-matrix) structure for the causal mask (Â§3; Fig. 1; Remark below Eq. 4). It is a framework that can wrap existing linear attention variants (demonstrated on Mamba-2 and Gated DeltaNet in Â§3.3).

## 3. Technical Approach
High-level idea: replace the single recurrent state used by linear attention with a small set of states organized hierarchically so that a query at time t only needs O(log t) summaries of the past. Use a structured mask M^H so the whole layer remains a matmul program for parallel training.

Step 1: Unified formulation
- Many efficient attention variants can be written as:
  - P = A âŠ™ M, O = P V  (Eq. 1), where A captures queryâ€“key interactions (e.g., QK^T) and M is a lower-triangular causal mask (Â§2).
- Different models differ in A and the structure of M (Table 1). Efficient training/inference hinges on Mâ€™s structure (Â§2 â€œRelationship between masking structure and efficient algorithmsâ€).

Step 2: Fenwick-tree partition of the past (what states to keep)
- Define a bucketization of the prefix [0, t) into disjoint segments whose lengths are powers of two, biased to keep the most recent tokens at highest resolution (Â§3.1; Fig. 2).
  - Use `lssb(t)`: index of the least significant set bit in tâ€™s binary representation to greedily subtract the largest power of two from the remaining prefix (formal recurrence on p. 4).
  - This yields at most L = O(log t) buckets B_t^(â„“), with sizes 1, 2, 4, â€¦; more recent segments are smaller and finer (Â§3.1).
- Maintain per-level hidden states S_t^(â„“) that summarize each bucket as a matrix of fast weights:
  - S_t^(â„“) = Î£_{s âˆˆ B_t^(â„“)} v_s k_s^T (Eq. 3, left term).
- Combine per-level states with data-dependent level weights:
  - Output at time t: o_t = Î£_{â„“=0}^{L-1} Î»_t^(â„“) q_t^T S_t^(â„“) (Eq. 3).
    - Î»_t^(â„“) are nonnegative scalars computed from the input at t (a small linear head), letting the model emphasize different temporal scales (Â§3.1). If all Î»â€™s are tied, the method collapses to vanilla linear attention (Â§3.1).

Step 3: Efficient parallel form for training (how to compute with matmuls)
- The above can be written as a pure matrix product with a structured causal mask:
  - O = (QK^T âŠ™ M^H) V, where M^H_{t,s} = Î»_t^{â„“(t,s)} if s â‰¤ t, else 0 (Eq. 4).
- Structure: M^H is a lower-triangular hierarchical matrix (specifically HODLR-like), with recursively partitioned off-diagonal blocks that are low rank (Â§3.1 Remark; App. B.1). This â€œquasi-Hâ€ structure enables both:
  - O(T log T) training via chunked parallelism (Â§3.2) and
  - O(log T) decoding time/space (Â§3.1 â€œMemory-efficient decodingâ€).

Step 4: O(log T) decoding algorithm (how to update states online)
- Keep only O(log t) states S_t^(â„“). When a new token arrives at t:
  - Insert v_t k_t^T at level â„“=0.
  - Merge and promote all â€œfullâ€ lower levels up to â„“ = lssb(t) into the next coarser level, and zero the merged levels (update rule in Â§3.1, â€œMemory-efficient decodingâ€).
  - This is the same binary-carry logic as Fenwick trees; updating and forming o_t uses only O(log t) states.
- Complexity: O(log T) time and memory per decoding step (Â§3.1).

Step 5: O(T log T) training via chunkwise parallel scan (how to keep it hardware-friendly)
- Split the sequence into chunks of length C and compute all chunks in parallel, passing summarized states between chunks where necessary (as in linear attention), but now for each hierarchy level (Â§3.2; Fig. 3 right; Algorithm 1).
- Decompose the hierarchical mask:
  - M^H = D + Î£_{â„“=1}^{L-1} M^(â„“), where D is block-diagonal (intra-chunk) and M^(â„“) captures inter-chunk dependencies at hierarchy level â„“ with blockwise low rank (Eq. in Â§3.2; Fig. 3 left).
- Two-stage computation (Â§3.2):
  - Intra-chunk (â„“=0): dense lower-triangular matmul per chunk; total O(TÂ·C).
  - Inter-chunk (â„“>0): each level reduces to a sequentially semiseparable (SSS) matrix pass; invoke the existing linear-attention â€œstate-passingâ€ primitive O(log (T/C)) times; each invocation is O(T) time and memory, so O(T log T) overall.
- Why this is fast in practice:
  - Itâ€™s â€œchunk-scanâ€: O(log T) independent scans over chunks, each scan rich in matmuls (Blelloch scan style), avoiding token-level scans that are bandwidth-limited (Â§3.2).

Step 6: Plug into existing models (how to use with Mamba-2 and Gated DeltaNet)
- Compose the original mask with the hierarchical mask:
  - Log-Linear Mamba-2: O = (QK^T âŠ™ M^S âŠ™ M^H) V, where M^S encodes data-dependent scalar forgetting (Eq. 2) (Â§3.3).
  - Log-Linear Gated DeltaNet: O = (QK^T âŠ™ L (I + KK^T âŠ™ (L âˆ’ I))^-1 âŠ™ M^S âŠ™ M^H) V (Â§3.3).
- The A-part (interaction structure) of each model stays unchanged; only the mask gains the hierarchical factor (Â§3.3).

Step 7: Implementation details that matter
- Custom Triton kernels fuse multiple levels per kernel to reduce overhead (â€œlevel fusionâ€) and share gradient computations across levels (Â§3.4; App. C). Fig. 4 shows runtime and throughput comparisons.

Optional generalization
- Appendix A shows how to extend beyond scalar gates to matrix-valued transitions using 4D â€œH-tensors,â€ enabling log-linear variants of more expressive linear RNNs with matrix-valued Ct (Â§3.5; App. A).

Key definitions used
- Fenwick tree (binary indexed tree): a structure supporting prefix sums and updates in O(log T) by maintaining overlapping power-of-two segments of the array (Â§3.1; [16, 54]).
- HODLR matrix: a recursively partitioned matrix whose off-diagonal blocks are low rank; supports O(T log T) storage and matvec (Â§3.1 Remark; App. B.1).
- SSS (sequentially semiseparable): a matrix whose off-diagonal blocks are low rank in a way that admits linear-time scan-like algorithms; many gated linear attentions yield SSS masks (Â§2; Â§3.2).

## 4. Key Insights and Innovations
- Logarithmically growing state via Fenwick-partitioned memory (fundamental)
  - Instead of a single fixed-size state, keep O(log T) per-level states S_t^(â„“) that each summarize a power-of-two-sized recent window, and weight them by learnable Î»_t^(â„“) (Eq. 3; Â§3.1; Fig. 1â€“2). This significantly increases expressive capacity for recall while preserving efficient O(log T) updates.
- Hierarchical-mask view of attention (theoretical framing + algorithmic pay-off)
  - The layer becomes O = (QK^T âŠ™ M^H) V where M^H is a lower-triangular hierarchical matrix (HODLR-like; â€œquasi-Hâ€) (Â§3.1 Remark; App. B.1â€“B.3). This perspective directly yields the O(T log T) chunk-scan algorithm (Â§3.2) and explains why decoding can be O(log T).
- Chunkwise parallel scan for hierarchical masks (engineering innovation)
  - Training reduces to O(log T) invocations of an existing inter-chunk primitive (the same used by linear attention/SSMs), plus small per-chunk dense work for level 0 (Algorithm 1; Â§3.2; Fig. 3). This preserves matmul-rich parallelism preferred by GPUs and outperforms FlashAttention-2 at long lengths for the implemented case (Fig. 4).
- Plug-and-play extension to multiple linear-time architectures (practical breadth)
  - By composing masks (M â† M âŠ™ M^H), the method upgrades Mamba-2 and Gated DeltaNet to log-linear variants with modest parameter overhead (<3% for Mamba-2; <0.4% for Gated DeltaNet; Â§4.2) and improved long-context behavior (Tables 2â€“4; Fig. 6â€“7).
- Optional: H-tensor generalization (incremental but enabling)
  - Appendix A shows how to extend to matrix-valued level weights and transitions (Ct), pointing to compatibility with more expressive linear RNNs beyond rank-1 gates (Â§3.5; App. A).

## 5. Experimental Analysis
Evaluation setup
- Implementation/runtime:
  - Custom Triton kernels; experiments on H100 for kernel benchmarks with 48 heads, head dim 64, state dim 128, chunk size 64 (Â§3.4; Fig. 4; App. Câ€“D).
- Pretraining:
  - 50B tokens on Long-Data-Collections at sequence length 16K; 21 layers; hidden size 1536; models sized ~700â€“825M parameters (Â§4.2). Also a 24-layer Transformer (~778M) for size matching.
- Benchmarks:
  - Synthetic associative recall (MQAR) (Fig. 5; Table 6).
  - Per-position loss on Book3 (39M tokens) to gauge use of long context (Fig. 6).
  - Needle-in-a-Haystack (RULER) single/multi-needle variants at 4Kâ€“16K context (Fig. 7; Table 7).
  - Standard language modeling and zero-shot reasoning: WikiText perplexity, LAMBADA, PIQA, HellaSwag, WinoGrande, ARC-e/c (Table 2).
  - Real retrieval tasks at multiple truncation lengths (SWDE, SQuAD, FDA, TriviaQA, DROP, NQ) (Table 3).
  - LongBench (14 long-context tasks including GovReport, QMSum, MultiNews, etc.) (Table 4).

Main quantitative findings
- Kernel/runtime
  - > â€œThe custom kernel for log-linear Mamba-2 outperforms FlashAttention-2 (forward + backward) at sequence lengths beyond 8Kâ€ (Â§3.4; Fig. 4). Log-linear Mamba-2 â€œsurpasses Transformer throughput at 32Kâ€ in a full stack with MLPs (Â§3.4).
- Synthetic recall (MQAR; Fig. 5 and Table 6)
  - Log-linear variants match or improve over their linear baselines. For example, at model dim 64, Gated DeltaNet already achieves â‰¥99%, and log-linear maintains that; at lower dims (32), Log-Linear Gated DeltaNet improves from 79.0% to 84.4%; Log-Linear Mamba-2 improves from 75.1% to 76.5% (Table 6).
- Language modeling and zero-shot reasoning (Table 2)
  - Short-context tasks show parity or modest gains:
    - Log-Linear Mamba-2 improves LAMBADA perplexity from 24.14 to 21.86 and slightly improves average zero-shot accuracy (44.8 â†’ 44.9).
    - Log-Linear Gated DeltaNet improves WikiText ppl (21.73 â†’ 21.44), LAMBADA ppl (19.71 â†’ 18.08), and average zero-shot accuracy (45.0 â†’ 45.6), outperforming the 21-layer Transformer on all metrics and the 24-layer Transformer on ~half (Table 2).
- Per-position loss (long-context usage; Fig. 6)
  - Across positions up to 16K, both log-linear variants show consistently lower smoothed loss than their linear counterparts, indicating better use of distant context (Â§4.2; Fig. 6). Log-Linear Gated DeltaNet approaches the 24-layer Transformerâ€™s curve but still leaves a gap to the parameter-matched Transformer.
- Needle-in-a-Haystack (Fig. 7; Table 7)
  - In single-needle tasks, Log-Linear Mamba-2 improves in 8/9 metrics and reaches 100% at 4Kâ€“8K on passkey retrieval (S-NIAH-1). Log-Linear Gated DeltaNet improves three metrics and keeps strong performance where baseline is already high.
  - In multi-needle tasks, Log-Linear Mamba-2 improves in 8/9 metrics; Log-Linear Gated DeltaNet improves across all metrics (Fig. 7; Table 7). Despite gains, Transformers still lead by a margin on several NIAH settings.
- In-context retrieval (Table 3)
  - Mixed but notable gains:
    - Log-Linear Mamba-2 improves SQuAD across lengths (e.g., 21.6% â†’ 25.9% at 512 tokens), but degrades FDA at 8Kâ€“16K (e.g., 38.0% â†’ 37.5% at 1024; larger drops at longer lengths).
    - Log-Linear Gated DeltaNet shows consistent improvements across SWDE, SQuAD, and FDA at most lengths (e.g., SWDE at 2048: 27.2% â†’ 35.3%), except DROP where improvements are not observed (Â§4.2; Table 3).
- LongBench (Table 4)
  - Both log-linear variants outperform their linear baselines on 8/14 tasks. Gains are task-dependent; e.g., Gated DeltaNet improves SamSum from 23.1 â†’ 23.2 and TriviaQA from 25.3 â†’ 41.1, but drops on MultiNews (11.6 â†’ 1.9).

Do the experiments support the claims?
- The runtime and throughput results (Fig. 4) support the efficiency claims at long sequence lengths with a custom kernel.
- The per-position loss curves (Fig. 6), MQAR (Fig. 5), and NIAH (Fig. 7; Table 7) substantively support improved long-range recall compared to linear baselines.
- On broader language modeling and reasoning (Table 2) and retrieval (Table 3), gains are modest or mixed; the approach narrows but does not close the gap to Transformers.
- Ablations:
  - The paper does not include systematic ablations of Î»-parameterization or hierarchy design; Â§5 notes limited hyperparameter exploration. Appendix B discusses trade-offs between â€œweaklyâ€ vs. â€œstronglyâ€ admissible hierarchical structures (B.4), with the latter giving marginal accuracy gains but up to 4Ã— slowdown.

## 6. Limitations and Trade-offs
- Fixed hierarchical inductive bias
  - Fenwick-based partitioning emphasizes recent tokens with fine granularity and compresses distant tokens aggressively (Â§3.1; Â§5). This is beneficial in many tasks but may be suboptimal when precise long-range details matter uniformly.
- Mixed task outcomes
  - Improvements are not universal: several retrieval and LongBench tasks do not improve or regress (Table 3â€“4). NIAH improves but still lags Transformers in many settings (Table 7).
- Engineering and implementation complexity
  - The algorithm requires specialized kernels (level fusion, unified grads for K/V and Î») and bespoke intra-chunk logic; the backward pass must handle Î»-gradients (Â§5; Â§3.4; App. C). This raises the bar for adoption versus standard linear attention kernels.
- Computational profile
  - Training time is O(T log T), not O(T), and intra-chunk computation is quadratic in chunk size C (though C is small; Â§3.2). For short sequences, overheads may outweigh benefits (Fig. 4 shows advantages emerge beyond 8K tokens).
- Limited hyperparameter exploration
  - Î»-parameterizations and other design knobs are not extensively tuned due to compute constraints (Â§5), leaving open whether better settings yield stronger or more consistent gains.
- Structural choice: â€œweakly admissibleâ€ H
  - The chosen hierarchical structure leaves roughly half of the O(log T) states inactive at a time (App. B.4), a speed/engineering trade-off; â€œstrong admissibilityâ€ activates all levels but slows training substantially with small accuracy gains (App. B.4).

## 7. Implications and Future Directions
- Broader impact on efficient sequence modeling
  - Log-Linear Attention shifts the design space between fixed-state RNNs and full attention: it offers a principled way to grow memory sublinearly with sequence length while preserving GPU-friendly parallel training (Â§3). This can strengthen linear-time model families for long contexts.
- Practical applications
  - Long-document QA/summarization, retrieval-augmented generation, and streaming where O(log T) decoding memory is critical (on-device, low-latency) and O(T log T) training is acceptable. The runtime results (Fig. 4) suggest competitiveness for 8Kâ€“128K contexts with the provided kernels.
- Research directions
  - Better Î»-parameterizations and learning strategies:
    - Explore richer gating (matrix-valued Î›^(â„“) from App. A), cross-level interactions, or attention over levels to adaptively select temporal scales.
  - Alternative hierarchical decompositions:
    - Learnable or data-dependent partitions beyond Fenwick trees; investigate â€œstrongly admissibleâ€ H structures (App. B.4) or hybrid schemes that selectively activate levels with favorable speed/accuracy trade-offs.
  - Integration with retrieval and memory modules:
    - Combine log-linear memory with external memory/keyâ€“value caches for long-document tasks; the O(log T) decoding footprint is attractive for hybrid designs.
  - Extending to more expressive linear RNNs:
    - Use the H-tensor extension (App. A) to incorporate matrix-valued transitions Ct in modern linear RNNs (e.g., RWKV-style dynamics), testing whether expressivity gains carry over with log-linear memory.
  - System-level generalization:
    - Bring the chunk-scan primitives to other efficient layers (Toeplitz/Hyena) and to multi-device training; explore mixed-precision, asynchronous, or flash-like kernel variants for further speedups.

Quotes and anchors
- Complexity summary:
  > â€œLog-Linear Mamba-2 â€¦ Training O(T log T) â€¦ Decoding Time and Space O(log T)â€ (Table 1).
- Output rule:
  > o_t = Î£_{â„“=0}^{L-1} Î»_t^(â„“) q_t^T S_t^(â„“) with S_t^(â„“) = Î£_{sâˆˆB_t^(â„“)} v_s k_s^T (Eq. 3, Â§3.1).
- Parallel form:
  > O = (QK^T âŠ™ M^H) V, M^H_{t,s} = Î»_t^{â„“(t,s)} (Eq. 4, Â§3.1).
- Training algorithm:
  > â€œEach level reduces to a computation involving a sequentially semi-separable (SSS) matrixâ€¦ inter-chunk computation requires only O(log (T/C)) invocationsâ€¦ Each invocation costs O(T) â€¦ total O(T log T)â€ (Â§3.2; Fig. 3; Algorithm 1).
- Kernel/runtime:
  > â€œOutperforms FlashAttention-2 (forward + backward) at sequence lengths beyond 8Kâ€ and â€œsurpasses Transformer throughput at 32Kâ€ (Â§3.4; Fig. 4).
- Long-context usage:
  > â€œExtending both Mamba-2 and Gated DeltaNet to their log-linear counterparts consistently reduces the (smoothed) loss across various positionsâ€ (Fig. 6, Â§4.2).
- Limitations:
  > â€œThis design reflects a natural assumptionâ€¦ interactions between distant elements can be approximated in low-rank formâ€¦ may not be optimal for all applicationsâ€ (Â§5).

# What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity

**ArXiv:** [2511.15593](https://arxiv.org/abs/2511.15593)

## üéØ Pitch

The paper identifies and quantifies ideation diversity‚Äîthe variety of initial model/architecture ideas an agent proposes‚Äîas a key bottleneck for LLM-driven AI research agents, using a large-scale analysis of 11,000 trajectories on MLE-bench and a controlled prompt intervention that causally reduces diversity and harms performance. This matters because increasing ideation diversity reliably improves agents‚Äô success (across multiple metrics), de-risks implementation failures, and points to practical design levers (scaffolds and prompts) for building more effective automated research systems.

---

## 1. Executive Summary (2-3 sentences)
This paper studies why some autonomous ‚ÄúAI research agents‚Äù (LLM-driven systems that iteratively propose, implement, and evaluate ML solutions) succeed more often than others on `MLE-bench`, a benchmark of 75 Kaggle-style ML engineering tasks. It argues that a key bottleneck is `ideation diversity`‚Äîhow varied an agent‚Äôs initial solution ideas are‚Äîand provides both large-scale correlational evidence (11,000 trajectories) and a controlled intervention showing that reducing diversity via prompt changes causes measurable performance drops (e.g., ‚àí6.9 to ‚àí8.4 medal-rate points on MLE-bench lite; Figure 6).

## 2. Context and Motivation
- **Problem / gap addressed.**
  - Research agents execute long, multi-step trajectories (ideate ‚Üí code ‚Üí run experiments ‚Üí debug ‚Üí iterate), so diagnosing failures is harder than in standard ML experiments (Introduction).
  - Large-scale ablations over agent design choices are expensive because each trajectory involves tool use, model training, and many steps (Introduction).
  - The paper targets a specific hypothesis: `ideation diversity` is a bottleneck for agent performance (Introduction; Section 4 title ‚ÄúThe Ideation Diversity Bottleneck‚Äù).

- **Why it matters.**
  - If research agents are to accelerate scientific/ML progress, we need to know which design levers (scaffolds, prompts, search policies) reliably improve outcomes (Introduction, Discussion Section 5).
  - The paper emphasizes that in today‚Äôs imperfect agents, success depends not only on having a good idea but on generating ideas that are implementable under time/compute constraints‚Äîso diversity may ‚Äúde-risk‚Äù getting stuck (Discussion Section 5).

- **Prior approaches and shortcomings (as positioned here).**
  - Prior agent work exists in tool-use LLM agents and research agents (e.g., HuggingGPT; AIDE; AIRA) and in evaluation frameworks like MLE-bench (Introduction; Related Work Section 6).
  - The paper positions itself as filling an analysis/diagnostics gap: it performs a ‚Äúfirst-of-its-kind, large-scale‚Äù trajectory study and explicitly measures/controls diversity (Section 1.1 Contributions).

- **How the paper positions itself.**
  - It combines:
    1) **Observational analysis** of a large trajectory bank on MLE-bench, and  
    2) **Controlled experiments** that intervene on diversity mechanisms (prompt edits; Appendix A.1 also explores temperature).
  - It also expands evaluation beyond the default Kaggle-medal metric with additional metrics (Section 4.3; Appendix A.2).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)
- The system is an `LLM-based agent` that repeatedly generates ML solution ideas, writes Python code, runs training/evaluation, debugs failures, and iterates to improve leaderboard performance on Kaggle-like tasks (Sections 2‚Äì3; Figure 2).
- The paper studies how varying the agent‚Äôs `idea-generation diversity` (how different the initial model/architecture plans are) relates to and affects task success on `MLE-bench` and a 22-task subset called `MLE-bench lite` (Sections 3‚Äì4).

### 3.2 Big-picture architecture (diagram in words)
- **Environment:** `MLE-bench` tasks provide datasets + docs + submission format + evaluation (Section 2).
- **Agent scaffold (outer loop):** a search procedure that builds a tree of candidate Python solutions (Section 3.1.3).
- **LLM backbone (inner engine):** generates text actions/plans/code for operators like Draft/Debug/Improve (Sections 2, 3.1.4).
- **Diversity measurement module (analysis-time):** extracts planned model architectures from the first draft ideas and computes entropy (Section 3.2).
- **Diversity control knob (experiment-time):** modifies the system prompt to encourage diverse vs similar ideas (Section 3.3) and compares outcomes (Section 4.2).

### 3.3 Roadmap for the deep dive
- I first explain what `MLE-bench` is and what agents must do, since the evaluation setting shapes what ‚Äúperformance‚Äù means (Section 2).
- Then I describe the `agent scaffolds` and how trajectories are structured as trees with operators (Section 3.1.3; Figure 2).
- Next I detail how the paper defines and computes `ideation diversity` from initial ideas (Section 3.2).
- Then I walk through the two empirical pillars: (i) large-scale trajectory analysis (Section 4.1; Figures 1, 3, 4) and (ii) the controlled intervention (Section 4.2; Figures 5‚Äì6).
- Finally I cover the expanded metric suite and what it changes (Section 4.3; Figure 9; Appendix A.2).

### 3.4 Detailed, sentence-based technical breakdown
This is an **empirical analysis + controlled ablation paper** whose core idea is: *measure diversity in an agent‚Äôs initial solution plans, and test whether increasing that diversity improves benchmark outcomes* (Sections 3‚Äì4).

**What happens first, second, third (pipeline narrative).**
1. **Task setup and interaction.** Each `MLE-bench` task provides standardized artifacts‚Äîproblem documentation, training data, held-out test set, submission format, and an automated evaluator‚Äîmirroring Kaggle workflows (Section 2).
2. **Agent executes a scaffolded search.** The agent‚Äôs outer loop (`agentic scaffold`) conducts a search over candidate Python solutions, represented as nodes in a tree (Section 3.1.3).  
   - Each node corresponds to a concrete code solution attempt (Section 3.1.3; Figure 2).
3. **Operators generate and refine nodes.** The scaffold uses three operators (Section 3.1.3):
   - `Draft` generates initial solution ideas and associated code attempts (the start of exploration).
   - `Debug` fixes errors when execution fails.
   - `Improve` modifies a working solution to raise performance.
4. **Execution and feedback.** The agent runs code (training + evaluation), observes errors or scores, and uses that feedback to decide the next nodes to create (illustrated in Figure 2 with an execution error followed by a bug fix and successful ROC-AUC).
5. **Evaluation and scoring.** The paper primarily uses `medal rate` (successfully achieving bronze/silver/gold thresholds) as the default performance measure (Section 3.1.2), and later adds alternative metrics (Section 4.3).

**Agent scaffolds studied (what changes across agents).**
- The paper formalizes research agents as a combination of a `search policy` (how to navigate the solution tree) and `operators` (how to generate new solutions) (Section 3.1.3).
- It studies three scaffolds (Section 3.1.3):
  - `AIDE` (Jiang et al., 2025a): a tree-search agent using a `Greedy` policy.
  - `AIRAGREEDY` (Toledo et al., 2025): another greedy tree-based policy with different operator designs, prompts, and memory scope.
  - `AIRAMCTS` (Toledo et al., 2025): uses `Monte Carlo Tree Search (MCTS)` as the search policy rather than greedy selection.
- The scaffold also includes a `memory configuration` controlling which past artifacts are shown to the LLM for each operator, motivated as preventing context overload, mode collapse, and debug loops (Section 3.1.3).

**LLM backbones (what changes across models).**
- For the large-scale trajectory analysis, the paper uses 6 backbones: `o3`, `gpt-oss-20B`, `gpt-oss-120B`, `Llama Maverick`, `Devstral`, and `CWM` (Section 3.1.4).
- For the controlled diversity experiment, it uses `DeepSeek R1` (Section 3.1.4).
- All LLMs are stated to use a `128K-token context window` ‚Äúto ensure input coverage without truncation‚Äù (Section 3.1.4).

**Scale of the trajectory dataset (why the analysis is statistically meaningful).**
- The paper reports a trajectory bank built from:
  - `75 tasks` (full MLE-bench),
  - `6 LLM backbones`,
  - `2 agent scaffolds` (for the large-scale study; Section 4.1 discusses AIDE vs AIRAGreedy as an illustrative comparison),
  - `10 to 20 random seeds`,
  - totaling `11,000 trajectories`, about `1,200,000 individual nodes`, and `264,000 GPU hours` (Introduction).
- This scale underpins the correlation analyses shown in Figure 1 and related figures.

**How ideation diversity is defined and measured (the core method).**
- The paper focuses on **diversity of ML models/architectures** proposed in the `ideation phase`, not diversity in preprocessing, feature engineering, etc. (Section 3.2).
- Each agent begins by generating **up to five initial ideas** using the `Draft` operator (Section 3.2):
  - ‚ÄúExactly five‚Äù for greedy searches, and ‚Äúup to five‚Äù for MCTS (Section 3.2).
- From these initial ideas, the paper extracts (Section 3.2):
  1) The **high-level ML approach / architecture** (examples given: `CNN`, `Transformer`, `Decision Trees`), and  
  2) The **specific model family** (e.g., variants grouped: ‚ÄúEfficientNet-B4 is grouped as EfficientNet‚Äù).
- It then computes `Shannon entropy` (base 2) over the **distribution of architectures** (Section 3.2).  
  - In plain language, Shannon entropy measures how ‚Äúspread out‚Äù the choices are: higher entropy means the agent‚Äôs first ideas are distributed across more different architectures rather than repeating the same few.

**How diversity is controlled (the causal test).**
- The intervention is implemented by changing the **system prompt** given to the LLM behind the agent (Section 3.3).
- The paper compares **baseline (control) agents** vs **agents with ablated diversity** (Sections 3.3.1‚Äì3.3.2):
  - Baseline agents include three diversity mechanisms (Section 3.3.1):
    1) `Sibling memory`: a new draft node sees descriptions of sibling solutions.
    2) `Prompt-adaptive complexity`: the prompt requests increasing complexity across the five initial ideas (minimal ‚Üí moderate ‚Üí advanced).
    3) Explicit `mention of diversity` in the system prompt (asking for different aspects each time).
  - Low-diversity agents (Section 3.3.2):
    - Remove prompt-adaptive complexity and the mention of diversity.
    - Reuse sibling memory, but repurpose it to request *similar* ideas.
- The paper‚Äôs intent is to affect *only* ideation diversity and not coding quality, while acknowledging later that second-order prompt effects are hard to fully rule out (Section 5 ‚ÄúLimitations of this study.‚Äù).

**Core configurations / hyperparameters (what is and is not specified).**
- The paper provides several key experimental-scale configurations, but **does not specify** many training hyperparameters typical for ML model training (optimizer settings, learning rate schedules, batch sizes, model layer counts, etc.), likely because the paper studies *agents across many tasks/models* rather than introducing a single new ML model.
- What *is* specified in the provided content:
  - `128K` context window for all LLMs (Section 3.1.4).
  - Trajectory scale and compute: `11,000 trajectories`, `~1.2M nodes`, `264,000 GPU hours` (Introduction).
  - Diversity computed from `‚â§5` initial draft ideas (Section 3.2).
  - Controlled experiment scope: `22 tasks` (MLE-bench lite), `10 seeds`, `2 scaffolds` (AIRAGreedy, AIRAMCTS), `DeepSeek R1` (Sections 3.1.1, 3.1.4, 4.2).
- What is *not* provided in the excerpt and therefore cannot be filled in without guessing:
  - Exact system prompts (full text), beyond the described mechanisms (Section 3.3).
  - Exact per-task compute limits in the experiments, except a mention in Figure 2 of a ‚Äú4 hours‚Äù time limit in an example run and Section 5 referencing ‚Äú24 hours allotted‚Äù (these appear as contextual statements, but not as a fully specified experimental parameter grid).
  - Implementation details of architecture/model extraction (e.g., whether extraction is rule-based, LLM-based labeling, or metadata-driven), including inter-annotator reliability or parsing error rates (Section 3.2 describes what is extracted, not how extraction is operationalized).

## 4. Key Insights and Innovations
- **1) A concrete, operational measure of ‚Äúideation diversity‚Äù for research agents.**
  - The paper proposes quantifying diversity by applying `Shannon entropy` to the distribution of **architectures** proposed in the first five draft ideas (Section 3.2).
  - Why it‚Äôs notable: ‚Äúdiversity‚Äù is often discussed qualitatively in agent work; this makes it measurable and comparable across scaffolds/models.

- **2) Large-scale trajectory analysis linking scaffold choice ‚Üí diversity distribution.**
  - Figure 3 shows scaffold-driven differences even with the same backbone (`o3`):  
    - `AIDE` concentrates heavily on a few architectures/models (Figure 3a, 3c), while `AIRAGreedy` spreads probability mass across more architectures/models (Figure 3b, 3d).
  - This supports the claim that *agent design* (prompting, operators, memory, search policy) materially shapes what the agent even tries (Section 4.1).

- **3) Evidence that diversity is not just correlated, but causally beneficial in this setting.**
  - The controlled prompt intervention reduces the number of distinct architectures used (Figure 5) and also reduces performance (Figure 6).
  - The causal logic is: same backbone (`DeepSeek R1`), same scaffolds, same tasks/seeds; only diversity mechanisms are removed/negated (Sections 3.3, 4.2).

- **4) Robustness across multiple performance metrics beyond Kaggle medals.**
  - The paper introduces and applies four additional metrics (valid submission rate, average normalized score, percentile, Elo; Section 4.3.1) and shows the ‚Äúlow diversity hurts‚Äù result persists across them (Figure 9; Section 4.3.3).
  - This addresses a recognized weakness of `medal rate` as a discrete, competition-dependent measure (Appendix A.2; Table 1; Figure 13).

## 5. Experimental Analysis
### Evaluation methodology
- **Benchmarks / task sets.**
  - `MLE-bench`: 75 Kaggle competition tasks spanning CV, NLP, time series, tabular, and multimodal domains (Section 2).
  - `MLE-bench lite`: a curated subset of `22 tasks` used for controlled experiments (Section 3.1.1).

- **Agent scaffolds evaluated.**
  - Large-scale analysis covers multiple scaffolds and backbones; Section 3.1.3 lists AIDE, AIRAGreedy, AIRAMCTS.
  - Controlled experiment explicitly uses `AIRAGreedy` and `AIRAMCTS` with `DeepSeek R1` (Sections 3.1.4, 4.2; Figures 5‚Äì6).

- **Primary and secondary metrics.**
  - Primary: `Medal Success Rate` (‚Äúmedal rate‚Äù)‚Äîpercentage of attempts earning bronze/silver/gold by task-specific percentile thresholds (Section 3.1.2).
  - Additional metrics (Section 4.3.1):
    1) `Valid Submission Rate`
    2) `Average Normalized Score` (0 = lowest human score, 1 = highest human score on that task)
    3) `Percentile` vs human submissions
    4) `ELO-Based Agent Ranking` built from head-to-head score comparisons; ‚Äú100 Elo ‚âà 64% expected win probability‚Äù (Section 4.3.1)

### Main quantitative results (with numbers and where they appear)
- **Correlation: diversity vs medal rate (large-scale analysis).**
  - Figure 1 reports a positive correlation between ideation diversity (entropy) and medal rate:  
    > `Pearson r = 0.57, p-value = 4.65e-14` (Figure 1)
  - The scatter in Figure 1 is described as forming two clusters: higher-performing agents (o3, gpt-oss 120B/20B) vs open-source LLMs (Llama Maverick, Devstral, CWM) (Section 4.1).

- **Scaffold affects diversity distribution (example: AIDE vs AIRAGreedy with o3).**
  - From Figure 3 and Section 4.1:
    - AIDE: top architectures `GBDT` and `CNN` account for `70%` of initial draft nodes (Figure 3a; Section 4.1 text).
    - AIRAGreedy: top four architectures (`CNN`, `Transformer`, `GBDT`, `Hybrid`) account for `68%` (Figure 3b; Section 4.1 text).
    - Model-family concentration:  
      > ‚ÄúLightGBM and EfficientNet represent 43%‚Äù for AIDE, while ‚Äúas many as 9 models represent this percentage‚Äù for AIRAGreedy (Section 4.1; Figures 3c‚Äì3d).

- **Tree-level diversity also correlates with performance.**
  - Figure 4 uses ‚Äútree-level diversity‚Äù = average number of distinct architectures in the first five nodes.
  - Section 4.1 states:  
    > High-performing models use ‚Äú`3.5` distinct architectures on average‚Äù vs ‚Äú`2.8`‚Äù for Llama Maverick/Devstral/CWM (Figure 4; Section 4.1).

- **Controlled experiment: prompt-ablated diversity reduces diversity and performance.**
  - **Did the intervention change diversity?**  
    - Figure 5 shows low-diversity agents more often stay within ‚â§2 architectures:
      > Baselines have ‚â§2 architectures in only `40%` of tasks, while low-diversity variants do so in `70%` of tasks (Section 4.2.1; Figure 5).
  - **Performance drop (medal rate).**  
    - Figure 6 reports medal rates on MLE-bench lite (22 tasks), with 95% CIs via stratified bootstrapping (`rliable`) (Figure 6 caption):
      > `AIRAGreedy R1`: `45.5%` vs `38.6%` (low diversity) ‚Üí `‚àí6.9` points  
      > `AIRAMCTS R1`: `47.0%` vs `38.6%` (low diversity) ‚Üí `‚àí8.4` points  
      (Figure 6; Section 4.2.2)

- **Robustness across alternative metrics (controlled experiment).**
  - Figure 9 shows the same performance gap across all metrics (Section 4.3.3). Key values visible in Figure 9:
    - Valid Submission Rate: `98% ‚Üí 92%` (AIRAGreedy) and `98% ‚Üí 90%` (AIRAMCTS).
    - Average Normalized Score: `89 ‚Üí 83` (AIRAGreedy) and `91 ‚Üí 82` (AIRAMCTS).  
    - Percentile: `64 ‚Üí 60` (AIRAGreedy) and `65 ‚Üí 60` (AIRAMCTS).  
    - Medal Rate: `45 ‚Üí 39` (AIRAGreedy) and `47 ‚Üí 39` (AIRAMCTS).  
    - Elo: `1004 ‚Üí 998` (AIRAGreedy) and `1017 ‚Üí 982` (AIRAMCTS).  
    (Figure 9)
  - The paper explains a concrete failure pattern: for two text-normalization competitions, low-diversity agents repeatedly attempt `T5` and time out, whereas baseline agents try other solutions and succeed more often (Section 4.3.3).

- **Temperature-based diversity control is inconclusive / mixed.**
  - Appendix A.1 varies temperature for a stripped-down AIRAGreedy setting and reports:
    - Medal rate stays around `44‚Äì46` across temperatures `0.05, 0.2, 0.6, 1, 2`, while Elo increases with temperature (Figure 12; Appendix A.1).
    - Figure 12 values (as shown):
      > Medal Rate: `44, 45, 45, 46, 46`  
      > Elo: `983, 1006, 999, 1015, 1030`  
      (Figure 12)

### Do experiments support the claims?
- **Correlation claim:** Supported by statistically significant correlations in Figures 1, 7, 8 (with reported Pearson r and p-values).
  - Figure 7: diversity vs average normalized score  
    > `r = 0.72, p = 1.24e-24`
  - Figure 8: diversity vs percentile  
    > `r = 0.66, p = 1.39e-19`
- **Causal claim:** The controlled prompt intervention (Sections 3.3, 4.2) provides direct evidence that reducing diversity reduces performance (Figure 6), and the ‚Äúare we actually influencing diversity?‚Äù check (Figure 5) verifies the manipulation affects diversity as intended.
- **Metric robustness claim:** Figure 9 shows consistent degradation across multiple metrics, strengthening the conclusion beyond medal rate.

## 6. Limitations and Trade-offs
- **Diversity definition is narrow (by design).**
  - The diversity metric only covers *model/architecture choices* in the first ‚â§5 draft ideas (Section 3.2). It does not measure diversity in:
    - data cleaning/preprocessing,
    - feature engineering,
    - validation strategy,
    - hyperparameter tuning,
    - ensembling strategy (except insofar as it is categorized as an ‚Äúarchitecture/approach‚Äù).
  - This means ‚Äúideation diversity‚Äù here is not a full proxy for overall research creativity; it is a specific, operational slice.

- **Potential confounding from prompt interventions.**
  - The controlled experiment modifies the system prompt and intends to only affect ideation diversity (Section 3.3.2), but the Discussion acknowledges ‚Äúsecond-order effects‚Äù are difficult to track (Section 5 ‚ÄúLimitations of this study‚Äù).
  - Practically, asking for ‚Äúsimilar ideas‚Äù might also change agent motivation, verbosity, or willingness to switch approaches‚Äînot purely diversity.

- **Implementation bottleneck complicates interpretation.**
  - The paper argues diversity helps partly because it avoids repeatedly choosing plans the agent cannot implement (Section 5; Section 4.3.3 example with T5 timeouts).
  - This implies the measured benefit of diversity may partly reflect *robustness to coding/engineering failures*, not only better exploration of the ML solution space.

- **Benchmark dependence and generalization.**
  - Results are shown only on `MLE-bench` / `MLE-bench lite` (Section 5 ‚ÄúGeneralization to other benchmarks‚Äù notes this explicitly).
  - The paper hypothesizes generalization due to task variety, but does not test it (Section 5).

- **MLE-bench medal metric is imperfect (and motivates extra metrics).**
  - The paper documents multiple issues (Appendix A.2; Table 1; Figure 13):
    - medal thresholds depend on number of Kaggle teams,
    - bronze threshold can be within ~3% of best score in many competitions,
    - agents are evaluated on custom test sets while medal thresholds derive from Kaggle private test sets,
    - older competitions may not reflect current ML standards.
  - Although the paper mitigates this by adding metrics (Section 4.3), any single-number summary remains a trade-off (Appendix A.2.2; Table 2).

- **Missing reproducibility details in the provided excerpt.**
  - While the paper reports aggregate compute (`264,000 GPU hours`) and large-scale counts (Introduction), it does not specify many run-level details (exact prompts, exact extraction procedure for architectures, full list of tasks in MLE-bench lite, per-task compute/time budgets) in the included text.

## 7. Implications and Future Directions
- **How this changes the landscape (within the scope of this paper).**
  - It suggests agent builders should treat `ideation diversity` as a first-class design objective, not just an emergent property, because scaffold/prompt design can measurably change it (Section 4.1; Figure 3) and it can causally affect outcomes (Section 4.2; Figure 6).
  - It reframes part of ‚Äúagent quality‚Äù as: *how well an agent allocates its limited trajectory budget across meaningfully different approaches* (Discussion Section 5).

- **Follow-up research enabled/suggested by the paper.**
  - **Disentangle ideation vs implementation models.** The paper explicitly proposes separating the LLM that ideates from the LLM that implements to better isolate diversity‚Äôs effect (Section 5 ‚ÄúLimitations of this study‚Äù).
  - **Repeat controlled studies as coding improves.** The Discussion argues that as implementation bottlenecks shrink, diversity might matter more for exploration efficiency rather than failure avoidance (Section 5).
  - **Test generalization beyond MLE-bench.** The paper suggests examining additional benchmarks (Section 5).
  - **Develop diversity-aware search/scaffolds.** The Conclusion recommends ‚Äúdiversity-aware methods‚Äù as future focus (Section 7).

- **Practical applications / downstream use cases.**
  - Any setting where an agent must solve heterogeneous ML engineering tasks under constraints (time, compute, tool reliability) could benefit from explicit diversity mechanisms:
    - varying architectural families early,
    - encouraging different complexity levels (prompt-adaptive complexity),
    - using memory constructs to avoid duplicative drafts (Section 3.3.1).

- **Repro/Integration Guidance: when to prefer what (based on the paper).**
  - If you are building an MLE-bench-like agent today (where implementation failures are common), the paper‚Äôs controlled experiment suggests you should **avoid forcing early homogeneity** in ideas, because low-diversity agents can get stuck repeatedly attempting an approach they cannot execute (Section 4.3.3).
  - If you already use a tree-search scaffold, the paper indicates you can increase ideation diversity via **prompt-level mechanisms** (mention diversity, prompt-adaptive complexity) and **memory design** (sibling memory) (Section 3.3.1).
  - The appendix suggests **temperature tuning alone** may not reliably change medal-rate performance, even if it affects Elo (Appendix A.1; Figure 12), so prompt/scaffold mechanisms appear more reliably tied to the diversity-performance link in this study.



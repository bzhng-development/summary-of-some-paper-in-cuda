# Gemini: A Family of Highly Capable Multimodal Models

**ArXiv:** [2312.11805](https://arxiv.org/abs/2312.11805)

## ğŸ¯ Pitch

This paper introduces Gemini, a new family of large-scale models natively trained to understand and reason across text, images, audio, and video, while also generating both text and images. By advancing state-of-the-art performance on a wide array of academic and real-world benchmarksâ€”including being the first to exceed human-expert scores on MMLUâ€”Gemini sets a new standard for generalist AI and delivers these capabilities from massive data-center deployments (Ultra, Pro) all the way to on-device efficiency (Nano). This leap in multimodal and cross-modal reasoning unlocks seamless applications in education, analytics, accessibility, and beyond, while incorporating rigorous evaluation and responsible deployment frameworks.

---

## 1. Executive Summary (2â€“3 sentences)
This paper presents Gemini, a family of â€œnatively multimodalâ€ large models that process and reason over interleaved text, images, audio, and video, and can also generate both text and images. The core significance is twofold: stateâ€‘ofâ€‘theâ€‘art performance across 30 of 32 evaluated benchmarks (with humanâ€‘expert MMLU performance) and a practical pathway from data center models (Ultra, Pro) to efficient onâ€‘device use (Nano) with a responsible deployment framework (Sections 1â€“2, Tables 1â€“2).

## 2. Context and Motivation
- Problem/gap
  - Prior foundation models largely excel in one modality or require external adapters (e.g., OCR for text-in-image) and often cannot natively combine modalities or output images. This limits crossâ€‘modal reasoning such as â€œread a chart, tie it to text, compute an answer, and format the resultâ€ (Sections 1â€“2; Figure 1, Figure 2).
  - Existing evaluation practices face contamination risks and uneven crossâ€‘modal coverage, making it hard to assess generalist capabilities reliably (Section 5.1.1; HellaSwag discussion).

- Why it matters
  - Realâ€‘world tasks frequently mix modalities: documents with charts, screenshots, and audio/video content. A single system that understands all of them reduces orchestration overhead and unlocks workflows in education, accessibility, analytics, and software engineering (Figures 1, 5; Table 13; Section 5.2).
  - On-device inference enables private, lowâ€‘latency user experiences (Section 5.1.3; Table 1).

- Prior approaches and shortfalls
  - Visionâ€‘language models like Flamingo/PaLI/CoCa improved imageâ€“text tasks but were not trained â€œfrom the beginningâ€ to be multimodal across image, audio, video and to emit images (Section 2).
  - Textâ€‘first LLMs extended to vision (e.g., via OCR) miss fine details or layout information and rely on brittle tool chains (Table 7 highlights â€œpixel onlyâ€ results without any external OCR).

- Positioning
  - Gemini is trained jointly across modalities, supports interleaved inputs, produces text and images via discrete image tokens, and is evaluated endâ€‘toâ€‘end across text, image, audio, and video, aiming to be both a generalist model and a competitive specialist in each domain (Sections 2, 5; Figure 2; Table 7).

## 3. Technical Approach
- Model family and sizes
  - `Ultra`: flagship, highest capability for complex reasoning and multimodal tasks.
  - `Pro`: performance/latencyâ€‘optimized for broad deployment.
  - `Nanoâ€‘1` (1.8B) and `Nanoâ€‘2` (3.25B): distilled, 4â€‘bit quantized models for onâ€‘device use (Table 1; Section 5.1.3).

- Core architecture
  - Decoderâ€‘only Transformer with efficient attention (e.g., multiâ€‘query attention) and a 32kâ€‘token context window (Section 2).
  - Native multimodality: one sequence can interleave text, image frames, audio features, and video frames; outputs can interleave text and images using discrete image tokens (Figure 2; Section 2).
  - Video is encoded as a frame sequence at variable resolution to trade compute for detail; audio ingested as 16 kHz features from Universal Speech Model (`USM`) to preserve nonâ€‘textual cues (Section 2).

- Training infrastructure and reliability (how training at scale is made practical)
  - Largeâ€‘scale training on TPUv4/v5e across multiple data centers using JAX + Pathways; model/data parallelism orchestrated by GSPMD and XLA (Section 3).
  - Reliability innovations:
    - Inâ€‘memory redundant model state with rapid replica recovery instead of periodic persistent checkpoints, increasing training â€œgoodputâ€ (time doing useful new steps) from 85% (PaLM/PaLMâ€‘2 scale) to 97% (Section 3).
    - Silent Data Corruption (SDC) detection via deterministic replay and proactive scanners with hot standbys to identify faulty hardware quickly (Section 3).
    - Optical reconfiguration of TPU SuperPods into 3D tori for flexible scaling and maintenance (Section 3).

- Data pipeline
  - Multimodal, multilingual mixture: web documents, books, code, images, audio, video; SentencePiece tokenizer trained on the full corpus for better nonâ€‘Latin coverage and speed (Section 4).
  - Quality and safety filtering; decontamination against evaluation data; staged mixture schedules that upâ€‘weight domainâ€‘relevant data toward the end of training (Section 4).

- Postâ€‘training into two productized variants (Section 6)
  - `Gemini Apps` (for conversational services like Gemini and Gemini Advanced) and `Gemini API` (for developers in AI Studio and Vertex AI). Both use a multiâ€‘stage â€œdata flywheelâ€:
    1) Curate prompts (single/multiâ€‘turn) representative of real use.
    2) Supervised Fineâ€‘Tuning (`SFT`) on highâ€‘quality demonstrations.
    3) Reward Model (`RM`) training on human preference data.
    4) Reinforcement Learning from Human Feedback (`RLHF`) to align outputs with preferences (Figure 7; Section 6.3).
  - Capabilityâ€‘specific postâ€‘training recipes for instruction following, tool use (codeâ€‘asâ€‘tool loops; Figure 8), multilinguality (translationability filtering + human validation), multimodal vision (SFT on curated imageâ€‘text), coding (human + synthetic supervision), and factuality (closedâ€‘book accuracy, attribution to provided sources, and â€œhedgingâ€ when unanswerable; Section 5.1.6; Table 6).

- â€œUncertaintyâ€‘routed chainâ€‘ofâ€‘thoughtâ€
  - For multipleâ€‘choice reasoning (e.g., MMLU), the model samples `k` reasoning traces; if a consensus exceeds a validationâ€‘tuned threshold, it picks the majority; otherwise it falls back to greedy decodingâ€”improving accuracy beyond plain CoT or greedy alone (Appendix 10.2; Figure 9).

- Evaluation setup (how the model is exercised)
  - 50+ benchmarks organized into capability clusters: Factuality, Longâ€‘Context, Math/Science, Reasoning, Summarization, Multilinguality (Appendix 10.3).
  - Vision: strict â€œpixel onlyâ€ inferenceâ€”no external OCR, zeroâ€‘shot or fewâ€‘shot instructions; video evaluated on 16 equally spaced frames per clip (Sections 5.2.1â€“5.2.2; Table 7; Table 10).

## 4. Key Insights and Innovations
- Native, endâ€‘toâ€‘end multimodality (fundamental)
  - The model ingests and reasons over interleaved text/image/audio/video and also generates images as tokens, enabling tasks like â€œread a chart + produce a table + explain errors in handwritten math + emit illustrative imagesâ€ within a single forward pass (Sections 2, 5.2; Figures 1â€“2, 5â€“6; Table 13). This removes fragile external adapters (e.g., OCR), shown by strong â€œpixel onlyâ€ scores in Table 7.

- Trainingâ€‘atâ€‘scale reliability and efficiency (enabler for capability scaling)
  - Checkpointâ€‘free replica recovery and SDC detection increase goodput to 97% at unprecedented TPU scale, allowing longer runs and larger models without proportional downtime (Section 3). This is an operational innovation that materially enables Ultraâ€‘scale training.

- â€œUncertaintyâ€‘routedâ€ CoT for examâ€‘style tasks (incremental but impactful)
  - Majorityâ€‘vote gating over sampled chains boosts MMLU to 90.04% (Appendix 10.2; Figure 9; Table 2), surpassing a reported humanâ€‘expert threshold (89.8%) and prior SOTA.

- Toolâ€‘use as code blocks inside the model loop (incremental but practical)
  - Treating tool calls as generated code that executes and returns results to the context allows the model to compose multiple tools per turn and reason over outputs. This delivers large gains in factual retrieval and math (Table 15), and powers `Gemini Extensions` for real products (Section 6.5.2; Figure 8).

- Responsibility stack integrated into the modeling pipeline (fundamental for deployment)
  - Factuality triad (closedâ€‘book accuracy, source attribution, calibrated hedging), adversarial multimodal safety datasets, and multilayer redâ€‘teamingâ€”plus evaluative assurances for â€œdangerous capabilitiesâ€â€”form a repeatable deployment process (Sections 5.1.6, 7.1â€“7.4; Table 6).

## 5. Experimental Analysis
- Evaluation methodology
  - Text (Table 2; Sections 5.1.1â€“5.1.6): MMLU, GSM8K, MATH, BIGâ€‘Benchâ€‘Hard, HumanEval, Natural2Code (heldâ€‘out Python code gen), DROP, HellaSwag (with decontamination and 10â€‘shot reporting), WMT23 (BLEURT). Longâ€‘context tested via synthetic retrieval and NLL vs position across 32k tokens (Figure 4).
  - Vision (Table 7; Section 5.2.1): MMMU (multiâ€‘discipline visual QA), TextVQA, DocVQA, ChartQA, InfographicVQA, MathVista, AI2D, VQAv2; multilingual captioning on XMâ€‘3600 subset (Table 9). All â€œpixel only,â€ zeroâ€‘shot, greedy, unless noted.
  - Video (Table 10; Section 5.2.2): VATEX (EN, ZH captioning, 4â€‘shot), YouCook2 (captioning, 4â€‘shot), and zeroâ€‘shot QA datasets (NextQA, ActivityNetâ€‘QA, Perception Test MCQA); 16 frames sampled per clip.
  - Audio (Table 11; Section 5.2.4): ASR on YouTube EN, MLS EN, FLEURS 62 langs, VoxPopuli 14 langs (WERâ†“); Speech translation on CoVoST2 21 langs (BLEUâ†‘). Note: FLEURS used in training; a noâ€‘FLEURS model still outperforms Whisper (WER 15.8 vs Whisper v3; Section 5.2.4).
  - Tool use (Table 15) and postâ€‘training changes (Table 17 for preâ€‘ vs postâ€‘trained vision; Table 14 instruction following; Table 6 factuality).

- Headline quantitative results (selected)
  - Text reasoning
    > â€œ`Gemini Ultra` reaches 90.04% on MMLU (CoT@32 with uncertainty routing), 94.4% on GSM8K (Maj1@32), 53.2% on MATH (4â€‘shot), and 83.6% on BIGâ€‘Benchâ€‘Hard (3â€‘shot)â€ (Table 2, Appendix 10.2).
    - HumanEval Pass@1 74.4% (0â€‘shot, postâ€‘trained API model), and 74.9% on the heldâ€‘out Natural2Code benchmark (Table 2).
  - Multilinguality
    > â€œOn WMT23 averaged across directions, BLEURT 74.4 for Ultra vs 73.8 GPTâ€‘4, 72.7 PaLM2â€‘Lâ€ (Table 4).  
    > â€œMGSM (8â€‘shot) 79.0 for Ultra vs 74.7 PaLM2â€‘Lâ€ (Table 5).
  - Longâ€‘context
    > â€œ98% retrieval accuracy for keyâ€‘value at the end of a 32k context; NLL decreases steadily across positions up to 32kâ€ (Section 5.1.5; Figure 4).
  - Vision (all zeroâ€‘shot unless stated)
    > â€œ`MMMU` pass@1 59.4% (Maj1@32 62.4%) for Ultra vs 56.8% GPTâ€‘4Vâ€ (Table 7, Table 8).  
    > Strong OCRâ€‘heavy tasks â€œpixelâ€‘onlyâ€: TextVQA 82.3%, DocVQA 90.9% (Table 7).  
    > MathVista 53.0% (testâ€‘mini), AI2D 79.5%, VQAv2 77.8% (Table 7).  
    > Multilingual captioning (XMâ€‘3600 subset): higher CIDEr than PaLIâ€‘X across seven languages (Table 9).
  - Video
    > â€œVATEX EN captioning CIDEr 62.7 (4â€‘shot), YouCook2 135.4 (4â€‘shot); NextQA WUPS 29.9 (0â€‘shot); ActivityNetâ€‘QA 52.2% (0â€‘shot); Perception Test MCQA 54.7% (0â€‘shot)â€ (Table 10).
  - Audio
    > â€œASR WER: YouTube EN 4.9% (Pro), MLS EN 4.8%, FLEURS 7.6% (62 langs), VoxPopuli 9.1% (14 langs); CoVoST2 BLEU 40.1â€ (Table 11). Qualitative examples show better rareâ€‘word/properâ€‘noun handling than USM (Table 12).
  - Tool use and instruction following
    > â€œWith tools vs without: GSM8K 80.1% vs 69.7%; MATH 41.8% vs 30.7%; NQ 68.0% vs 59.0%; RealTimeQA 70.8% vs 39.2%â€ (Table 15).  
    > Instruction following on complex prompts: perâ€‘instruction accuracy 87.4% and fullâ€‘response accuracy 54.1% for Gemini Advanced (Ultra) (Table 14).
  - Factuality/Attribution/Hedging
    > â€œInaccuracy rate halved (6.7% â†’ 3.8%), attribution AIS up (40.2% â†’ 60.0%), hedging accuracy 69.3% from 0% after factualityâ€‘focused postâ€‘training on Proâ€ (Table 6).
  - Onâ€‘device Nano models
    > â€œDespite 1.8B/3.25B size, Nanoâ€‘2 achieves 0.83Ã— of Pro on NQâ€‘Retrieved and 0.78Ã— on MMLU; Nanoâ€‘1 achieves 0.69Ã— and 0.64Ã— respectivelyâ€ (Table 3; Figure 3).

- Ablations and robustness checks
  - HellaSwag dataâ€‘sensitivity: fineâ€‘tuning on websites corresponding to HellaSwag training set (not used in pretraining) moves 1â€‘shot validation accuracy to 96.0% (Ultra) and 89.6% (Pro), showing metric sensitivity to pretraining mixtures (Section 5.1.1). To mitigate such effects, the paper reports decontaminated 10â€‘shot numbers in Table 2.
  - Preâ€‘ vs postâ€‘trained vision: SFT improves several image benchmarks meaningfully (e.g., +3.3% on VQAv2, +2.9% AI2D, +2.4% InfographicVQA), aligning outputs to task references while the base model is already strong (Table 17).
  - Longâ€‘context synthetic retrieval: explicit 32k stress test (Section 5.1.5).

- Do the experiments support the claims?
  - The breadth (50+ tasks), transparency on contamination risks, and crossâ€‘modal â€œpixelâ€‘onlyâ€ protocol together substantiate claims of multimodal competence and stateâ€‘ofâ€‘theâ€‘art performance in many areas. Caveats remain where comparisons are not applesâ€‘toâ€‘apples (e.g., some baselines are fineâ€‘tuned vs Gemini zeroâ€‘shot; section notes this in Table 7), or evaluations depend on model sampling strategies (Appendix 10.2).

## 6. Limitations and Tradeâ€‘offs
- Data and evaluation sensitivity
  - Benchmarks like HellaSwag are highly sensitive to training data composition; despite decontamination efforts, the field still lacks uniformly leakageâ€‘free, robust benchmarks (Section 5.1.1).
  - Some reported baselines are via external APIs at a specific time (Table 2 notes â€œselfâ€‘collected via API in Nov 2023â€), which can drift and complicate strict comparability.

- Compute and reproducibility
  - Compute requirements and parameter counts for Ultra/Pro are not disclosed in the model card (â€œCompute Requirements: Not reportedâ€; Appendix 10.1), which limits independent reproduction and precise scaling analyses. The uncertaintyâ€‘routed CoT uses many samples (e.g., 32), increasing inference cost (Appendix 10.2).

- Modal coverage specifics
  - Video evaluation samples only 16 frames per clip (Section 5.2.2), potentially missing fine temporal events.
  - Audio: Ultra is not yet evaluated; FLEURS is in training data, making those ASR numbers less comparable (Section 5.2.4), though a control without FLEURS still beats Whisper.

- Safety and reliability
  - Despite improvements, safety assessments flag areas â€œwith particular room for improvement,â€ e.g., medical advice and harassment in textâ€‘toâ€‘text (Section 7.4.1.1). Image/video tests show the model can make ungrounded inferences about people; no consistent bias pattern observed, but the behavior remains a risk (Section 7.4.1.2).

- Scope limits
  - Context length is capped at 32k tokens; truly longâ€‘form video or multiâ€‘document corpora may exceed this.
  - Toolâ€‘use results are shown for specific tools and tasks; broader compositional tool chains or securityâ€‘hardened tool execution are not exhaustively studied (Section 6.5.2).

## 7. Implications and Future Directions
- Field impact
  - A practical template for â€œone model, many modalitiesâ€ that competes with specialized systems while simplifying pipelines (Figures 5â€“6; Table 7). The integrated responsibility stackâ€”factuality triad, adversarial multimodal safety sets, red teamingâ€”sets a deployment bar for generalist agents (Sections 5.1.6, 7.1â€“7.4).

- Enabled research
  - Agents that combine Geminiâ€‘class reasoning with tools and search (AlphaCode 2 demonstrates topâ€‘15% Codeforces performance by coupling Gemini Pro with search, clustering, and reranking; Section 5.1.7). Future work can extend this paradigm to planning, retrievalâ€‘augmented generation across modalities, and robust toolâ€‘use security.
  - Evaluation science: the paperâ€™s contamination analysis and â€œpixelâ€‘onlyâ€ protocols motivate new, leakageâ€‘resistant multimodal benchmarks and standardized sampling/reporting practices.

- Applications and downstream use
  - Education and accessibility: grading handwritten work, captioning images/videos, explaining charts/diagrams, and audioâ€‘visual tutoring (Figures 1, 10, 13, 23; Table 13).
  - Enterprise productivity: longâ€‘context summarization/search over mixedâ€‘media documents; robust chart/table understanding (Figure 10; Section 5.1.5).
  - Software engineering and data tasks: strong coding and math performance, boosted further by tools (Table 2, Table 15); image generation to support content creation (Figure 6).
  - Onâ€‘device experiences: summarization, retrieval QA, and reasoning at the edge with `Nano` models (Table 3; Figure 3).

---

> Selected results to remember (with sources):
> - â€œMMLU 90.04% with uncertaintyâ€‘routed CoT@32â€ (Table 2; Appendix 10.2, Figure 9).
> - â€œMMMU pass@1 59.4% (Maj1@32 62.4%) â€˜pixelâ€‘onlyâ€™ zeroâ€‘shot; GPTâ€‘4V 56.8%â€ (Table 7, Table 8).
> - â€œWMT23 average BLEURT 74.4 (Ultra) vs 73.8 (GPTâ€‘4)â€ (Table 4).
> - â€œTool use boosts NQ from 59.0% â†’ 68.0% and RealTimeQA from 39.2% â†’ 70.8%â€ (Table 15).
> - â€œFactuality inaccuracy halved (6.7% â†’ 3.8%); attribution up (40.2% â†’ 60.0%); hedging to 69.3%â€ (Table 6).
> - â€œGoodput 97% at Ultra scale via inâ€‘memory recovery and SDC scanningâ€ (Section 3).

Definitions of less-common terms used above:
- `BLEURT`: a learned MT quality metric that correlates with human judgments better than BLEU (Table 4).
- `AIS`: â€œAttributable to Identified Sources,â€ a humanâ€‘rated measure of whether generated text is faithful to provided sources (Section 5.1.6; Table 6).
- `WER`: Word Error Rate, standard ASR metric (Table 11).
- `CIDEr`: Image/video captioning metric measuring consensus with reference captions (Table 10).
- `Goodput`: fraction of total training time spent doing useful new steps, not recovery/overhead (Section 3).

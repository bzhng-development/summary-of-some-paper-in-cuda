# Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention

**ArXiv:** [2502.11089](https://arxiv.org/abs/2502.11089)

## ğŸ¯ Pitch

Native Sparse Attention (NSA) introduces a novel, hierarchical sparse attention mechanism that is natively trainable and specifically optimized for modern hardware, enabling unprecedented efficiency for long-context language models. By combining token compression, blockwise token selection, and a local sliding windowâ€”alongside custom low-level kernelsâ€”NSA delivers dramatic real-world speedups (up to 11.6Ã— for decoding) without sacrificing, and sometimes even surpassing, the accuracy of full attention models on challenging benchmarks. This innovation addresses key shortcomings of previous sparse methods, making it possible to efficiently train and deploy LLMs at long context lengths that are essential for tasks like codebase completion, document reasoning, and multi-turn conversational AI.

---

## 1. Executive Summary
This paper introduces NSA (Native Sparse Attention), a new attention mechanism that is both natively trainable and hardware-aligned for efficient longâ€‘context language modeling. NSA replaces full quadratic attention with a hierarchical, queryâ€‘dependent sparse scheme that combines token compression, blockwise token selection, and a local sliding window, and it comes with specialized kernels that translate theoretical sparsity into real speedups. On 64k-token sequences it reports up to 11.6Ã— faster decoding and 9.0Ã—/6.0Ã— speedups for forward/backward passes while matching or outperforming full attention on general, longâ€‘context, and chainâ€‘ofâ€‘thought evaluations (Figure 1; Tables 1â€“3).

## 2. Context and Motivation
- Problem addressed
  - Longâ€‘context LLMs are constrained by the high cost of standard attention, whose latency dominates as context grows (Section 1). The authors estimate that with softmax attention architectures, attention accounts for 70â€“80% of total latency at 64kâ€‘token decoding.
  - Many recent sparse attention methods reduce theoretical computation but often fail to deliver endâ€‘toâ€‘end wallâ€‘clock speedups or support training (Section 2).

- Why this matters
  - Real applicationsâ€”repositoryâ€‘level code, long multiâ€‘turn interactions, documentâ€‘level reasoningâ€”require long contexts. Making both training and inference efficient expands what models can practically handle (Section 1).

- Where prior approaches fall short (Section 2)
  - Phase-restricted sparsity: Some techniques only help during decoding (e.g., KV eviction like H2O) or only during prefilling; endâ€‘toâ€‘end latency remains high because at least one stage behaves like full attention.
  - Incompatibility with modern attention architectures: Methods that sparsify independently per attention head do not align with Multiple/Groupedâ€‘Query Attention (MQA/GQA), where heads share the KV cache; the union of selected KV across heads can negate memory gains (Section 2.1).
  - Not trainable endâ€‘toâ€‘end: Discrete or heuristic selection (e.g., Kâ€‘means in ClusterKV, SimHash in MagicPIG) breaks differentiability; gradients cannot flow to learn sparse patterns (Section 2.2).
  - Inefficient backward passes: Tokenâ€‘level random access prevents reuse of fast blockwise kernels such as FlashAttention, causing low hardware utilization during training (Section 2.2).

- Positioning
  - NSA is designed as a native, differentiable sparse attention that:
    - Works in all phases (prefilling, decoding, training).
    - Aligns with GQA/MQA to minimize KV memory traffic.
    - Uses blockwise access patterns to fit highâ€‘throughput GPU kernels (Sections 3.2â€“3.4).

Definitions used in this section:
- `KV cache`: the stored keys and values from previous tokens used during decoding.
- `Prefilling`: processing the initial prompt where many tokens are handled in parallel.
- `Decoding`: generating one token at a time autoregressively; this phase is typically memoryâ€‘bandwidth bound.
- `Arithmetic intensity`: ratio of computation to memory access; high intensity favors computeâ€‘bound kernels, low intensity is memoryâ€‘bound (Section 3.1).
- `GQA/MQA`: architectures where multiple query heads share the same K and V, reducing KV memory bandwidth during decoding (Section 2.1; citations to Ainslie 2023; Shazeer 2019).

## 3. Technical Approach
NSA constructs, for each query token `q_t`, a compact set of contextual key/value pairs `KÌƒ_t, VÌƒ_t` and computes attention only over these, instead of over all past tokens (Equations 3â€“4). The final output is a gated sum of three attention branches (compression, selection, sliding window) as in Equation (5) and Figure 2.

Step-by-step overview:
1. Replace the full context with a compact, queryâ€‘dependent set
   - Formally: compute `KÌƒ_t = f_K(q_t, k_{:t}, v_{:t})`, `VÌƒ_t = f_V(q_t, k_{:t}, v_{:t})` (Eq. 3), then attend: `o*_t = Attn(q_t, KÌƒ_t, VÌƒ_t)` (Eq. 4).
   - Combine multiple remappings: `o*_t = Î£_{c âˆˆ {cmp, slc, win}} g^c_t Â· Attn(q_t, KÌƒ^c_t, VÌƒ^c_t)` (Eq. 5), where `g^c_t âˆˆ [0,1]` are learned gate scores from an MLP.

2. Branch A â€” Token compression (Section 3.3.1; Eq. 7)
   - Idea: summarize consecutive tokens into a single â€œcompressedâ€ token so that a query can cheaply scan the whole history at a coarse granularity.
   - Mechanism:
     - Partition the sequence into overlapping blocks of length `l` with stride `d` (`d < l` to reduce fragmentation).
     - Apply a learnable function `Ï†` (an MLP with intraâ€‘block position encoding) to the keys in each block to produce one compressed key; similarly compress values.
     - This yields compressed `KÌƒ_cmp_t, VÌƒ_cmp_t` containing roughly `(t - l)/d` items instead of `t`.

3. Branch B â€” Blockwise selection (Section 3.3.2; Eqs. 8â€“12)
   - Motivation: compressed tokens may miss fine details; selection retains a small number of fullâ€‘fidelity blocks.
   - Key design choices and why:
     - Blockwise (not tokenwise): contiguous blocks align with GPU memory and Tensor Core usage, and match empirical attention continuity (Section 3.3.2 and visualization in Figure 8).
     - Importance scoring with negligible overhead: reuse the attention scores computed between `q_t` and the compressed keys (Eq. 8) to derive blockâ€‘level importance.
     - When compression and selection blockings differ, map scores using a structured aggregation (Eq. 9).
     - GQA/MQA alignment: aggregate importance across heads within a GQA group (Eq. 10) so all heads share one subset of selected KV blocks, minimizing KV cache loads.
     - Topâ€‘n selection: choose indices `I_t` of the `n` most important blocks (Eq. 11) and concatenate their tokens to form `KÌƒ_slc_t, VÌƒ_slc_t` (Eq. 12).

4. Branch C â€” Sliding window (Section 3.3.3)
   - Preserve local context by always keeping the most recent `w` tokens intact: `KÌƒ_win_t = k_{tâˆ’w:t}`, `VÌƒ_win_t = v_{tâˆ’w:t}`.
   - Prevent â€œshortcut learningâ€: each branch has its own K/V and attention computation, and outputs are combined via learned gates `g_t^c` (Eq. 5), which encourages compression/selection to specialize in longerâ€‘range patterns instead of relying on the local window.

5. Training and gating
   - All operations are differentiable. The gates `g_t^c` come from an MLP with sigmoid, enabling endâ€‘toâ€‘end learning of how much each branch should contribute (Section 3.2).

6. Hardwareâ€‘aligned kernel design (Section 3.4; Figure 3)
   - Compressed and slidingâ€‘window branches reuse FlashAttentionâ€‘2 kernels.
   - A specialized Triton kernel accelerates the selection branch:
     - Groupâ€‘centric query loading: for each time `t`, load all query heads in a GQA group together because they share the same selected KV blocks.
     - Shared KV fetching: load the selected KV blocks once into onâ€‘chip SRAM in continuous chunks (`B_k` divides the selection block size `l'`), then compute attention for all heads in the group.
     - Grid scheduling: put the query/output loops on Tritonâ€™s grid (outer loop) since the inner loop length (â‰ˆ number of selected blocks) is similar across positions.
   - This design:
     - Maximizes arithmetic intensity during prefilling/training.
     - Minimizes KV memory traffic during decoding by sharing selected blocks across heads.

7. Decoding memory model (Section 5.2; Table 4)
   - Per decoding step, NSA loads at most:
     - ~`(sâˆ’l)/d` compressed tokens,
     - `nÂ·l'` tokens from selected blocks, and
     - `w` recent tokens,
     where `s` is current sequence length.
   - This yields a nearâ€‘linear relationship between reduced memory access volume and speedup (expected up to 11.6Ã— at 64k; Table 4).

Example hyperparameters used for the 27B model (Section 4.1):
- Compression: block `l=32`, stride `d=16`.
- Selection: block `l'=64`, topâ€‘`n=16` blocks (including one initial and two local blocks).
- Sliding window: `w=512`.
- Architecture: 27B MoE+GQA (4 groups, 64 heads total; `d_k = d_q = 192`, `d_v = 128`) trained on 270B tokens, then longâ€‘context adaptation to 32k with YaRN.

## 4. Key Insights and Innovations
1. Hierarchical, queryâ€‘conditioned sparsity that is fully differentiable (Sections 3.2â€“3.3)
   - Novelty: combines three branchesâ€”compressed scan for global context, blockwise selection for detail, and a local windowâ€”into a gated mixture (Eq. 5).
   - Significance: preserves both global awareness and local precision while allowing endâ€‘toâ€‘end learning of sparse patterns. This contrasts with inferenceâ€‘only sparsity and heuristic selection in prior work.

2. Hardwareâ€‘aligned blockwise selection with GQA/MQA compatibility (Sections 2.1, 3.3.2, 3.4)
   - Novelty: importance scores are computed via alreadyâ€‘available compressed attention (Eq. 8), then aggregated across heads in a GQA group (Eq. 10) to ensure shared KV subsets.
   - Significance: converts theoretical sparsity into real speed by minimizing redundant KV loads and maximizing continuous memory access, a common bottleneck for sparse methods.

3. Specialized Triton kernel for selection that balances arithmetic intensity (Section 3.4; Figure 3)
   - Novelty: groupâ€‘centric query loading and shared KV fetching in continuous chunks; outerâ€‘loop grid scheduling.
   - Significance: achieves FlashAttentionâ€‘level throughput on sparse attention during training/prefilling and large speedups during decoding (Figure 6; Figure 1 right).

4. Native trainability without auxiliary losses or discrete operators (Sections 2.2, 6.1)
   - Novelty: avoids nonâ€‘differentiable operations (e.g., Kâ€‘means, LSH) and the overhead/instability of auxiliary supervision for selection.
   - Evidence: alternative trainable designs (auxiliaryâ€‘loss or heuristic selection) underperform on loss curves compared to NSA (Figure 7).

These are fundamental innovations rather than small tweaks: they change how sparse attention is constructed (hierarchical, differentiable), how it interacts with modern attention architectures (GQA/MQA), and how the kernels are scheduled to realize speedups across all phases.

## 5. Experimental Analysis
- Evaluation setup (Section 4)
  - Models: 27B MoE+GQA backbone; both NSA and full attention are pretrained on 270B tokens (8k context) and then continued to 32k with YaRN. NSA uses the hyperparameters listed above.
  - Benchmarks:
    - General knowledge/reasoning/coding: MMLU, MMLUâ€‘PRO, CMMLU, BBH, GSM8K, MATH, DROP, MBPP, HumanEval (Table 1).
    - Longâ€‘context: LongBench subsets (Table 2), Needleâ€‘inâ€‘aâ€‘Haystack (NiH) 64k (Figure 5).
    - Chainâ€‘ofâ€‘thought reasoning: AIMEâ€™24 via SFT distilled from DeepSeekâ€‘R1; evaluate with 16 samples per question at T=0.7, topâ€‘p=0.95 under 8k and 16k generation limits (Table 3).
  - Kernel speed comparison: Triton NSA kernel vs Triton FlashAttentionâ€‘2 (Figure 6). Decoding memoryâ€‘access analysis (Table 4).

- Main results
  - General benchmarks (Table 1):
    - NSA achieves higher average score than full attention: 
      > Average 0â€‘shot: NSA 0.456 vs Full 0.443.
    - Notable gains: 
      > GSM8K +0.034 (0.520 vs 0.486), DROP +0.042 F1 (0.545 vs 0.503), BBH +0.024 (0.521 vs 0.497).
    - Slight tradeâ€‘offs in some coding metrics (MBPP 0.466 vs 0.482).
    - Training loss curves show stable convergence with NSA lower than full attention (Figure 4).

  - Longâ€‘context accuracy:
    - NiH retrieval at 64k: 
      > Perfect retrieval across all positions (Figure 5).
    - LongBench (Table 2): NSA has the best average,
      > Avg 0.469 vs Full 0.437 and vs Exactâ€‘Top 0.423.
      - Multiâ€‘hop QA improvements:
        > HPQ: 0.437 vs 0.350 (+0.087); 2Wiki: 0.356 vs 0.305 (+0.051).
      - Code understanding:
        > LCC: 0.232 vs 0.163 (+0.069).
      - Passage retrieval:
        > PassRâ€‘en: 0.905 vs 0.830 (+0.075).

  - Chainâ€‘ofâ€‘thought math (Table 3):
    - After SFT, NSAâ€‘R outperforms Full Attentionâ€‘R at both generation limits:
      > 8k: 0.121 vs 0.046; 16k: 0.146 vs 0.092.
    - Interpretation given in the paper: the pretrained sparse patterns help capture longâ€‘range logical dependencies important to math reasoning (Section â€œChainâ€‘ofâ€‘Thought Reasoning Evaluationâ€).

  - Efficiency (Figures 1 and 6; Table 4):
    - Kernel speed (training/prefilling):
      > At 64k, NSA achieves ~9.0Ã— forward and ~6.0Ã— backward speedups over FlashAttentionâ€‘2 (Figure 6).
      - Speedup increases with sequence length (8kâ†’64k).
    - Decoding memory traffic (Table 4): expected speedup â‰ˆ linear in reduced KV loads:
      > 8k: 4Ã—; 16k: 6.4Ã—; 32k: 9.1Ã—; 64k: 11.6Ã—.
    - Endâ€‘toâ€‘end stages (Figure 1 right):
      > Decode 11.6Ã—, Forward 9.0Ã—, Backward 6.0Ã— on 64k sequences.

- Ablations and qualitative insights (Section 6)
  - Alternative selection strategies:
    - Auxiliaryâ€‘lossâ€‘based and heuristic parameterâ€‘free selection both yield worse training loss than NSA (Figure 7).
  - Attention visualization:
    - Full attention maps show blockwise clusteringâ€”nearby keys share similar importance (Figure 8), motivating blockwise selection.

- Overall assessment
  - The experiments comprehensively evaluate capability and efficiency. Gains are consistent on longâ€‘context tasks and reasoning, with competitive general performance.
  - Speedups are grounded in kernelâ€‘level comparisons and decoding memory modeling, not just FLOP counts.
  - NSAâ€™s native trainability is evidenced by successful pretraining to convergence and improved downstream SFT results.

## 6. Limitations and Trade-offs
- Assumptions about attention structure
  - The approach leverages empirical â€œspatial continuityâ€ of attention (Figure 8). Tasks where important tokens are highly scattered at fine granularity may be less amenable to blockwise selection, though the sliding window mitigates some risk.

- Hyperparameter sensitivity
  - NSA introduces several structural choicesâ€”`l`, `d`, `l'`, `n`, `w`â€”that may require tuning across domains and model sizes (Section 4.1 uses specific values). Fixed `n` imposes a hard cap on selected detail.

- Overheads and memory
  - Maintaining three branches with separate K/V and gates adds parameters and some memory/compute overhead. Compression and selection preâ€‘compute introduce extra steps, although they are designed to be lightweight and GPUâ€‘friendly (Section 3.4).

- Hardware and architecture coupling
  - The key speed wins rely on GQA/MQA and the custom Triton kernel that exploits groupâ€‘wise sharing and continuous memory. Benefits may diminish on architectures lacking such features, or on very different hardware/memory hierarchies.

- Scope of evaluation
  - Results center on one 27B MoE+GQA backbone and A100 GPUs. Generalization to other sizes (especially very small or very large dense models), other accelerators, or multimodal settings remains to be validated.

- Shortâ€‘sequence regimes
  - For short contexts (e.g., 8k), speedups and even accuracy gains are smaller; full attention may suffice when latency is already low (Figure 6 shows modest gains at 8k).

## 7. Implications and Future Directions
- Impact on the field
  - NSA demonstrates that sparse attention can be made both trainable and hardwareâ€‘efficient, removing a key barrier to longâ€‘context LLMs. This can shift pretraining and fineâ€‘tuning practices toward natively sparse models rather than applying sparsity only at inference.

- Practical applications
  - Faster and cheaper training/inference for long documents, repositoryâ€‘level code modeling, extended multiâ€‘turn agents, and complex multiâ€‘document reasoning. The perfect NiH retrieval at 64k (Figure 5) and LongBench gains (Table 2) specifically recommend NSA for retrievalâ€‘heavy and multiâ€‘hop tasks.

- Research avenues
  - Adaptive hyperparameters: learning `n`, `l`, `w` per layer or per sequence.
  - Richer compression: explore alternative learnable summarizers (e.g., attentionâ€‘based compressors) that remain kernelâ€‘friendly.
  - Multiâ€‘modal extension: extend hierarchical sparsity to vision/audio sequences with similar kernel principles.
  - Theoretical analysis: bounds on approximation error introduced by compression/selection and formal connections to attentionâ€™s intrinsic sparsity.
  - Integration with retrieval systems: combine NSAâ€™s compressed scan with external memory indexes for even longer contexts.
  - Portability and systems work: optimized kernels for newer GPUs (e.g., Hopper) and other accelerators; plugâ€‘andâ€‘play support within popular inference/training stacks.

In sum, NSA is a coherent algorithmâ€‘systems coâ€‘design: a hierarchical sparse attention that is natively trainable and paired with kernels that ensure arithmeticâ€‘intensity balance and KVâ€‘sharing in GQA/MQA. The paper provides evidence of both capability (Tables 1â€“3; Figure 5) and endâ€‘toâ€‘end efficiency (Figure 1; Figure 6; Table 4), positioning NSA as a strong candidate for longâ€‘context LLMs.

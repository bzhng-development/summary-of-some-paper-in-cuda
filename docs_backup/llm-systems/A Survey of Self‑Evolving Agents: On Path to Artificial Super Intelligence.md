# A Survey of Selfâ€‘Evolving Agents: On Path to Artificial Super Intelligence

**ArXiv:** [2507.21046](https://arxiv.org/abs/2507.21046)
**Authors:** Huanâ€‘ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenghailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang
**Institutions:** Various institutions across contributing authors (not individually specified in abstract)

## ğŸ¯ Pitch

This paper introduces a comprehensive framework for self-evolving agents, integrating LLMs that adapt and improve autonomously across tasks and environments. By formalizing agent evolution, decision-making, and feedback-driven transformation, it offers a foundational blueprint for developing adaptive systems crucial for dynamic fields like digital assistants and healthcare, ensuring robustness and scalability in real-world applications.

---

## 1. Executive Summary (2-3 sentences)
This survey systematizes the rapidly growing area of selfâ€‘evolving agentsâ€”LLMâ€‘based systems that can improve themselves during and between tasksâ€”into a unified, formal framework. It answers three core questionsâ€”what to evolve, when to evolve, and how to evolveâ€”while adding â€œwhere to evolveâ€ (applications) and â€œhow to evaluate,â€ with formal definitions (Section 2.1, Eqs. 1â€“3), a complete taxonomy (Figures 2â€“3), and an evaluation roadmap (Section 7, Figures 9, Tables 5â€“7).

## 2. Context and Motivation
- Problem addressed
  - LLMs and many â€œagentsâ€ that wrap them are fundamentally static: they do not adapt their internal parameters or nonâ€‘parametric components (memory, tools, workflows) as they interact with the world (Introduction, p. 4). This is a critical bottleneck for openâ€‘ended, interactive deployments where tasks, tools, and environments change.
- Importance
  - Realâ€‘world impact: Digital assistants, coding agents, GUI/web automation, healthcare, and education all require continual adaptation and robust retention of past experience (Sections 6.1â€“6.2).
  - Theoretical significance: The work gathers scattered ideas into a principled formulation of agent evolution as decisionâ€‘making in partially observable environments with explicit objectives (Section 2.1), providing foundations for analyzing safety, stability, and coâ€‘evolution.
- Prior approaches and gaps
  - Curriculum learning, lifelong learning, model editing, and unlearning each cover a slice of the problem but focus mainly on parameter updates over static data and usually lack autonomous exploration, architectural selfâ€‘modification, or tool evolution (Section 2.2; Table 1).
  - Existing surveys treat evolution as a small component of agent taxonomies or study model selfâ€‘improvement divorced from tools, memory, and system architecture (Introduction; Section 2.2).
- Positioning
  - The survey provides the first endâ€‘toâ€‘end framework that:
    - Formally defines environments, agents, and selfâ€‘evolution as a transformation problem (Section 2.1; Eqs. 1â€“3).
    - Organizes the field across â€œwhat/when/how/where,â€ with crossâ€‘cutting dimensions (online/offline, on/offâ€‘policy, reward granularity) and evaluation principles (Figures 2â€“3, 7, 9; Tables 3â€“4, 5â€“7).

## 3. Technical Approach
This is a survey with a formal framework. The â€œapproachâ€ is the organization and definitions that make disparate work comparable.

- Formal problem setup (Section 2.1)
  - Environment as a POMDP: `E=(G,S,A,T,R,Î©,O,Î³)`
    - `G`: goals (e.g., a user request).
    - `S`: environment states; `Î©`: observations the agent can read; `O`: observation model.
    - `A`: actions that include natural language, retrieval, and tool calls.
    - `T`: transition dynamics; `R`: feedback (a scalar or text) conditioned on goal `gâˆˆG`.
    - `Î³`: discount factor.
  - Agent system: `Î =(Î“,{Ïˆ_i},{C_i},{W_i})`
    - `Î“`: architecture/topology (workflow or code graph organizing nodes `N_i`).
    - At each node `N_i`: model `Ïˆ_i`, context `C_i` (prompt `P_i`, memory `M_i`), and tools `W_i`.
    - Policy at node `i`: `Ï€_{Î¸_i}(Â·|o)` with `Î¸_i=(Ïˆ_i, C_i)`; actions live in language space âˆª tool space `W_i`.
  - Selfâ€‘evolving strategy (Eq. 1):  
    > `f(Î , Ï„, r) = Î â€² = (Î“â€², {Ïˆâ€²_i}, {Câ€²_i}, {Wâ€²_i})`  
    The agent transforms itself into a new system `Î â€²` based on the trajectory `Ï„` and feedback `r`.
  - Objective over a task sequence (Eq. 3):  
    > Maximize `Î£_j U(Î _j, T_j)` where `Î _{j+1} = f(Î _j, Ï„_j, r_j)` (Eq. 2).  
    `U` is a utility that can be derived from rewards, time, accuracy, robustness, etc.

- Taxonomy: what, when, how, where (Figures 2â€“3)
  - What to evolve (Section 3; Table 2)
    1) Models: update policies with selfâ€‘generated data, feedback, or RL (e.g., SCA generates and solves codeâ€‘tasks; SELF, SCoRe, PAG use execution traces or critiques as signals; TextGrad treats textual feedback as â€œgradientsâ€).
    2) Context: memory and prompt evolution.
       - Memory management mechanisms (Section 3.2.1): e.g., SAGE uses the Ebbinghaus curve to decide what to forget; Mem0 supports ADD/MERGE/DELETE to maintain coherent longâ€‘term memory; Agent Workflow Memory stores reusable subâ€‘task workflows.
       - Prompt optimization (Section 3.2.2): searchâ€‘based (APE), iterative rewriting (ORPO), â€œtextual gradientâ€ edits (ProTeGi), MCTS (PromptAgent), evolutionary (PromptBreeder), and fully selfâ€‘supervised loops (SPO).
    3) Tools: creation (Voyager skill library, CREATOR abstracts tool creation), mastery via iterative refinement (LearnAct, DRAFT), and scalable management/selection (ToolGen encodes tools as tokens; AgentSquare searches modular agent designs; Darwin GÃ¶del Machine rewrites its own code) (Section 3.3).
    4) Architecture: optimize singleâ€‘agent nodes and code (TextGrad; GÃ¶del Agent; AlphaEvolve) and evolve multiâ€‘agent workflows (ADAS, AFlow with MCTS; ScoreFlow/FlowReasoner learn to generate queryâ€‘specific workflows) or learn coordination via MARL (ReMA, GiGPO) (Section 3.4).
  - When to evolve (Section 4; Figure 5)
    - Intraâ€‘testâ€‘time (during solving the current task): via inâ€‘context learning (ICL), supervised fineâ€‘tuning (SFT), or reinforcement learning (RL).
      - Examples: Reflexion stores naturalâ€‘language reflections midâ€‘episode; AdaPlanner revises plans on outâ€‘ofâ€‘plan feedback using an `ask_LLM()` action; Selfâ€‘Adapting LMs produce â€œselfâ€‘editsâ€ that trigger immediate SFT; LADDER triggers targeted testâ€‘time RL for hard problems.
    - Interâ€‘testâ€‘time (between tasks): offline or online learning over collected trajectories.
      - Examples: SELF/STaR/Quietâ€‘STaR/SiriuS for selfâ€‘training with selfâ€‘generated rationales; RAGEN/DYSTIL/WebRL/DigiRL for RL across multiâ€‘turn environments.
  - How to evolve (Section 5; Figure 6; Table 3; Figure 7)
    - Rewardâ€‘based: textual feedback (Reflexion, Selfâ€‘Refine), internal rewards (confidence/certainty), external rewards (environment, verification rules, majority vote), and implicit rewards (inâ€‘context RL or logitsâ€‘derived â€œendogenousâ€ rewards).
    - Imitation/demonstration: selfâ€‘generated (STaR and variants), crossâ€‘agent (SiriuS), and hybrid (RISE, confidenceâ€‘filtered).
    - Populationâ€‘based/evolutionary: singleâ€‘agent (Darwin GÃ¶del Machine code evolution; GENOME parameter evolution; selfâ€‘play methods like SPIN/SPC/STL) and multiâ€‘agent (EvoMAC team/backpropâ€‘like updates; Puppeteer learning orchestration; MDTeamGPT/MedAgentSim knowledgeâ€‘base evolution).
    - Crossâ€‘cutting dimensions (Figure 7; Section 5.4; Table 4): online vs offline learning, onâ€‘policy vs offâ€‘policy, and reward granularity (processâ€‘ vs outcomeâ€‘ vs hybrid).
  - Where to evolve (Section 6; Figure 8)
    - Generalâ€‘purpose agents: memory mechanisms (Mobileâ€‘Agentâ€‘E â€œTips/Shortcutsâ€), modelâ€‘agent coâ€‘evolution (UIâ€‘Genie coâ€‘trains reward model and agent; WebEvolver coâ€‘trains a world model), and curriculumâ€‘driven training (WebRL adaptive curricula; Voyagerâ€™s bottomâ€‘up tasks).
    - Specialized domains: coding (SICA, EvoMAC), GUI/web (WindowsAgentArena/Navi; WebVoyager; ReAP), finance (QuantAgent), medical (Agent Hospital, MedAgentSim, DoctorAgentâ€‘RL), education, and more.

- Evaluation framework (Section 7; Figure 9; Tables 5â€“7)
  - Goals: adaptivity, retention, generalization, efficiency, safety (Table 5).
  - Paradigms: static, shortâ€‘horizon adaptation, and longâ€‘horizon lifelong learning (Sections 7.2.1â€“7.2.3; Table 6).
  - Benchmarks: a catalog by domain (Table 7), plus longâ€‘term memory (LTMBenchmark) and lifelong agents (LifelongAgentBench).

## 4. Key Insights and Innovations
1) A formal, general definition of selfâ€‘evolution in agents (Section 2.1)
   - Innovation: Eqs. (1)â€“(3) express selfâ€‘evolution as a transformation `f` over all agent componentsâ€”not just weightsâ€”conditioned on observed trajectories and feedback. This unifies parameter updates, prompt/memory editing, tool creation, and workflow search as firstâ€‘class optimization targets.
   - Significance: Enables rigorous reasoning about adaptive agents beyond fineâ€‘tuning and connects to utility maximization over task streams.

2) A comprehensive, actionable taxonomy of evolution (Figures 2â€“3; Sections 3â€“5)
   - Difference from prior work: Goes beyond â€œmodel selfâ€‘improvementâ€ to cover nonâ€‘parametric context, tool ecosystems, and architecture (single vs multiâ€‘agent), and ties each to specific methods (Table 2) and learning paradigms (ICL/SFT/RL).
   - Significance: Provides a design mapâ€”from prompt search to codeâ€‘level selfâ€‘modificationâ€”with concrete exemplars for each branch.

3) Crossâ€‘cutting lenses that explain design tradeâ€‘offs (Figure 7; Table 4)
   - Novelty: The online/offline, on/offâ€‘policy, and rewardâ€‘granularity axes expose why certain approaches are sampleâ€‘efficient but brittle (e.g., imitation), while others are stable yet expensive (e.g., outcomeâ€‘only RL).
   - Utility: These lenses guide practitioners to mix strategies (e.g., hybrid reward; offline SFT + online RL) for a target domain.

4) Evaluation program tailored to evolving agents (Section 7; Figure 9; Tables 5â€“7)
   - Contribution: Defines retention with explicit formulas for forgetting and backward transfer (FGT/BWT), distinguishes shortâ€‘ vs longâ€‘horizon evaluation, and curates benchmarks that stress adaptation and memory (LTMBenchmark, LifelongAgentBench).
   - Impact: Shifts assessment from singleâ€‘shot accuracy to longitudinal competence and safety.

These are fundamental organizing contributions rather than incremental empirical improvements.

## 5. Experimental Analysis
This survey does not introduce a new model; instead, it synthesizes methods and evaluation practices. It still grounds effectiveness with representative evidence and provides a concrete evaluation blueprint.

- Evaluation methodology (Section 7)
  - Metrics (Table 5)
    - Adaptivity: successâ€‘rate by iteration, adaptation speed.
    - Retention:  
      > Forgetting `FGT_t = (1/(t-1)) Î£_{i=1}^{t-1} (max_{jâˆˆ{i,â€¦,t}} J_{j,i} âˆ’ J_{t,i})`  
      > Backward transfer `BWT_t = (1/(t-1)) Î£_{i=1}^{t-1} (J_{t,i} âˆ’ J_{i,i})`  
      where `J_{j,i}` is performance on task `i` after finishing task `j` (Section 7.1).
    - Generalization: aggregate crossâ€‘domain performance and outâ€‘ofâ€‘distribution tests.
    - Efficiency: token cost, time, action steps, tool productivity.
    - Safety: safety score, harm score, completion under policy (CuP), risk ratio, refusal rate, leakage rate.
  - Paradigms (Figure 9; Table 6)
    - Static assessment: endâ€‘toâ€‘end competence snapshots (e.g., AgentBench, SWEâ€‘bench, OSWorld).
    - Shortâ€‘horizon adaptation: performance vs. iteration (examples in Section 7.2.2; MemoryAgentBench includes builtâ€‘in testâ€‘time learning tasks).
    - Longâ€‘horizon: lifelong memory/learning (LTMBenchmark; LifelongAgentBench), including dynamic/evolving test suites (Section 7.2.3).

- Benchmarks and datasets (Table 7; Section 7.2)
  - Web/GUIs: WebShop, WebArena, Mind2Web, BrowseComp; OSWorld; Mobileâ€‘Evalâ€‘E.
  - Software engineering: SWEâ€‘bench (and variants).
  - Planning/Tools/Memory/Multiâ€‘agent: PlanBench, ToolBench family, MemoryAgentBench, MultiAgentBench, SwarmBench.
  - General assistants: AgentBench, GAIA, TheAgentCompany.

- Representative quantitative evidence from cited systems within the survey (Section 6.2)
  - GUI agents:
    - WindowsAgentArenaâ€™s Navi agent â€œdoublesâ€ taskâ€‘completion after replayâ€‘andâ€‘critique selfâ€‘evolution (Section 6.2, â€œGraphical User Interfacesâ€).
    - WebVoyager improves success on unseen sites from â€œ30% to 59%â€ via selfâ€‘fineâ€‘tuning (Section 6.2).
    - ReAP adds episodic memory and â€œrecovers a further 29â€‘percentageâ€‘point marginâ€ on previously failed queries (Section 6.2).
  - These are examples of selfâ€‘evolutionâ€™s impact across environments; the survey reports them to illustrate effectiveness, not as new experiments.

- Do the compiled results support the claims?
  - The evidence spans many domains and mechanisms, showing selfâ€‘evolution can:
    - Increase success rates through memory and plan revision (Sections 3.2, 4.1).
    - Learn tools from scratch and then master them (Section 3.3).
    - Benefit from hybrid evolution strategies (Table 4 shows tradeâ€‘offs motivating combinations).
  - Robustness considerations:
    - The survey highlights pitfalls such as â€œfeedback frictionâ€ (agents underâ€‘use external feedback; Table 3), reward sparsity (Sections 5.1, 5.4.3), distribution shift in offâ€‘policy learning (Section 5.4.2), and expensive workflow search (Section 3.4.2), along with mitigations like Agentic Predictor (Section 3.4.2) and hybrid rewards (Section 5.4.3).

- Ablations/failure modes discussed
  - Reward granularity ablations: processâ€‘ vs outcomeâ€‘based rewards and hybrid methods (Section 5.4.3) detail when each improves stability/learning signal density.
  - Stability/sampleâ€‘efficiency comparisons across method families (Table 4) serve as a qualitative ablation over design axes.

Overall, while no new experiments are run, the surveyâ€™s evaluation section makes the case for longitudinal, safetyâ€‘aware assessment and provides concrete metrics and benchmarks to implement it.

## 6. Limitations and Trade-offs
- Scope and assumptions
  - The framework assumes environments can supply some evaluative signalâ€”textual or scalarâ€”even if implicit (Section 5.1). In domains lacking verifiable outcomes or reliable critics, evolution can stall or drift.
  - The POMDP formalization presumes taskâ€‘conditioned rewards `R(s,a,g)`, which may be hard to craft or infer in messy realâ€‘world settings (Section 2.1).
- Methodâ€‘level tradeâ€‘offs (Table 4; Figure 7)
  - Imitation/demonstration: high sample efficiency but brittle if demonstrations are biased or scarce.
  - Rewardâ€‘based RL: flexible but sensitive to reward design and sparsity; can rewardâ€‘hack without careful verification (Sections 5.1, 5.4.3).
  - Populationâ€‘based evolution: broad exploration and architectural novelty but computeâ€‘intensive and slower to converge.
  - Onâ€‘policy vs offâ€‘policy: onâ€‘policy is stable but dataâ€‘hungry; offâ€‘policy is efficient but risks distribution mismatch (Section 5.4.2).
  - Outcome vs process rewards: outcomes are cheap but sparse; process rewards are informative but require validation or annotation surrogates (Section 5.4.3).
- Practical constraints
  - Compute and latency: Dynamic reasoning, workflow search, and multiâ€‘agent rollouts incur large costs during testâ€‘time (Sections 3.4.2, 5.4.1; see also Figure 7â€™s online learning path).
  - Safety and controllability remain challenging in open environments; agents can leak sensitive data or pursue unsafe strategies despite constitutions or rules (Section 8.3; Table 5 safety metrics).
- Open questions
  - Catastrophic forgetting and stabilityâ€‘plasticity balance in longâ€‘horizon settings (Sections 7.1, 8.2).
  - Knowledge transfer across agents and tasks; evidence suggests current agents often fail to propagate learning reliably (Section 8.2).
  - Reliable evaluation under data contamination and evolving benchmarks (Section 7.2.3).

## 7. Implications and Future Directions
- Fieldâ€‘level impact
  - By treating prompts, memory, tools, and architectures as evolvable components (Figures 2â€“3), the survey reframes â€œagent trainingâ€ from modelâ€‘only fineâ€‘tuning to fullâ€‘stack adaptation. This widens the design space for practical systems and aligns research on the path from â€œfoundation agentsâ€ to â€œselfâ€‘evolving agentsâ€ and ultimately ASI (Figure 1).
- Enabled research directions (Section 8)
  - Personalized agents: selfâ€‘generated preference data, coldâ€‘startâ€‘aware memory and profile building, and evaluation that tracks evolving personalization quality.
  - Generalization: scalable architectures, testâ€‘time scaling of compute, metaâ€‘learning for crossâ€‘domain adaptation, and explicit mechanisms to form robust world models.
  - Safety and control: richer constitutions and case libraries, improved feedback verification and sandboxing, privacyâ€‘preserving memories, and safer training algorithms.
  - Multiâ€‘agent ecosystems: dynamic role formation, orchestration learning (e.g., Puppeteer), knowledgeâ€‘base evolution (MDTeamGPT), and dynamic benchmarks capturing longâ€‘term collaboration.
- Practical applications
  - Software engineering: selfâ€‘improving coding agents that evolve workflows and tests (Section 6.2).
  - GUI/web automation: memoryâ€‘augmented, curriculumâ€‘trained agents that adapt to new apps/sites (Section 6.2).
  - Healthcare and education: hospitalâ€‘scale simulations for evolving diagnostic strategies; multiâ€‘agent tutors that refine pedagogy via selfâ€‘play and feedback (Section 6.2).
  - Finance and science: selfâ€‘evolving consultation/trading frameworks; principleâ€‘aware scientific discovery with multiâ€‘agent collaboration (Sections 6.2, 5.3.2).

In short, the survey provides a rigorous blueprintâ€”formalism, taxonomy, and evaluation toolkitâ€”that allows researchers and practitioners to design, analyze, and measure truly adaptive agentic systems. It clarifies which levers to pull (what/when/how), where to apply them, and how to know whether evolution is working over the short and long term.

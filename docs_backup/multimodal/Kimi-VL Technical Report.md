# Kimi-VL Technical Report

**ArXiv:** [2504.07491](https://arxiv.org/abs/2504.07491)
**Authors:** Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen, Zongyu Lin
**Institutions:** Kimi Team

## ğŸ¯ Pitch

Kimi-VL introduces a groundbreaking vision-language model utilizing a Mixture-of-Experts language decoder combined with a high-resolution vision encoder, enabling efficient multimodal reasoning and long-context understanding with minimal computational activation. This innovation significantly narrows the gap between open-source and proprietary models, offering cost-effective, accessible solutions for complex tasks like high-res UI understanding and long-document/video comprehension, driving advancements in areas such as enterprise document intelligence and video analytics.

---

## 1. Executive Summary
Kimi-VL is an efficient, open-source visionâ€“language model (VLM) that combines a Mixture-of-Experts (MoE) language decoder with a nativeâ€‘resolution vision encoder to deliver strong multimodal reasoning, longâ€‘context understanding (up to 128K tokens), and GUI agent capabilities while activating only about 3B parameters at inference (Section 1; Figure 3). A longâ€‘thinking variant, `Kimiâ€‘VLâ€‘A3Bâ€‘Thinkingâ€‘2506`, adds chainâ€‘ofâ€‘thought (CoT) supervised fineâ€‘tuning and reinforcement learning (RL) to reach stateâ€‘ofâ€‘theâ€‘art performance among openâ€‘source efficient VLMs on several reasoning and video benchmarks (Section 4.2; Table 4; Table 5).

## 2. Context and Motivation
- Problem gap:
  - Open-source VLMs have lagged behind proprietary models (e.g., GPTâ€‘4o, Gemini) in three areas: efficiency at scale, longâ€‘context processing, and advanced â€œlong thinkingâ€ (multiâ€‘step CoT) reasoning (Section 1).
  - Existing open VLMs:
    - Dense models (e.g., `Qwen2.5â€‘VLâ€‘7B`, `Gemmaâ€‘3â€‘12Bâ€‘IT`) are competitive but computeâ€‘heavier and generally lack longâ€‘CoT reasoning out of the box (Section 1).
    - Early MoE VLMs (e.g., `DeepSeekâ€‘VL2`, `Aria`) show promise but have key limits: fixedâ€‘size vision encoders, short context (4K), weaker fineâ€‘grained perception, and no longâ€‘thinking support (Section 1).
- Why this matters:
  - Real use cases require: highâ€‘resolution OCR and UI understanding, multiâ€‘image/video reasoning, and longâ€‘document/video comprehension (e.g., enterprise documents, software agents). Achieving this with small activated compute is crucial for accessibility, cost, and latency.
- Positioning:
  - Kimiâ€‘VL integrates three fronts in one open model: an MoE text decoder for computeâ€‘efficiency, a nativeâ€‘resolution vision encoder for highâ€‘fidelity perception, and a training recipe enabling 128K multimodal context plus longâ€‘CoT reasoning (Figures 3â€“5; Figure 4 on preâ€‘training stages; Section 2.3â€“2.4).

## 3. Technical Approach
Stepâ€‘byâ€‘step overview (with selective definitions of nonstandard terms):

- Core architecture (Figure 3; Section 2.1):
  - `MoonViT` (vision encoder):
    - Nativeâ€‘resolution processing: images are patchified, flattened, and concatenated into variableâ€‘length 1D sequences so the model can â€œpackâ€ different sizes efficiently (NaViTâ€‘style packing) without splitting images into tiles (Section 2.1, â€œMoonViTâ€).
    - Spatial encoding: combines interpolated absolute positional embeddings from SigLIP with 2D RoPE (rotary position embeddings) along height and width to preserve fineâ€‘grained spatial relations at high resolution (Section 2.1).
      - Definition: `2D RoPE` rotates query/key vectors as a function of 2D position, improving generalization to large images compared with fixed embeddings.
    - In the `2506` thinking variant, MoonViT is continually trained to encode up to ~3.2M pixels per image (4Ã— the prior limit), enabling ultraâ€‘highâ€‘resolution perception (Section 2.1).
  - `MLP projector`:
    - Uses a `pixel shuffle` step (2Ã—2 spatial downsampling in exchange for more channels) followed by a twoâ€‘layer MLP to map vision features to the language model embedding space (Section 2.1).
  - `Moonlight` MoE language decoder:
    - An MoE LLM with 2.8B activated and 16B total parameters (Section 2.1). â€œActivated parametersâ€ are the subset of expert weights used per token (typical in MoE routing), so inference cost follows ~3B parameters instead of the full 16B.
    - Architecture similar to `DeepSeekâ€‘V3` (nonâ€‘shared and shared experts plus a router); initialized from a 5.2Tâ€‘token textâ€‘pretrained checkpoint with 8K context (Section 2.1).

- Optimization and scaling (Section 2.2; 2.5):
  - `Enhanced Muon` optimizer with weight decay and perâ€‘parameter update scaling, implemented in a memoryâ€‘efficient ZeROâ€‘1 style (Section 2.2).
  - 4D parallelism for throughput and long sequences: Data Parallelism, Expert Parallelism, Pipeline Parallelism, and Context Parallelism (the latter splits the sequence length across devices to train 128K efficiently with FlashAttention) (Section 2.5).
  - Additional memory tactics: Selective activation checkpointing and ZeROâ€‘1 optimizer state sharding; recomputation increases for ultraâ€‘long sequences (Section 2.5).
  - Reported training throughput: ~60% higher than a 7B dense VLM baseline after parallelism optimization (Section 2.5).

- Multiâ€‘stage preâ€‘training to preserve text ability while adding vision and long context (Figure 4; Table 1; Section 2.3):
  - Stage A: ViT training (2.0T tokens + 0.1T alignment)
    - CoCaâ€‘like objective: contrastive `SigLIP` loss plus caption generation loss (weights L = L_siglip + 2Â·L_caption). A tiny text decoder is used here only; later alignment swaps to the MoE LLM (Section 2.3 â€œViT Training Stagesâ€).
    - Progressive image resolution sampling; observation that OCR skill emerges in the caption decoder as OCR data scales (Section 2.3).
    - Alignment step (0.1T) updates only MoonViT+projector to reduce perplexity when feeding vision tokens into the MoE LLM (Section 2.3).
  - Stage B: Joint preâ€‘training (1.4T tokens)
    - Mixes the original text corpus and diverse multimodal data; gradually increases the multimodal ratio to preserve language skill while learning vision grounding (Section 2.3 â€œJoint Preâ€‘training Stageâ€).
  - Stage C: Joint cooldown (0.6T tokens)
    - Highâ€‘quality text and multimodal data, augmented with verified synthetic QA pairs in math, knowledge, and code to sharpen capabilities (with rejection sampling) (Section 2.3 â€œJoint Cooldown Stageâ€).
    - Visual QA pairs are kept at a low ratio to avoid overfitting to QA patterns, serving to â€œactivateâ€ capabilities needed to learn from higherâ€‘quality data (Section 2.3).
  - Stage D: Joint longâ€‘context activation (0.3T tokens)
    - Extends context from 8K â†’ 32K â†’ 128K by resetting RoPE base from 50,000 to 800,000 and running two subâ€‘stages (each 4Ã— length increase) (Section 2.3).
    - 25% of tokens are â€œlong dataâ€ (long text, long interleaved imageâ€“text, long video, long document), 75% replay shorter data to retain shortâ€‘context skills (Section 2.3).
    - Validation: â€œNeedle in a Haystackâ€ (NIAH) recall is near 100% up to 64K for both text and video haystacks, and 87â€“92% at 128K (Table 2).

- Postâ€‘training (Figure 5; Section 2.4):
  - Instruction SFT:
    - 1 epoch at 32K, then 1 epoch at 128K, mixing pure text and multimodal chat data in ChatML format; supervision on answers and special tokens only; formatâ€‘aware packing preserves dialogue structure and crossâ€‘modal alignment (Section 2.4 â€œJoint SFTâ€).
  - Longâ€‘CoT SFT (thinking warmâ€‘start):
    - A small, highâ€‘quality multimodal CoT set is created via promptâ€‘engineered sampling and verification, explicitly teaching â€œplanning, evaluation, reflection, explorationâ€ reasoning patterns (Section 2.4 â€œLongâ€‘CoT SFTâ€).
  - RL for reasoning:
    - Online `policy mirror descent` with a 0/1 reward on answer correctness and a KL regularization term to stabilize updates (Equation (1); Section 2.4 â€œReinforcement Learningâ€).
      - Plainâ€‘language: the model generates answers, receives a binary correctness reward, and its policy is nudged toward outputs that earned higher reward while staying close to the current policy (the KL term).
    - Length penalty discourages unnecessary long CoTs (â€œoverthinkingâ€); difficultyâ€‘aware curriculum and prioritized sampling focus compute where it teaches most (Section 2.4).

- Data construction highlights (Section 3):
  - Six multimodal categories curated at scale: `caption`, `interleaving`, `OCR`, `knowledge`, `video`, `agent` (Section 3.1).
  - OCR data includes multiâ€‘page documents, figures/tables/diagrams, heavy augmentations; supports longâ€‘document OCR and layout understanding (Section 3.1 â€œOCR Dataâ€).
  - Agent data gathered from largeâ€‘scale virtualized environments with dense grounding labels and multiâ€‘step trajectories (Section 3.1 â€œAgent Dataâ€).

## 4. Key Insights and Innovations
- Efficient MoE VLM with only ~3B activated parameters for strong, general multimodal ability (Figure 3; Table 3).
  - Why it matters: Competes with or surpasses larger dense VLMs while lowering inference cost. For example, on InfoVQA (OCR), Kimiâ€‘VL scores 83.2 vs GPTâ€‘4o at 80.7 and Qwen2.5â€‘VLâ€‘7B at 82.6, with a fraction of activated parameters (Table 3).
  - Distinction: Prior open VLMs either use dense decoders or earlier MoE designs with short context or weaker perception.

- Nativeâ€‘resolution `MoonViT` with 2D RoPE + NaViTâ€‘style packing for highâ€‘res, variableâ€‘aspect inputs (Section 2.1; Figure 3).
  - Why it matters: Avoids tiling, preserves fine spatial cues across huge images and UI screenshots. The `2506` variant expands to ~3.2M pixels per image and posts large gains in highâ€‘res/OS UI tasks (Table 5: ScreenSpotâ€‘Pro 52.8 and OSWorldâ€‘G 52.5).

- Longâ€‘context activation across modalities with demonstrated retrieval up to 128K (Section 2.3; Table 2).
  - Why it matters: Long PDFs/videos and interleaved sequences are firstâ€‘class citizens. The data mix and staged extension (replaying short data) maintain shortâ€‘context quality while enabling longâ€‘form reasoning; this is nontrivial and often brittle without such recipes.

- Integrated longâ€‘thinking via CoT SFT + online RL that both improves accuracy and reduces CoT length (Figure 5; Table 4).
  - Why it matters: `Kimiâ€‘VLâ€‘A3Bâ€‘Thinkingâ€‘2506` lifts reasoning benchmarks substantially (e.g., MathVision 56.9 and MathVista 80.1) while cutting average output tokens by ~20% on MMMUâ€‘val and MathVision (Section 4.3), improving latency and cost. Figure 13 shows testâ€‘time scaling: more thinking tokens â†’ better accuracy up to a point.

These are fundamental advances in training recipe and system design rather than only incremental tuning.

## 5. Experimental Analysis
- Evaluation setup (Sections 4, B):
  - Breadth: collegeâ€‘level (MMMU/MMVU), general VLM (MMBench, MMVet, MMStar, RealWorldQA, AI2D), math (MathVista, MathVision), multiâ€‘image (BLINK), OCR (InfoVQA, OCRBench), long document (MMLongBenchâ€‘Doc), long video (Videoâ€‘MME, MLVU, LongVideoBench), video perception (EgoSchema, VSIâ€‘Bench, TOMATO), and agent/GUI grounding (ScreenSpotâ€‘V2/Pro, OSWorld, WindowsAgentArena).
  - Metrics: accuracy or Pass@1 for MCQ/VQA; OCRBench out of 1000; with/without subtitles for Videoâ€‘MME to isolate frame understanding from textual leakage.
  - Baselines: GPTâ€‘4o/â€‘mini (numbers shown for context; GPTâ€‘4o appears grayed in Table 3), Qwen2.5â€‘VLâ€‘7B, Gemmaâ€‘3â€‘12Bâ€‘IT, Llamaâ€‘3.2â€‘11Bâ€‘Instruct, DeepSeekâ€‘VL2. Some competitor entries are unavailable where models cannot handle task context lengths (Table 3).

- Main results for `Kimiâ€‘VLâ€‘A3B` (Instruct) (Table 3; Figure 2):
  - General:
    - â€œMMBenchâ€‘ENâ€‘v1.1â€: 83.1, matching GPTâ€‘4o and ahead of other open baselines.
    - â€œRealWorldQAâ€: 68.1â€”near Qwen2.5â€‘VLâ€‘7B (68.5), above Gemmaâ€‘3â€‘12Bâ€‘IT (59.1).
  - Math:
    - â€œMathVistaâ€: 68.7, better than GPTâ€‘4o (63.8), Qwen2.5â€‘VLâ€‘7B (68.2).
    - â€œMathVisionâ€: 21.4 (lower; later improved by thinking variants).
  - OCR:
    - â€œInfoVQAâ€: 83.2â€”tops GPTâ€‘4o (80.7), DeepSeekâ€‘VL2 (78.1); â€œOCRBenchâ€: 867/1000 (Table 3).
  - Long context:
    - â€œMMLongBenchâ€‘Docâ€: 35.1â€”above Qwen2.5â€‘VLâ€‘7B (29.6) and GPTâ€‘4oâ€‘mini (29.0), though below GPTâ€‘4o (42.8).
    - â€œLongVideoBenchâ€: 64.5â€”near GPTâ€‘4o (66.7).
  - Video:
    - â€œVideoâ€‘MMEâ€ w/o sub: 67.8 (strong without subtitles); with sub: 72.6 (Table 3).
    - MLVU MCQ: 74.2 (SoTA vs GPTâ€‘4o 64.6; Qwen2.5â€‘VLâ€‘7B 70.2).
    - EgoSchema: 78.5 vs GPTâ€‘4o 72.2.
  - Multiâ€‘image:
    - BLINK: 57.3â€”above Qwen2.5â€‘VLâ€‘7B (56.4), GPTâ€‘4oâ€‘mini (53.6).
  - Agent:
    - â€œScreenSpotâ€‘V2â€: 92.8; â€œScreenSpotâ€‘Proâ€: 34.5; â€œOSWorldâ€ Pass@1: 8.22 vs GPTâ€‘4o 5.03; â€œWindowsAgentArenaâ€: 10.4 vs GPTâ€‘4o 9.4 (Table 3).

- Reasoning variants (Table 4; Figure 13):
  - `Kimiâ€‘VLâ€‘A3Bâ€‘Thinking`:
    - Inferenceâ€‘time longer CoT boosts accuracy (Figure 13). At 16k thinking tokens, MathVision rises to 36.8, MMMU to 61.7, MathVista to 71.3.
  - `Kimiâ€‘VLâ€‘A3Bâ€‘Thinkingâ€‘2506` (integrated thinking model):
    - MathVision 56.9, MathVista 80.1, MMMU 64.0, MMMUâ€‘Pro 46.3, VideoMMMU 65.2 (Table 4).
    - General/perception retained or improved (Table 5): MMBench 84.4; MMVet 78.1; RealWorldQA 70.0; MMStar 70.4; Videoâ€‘MME with sub 71.9; MMLongBenchâ€‘Doc 42.1 (matching GPTâ€‘4oâ€™s 42.8, and +10 points over the earlier thinking model).
    - Highâ€‘res agent grounding: ScreenSpotâ€‘Pro 52.8; OSWorldâ€‘G 52.5; ScreenSpotâ€‘V2 91.4 (Table 5).

- Qualitative evidence:
  - Examples include multiâ€‘image spatial reasoning, video game scene recognition, landmark identification, longâ€‘document OCR (Figure 7; Figure 9), stepâ€‘byâ€‘step GUI actions (Figure 10), long video scene segmentation (Figure 11).
  - There is also a demo of author inference from historical manuscripts (Figure 6). Note that identifying specific real persons from images can be sensitive; the figure is presented in the paper as a qualitative demonstration only.

- Do the experiments support the claims?
  - Breadth and consistency are strong: across OCR, multiâ€‘image, longâ€‘video/document, and agent tasks, `Kimiâ€‘VLâ€‘A3B` is competitive with models 2â€“4Ã— larger; the `2506` variant demonstrably improves reasoning while retaining perception and longâ€‘context performance (Table 3, Table 4, Table 5).
  - Particularly convincing:
    - Longâ€‘video/document capability with 128K context (Table 2; Table 3; Table 5).
    - Highâ€‘resolution UI grounding and OS agents (Table 3; Table 5).
    - Testâ€‘time CoT scaling behavior (Figure 13).
  - Less explored or missing:
    - Limited ablations isolating the effects of 2D RoPE, alignment stage, cooldown synthetic data, and longâ€‘context composition.
    - No detailed failure analysis; qualitative examples are curated.

- Tradeâ€‘offs and conditional results:
  - The base instruct model underperforms proprietary larger models on some academic reasoning (e.g., GPTâ€‘4o MMMU 69.1 vs Kimiâ€‘VL 57.0; Table 3), while the thinking variant narrows or reverses gaps on reasoningâ€‘heavy tests but increases generation cost unless promptâ€‘level controls are used (Figure 13, Section 4.3 on token reductions in `2506`).
  - With subtitles, video accuracy increases for most models; evaluating without subtitles (Table 3) is a fairer test of spatiotemporal perception.

## 6. Limitations and Trade-offs
- Capacity vs. breadth:
  - Only ~2.8B activated parameters in the decoder (plus ~0.4B in vision). While efficient, this caps raw language capacity, which may limit extremely specialized or knowledgeâ€‘heavy tasks (Section 5 â€œConclusion, Limitation, and Future Workâ€, point 1).
- Longâ€‘context constraints:
  - Although context is 128K, attention capacity still corresponds to a ~3B model, so performance on very long and complex sequences may lag larger decoders (Conclusion, point 3).
- Reasoning supervision and RL:
  - Rewards are binary (correct/incorrect), which may insufficiently capture reasoning quality; risk of spurious but correct answers being reinforced (Section 2.4).
  - CoT SFT and RL depend on synthetic and promptâ€‘engineered data; coverage and bias depend on generation quality and rejection sampling efficacy (Section 2.4; Section 3.3).
- Evaluation coverage:
  - Extensive, but with few ablations and limited error analysis. Some competitor numbers are absent due to context/ability limits, complicating perfect applesâ€‘toâ€‘apples comparisons (Table 3).
- Practical compute costs:
  - While more efficient than dense peers, longâ€‘context training/inference and long CoTs still incur real costs. The `2506` tokenâ€‘length reductions help (~20% shorter answers), but practitioners must tune thinking depth (Figure 13; Section 4.3).

## 7. Implications and Future Directions
- Field impact:
  - Establishes a strong template for efficient multimodal systems: MoE text decoding + nativeâ€‘resolution vision + staged longâ€‘context training. Demonstrates that a ~3Bâ€‘activated VLM can be competitive across OCR, highâ€‘res UI grounding, long videos/docs, andâ€”with CoT+RLâ€”hard reasoning (Figures 2â€“5; Tables 3â€“5).
- Enabled research directions:
  - MoEâ€‘centric VLM scaling: explore larger total parameters while keeping low activated compute; study expert specialization across modalities and context lengths.
  - Longâ€‘context learning: ablations on RoPE scaling, data composition, and crossâ€‘modal ordering to further improve 128K+ stability and retrieval.
  - Reasoning training: richer reward models (stepâ€‘level, structureâ€‘aware), verifiable toolâ€‘use, and hybrid search (the model already encodes planning/evaluation/reflection patterns; Figure 5).
  - Highâ€‘resolution perception: leverage the 3.2Mâ€‘pixel capability for CAD/medical/doc layouts and professional GUI agents (Table 5 improvements on ScreenSpotâ€‘Pro and OSWorldâ€‘G).
- Practical applications:
  - Enterprise document intelligence (OCR, table/form understanding, longâ€‘document Q&A).
  - Video analytics at scale (surveillance summaries, sports highlights, instructional video comprehension).
  - Software agents for desktop/web/mobile automation with robust grounding and multiâ€‘step plans (Figure 10; Table 3; Table 5).
  - Education and STEM assistance (math visual reasoning; Table 4).

> Representative results to remember:
> - Efficiency with reach: â€œMMBenchâ€‘ENâ€‘v1.1 = 83.1â€ for Kimiâ€‘VL (Table 3), matching GPTâ€‘4o, and â€œInfoVQA = 83.2â€ (OCR).
> - Longâ€‘form understanding: â€œMMLongBenchâ€‘Doc = 42.1â€ for `Thinkingâ€‘2506`, matching GPTâ€‘4oâ€™s 42.8 (Table 5).
> - Reasoning with small compute: `Thinkingâ€‘2506` hits â€œMathVista = 80.1â€, â€œMathVision = 56.9â€, and â€œVideoMMMU = 65.2â€ (Table 4), with only ~3B activated parameters.

Overall, Kimiâ€‘VL and `Kimiâ€‘VLâ€‘A3Bâ€‘Thinkingâ€‘2506` show that careful architectural choices plus a multiâ€‘stage multimodal training recipe can deliver a broadly capable, longâ€‘context, and reasoningâ€‘enabled VLM at low activated computeâ€”pushing openâ€‘source systems closer to proprietary leaders while making them more practical to deploy.

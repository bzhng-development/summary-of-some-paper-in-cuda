# InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU

**ArXiv:** [2502.08910](https://arxiv.org/abs/2502.08910)

## ğŸ¯ Pitch

InfiniteHiP introduces a novel, training-free framework that enables large language models to handle up to 3 million tokens on a single commodity GPUâ€”achieving this by uniting efficient hierarchical block-sparse attention, dynamic position encoding for out-of-length generalization, and smart KV-cache offloading that avoids forgetting any context. This combination delivers unprecedented scalability and speed, empowering production LLM deployments for massive-context applications like retrieval-augmented generation and long-form analysis, without expensive retraining or hardware upgrades.

---

## 1. Executive Summary
InfiniteHiP is a trainingâ€‘free inference framework that lets existing large language models process millionâ€‘token prompts on a single GPU by combining three ideas: a modular hierarchical pruning scheme that builds a blockâ€‘sparse attention mask, a dynamic positional encoding strategy for outâ€‘ofâ€‘length (OOL) generalization, and a KVâ€‘cache offloading system that spills rarely used tokens to host memory. As a result, it processes up to 3M tokens on a single 48GB L40s without permanently discarding context and achieves large speedups over dense attention and prior sparse/offloading systems (Abstract; Figures 1, 5; Tables 3â€“4).

## 2. Context and Motivation
- The problem
  - Long prompts make attention slow (quadratic in sequence length) and inflate the keyâ€‘value (`KV`) cache linearly with length, stressing GPU memory during generation (Introduction).
  - Most pretrained LLMs do not generalize beyond their training context window because their positional encoding (e.g., RoPE) does not extrapolate well (Introduction).
- Why this matters
  - Practical applications (RAG systems, multiâ€‘document QA, longâ€‘form analysis) require hundreds of thousands to millions of tokens while keeping prior content accessible (Introduction; Section 5).
  - Production deployments need both speed and faithful retrieval of any part of the context (no â€œforgettingâ€ due to eviction).
- Prior approaches and gaps
  - FlashAttention2 reduces memory traffic but not compute; it still computes dense attention (Related Works; Table 3).
  - Tokenâ€‘selection methods speed prefill but not decoding (e.g., MInference), or rely on fixed representatives that may miss queryâ€‘specific tokens (InfLLM) (Related Works).
  - KV eviction (e.g., H2O) saves memory but permanently forgets information that may be needed later (Related Works).
  - HiP Attention offloads KV to CPU and fetches on demand, but its pruning uses heuristics with global synchronizations that limit GPU parallelism and speed (Related Works; Section 1).
  - RoPE extrapolation methods (Dynamicâ€‘NTK, Selfâ€‘Extend) extend position indexing but do not address compute/memory cost (Section 5.1).
- Positioning
  - InfiniteHiP integrates three componentsâ€”modular pruning, dynamic RoPE, and efficient offloadingâ€”into a single inference pipeline that targets both speed and OOL generalization without training (Section 4; Figure 1).

## 3. Technical Approach
The system consists of three interlocking parts (Figures 1â€“2; Section 4; Appendix Aâ€“D).

1) Modular hierarchical context pruning (how the sparse mask is built)
- Key observation (chunk sparsity)
  - In real attention maps, â€œtopâ€‘kâ€ keys concentrate in a small number of contiguous chunks: in a 128K context using Llamaâ€‘3.1â€‘8B, fewer than 2% of chunks contain more than 12.5% of the topâ€‘2048 keys, and about 75% of 64â€‘token chunks contain none of the topâ€‘2048 keys (Figure 2a).
- Goal
  - For each query block, approximate the topâ€‘K highestâ€‘score key blocks and compute attention only there (blockâ€‘sparse attention).
- Pruning stages (S(i))
  - Each stage S(i) is parameterized by `(bq, lc, k)` (query block size, key chunk size, tokens to keep); see Section 4 (â€œIn formal notationâ€¦â€) and Appendix A.1.
  - Pipeline per stage (Figure 2b; Algorithm 2):
    1) Partition candidate key indices from the previous stage into equalâ€‘size chunks of length `lc`.
    2) For each chunk, pick a representative token via hierarchical topâ€‘1 selection (Algorithm 3): recursively split the chunk in half, compare one token from each half against the current query block (maximum over queries and heads), and keep the better half; this takes `O(log lc)` steps and reads at most two keys per split. This avoids scanning all keys and is highly parallelizable on the GPU.
    3) Compute an estimated chunk score by taking the maximum attention score between the query block and that chunkâ€™s representative token, maximized over heads and queries in the block.
    4) Keep only the top `K = k / lc` chunks and pass their indices to the next stage (Equations (1)â€“(3)).
- Design details that make it practical
  - Sink and streaming tokens: always retain the first `nsink` prompt tokens and the most recent `nstream` tokens to preserve global anchors and recency (Section 4).
  - Stage caching: during decoding, the sparse mask is not recomputed every step; each stage refreshes every `n_refresh^(i)` steps to exploit temporal locality (Section 4 â€œSparse Attention Mask Cachingâ€; Algorithm 4; Table 4 shows hit ratios rise steeply as more stages are cached).
  - Default hyperparameters (Appendix F) use three stages with progressively smaller chunks `(256, 32, 8)` and `k` shrinking `(32K, 8K, 2â€“4K)`, query blocks of size 64, sink=256, stream=1024.

2) Dynamic RoPE for outâ€‘ofâ€‘length generalization (how positions are assigned when context > training window)
- Terms
  - `RoPE` (rotary positional embedding) rotates queries/keys with sinusoidal matrices parameterized by position id; it encodes order without positional vectors.
  - â€œOOL generalizationâ€ means handling positions longer than those seen during pretraining without fineâ€‘tuning.
- Why RoPE needs care here
  - When contexts exceed the training length, naive RoPE indexing harms attention patterns. Also, early layers of many LLMs exhibit slidingâ€‘windowâ€‘like locality that blockâ€‘sparse attention must approximate (Appendix D; Figure 10).
- Strategy (Section 4 â€œDynamic RoPEâ€¦â€; Appendix A.1, Bâ€“D)
  - During pruning (mask selection), mix two styles:
    - Chunkâ€‘indexed RoPE for the first 3 layers: assign one position id per key chunk (offset by `nstream` from the current query); this injects strong locality and helps approximate sliding windows in early layers.
    - Relativeâ€‘style RoPE for later layers: assign two nearby position ids to the left/right halves during topâ€‘1 selection, which emphasizes content while preserving relative proximity.
  - During the final sparse attention compute, use StreamingLLMâ€‘style RoPE: assign sequential position ids to the selected keys (including sink and stream), with the most recent key sharing the queryâ€™s position (Section 4; Xiao et al., 2024b).
- Evidence the mix matters
  - Ablation with âˆBench En.MC at 300K context shows using chunkâ€‘indexed in layers 1â€“3 and relative in 4â€“32 yields 74.23% vs 68.55% for relativeâ€‘only (Appendix D, Table 6).
  - A crossâ€‘combination ablation (Table 5) indicates Relative (pruning) + Streaming (sparse attention) is among the strongest pairings.

3) KVâ€‘cache offloading with unified memory and LRU (how memory fits on a single GPU)
- Terms
  - `KV cache` stores past keys and values so the model does not recompute them during decoding; it grows linearly with context length.
  - `UVM` (Unified Virtual Memory) lets GPU kernels access CPU memory pages that are transparently fetched over PCIe on demand.
  - `LRU` eviction discards the least recently used entries when the GPU cache is full.
- Mechanism (Section 4 â€œKV Cache Offloadingâ€; Algorithm 4)
  - Maintain two GPUâ€‘resident key banks: one for maskâ€‘selection kernels and one for the final blockâ€‘sparse attention (separate access patterns).
  - Keep a GPU page table mapping global token indices to their current GPUâ€‘bank slot.
  - On a GPU cache miss, fetch the missing KV pages from host memory via UVM and place them into the GPU bank; evict cold pages using LRU.
  - Stageâ€‘wise mask caching reduces how often Stageâ€‘1 (the only part linear in context length) runs and therefore how often CPU memory must be touched (Table 4, â€œCached Stagesâ€).
- Paged blockâ€‘sparse attention kernel
  - A FlashAttentionâ€‘like kernel adapted to blockâ€‘sparse masks and paged KV (Section 4 â€œImplementationâ€), with FlashDecode for decoding and PagedAttentionâ€‘style memory management.

Complexity and parallelism
- Stageâ€‘1 pruning is `O(TqÂ·Tkv)` but reads at most two keys per chunk during topâ€‘1 selection and is implemented as a single Triton kernel with no global synchronization (Appendix A.1). Later stages are `O(Tq)`. In practice, heavy GPU parallelism and caching make it faster than earlier hierarchical methods (Table 3).

## 4. Key Insights and Innovations
- Modular, GPUâ€‘friendly pruning that exploits chunk sparsity
  - Innovation: a stack of pruning stages that each (a) select a representative key per chunk using a binaryâ€‘searchâ€‘like topâ€‘1 estimator and (b) pass only the best chunks onward (Figure 2b; Algorithms 2â€“3).
  - Why it matters: enables highâ€‘recall, queryâ€‘adaptive masks with strong GPU parallelism. The selectedâ€‘key recall is higher than HiP and InfLLM by 4.72 and 1.57 percentage points, respectively (Figure 6a).
- A layerâ€‘wise mix of RoPE indexers geared to attention locality
  - Innovation: combine chunkâ€‘indexed RoPE in early layers with relative/streaming styles later (Section 4; Appendices Bâ€“D).
  - Why it matters: preserves slidingâ€‘windowâ€‘like behavior in shallow layers (Figure 9, 10) and yields stronger OOL generalization; En.MC rises from 68.55% (relativeâ€‘only) to 74.23% with the mixed scheme (Appendix D, Table 6).
- Stageâ€‘wise mask caching plus LRUâ€‘backed UVM offloading
  - Innovation: cache masks per pruning stage and maintain a separate GPU KV bank for masking vs attention; fetch misses from CPU memory and evict via LRU (Algorithm 4; Table 4).
  - Why it matters: drastically cuts decoding latency under offloading. At 256K, â€œRuntime (Flash)â€ falls to 325 Âµs vs InfLLMâ€™s 1,186 Âµs (â‰ˆ3.65Ã— faster), and mask hit ratios exceed 98% when early stages are cached (Table 4).
- Practical singleâ€‘GPU millionâ€‘token inference
  - Innovation: an endâ€‘toâ€‘end SGLang integration with paged blockâ€‘sparse kernels and UVM offloading (Section 4 â€œImplementationâ€; Figure 5).
  - Why it matters: delivers usable throughput on commodity or cloud GPUs. On a 24GB RTX 4090 at 1M tokens, throughput is â‰ˆ3.20Ã— higher than the SGLang Runtime with FlashInfer (SRT) estimate (Figure 5, left). On a 48GB L40s at 3M tokens, throughput is â‰ˆ7.25Ã— higher than SRT estimate (Figure 5, right).

## 5. Experimental Analysis
- Evaluation setup (Section 5)
  - Benchmarks
    - LongBench (average length â‰ˆ32K) spanning QA, summarization, fewâ€‘shot, code (Table 1).
    - âˆBench (beyond 100K tokens) covering synthetic retrieval tasks and NLU (Table 2).
    - Additional: RULER (Appendix E.2), passkey retrieval on DeepSeekâ€‘R1â€‘distilled Qwen2.5â€‘14B (Appendix E.1), scaling curves (Figures 3â€“4).
  - Models: Llamaâ€‘3/3.1â€‘8B Instruct and Mistral 0.2â€‘7B Instruct; additional tests on Gemma2â€‘9B and EXAONEâ€‘7.8B (Figures 3â€“4; Table 10).
  - Baselines (Section 5.1): FA2 (dense, truncation), Dynamicâ€‘NTK, Selfâ€‘Extend (RoPE tricks), LMâ€‘Infinite and StreamingLLM (sink+stream windows), H2O (eviction), InfLLM (chunk representatives), and HiP Attention.
  - System setup: Triton kernels, SGLang integration; decoding latency and throughput measured on RTX4090 and L40s (Tables 3â€“4; Figure 5; Appendix E.4).
- Main quantitative results
  - LongBench (Table 1)
    - Llamaâ€‘3â€‘8B: Absolute average 47.72 for InfiniteHiP vs 44.47 (InfLLM) and 42.47 (FA2). Relative score 100.00 vs 92.83 and 87.69.
    - Mistralâ€‘7B: Absolute average 42.71 for InfiniteHiP vs 41.46 (best baseline). Relative score 99.85 vs 96.99 (InfLLMâ€‘12K).
  - âˆBench (Table 2)
    - Llamaâ€‘3â€‘8B: Relative score 98.17 (3K window) vs 89.07 (InfLLM). With different refresh schedules (3Kâ€‘fast/flash), relative scores remain â‰ˆ98.
    - Mistralâ€‘7B: Relative score up to 99.09 (5K) vs 94.77 (InfLLMâ€‘16K).
  - OOL scaling (Figures 3â€“4)
    - En.MC rises with context for InfiniteHiP even beyond the pretrained window, while dense baselines degrade after 128K.
    - On shortâ€‘context models (Gemma2 8K, EXAONE 3/3.5), InfiniteHiP yields large gains at extended lengths (Figure 4; Table 10).
  - Latency without offloading (Table 3; 1Mâ€‘token context)
    - Decoding attention latency: 234 Âµs (InfiniteHiP 3K) vs 4,645 Âµs (FA2) and 1,222 Âµs (InfLLMâ€‘12K), i.e., â‰ˆ19.85Ã— and â‰ˆ4.98Ã— faster, respectively. Prefill is also faster than FA2 (20.29Ã—) and slightly faster than InfLLM.
    - Enabling dynamic RoPE (â€œExtendâ€) adds â‰ˆ1.6Ã— overhead to prefill and â‰ˆ5% to decoding (Table 3, â€œOurs with Extendâ€).
  - Latency with KV offloading (Table 4)
    - At 256K, â€œRuntime (Flash)â€ is 325 Âµs (InfiniteHiP 3Kâ€‘fast) vs 1,186 Âµs (InfLLMâ€‘12K), â‰ˆ3.65Ã— faster; similar factors at 512K and 1,024K.
    - Stage caching reduces decoding from 9,803 Âµs (no caches) to 110 Âµs (all stages cached) at 256K; mask hit ratio jumps from 71.7% to 98.8% (Table 4).
  - Throughput (Figure 5; Appendix E.4 Tables 11â€“12)
    - RTX4090 @1M: Estimated 40 tokens/s (offloadâ€‘flash) vs 12.5 (SRT estimate), â‰ˆ3.2Ã— higher.
    - L40s @3M: Estimated 23.8 tokens/s (offloadâ€‘flash) vs 3.3 (SRT estimate), â‰ˆ7.25Ã— higher.
- Ablations and diagnostics
  - Mask quality: selectedâ€‘key recall beats HiP by 4.72 points and InfLLM by 1.57 points (Figure 6a).
  - Depth of pruning: more stages (N=3) improve En.MC over N=2 (74.24 vs 70.31), both surpass dense FA2 at 128K (67.25) (Figure 6b).
  - RoPE combinations: Relative (pruning) + Streaming (sparse attention) among the strongest; chunkâ€‘indexed everywhere hurts, but using it only in early layers helps (Table 5; Appendix D, Table 6).
- Do results support the claims?
  - Speedups: Yes; multiple tables show orderâ€‘ofâ€‘magnitude decoding gains vs FA2 and multiâ€‘fold gains vs InfLLM, both with and without offloading (Tables 3â€“4).
  - Accuracy: On LongBench and âˆBench, InfiniteHiP matches or exceeds best baselines while pruning more aggressively (Table 1â€“2). OOL generalization is supported by scaling curves and by strong performance on shortâ€‘context models when extended (Figures 3â€“4; Table 10).
  - Practicality: SGLang throughput plots demonstrate singleâ€‘GPU millionâ€‘token decoding with realistic tokens/s (Figure 5).

> â€œInfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU â€¦ without any permanent loss of context information.â€ (Abstract; Figure 1; Section 4 â€œKV Cache Offloadingâ€)

> â€œOur framework achieves an 18.95Ã— speedup in attention decoding for a 1 million token context without requiring additional training.â€ (Abstract; corroborated by Table 3, which shows â‰ˆ19.85Ã— vs FA2)

> â€œWe implement InfiniteHiP on the SGLang serving framework, achieving a 7.24Ã— speedup in endâ€‘toâ€‘end decoding on a 3M token context while using only 3.34% of the VRAM required by FA2.â€ (Section 1, contributions)

## 6. Limitations and Trade-offs
- Assumptions about attention structure
  - The pruning relies on â€œchunk sparsityâ€ and â€œattention locality.â€ While empirically supported (Figure 2a; Figures 9â€“10), pathological inputs with uniformly distributed longâ€‘range attention could reduce pruning efficacy.
- Stageâ€‘1 cost and prefill latency
  - Stageâ€‘1 is `O(TqÂ·Tkv)` and dominates when masks must be computed; caching amortizes it in decoding but not in firstâ€‘token latency. Appendix G acknowledges that even linearâ€‘ish systems still yield multiâ€‘minute TTFT for 1Mâ€‘token prompts on consumer hardware.
- Memoryâ€‘bandwidth sensitivity under offloading
  - Accessing CPU memory over PCIe adds â‰ˆ31.5Ã— latency versus VRAM (Section 5.3 â€œLatency with KV Offloadingâ€). InfiniteHiP mitigates this via caching and masking, but decoding remains memoryâ€‘bound at extreme lengths (Table 4 â€œOffload (Âµs)â€ rows).
- Overheads from dynamic RoPE
  - OOL generalization adds â‰ˆ1.6Ã— prefill overhead and â‰ˆ5% decoding overhead due to extra sin/cos reads (Table 3).
- Hyperparameter tuning
  - Performance depends on stage sizes, keepâ€‘rates, and refresh intervals (Appendix F). Defaults work broadly, but taskâ€‘specific tuning can shift the accuracy/latency tradeâ€‘off (Appendix G discusses blockâ€‘size tradeâ€‘offs).
- Hardware and software dependencies
  - Relies on Nvidia UVM and Triton kernels; portability to other accelerators or runtimes is nonâ€‘trivial.
- Scope
  - The work focuses on inference, not training; it does not address training with millionâ€‘token sequences or modelâ€‘internal reparameterizations.

## 7. Implications and Future Directions
- How it shifts the landscape
  - Demonstrates that millionâ€‘token prompts are feasible on a single commodityâ€‘class GPU without discarding context and without fineâ€‘tuning, by coâ€‘designing sparse attention, positional indexing, and memory management. This lowers the barrier to realâ€‘world longâ€‘context applications.
- Practical applications
  - Retrievalâ€‘augmented generation over massive corpora, contract or codebase analysis, longâ€‘horizon agents and logs, multiâ€‘document summarization and QA, and interactive assistants that must retain full conversation history.
- Followâ€‘up research
  - Reducing TTFT: integrate speculative decoding/prefill or lazy initialization specifically tailored to modular pruning (Appendix G).
  - Memory efficiency: combine with KV quantization/compression (e.g., KVQuant, DeepSeekâ€‘style compression) inside the same offloading framework (Appendix G).
  - Adaptive pruning policies: learn perâ€‘layer or perâ€‘task stage parameters online; develop reliability checks that detect when chunk sparsity fails and fall back gracefully.
  - Broader hardware support: extend UVMâ€‘style paging and graphâ€‘capturable kernels to nonâ€‘Nvidia accelerators; multiâ€‘GPU paging across NVLink.
  - Trainingâ€‘time synergy: pretraining with objectives that sharpen chunk sparsity or encourage stable attention locality could further improve pruning accuracy and OOL robustness.

Overall, InfiniteHiP is a wellâ€‘engineered, trainingâ€‘free path to practical longâ€‘context LLM inference. Its key strength is the coherent combination of algorithmic, architectural, and systems ideas that together deliver both speed and accuracy at unprecedented context scales, as evidenced across benchmark quality, latency, and throughput (Tables 1â€“5; Figures 3â€“5).

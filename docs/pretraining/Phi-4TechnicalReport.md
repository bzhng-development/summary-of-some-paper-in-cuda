# Phi-4 Technical Report

**ArXiv:** [2412.08905](https://arxiv.org/abs/2412.08905)

## ğŸ¯ Pitch

Phi-4 introduces a 14-billion-parameter language model that sets a new standard for small models by leveraging a data-centric approach: extensive, curated synthetic data and an innovative token-level preference optimization during post-training. This design allows phi-4 to rival or surpass much larger modelsâ€”like outperforming GPT-4o on complex STEM and math tasksâ€”while keeping inference cost and latency low. The significance: Phi-4 proves high-quality data and training curriculum can unlock remarkable reasoning ability in compact models, expanding access to capable AI systems and establishing new benchmarks for efficient, trustworthy small-model deployment.

---

## 1. Executive Summary
Phiâ€‘4 is a 14â€‘billionâ€‘parameter language model built around a dataâ€‘centric recipe: heavy, carefully designed synthetic data during pretraining and midtraining, plus a new tokenâ€‘level preference optimization method in postâ€‘training. The result is a small model that matches or surpasses much larger systems on reasoningâ€‘heavy tasksâ€”e.g., it beats GPTâ€‘4o on graduateâ€‘level STEM QA (GPQA) and math competition problems (MATH)â€”while being evaluated on fresh, decontaminated data such as the November 2024 AMC exams (Figure 1; Table 1).

## 2. Context and Motivation
- Problem/gap addressed
  - Small models rarely rival large frontier models on complex reasoning without expensive inference-time tricks (e.g., long chains of thought). Even when small models are strong, benchmark contamination makes results hard to trust. This paper targets both: (1) how to train a small model to reason well at low inference cost, and (2) how to evaluate it credibly (Section 1.1).
- Why it matters
  - Practical impact: lower latency and cost at deployment compared to very large models or long-chain-of-thought systems (Section 1.1 notes QwQâ€‘32B uses >4Ã— tokens per solution and >2Ã— parameters, making its inference â€œan order of magnitudeâ€ costlier).
  - Scientific value: shows how data quality and curriculum can substitute for bruteâ€‘force scaling (Introduction, Section 1).
- Prior approaches and shortcomings
  - â€œOrganicâ€ webâ€‘heavy pretraining often misaligns with inference settings (e.g., forum style vs. chat style) and can be contaminated by leaked benchmarks (Sections 1 and 2.1).
  - Earlier Phi models heavily distilled from a teacher (GPTâ€‘4), but still trailed frontier models on advanced reasoning (Introduction).
  - Recent long chainâ€‘ofâ€‘thought models (O1/Oâ€‘series style) perform well but at high inference cost (Section 1.1).
- Positioning
  - Phiâ€‘4 keeps the phiâ€‘3 architecture but overhauls the data pipeline: synthetic data is central in all stages; web data is curated and filtered as â€œseeds,â€ and postâ€‘training introduces Pivotal Token Search (PTS), a tokenâ€‘level DPO method (Sections 2, 3, 4).

## 3. Technical Approach
This section unpacks how phiâ€‘4 is built and aligned.

- Model and training overview
  - Architecture: decoderâ€‘only transformer, 14B parameters; 4K context in pretraining extended to 16K in midtraining; `tiktoken` tokenizer (100,352 vocab, padded); full attention at 4K (Section 3).
  - Pretraining schedule: ~10T tokens with linear warmâ€‘up/decay, peak LR 3eâ€‘4, weight decay 0.1, global batch 5760; midtraining adds 250B tokens for 16K context with a 10Ã— smaller LR and RoPE base 250K (Section 3, 3.3).
  - Instruction following is learned later; hence pretraining evaluation uses logâ€‘likelihood and fewâ€‘shot formats rather than strict 0â€‘shot templates (Table 2).

- Data recipe (the core of the method)
  - What â€œsynthetic dataâ€ means here: text generated by LLMs in carefully designed workflows so every next token is predicted inâ€‘distribution, often with explicit stepâ€‘byâ€‘step reasoning (â€œspoonfeeding,â€ Section 2.1).
  - Three pillars (Section 1):
    1) Synthetic data for pre/midtraining; 2) meticulous curation/filtering of organic sources (web, books, code) as seeds and as complementary data; 3) new postâ€‘training techniques.

- Synthetic data generation (Section 2.2; Appendix D)
  - 50 dataset types (~400B unweighted tokens) produced by multiâ€‘stage prompting and quality control. Key elements:
    - Seed curation
      - Web/code snippets and book/paper passages selected via twoâ€‘stage filtering for â€œeducational potential,â€ reasoning depth, and factual content (Section 2.2, â€œWeb and Codeâ€‘based Seedsâ€).
      - Large Q&A pools collected and filtered by plurality voting: discard tooâ€‘easy (all answers agree) and tooâ€‘ambiguous (no agreement) items; keep â€œchallenging but approachableâ€ ones; use plurality answers for rejection sampling during generation (Section 2.2, â€œQuestion Datasetsâ€).
      - Extract Q&A from reasoning chains embedded in organic text by detecting deduction steps and reformulating them into questions/answers (Section 2.2, â€œCreating Questionâ€‘Answer pairsâ€¦â€).
    - Rewrite and augment: turn informative passages into exercises, discussions, or structured reasoning tasks (Section 2.2, â€œRewrite and Augmentâ€).
    - Selfâ€‘revision: generate â†’ critique â†’ revise with rubrics targeting reasoning and accuracy (Section 2.2, â€œSelfâ€‘revisionâ€; Appendix D.1.2).
    - Instruction reversal (notable for code): start from code; generate the corresponding instruction so the pair is in â€œinstruction â†’ codeâ€ order; only keep highâ€‘fidelity pairs where regenerated code matches original (Section 2.2).
    - Validation: run code/tests; for science QA, apply extraction pipelines that ensure relevance and difficulty balance (Section 2.2).

- Organic (human) data curation and filtering (Section 2.3)
  - Targeted acquisitions: arXiv, PubMed Central, GitHub, licensed books (reasoningâ€‘dense corpora).
  - Filtering web dumps: small nonâ€‘LLM classifiers trained on ~10^6 LLMâ€‘generated annotations to select highâ€‘quality pages; extra pipeline to avoid STEM overâ€‘bias and amplify nonâ€‘STEM content; remove corrupted artifacts using nâ€‘gram/compression heuristics.
  - Multilingual coverage: fastTextâ€‘based language ID for 176 languages; quality filtering with the same classifiers distilled to multilingual (Section 2.3).
  - Custom extraction/cleaning: robust HTMLâ€‘toâ€‘text preserving equations, code, tables, thread structure; parsers for TeX, EPUB/XML, Word, PDFs (Section 2.3).

- How much of each data type? (Sections 3.1â€“3.2)
  - Ablations establish two principles:
    - More epochs on highâ€‘quality synthetic beats adding more unique web tokens for reasoning benchmarks (Figure 2).
    - Purely synthetic models underperform on knowledge retrieval (large gap on TriviaQA, Table 3).
  - Final pretraining mixture (Table 5):
    - `Synthetic` 40% (â‰ˆ290B unique tokens, 13.8 epochs),
    - `Web rewrites` 15% (â‰ˆ290B, 5.2 epochs),
    - `Filtered web` 15% (â‰ˆ1.3T, 1.2 epochs),
    - `Code` 20% (â‰ˆ820B, 2.4 epochs),
    - `Acquired sources` 10% (â‰ˆ580B, 1.7 epochs).

- Midtraining for long context (Section 3.3; Table 6)
  - Strategy: prefer inherently long inputs (books, academic, code) over concatenated padding; upâ€‘weight â‰¥8K/16K samples; add new synthetic long sequences; final mix = 30% new longâ€‘context data + 70% recall tokens from pretraining.
  - Outcome: at 16K, improved manyâ€‘shot ICL and longâ€‘doc QA/summarization (Table 6).

- Postâ€‘training to become an assistant (Section 4)
  - Format: `chatml` schema (two turns example provided).
  - SFT: ~8B tokens covering math, coding, reasoning, general chat, safety, model identity, and 40 languages (Section 4.1).
  - Two rounds of DPO (Direct Preference Optimization; Section 4.2):
    - Stage 1: Pivotal Token Search (PTS) DPO (novel) using tokenâ€‘level preferences (Section 4.3; Tables 7 for data mix).
    - Stage 2: judgeâ€‘guided DPO with ~850k fullâ€‘response pairs; responses from GPTâ€‘4o, GPTâ€‘4t, and phiâ€‘4 judged by GPTâ€‘4o with rubrics scoring accuracy/style/detail (Appendix A.2; Table 8).

- What is PTS and how it works? (Section 4.3; Figures 3â€“5)
  - â€œPivotal tokensâ€ are individual tokens where choosing one continuation vs. another causes a sharp change in the chance that the final answer is correct.
  - Estimating â€œprobability of successâ€: from each prefix `t1..ti`, sample many rollouts and use an oracle to score correctness (unit tests for code; exact answer for math/QA).
  - Algorithm (Figure 4): recursively subdivide the completion into segments and keep the single tokens where |Î” p(success)| â‰¥ threshold `pgap`; build DPO pairs where the prompt is the prefix and the accepted vs. rejected completions are the two candidate tokens.
  - Why itâ€™s needed: standard DPO spreads learning signal across whole sequences; lowâ€‘probability but harmful tokens can get incorrectly reinforced; PTS isolates the exact decision points that flip success (Figure 3 shows such flips in a math solution).

- Hallucination mitigation (Appendix A.1; Figure 6)
  - Goal: when the model is unlikely to know an obscure fact, prefer refusal over guessing.
  - Data creation: for each trivia item (from TriviaQA seeds), estimate phiâ€‘4â€™s success rate; generate correct answers and refusal messages; also generate â€œbogus but plausibleâ€ variants and paired refusals; then use SFT and DPO on short (first 5 tokens) responses to nudge behavior (Appendix A.1.1).
  - Effect: large rise in â€œnot attemptedâ€ on SimpleQA; fewer incorrect guesses (Figure 6).

- Robust decontamination (Appendix B)
  - Hybrid 13â€‘gram and 7â€‘gram decontamination with thresholds; safelist of ubiquitous 13â€‘grams (Algorithm 1). Example shows detection against AGIEval with overlapping nâ€‘grams (Appendix B, last page).

## 4. Key Insights and Innovations
- Pivotal Token Search (PTS) for tokenâ€‘level preference optimization
  - Whatâ€™s new: preference pairs operate at the singleâ€‘token level at precisely the decision points that flip correctness (Section 4.3; Figures 3â€“4).
  - Why it matters: concentrates gradient where it counts; reduces noise from long completions; complements standard DPO. Empirically, PTS boosts reasoning tasksâ€”e.g., GPQA rises from 47.3 (SFT) â†’ 53.6 (Stage 1 PTS DPO) â†’ 56.1 (final) and MATH from 77.1 â†’ 80.5 â†’ 80.4 (Table 9).
  - Contrast with prior â€œcritical tokenâ€ work (e.g., [LLX+24]): PTS estimates success probabilities directly from rollouts and works for both accepted and rejected tokens (Section 4.3, â€œRelated Workâ€).
- A syntheticâ€‘first curriculum that still preserves knowledge
  - Whatâ€™s new: 40% synthetic + 30% web/webâ€‘rewrites + 20% code + 10% acquisitions (Table 5), after ablations show that extra synthetic epochs beat adding web tokens for reasoning (Figure 2) while some web/knowledge sources are necessary for factual tasks (Table 3, TQA gap).
  - Why it matters: balances reasoning (synthetic) and knowledge (clean web/books/code) while ensuring style alignment with inference contexts (Section 2.1â€“2.3).
- Instructionâ€‘reversal and selfâ€‘revision pipelines at scale
  - Whatâ€™s new: reverse engineer instructions from code to create faithful instructionâ†’solution pairs; multiâ€‘agent selfâ€‘critique/refinement (Section 2.2; Appendix D).
  - Why it matters: improves alignment with user prompts, raises solution fidelity in code/science, and makes pretraining contexts closer to inference distribution (Section 2.1).
- Longâ€‘context midtraining that favors naturally long samples
  - Whatâ€™s new: rather than padding short items, curate inherently long (>8K/16K) academic/books/code data and add long synthetic sequences (Section 3.3).
  - Why it matters: improves manyâ€‘shot ICL and longâ€‘document tasks at 16K (Table 6), showing the importance of â€œnaturalâ€ long inputs.

## 5. Experimental Analysis
- Evaluation design
  - Academic benchmarks via the reproducible `simple-evals` framework at temperature 0.5 with fixed prompts/extraction (Table 1).
  - Contaminationâ€‘resistant tests:
    - Fresh AMCâ€‘10/12 November 2024 exams (Figure 1; Appendix C details 100 runs per test, t=0.5; GPTâ€‘4o used only to extract the final boxed option from long solutions).
    - GPQA Diamond (new, webâ€‘proof) and an internal teamâ€‘written benchmark, PhiBench (Section 1.1; Section 5).
  - Longâ€‘context evaluation: HELMET tasks including recall, RAG, reâ€‘ranking, inâ€‘context learning, QA, and summarization (Section 3.3; Table 6).

- Headline results
  - Crossâ€‘benchmark performance (Table 1):
    > GPQA: 56.1 (phiâ€‘4) vs 49.1 (GPTâ€‘4o), 50.6 (Qwenâ€‘72B), 49.0 (Llamaâ€‘70B).
    >
    > MATH: 80.4 (phiâ€‘4) vs 74.6 (GPTâ€‘4o), 80.0 (Qwenâ€‘72B), 66.3 (Llamaâ€‘70B, extraction caveat).
    >
    > MMLU: 84.8 (phiâ€‘4) vs 88.1 (GPTâ€‘4o), 86.3 (Llamaâ€‘70B), 85.3 (Qwenâ€‘72B).
    >
    > HumanEval+: 82.8 (phiâ€‘4) vs 82.0 (GPTâ€‘4oâ€‘mini), 78.4 (Qwenâ€‘72B), 77.9 (Llamaâ€‘70B).
    - Interpretation: phiâ€‘4 is â€œsmallâ€ (14B) yet competitive with or better than much larger open models, especially on reasoning (GPQA, MATH) and coding (HumanEval/HumanEval+).
  - Fresh AMC exams (Figure 1):
    > Average score (max 150): 91.8 (phiâ€‘4) vs 89.8 (Gemini Pro 1.5), 78.7 (GPTâ€‘4o), 77.9 (GPTâ€‘4oâ€‘mini), 78.2 (Qwenâ€‘72B), 66.4 (Llamaâ€‘3.3â€‘70B).
    - Significance: protects against contamination (Section 1.1; Appendix C).
  - Longâ€‘context (HELMET, Table 6):
    > At 16K: phiâ€‘4 improves ICL from 68.0 â†’ 77.0 and QA 26.7 â†’ 36.0; summarization 38.3 â†’ 40.5.
    - Tradeâ€‘offs: Reâ€‘ranking drops (65.3 â†’ 54.4) and RAG slightly decreases (58.1 â†’ 57.1), suggesting taskâ€‘specific effects of longer context.
  - Postâ€‘training ablations (Table 9):
    > SFT â†’ PTS DPO (stage 1) â†’ judgeâ€‘guided DPO (stage 2): GPQA 47.3 â†’ 53.6 â†’ 56.1; MATH 77.1 â†’ 80.5 â†’ 80.4; ArenaHard 56.7 â†’ 66.5 â†’ 75.4.
    - Interpretation: PTS is especially helpful on reasoning; judgeâ€‘guided DPO shines on humanâ€‘preferenceâ€‘style judgments (ArenaHard). Combined stages are complementary.
  - Safety/RAI (Table 10):
    > â€œJailbreak (DR1)â€ defect rate 0.073 (phiâ€‘4) lower than several 7â€‘8B baselines; â€œGroundingâ€ 4.619/5.0; harmful content continuation DR3 = 0.036.
    - Plus dedicated redâ€‘teaming by Microsoft AIRT and safety postâ€‘training (Section 7).

- Pretraining improvements relative to phiâ€‘3 (Table 2)
  > After pretraining, phiâ€‘4 gains on MMLU (+3.0), MMLUâ€‘pro (+10.3), HumanEval (+7.8), MBPP (+6.8), MATH (+8.9).
  - Supports the claim that the new data/curriculum materially improves core capabilities before any instruction tuning.

- Ablations that justify the mixture (Tables 3â€“4; Figure 2)
  - â€œSynthetic onlyâ€ improves reasoning/coding but hurts TriviaQA by âˆ’14.8 (Table 3).
  - â€œMore synthetic epochsâ€ beats â€œmore fresh web tokensâ€ on MMLU (Figure 2).
  - Mixture search shows uniform allocation is suboptimal; syntheticâ€‘heavy variants edge out others but the final mixture balances knowledge benchmarks and later gains from postâ€‘training (Table 4; Section 3.2).

- Hallucination behavior (Figure 6; Appendix A.1)
  > SimpleQA â€œNot Attemptedâ€ rises to 81.1% with incorrect reduced to 15.8% (final), reflecting safer behavior even though F1 drops.

- Do the experiments support the claims?
  - Yes for reasoning: multiple independent indicators (GPQA, MATH, AMC) show strong gains with contamination control (Section 1.1; Appendix C).
  - Mixed for instruction following: IFEval is relatively weak (63.0; Table 1), consistent with the paperâ€™s own assessment (Section 8).
  - Longâ€‘context is improved in some categories but not uniformly (Table 6), a useful nuance.

## 6. Limitations and Trade-offs
- Reliance on synthetic data requires immaculate seeds and validation; small errors in seeds can propagate and degrade synthetic generations (Section 2.3).
- Knowledge limitations remain: without tools/search, the model still hallucinates factual biographies/names and obscure facts (Section 8). The paper mitigates this by training refusals but does not eliminate the issue.
- Instruction following is a weakness: strict format adherence (e.g., tables, bullet structure) is inferior to peers (IFEval 63.0; Table 1; Section 8).
- Verbosity: chainâ€‘ofâ€‘thoughtâ€‘heavy training can yield overly long answers even for simple queries (Section 8).
- Singleâ€‘turn optimization: the model is tuned primarily for singleâ€‘turn tasks; multiâ€‘turn reliability may lag (Section 8).
- Longâ€‘context tradeâ€‘offs: 16K improves ICL/QA but can reduce reâ€‘ranking performance (Table 6).
- Compute/data cost: although inference is smallâ€‘modelâ€‘class, training uses ~10T pretraining tokens + 250B midtraining (Section 3), so the training budget is substantial.
- Residual contamination risk: even with hybrid 7â€‘gram/13â€‘gram decontamination (Appendix B), paraphraseâ€‘level leakage canâ€™t be completely ruled out (Section 5).
- LLMâ€‘asâ€‘judge dependence: judgeâ€‘guided DPO and some evaluations (ArenaHard, parts of HELMET scoring) can reflect judge biases (Section 5).

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that a dataâ€‘centric recipeâ€”diverse synthetic generation, seed curation, and tokenâ€‘level preference optimizationâ€”can push small models into â€œreasoning parityâ€ with much larger systems (Table 1; Figure 1). This shifts attention from pure scale to curriculum and postâ€‘training design.
  - PTS opens a new granularity for preference learningâ€”tokenâ€‘level at pivotal decisionsâ€”which could benefit many domains (code repair, mathematical proof search, toolâ€‘use planners).
- Followâ€‘up research enabled
  - Endâ€‘toâ€‘end optimization of pretraining mixtures that account for postâ€‘training effects (Section 3.2 notes this as promising).
  - Extending PTS: beyond binary accepted/rejected tokens to structured action spaces, multiâ€‘token decisions, or reinforcement learning settings (Section 4.3).
  - Better contaminationâ€‘proof and processâ€‘based benchmarks; more â€œfreshâ€ evaluations like AMC (Section 1.1; Section 5).
  - Strengthening instruction following with targeted synthetic datasets (Section 6 notes this as a likely fix).
  - Toolâ€‘augmented phiâ€‘4 (search, calculators, code runners) to reduce factual hallucinations (Section 8 suggests this).
- Practical applications
  - Costâ€‘sensitive reasoning: education (competition math tutors), scientific QA, coding assistants that run locally or at low latency (Table 1; Figure 1).
  - Longâ€‘document tasks within 16K window: technical document QA and legal summarization (Table 6).
  - Safer deployment defaults: refusalâ€‘toâ€‘hallucinate behavior for knowledgeâ€‘sparse queries (Appendix A.1; Figure 6).

Overall, phiâ€‘4 shows that carefully engineered data pipelines plus targeted tokenâ€‘level preference learning can move the performance needle more efficiently than parameter count aloneâ€”especially for reasoningâ€‘centric tasksâ€”while flagging clear next steps for instruction following, longâ€‘context tradeâ€‘offs, and tool integration.

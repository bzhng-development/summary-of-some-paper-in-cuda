# Why Do Multiâ€‘Agent LLM Systems Fail?

**ArXiv:** [2503.13657](https://arxiv.org/abs/2503.13657)
**Authors:** Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica
**Institutions:** University of California, Berkeley (Sky Computing Lab), UC Berkeley, Stanford University, University of Illinois Urbanaâ€“Champaign

## ðŸŽ¯ Pitch

This paper introduces the MAST framework, the first comprehensive taxonomy and dataset for diagnosing failure modes in multi-agent systems constructed from large language models. By elucidating 14 specific failure types and providing automated labeling tools, it enables precise, scalable analysis critical for improving system reliability and efficiency, addressing failures rooted in design, coordination, and verification rather than relying merely on stronger base models. This work paves the way for targeted debugging and principled system design, crucial for advancing real-world MAS applications.

---

## 1. Executive Summary (2â€“3 sentences)
This paper builds the first empirically grounded framework for understanding why multiâ€‘agent systems built from large language models (LLMs) fail. It contributes (a) MAST, a taxonomy of 14 concrete failure modes grouped into three categories, (b) MASTâ€‘Data, 1,642 execution traces from seven popular multiâ€‘agent systems labeled with those failures, and (c) an LLMâ€‘asâ€‘aâ€‘Judge pipeline that scales failure annotation with strong agreement to human experts. Together, these tools show that most failures stem from system design, interâ€‘agent misalignment, and inadequate verificationâ€”problems that require architectural fixes beyond simply using stronger base models.

## 2. Context and Motivation
- Problem the paper addresses
  - Multiâ€‘agent LLM systems (MAS)â€”sets of specialized LLM â€œagentsâ€ that collaborate via messages and toolsâ€”are increasingly used in software engineering, web/task agents, and science assistants. Yet their performance gains are inconsistent and often marginal.
  - The paper identifies a central gap: there is no principled, fineâ€‘grained, crossâ€‘system account of why MAS fail. Without shared definitions and datasets, debugging and improving these systems is ad hoc.

- Why this matters
  - Realâ€‘world impact: MAS are deployed for complex tasks such as software development (e.g., ChatDev, MetaGPT) and web workflows (e.g., AppWorld, Magenticâ€‘One). Failures can waste compute and human time, and can ship incorrect code or unsafe actions.
  - Evidence of the problem: On six openâ€‘source MAS, failure rates range from â€œ41% to 86.7%â€ (Figure 5; Appendix B), i.e., many runs do not achieve the intended objective.
  - Engineering significance: Reliability demands more than aggregate success/failure rates; we need to know what fails, when, and why (root causes and failure dynamics).

- Prior approaches and their gaps
  - Benchmarks evaluate overall success but not granular failure causes (Section 2.1; citations [27â€“32]).
  - Design checklists and singleâ€‘agent principles exist, but do not systematize multiâ€‘agent failure patterns (Section 2.2).
  - There was no large, labeled corpus of MAS traces, and no validated taxonomy tailored to multiâ€‘agent coordination problems.

- How this work positions itself
  - Provides a bottomâ€‘up, dataâ€‘driven taxonomyâ€”MASTâ€”derived via qualitative analysis of real agent traces (Grounded Theory).
  - Publishes a large crossâ€‘framework datasetâ€”MASTâ€‘Dataâ€”annotated using that taxonomy.
  - Supplies an automated annotator (LLMâ€‘asâ€‘aâ€‘Judge) to scale labeling and enable quantitative analysis across systems, tasks, and models.

## 3. Technical Approach
This section explains how the study builds MASTâ€‘Data, defines the MAST taxonomy, and calibrates the LLMâ€‘asâ€‘aâ€‘Judge annotator.

- What is a â€œtraceâ€ and what counts as â€œfailureâ€?
  - A â€œtraceâ€ is the full conversation and toolâ€‘use log across agents for a task. A â€œfailureâ€ is when the MAS does not achieve the intended task objective (Section 3).

- Step 1 â€” Discover failure patterns with Grounded Theory (Section 3.1)
  - Grounded Theory is a qualitative method where categories emerge from data rather than from predefined hypotheses.
  - Procedure:
    - Collect an initial 150 traces across five MAS and two task types (programming and math): HyperAgent, AppWorld, AG2 (MathChat), ChatDev, and MetaGPT (Section 3.1).
    - Use â€œopen codingâ€ to label observed failure behaviors; iteratively compare cases and memo findings until theoretical saturation (no new failure types appear).
    - Output: a candidate set of failure modes and draft definitions.

- Step 2 â€” Turn the patterns into a precise taxonomy and validate human agreement (Section 3.2; Appendix A)
  - Interâ€‘Annotator Agreement (IAA): Three expert annotators independently label batches of traces with the draft taxonomy; disagreements are discussed and definitions refined.
  - After three IAA rounds, the finalized taxonomy achieves â€œÎº = 0.88â€ Cohenâ€™s Kappa (strong agreement), demonstrating unambiguous definitions (Section 3.2).
  - The paper also releases a small, tripleâ€‘labeled subset â€œMASTâ€‘Dataâ€‘humanâ€ for transparency and future calibration.

- Step 3 â€” The MAST taxonomy (Figure 1; Appendix A)
  - 14 failure modes grouped into three categories and aligned to stages of an MAS run (Preâ€‘Execution, Execution, Postâ€‘Execution):
    - FC1 System Design Issues (e.g., `FMâ€‘1.1 Disobey Task Specification`, `FMâ€‘1.3 Step Repetition`, `FMâ€‘1.5 Unaware of Termination Conditions`).
    - FC2 Interâ€‘Agent Misalignment (e.g., `FMâ€‘2.2 Fail to Ask for Clarification`, `FMâ€‘2.4 Information Withholding`, `FMâ€‘2.6 Reasoningâ€“Action Mismatch`).
    - FC3 Task Verification (e.g., `FMâ€‘3.1 Premature Termination`, `FMâ€‘3.2 No or Incomplete Verification`, `FMâ€‘3.3 Incorrect Verification`).
  - Figure 1 also gives prevalence across â€œ1642 MAS execution traces,â€ e.g., `FMâ€‘1.3 Step Repetition` at 15.7% and `FMâ€‘2.6 Reasoningâ€“Action Mismatch` at 13.2%, with overall category shares of 44.2% (FC1), 32.3% (FC2), and 23.5% (FC3).

- Step 4 â€” Build an LLMâ€‘asâ€‘aâ€‘Judge annotator for scalable labeling (Section 3.3; Table 2; Appendix N)
  - Setup:
    - Input: a full trace, MAST definitions, and fewâ€‘shot examples from humanâ€‘labeled traces.
    - Model: OpenAIâ€™s `o1`. The annotator outputs which failure modes occurred and textual justifications.
  - Calibration and performance:
    - Without fewâ€‘shot: accuracy 0.89, Îº = 0.58.
    - With fewâ€‘shot: â€œaccuracy 0.94 â€¦ Îº = 0.77â€ (Table 2), showing substantial agreement with experts.
  - Generalization:
    - On two unseen MAS and benchmarksâ€”OpenManus and Magenticâ€‘One; MMLU and GAIAâ€”it achieves â€œÎº = 0.79â€ (Section 3.4).

- Step 5 â€” Construct MASTâ€‘Data and analysis tools (Section 3.4; Table 1; Figure 2)
  - Data composition:
    - 1,642 annotated traces spanning seven MAS frameworks and multiple benchmarks (Table 1): ChatDev, MetaGPT, HyperAgent, AppWorld, AG2 (MathChat), Magenticâ€‘One, OpenManus.
    - Models include `GPTâ€‘4`, `GPTâ€‘4o`, `Claudeâ€‘3.7â€‘Sonnet`, and openâ€‘source `Qwen2.5â€‘Coderâ€‘32Bâ€‘Instruct` and `CodeLlamaâ€‘7Bâ€‘Instruct` (Table 1; Appendix I).
  - Tooling:
    - A Python package `agentdash` exposes the annotator and taxonomy for developers (Appendix C gives a usage example).

- How the pieces work together
  - MAST gives precise labels; the LLMâ€‘asâ€‘aâ€‘Judge scales those labels to thousands of traces; MASTâ€‘Data then supports crossâ€‘system, crossâ€‘model, and perâ€‘benchmark analyses and interventions.

## 4. Key Insights and Innovations
- A validated, fineâ€‘grained failure taxonomy for MAS
  - Novelty: Prior work discussed challenges qualitatively; MAST provides 14 wellâ€‘defined modes with stage alignment and prevalence numbers (Figure 1; Appendix A).
  - Significance: Enables applesâ€‘toâ€‘apples diagnosis across frameworks and tasksâ€”critical for engineering reliable multiâ€‘agent systems.

- A large, publicly released dataset of labeled MAS traces
  - 1,642 traces with failure annotations plus a humanâ€‘labeled subset for calibration (Table 1; Section 3.4).
  - Significance: Establishes a common empirical basis for MAS reliability research.

- A calibrated LLMâ€‘asâ€‘aâ€‘Judge for failure labeling
  - With fewâ€‘shot prompting, the judge reaches strong agreement with human experts (accuracy 0.94; Îº = 0.77; Table 2).
  - Significance: Dramatically reduces the cost/time of largeâ€‘scale failure analysis while preserving definitional fidelity.

- Three design insights grounded in the taxonomy (Section 4)
  - Insight 1 (System Design): Many failures arise from architecture and prompt specifications, not just baseâ€‘model limits. Example: improving agent role specifications in ChatDev yields â€œ+9.4%â€ success gains without changing the LLM (Section 4 and Appendix H).
  - Insight 2 (Interâ€‘Agent Misalignment): Communication problems often reflect weak â€œtheory of mindâ€ between agents (e.g., `FMâ€‘2.4 Information Withholding`, Figure 3), suggesting the need for deeper communicative intelligence, not just better message formats (Section 4, FC2).
  - Insight 3 (Verification): Singleâ€‘stage or shallow verifiers miss real errors; multiâ€‘level verification is needed. Adding a highâ€‘level objective check to ChatDev improves success by â€œ+15.6%â€ on ProgramDev (Section 4 and Appendix H).

## 5. Experimental Analysis
- Evaluation methodology
  - Systems and tasks: Seven MAS frameworks, spanning software engineering (ProgramDev, SWEâ€‘Bench Lite), math (GSMâ€‘Plus, OlympiadBench), general agent tasks (GAIA), and knowledge tests (MMLU) (Table 1; Appendices B, D).
  - Labels: Failure modes assigned by human experts for calibration and by the LLMâ€‘asâ€‘aâ€‘Judge for scale (Sections 3.2â€“3.4).
  - Agreement checks: Humanâ€“human Îº = 0.88; LLMâ€‘judge vs human Îº = 0.77 on heldâ€‘out traces; generalizes to unseen systems/benchmarks with Îº = 0.79 (Sections 3.2â€“3.4).

- How often do systems fail?
  - Figure 5 reports failure rates by system/benchmark (not meant for direct crossâ€‘system comparison). The failures range broadlyâ€”e.g., AppWorld (Testâ€‘C) shows â€œ86.7% failure,â€ while AG2 (OlympiadBench) shows â€œ41.0% failure.â€
  - The study uses these rates illustratively and emphasizes perâ€‘system failure profiles rather than leaderboardâ€‘style comparisons.

- What fails and where in the pipeline?
  - Global distribution (all 1,642 traces): System Design Issues â‰ˆ 44.2%, Interâ€‘Agent Misalignment â‰ˆ 32.3%, Task Verification â‰ˆ 23.5% (Figure 1).
  - Perâ€‘system profiles (first 30 traces/system; n=210): System Design 41.8%, Interâ€‘Agent 36.9%, Verification 21.3% (Figure 4). The mix differs by framework (e.g., AppWorld is prone to `FMâ€‘3.1 Premature Termination`; HyperAgent shows `FMâ€‘1.3 Step Repetition` and `FMâ€‘3.3 Incorrect Verification`).

- Are the failure categories distinct?
  - A correlation analysis shows low crossâ€‘category correlations (0.17â€“0.32; Figure 6), suggesting the taxonomy captures distinct aspects of failure.
  - Some modes with similar symptoms correlate moderately (max 0.63; Figure 7), warning that automated annotators can conflate causes if not carefully prompted.

- Model choice vs. architecture: Which matters more?
  - Effect of LLMs within the same architecture (MetaGPT): With GPTâ€‘4o vs Claudeâ€‘3.7â€‘Sonnet on ProgramDevâ€‘v2, GPTâ€‘4o shows substantially fewer System Design and Interâ€‘Agent failures; both still suffer many Verification failures (Figure 8).
  - Effect of MAS architecture with the same LLM (GPTâ€‘4o): MetaGPT vs ChatDev on ProgramDevâ€‘v2â€”MetaGPT has far fewer System Design and Interâ€‘Agent failures but more Verification failures than ChatDev (Figure 9). Architectural choices shift which problems dominate.

- Openâ€‘source models
  - With `Qwen2.5â€‘Coderâ€‘32Bâ€‘Instruct` vs `CodeLlamaâ€‘7Bâ€‘Instruct` on ChatDev and MetaGPT, Qwen is markedly more robust (fewer failures), but both lag behind GPTâ€‘4o/Claude (Appendix I; Table 6).
  - Example counts (Table 6): on ChatDev, `FMâ€‘1.3 Step Repetition` occurs 96 times with Qwen vs 97 with CodeLlama; more stark gaps appear in `FMâ€‘1.5 Unaware of Termination Conditions` (Qwen 94 vs CodeLlama 97) and misalignment modes (`FMâ€‘2.5` 2 vs 42).

- Do failure modes predict success?
  - Table 7 separates traces by final outcome and shows that some modes are â€œfatalâ€ (e.g., `FMâ€‘1.5 Unaware of Termination Conditions` and `FMâ€‘2.4 Information Withholding` appear almost only in failed runs), while verification failures (`FMâ€‘3.2`, `FMâ€‘3.3`) often appear even in successful runsâ€”indicating systemic verifier weakness that doesnâ€™t always flip the final outcome.

- Benchmark difficulty matters
  - With AG2 (GPTâ€‘4o) across benchmarks (Table 8): failure rates per trace rise with task difficultyâ€”e.g., OlympiadBench shows higher System Design (1.19) and Verification (0.67) failure rates than GSM (0.53; 0.37).

- Case studies: Can MAST guide improvements?
  - AG2 MathChat (Section H.1; Table 5):
    - Baseline vs Prompt improvement vs New topology, repeated 6Ã— runs.
    - GPTâ€‘4: 84.75% â†’ 89.75% (prompt) â†’ 85.50% (topology).
    - GPTâ€‘4o: 84.25% â†’ 89.00% (prompt) â†’ 88.83% (topology).
    - Statistical note: on GPTâ€‘4, only the prompt change gives significant gains; on GPTâ€‘4o, both prompt and topology yield significant improvements (Wilcoxon p = 0.03 vs baseline).
  - ChatDev (Section H.2; Table 5):
    - ProgramDevâ€‘v0: 25.0% â†’ 34.4% (prompt) â†’ 40.6% (topology).
    - HumanEval: 89.6% â†’ 90.3% (prompt) â†’ 91.5% (topology).
    - A separate workflow tweak (â€œCEO final sayâ€) earlier yields â€œ+9.4%â€ (Section 1 and Section 4).
  - MAST detects how interventions change failure profiles, not just success rates (Appendix H.3; Figures 10â€“11).

- Cost of automated annotation
  - Average $1.80 per trace; costs vary with trace length (Table 9), e.g., OpenManus â‰ˆ $4.14/trace; MetaGPT â‰ˆ $2.45/trace.

- Representative failures
  - The paper provides concrete trace snippets for each mode (Appendix N). Example: Figure 3 shows `FMâ€‘2.4 Information Withholding`â€”a Phone Agent fails to tell the Supervisor that the API expects a phone number, causing repeated login failures.

- Do the experiments support the claims?
  - The taxonomyâ€™s reliability is supported by high human agreement (Îº = 0.88) and strong judge agreement (Îº = 0.77) on heldâ€‘out traces and across new systems (Sections 3.2â€“3.4).
  - Crossâ€‘system analyses repeatedly show that:
    - Many failures cluster in System Design and Interâ€‘Agent categories (Figures 1 and 4).
    - Verification is a persistent weak link across LLMs and frameworks (Figures 8â€“9).
  - Intervention studies demonstrate that MASTâ€‘guided changes measurably improve outcomes (Section H; Table 5), though not universally or completelyâ€”suggesting deeper, structural fixes are needed (Section 5.3; Appendix G).

## 6. Limitations and Tradeâ€‘offs
- Scope of taxonomy
  - The taxonomy is comprehensive but not exhaustive: â€œwe do not claim MAST covers every potential failure patternâ€ (Section 4). New domains (e.g., robotics) may surface additional modes.
- Rootâ€‘cause certainty
  - Labels are derived from traces; some modes have similar surface symptoms (Appendix E), and the judgeâ€™s recall is 0.77 (Table 2), so subtle cases can be misclassified.
- Dataset breadth and comparability
  - Benchmarks and tasks vary per system (Table 1; Figure 4 caption), so performance numbers are illustrative, not headâ€‘toâ€‘head comparisons.
  - Closedâ€‘source systems (e.g., Manus) could not be included in failure analyses due to missing full traces (Appendix B.3).
- Generalization beyond studied settings
  - Most traces are programming/math/web tasks. Other domains (embodied agents, safetyâ€‘critical control) may display different failure dynamics.
- Verification ground truth
  - For some tasks, final success/failure requires human evaluation (Table 1 â€œHEâ€). Rootâ€‘cause verification can be subjective without formal specs or unit tests.
- Cost/compute
  - Automated annotation has nonâ€‘trivial cost (Table 9) and depends on access to a highâ€‘end model (`o1`). Reproducibility may be limited by API/price changes.

## 7. Implications and Future Directions
- How this changes the landscape
  - Provides a shared language and dataset for MAS reliability. Researchers and practitioners can now measure not only whether a system fails but how and where it failsâ€”enabling targeted debugging and principled system design.
  - The taxonomy reveals that many issues are organizational: agent roles, workflows, and verification pipelinesâ€”echoing insights from reliability engineering (Section 5.3).

- Practical recommendations (Appendix G; Section 4)
  - Prioritize structural fixes:
    - Multiâ€‘level verification (unit tests, runtime checks, highâ€‘level objective validation) rather than only final â€œdoes it compile?â€ checks.
    - Standardized, structured interâ€‘agent communication (beyond freeâ€‘form chat) and protocols that surface assumptions and uncertainties.
    - Memory/state management for longâ€‘horizon coordination; avoid conversation resets and context loss (`FMâ€‘1.4`, `FMâ€‘2.1`).
    - Incorporate uncertainty/confidence thresholds to trigger clarification (`FMâ€‘2.2`) and avoid premature termination (`FMâ€‘3.1`).
  - Tactical measures help but are inconsistent:
    - Better prompts and clearer role specs can reduce `FMâ€‘1.x` failures; majorityâ€‘vote/resampling helps only if backed by real verifiers (Appendix G.1, G.2; [67, 68]).

- Research directions enabled by MASTâ€‘Data
  - Train â€œsocially awareâ€ agents: datasets for modeling other agentsâ€™ information needs and improving `FMâ€‘2.x` cases (Section 4, FC2).
  - Learn verifiers: domainâ€‘adaptive, multiâ€‘granular verifiers that combine symbolic checks and test generation; move toward provable guarantees in constrained domains (Section 4, FC3; Appendix G.2).
  - Automated failure attribution and repair: integrate MASTâ€‘style labels with causal debugging tools and agentâ€‘level credit assignment (Appendix H; related to [41, 42]).
  - Taxonomy expansion: incorporate new domains (embodied agents, safety/security) and refine ambiguous boundary cases identified by the correlation analysis (Appendix E).

- Downstream applications
  - Engineering dashboards that track failure modes over time during development and after deployment.
  - Model selection and system design: choose LLMs and architectures based on expected failure profiles (Figures 8â€“9), not just average accuracy.
  - Continuous integration for agents: enforce verification gates tied to `FMâ€‘3.x`, regression tests for `FMâ€‘1.x` design regressions, and communication linting for `FMâ€‘2.x`.

> In summary, the paper provides the field with a vocabulary (MAST), an evidence base (MASTâ€‘Data), and a scalable instrument (LLMâ€‘asâ€‘aâ€‘Judge) to diagnose and reduce MAS failures. The main takeaway is actionable: most problems trace to design, coordination, and verification; fixing them requires architectural rigor and better verifiersâ€”not merely stronger base models.

# Training language models to follow instructions with human feedback

**ArXiv:** [2203.02155](https://arxiv.org/abs/2203.02155)

## ðŸŽ¯ Pitch

This paper presents InstructGPT, a family of large language models aligned to follow user instructions by fine-tuning GPT-3 with human feedback using reinforcement learning (RLHF) on real-world prompts. InstructGPT models are rated as more helpful, truthful, and less toxic than much larger GPT-3 models, marking a major advance in making language models practically useful, safer, and more controllable across broad tasksâ€”setting a new paradigm for aligning AI systems with human intent.

---

## 1. Executive Summary (2â€“3 sentences)
This paper shows how to align large language models with user intent by fineâ€‘tuning them using human feedback (reinforcement learning from human feedback, RLHF). The resulting models, called `InstructGPT`, produce responses that human evaluators prefer over those from much larger GPTâ€‘3 models, improve truthfulness and reduce toxic output on several measures, andâ€”when combined with a simple training modificationâ€”avoid major regressions on standard NLP benchmarks (Figures 1, 3, 4, 6, 7; Section 4.1â€“4.2).

## 2. Context and Motivation
- Problem/gap:
  - Pretrained language models are optimized to predict the next token on internet text, not to â€œfollow a userâ€™s instructions helpfully and safely.â€ This mismatch leads to unhelpful, untruthful, or toxic outputs (Introduction; Section 1).
  - Prior RLHF work focused on narrow tasks like summarization (Section 2), leaving open whether instruction-following could be aligned across a broad, realâ€‘world distribution of tasks.
- Realâ€‘world importance:
  - Aligned models are easier to deploy in applications without extensive prompt engineering, reduce risk of harmful outputs, and better match user expectations (Section 1; Table 3 for safety metadata tracked).
- Prior approaches and their limits:
  - Prompting or fewâ€‘shot learning improves instruction following but is brittle and still misaligned (Figure 1 â€œGPT (prompted)â€; Section 4.1).
  - Multiâ€‘task fineâ€‘tuning on public NLP datasets (e.g., FLAN, T0) boosts zeroâ€‘shot transfer but underperforms on real API prompts dominated by openâ€‘ended generation and brainstorming (Figure 5; Table 1).
- Positioning:
  - The paper scales RLHF to a wide distribution of instructionâ€‘like prompts sourced from real API users, and compares against strong baselines including fewâ€‘shot GPTâ€‘3, SFTâ€‘only models, and FLAN/T0 (Sections 3.2, 3.5; Figures 1, 3, 5).

## 3. Technical Approach
The method has three stages (Figure 2; Section 3.1). Terms used below:
- `SFT` (Supervised Fineâ€‘Tuning): standard fineâ€‘tuning on inputâ€“output demonstrations.
- `Reward model` (`RM`): a model that maps (prompt, completion) to a scalar score trained to predict human preferences.
- `RLHF` with `PPO`: optimize the policy (the language model) via Proximal Policy Optimization using the RM as the reward.
- `KL penalty`: a perâ€‘token penalty that discourages the policy from drifting too far from the SFT model.
- `PPO-ptx`: PPO training mixed with gradients from the original pretraining distribution to mitigate benchmark regressions (Equation (2); Section 3.5).

Stepâ€‘byâ€‘step:
1) Data and labeling pipeline (Sections 3.2â€“3.4; Appendix A, B)
   - Prompt sources:
     - Real prompts submitted to early InstructGPT models via the OpenAI API Playground (with user consent, deduplication, PII filtering; train/val/test split by user ID).
     - Labelerâ€‘written prompts used to bootstrap the first SFT models.
   - Datasets (Table 6):
     - `SFT` train: ~13k prompts (11,295 labeler; 1,430 customer), plus validation.
     - `RM` train: ~33k prompts with 4â€“9 model responses per prompt ranked by labelers (generates up to K(Kâˆ’1)/2 pairwise comparisons per prompt).
     - `PPO` train: ~31k customer prompts for RL rollouts; separate validation.
   - Labelers:
     - ~40 trained contractors selected via screening for sensitivity to harmful content, preference agreement, and ability to write nuanced demos (Section 3.4; Appendix B.1).
     - Interâ€‘annotator agreement: training labelers 72.6 Â± 1.5%; heldâ€‘out labelers 77.3 Â± 1.3% (Section 3.4).
   - Task diversity:
     - Prompts span 10 useâ€‘case categories; majority are openâ€‘ended generation/brainstorming (Table 1; examples in Tables 2 and Appendix A.2). Dataset is >96% English (Appendix A.4).

2) Stage 1 â€” Supervised Fineâ€‘Tuning (`SFT`) (Section 3.5 â€œSupervised fine-tuningâ€)
   - Fineâ€‘tune GPTâ€‘3 on labeler demonstrations for 16 epochs (cosine LR decay; residual dropout 0.2). Although validation loss overfits after ~1 epoch, continuing training improves humanâ€‘aligned metrics (reward model score and human preferences).

3) Stage 2 â€” Reward Model (`RM`) training (Section 3.5 â€œReward modelingâ€; Equation (1))
   - Input: prompt x and completion y; output: scalar reward rÎ¸(x, y).
   - Labelers rank K samples per prompt; training uses all pairwise comparisons from these rankings but computed efficiently:
     - Instead of treating each pair as a separate example (which overfits because pairs share the same completions), all K completions for a prompt are processed in one batch, and the loss aggregates the pairwise Bradleyâ€‘Terry style comparisons:
       - Equation (1): minimize âˆ’E[log Ïƒ(rÎ¸(x, y_w) âˆ’ rÎ¸(x, y_l))], where y_w is the preferred completion, y_l the less preferred one.
   - Practical choices:
     - Use a 6B RM (not 175B) for stability and compute efficiency; 175B RM training was less stable and too costly to use as a value function during RL (Appendix C.2).
     - Normalize RM scores so labeler demonstrations have mean 0 (Section 3.5).

4) Stage 3 â€” RLHF with PPO (Section 3.5 â€œReinforcement learning (RL)â€)
   - Environment: contextual banditâ€”sample a prompt, the policy generates a completion, the RM returns a scalar reward; the episode ends (Section 3.5).
   - Objective (Equation (2)):
     - Maximize E[rÎ¸(x, y)] minus a `Î²` KL penalty from the SFT policy (to prevent overâ€‘optimization of the RM and preserve language quality), plus an optional pretraining logâ€‘likelihood term with weight `Î³` (`PPO-ptx`) that mixes in gradients from the original pretraining distribution:
       - objective(Ï†) = E[rÎ¸(x, y) âˆ’ Î² log(Ï€RL(y|x)/Ï€SFT(y|x))] + Î³ Exâˆ¼Dpretrain log Ï€RL(x)
   - Training details (Appendix C.4):
     - Initialize the RL policy from the SFT model (with 10% pretraining mix in SFT init; Appendix C.3).
     - Train 256k episodes over ~31k unique, filtered prompts; batch size 512; PPO clip 0.2; sampling temperature 1.
     - Value function initialized from the same 6B RM.
     - Example hyperparameters: Î² = 0.02; Î³ tuned (â‰ˆ27.8 worked well and mitigated regressions; Figure 33).

5) Baselines (Section 3.5)
   - `GPTâ€‘3` (no instruction prefix).
   - `GPTâ€‘3 (prompted)`: a handâ€‘crafted instructionâ€‘following prefix found via an internal â€œprefix competition.â€
   - `SFT`: supervised fineâ€‘tuned on demonstrations.
   - `FLAN` and `T0`: 175B GPTâ€‘3 fineâ€‘tuned on public instruction datasets; checkpoints selected by highest RM score (Appendix C.5; Figure 13).

Why these design choices?
- Preferenceâ€‘trained RM gives a taskâ€‘general reward signal aligned with labelersâ€™ judgments (Figure 2; Section 3.1).
- KL penalty avoids pathological overâ€‘optimization of the RM; mixing in pretraining (`PPO-ptx`) retains general NLP capabilities (Figures 28â€“29, 33â€“34).
- Training the RM and value function at 6B improves stability and reduces compute (Appendix C.2, C.4).

## 4. Key Insights and Innovations
1) Broadâ€‘distribution RLHF for instruction following
   - Innovation: Scale RLHF beyond a single task to the diverse, realâ€‘world prompt distribution from API users (Section 3.2; Table 1).
   - Significance: Even a 1.3B `InstructGPT` is preferred to a 175B GPTâ€‘3 on these prompts (Figure 1), demonstrating that alignment beats brute model scaling for usability.

2) Efficient RM training from rankings
   - Innovation: Train the RM using all K ranked completions per prompt in a single batch to avoid overfitting from correlated pairwise comparisons and to save compute (Section 3.5; Equation (1)).
   - Significance: Higher validation accuracy and better use of human data enable reliable PPO training.

3) `PPO-ptx`: Mixing pretraining updates during RL
   - Innovation: Add a pretrainingâ€‘likelihood term during PPO to mitigate regressions (â€œalignment taxâ€) on standard NLP benchmarks (Equation (2); Figures 28â€“29, 33â€“34).
   - Significance: Recovers much of the lost performance (and even surpasses GPTâ€‘3 on HellaSwag in some settings; Section 4.2) while maintaining preference gains.

4) Realâ€‘world evaluation design
   - Innovation: Evaluate on a heldâ€‘out slice of real API prompts with human preferences, Likert quality ratings, and fineâ€‘grained safety/quality metadata (Section 3.6; Table 3).
   - Significance: Shows generalization to heldâ€‘out labelers and highlights concrete behavioral improvements (Figures 3â€“4).

## 5. Experimental Analysis
Evaluation methodology (Section 3.6):
- Two regimes:
  1) Real API prompts (â€œInstructGPT distributionâ€) and prompts crafted for GPTâ€‘3 (â€œGPT distributionâ€); test users held out by organization ID (Sections 3.2, 4.1; Figure 3).
  2) Public NLP benchmarks for safety (toxicity, bias, truthfulness) and capabilities (QA, reading comprehension, summarization, translation; Appendix D; Figures 28â€“29; Table 14).
- Metrics:
  - Human pairwise preference winâ€‘rate vs a 175B SFT baseline (Figures 1, 3).
  - Perâ€‘output Likert quality (1â€“7) and metadata (e.g., instruction following, hallucination, safety flags; Figure 4; Table 3).
  - Automatic task metrics (e.g., F1, accuracy, BLEU, ROUGE; Table 14; Appendix D).
  - TruthfulQA human evaluation (true; true+informative; Figure 6).
  - RealToxicityPrompts via Perspective API and human toxicity/continuity ratings (Figures 7, 39â€“41).
  - Bias via entropy over balanced choices (higher entropy = less bias; Winogender and CrowSâ€‘Pairs; Figure 32; Table 14).

Main quantitative results:
- Preference and quality on real prompts
  - Strong preference for `InstructGPT`:
    - > â€œ175B `InstructGPT` is preferred to 175B GPTâ€‘3 85 Â± 3% of the time, and 71 Â± 4% vs fewâ€‘shot GPTâ€‘3â€ (Section 1; Figure 1).
    - Even 1.3B `InstructGPT` beats 175B GPTâ€‘3 (Figure 1).
    - Results hold on both â€œInstructâ€ and â€œGPTâ€ prompt distributions and for heldâ€‘out labelers (Figure 3).
  - Concrete behavioral improvements (Figure 4):
    - Fewer hallucinations on closedâ€‘domain tasks; better adherence to explicit constraints; higher rate of attempting the correct instruction; more appropriate as a customer assistant.
    - The paper quantifies hallucination decreases by â€œabout halfâ€ (21% vs 41% on closedâ€‘domain tasks; Section 1 bullets).

- Comparison to FLAN/T0 (public instruction datasets)
  - FLAN/T0 improve over default GPTâ€‘3 but underperform SFT and are far behind `InstructGPT` on API prompts:
    - Likert scores show `InstructGPT` highest; FLAN/T0 near â€œGPTâ€‘3 (prompted)â€ (Figure 5).
    - Headâ€‘toâ€‘head: 175B `InstructGPT` wins 78 Â± 4% vs FLAN, 79 Â± 4% vs T0 (Section 4.1).

- Truthfulness (TruthfulQA; Figure 6)
  - PPO models generate more truthful and informative answers; with an explicit â€œhelpful instructionâ€ prompt, RLHF models â€œerr on the side of being truthful and uninformative rather than confidently saying a falsehood,â€ whereas GPTâ€‘3 does not (Section 4.2).
  - Caveat: the 1.3B `PPO-ptx` slightly underperforms GPTâ€‘3 at that size on some TruthfulQA settings (Figure 6).

- Toxicity (RealToxicityPrompts; Figures 7, 39â€“41; Table 14)
  - With a respectful instruction, RLHF models output less toxic text than GPTâ€‘3 (both Perspective API and human ratings; Figure 7).
  - Without the instruction, the advantage largely disappears; with an explicitly biased instruction, RLHF models produce very toxic text (Figure 39).
  - Human study: SFT is least toxic but lowest continuity/preferenceâ€”suggesting overly terse or degenerate responses (Figure 40).

- Bias (Winogender, CrowSâ€‘Pairs; Figure 32; Table 14)
  - No significant reduction in measured social bias. Instructed models can show lower entropy (higher certainty) regardless of stereotype direction, complicating interpretation (Section 4.2).

- Public NLP benchmarks (Figures 28â€“29; 33â€“35; Table 14)
  - Plain PPO suffers an â€œalignment taxâ€ (regressions on SQuADv2, DROP, HellaSwag, WMTâ€™15 Frâ†’En).
  - `PPO-ptx` (Î³ > ~20) largely recovers performance (Figure 33), even surpassing GPTâ€‘3 on HellaSwag in fewâ€‘shot at 175B (Figure 29), while preserving validation reward; increasing only the KL coefficient cannot fully fix regressions (Figure 34).
  - Training too long (512k episodes) can reintroduce regressions (Figure 35).

- Generalization and failure cases
  - Generalization: examples show instruction following in other languages and on code questions, despite such data being rare in fineâ€‘tuning (Figure 8; Section 4.3).
  - Failure cases:
    - Accepts false premises; overly hedges; struggles with prompts containing multiple hard constraints (Figure 9; Section 4.3).

Ablations/robustness:
- Heldâ€‘out labelers: preferences generalize (Figure 3 top); RM predicts heldâ€‘out labeler preferences at 69.6 Â± 0.9% vs 72.4 Â± 0.4% withinâ€‘group (Section 4.1; Appendix E.2).
- Hyperparameter studies: best KL â‰ˆ 0.01â€“0.02 (Figure 36); learningâ€‘rate sensitivity and pretraining mix evaluated (Figure 38; Appendix E.8â€“E.11).

Assessment:
- The experiments are broad, use both human and automatic metrics, and include useful ablations. The strongest claimâ€”human preference dominance on real promptsâ€”is well supported (Figures 1, 3). Safety results are nuanced: improvements depend on giving the model a respectful instruction; bias remains unresolved (Figures 7, 32, 39â€“41). Capability regressions are documented and mitigated via `PPO-ptx` (Figures 28â€“29, 33â€“35).

## 6. Limitations and Trade-offs
- What is being aligned, and to whom? (Section 5.2)
  - Alignment is to the preferences of a specific labeler pool, under instructions authored by the research team, and on prompts coming from OpenAI API users. This is not a universal notion of â€œhuman values.â€
  - Labelers are mostly Englishâ€‘speaking and from the US or Southeast Asia; dataset is >96% English (Appendix A.4; B.3). Preferences and judgments may not represent broader populations or domainâ€‘expert contexts.

- Safety tradeâ€‘offs (Sections 3.4, 5.3â€“5.4)
  - During training, helpfulness had priority over truthfulness/harmlessness; in evaluation, truthfulness/harmlessness were emphasized (Section 3.4; B.2). This can produce models that comply even with harmful instructions (Figure 39), or that hedge too much when labelers reward epistemic humility (Figure 9).

- Residual harms
  - Bias not substantially reduced on Winogender or CrowSâ€‘Pairs (Figure 32); toxicity improvement hinges on the presence of a respectful prompt (Figure 7).

- Alignment tax and mitigation
  - Plain PPO harms some benchmark performance (Figures 28â€“29). `PPO-ptx` mitigates but may not fully eliminate regressions (Section 4.2).

- Data and compute constraints
  - Human data is costly; RM and RL training still require substantial compute (though far less than pretraining; Section 5.1, point 1).
  - RM uses 6B for stability; 175B RM training was unstable (Appendix C.2).

- Edge cases not addressed
  - Little multilingual supervision and no domainâ€‘expert pipelines; models can follow prompts with false premises; complex constraint satisfaction remains fragile (Section 4.3).
  - Refusal behaviors and contextâ€‘sensitive safety policies are not the primary focus here (Section 5.4).

## 7. Implications and Future Directions
- How this changes the landscape (Section 5.1)
  - RLHF at scale provides a practical path to align models with user intent across diverse tasks, delivering large usability gains without scaling parameter count. The alignment tax can be made small with `PPO-ptx`, making adoption more attractive.

- What it enables
  - General instructionâ€‘following assistants usable with minimal prompt engineering; safer defaults when paired with appropriate system prompts; potential to condition on different communitiesâ€™ preferences (Section 5.2).

- Promising research directions (Sections 5.4, 5.2)
  - Safety:
    - Train refusal behaviors and contextâ€‘dependent safety policies; adversarial data collection to fix failure modes (e.g., false premises, jailbreaks).
    - Combine RLHF with pretraining data filtration or truthfulness tools (e.g., webâ€‘augmented QA).
  - Reward modeling:
    - Use richer feedback (edits, critiques), better interfaces, and methods beyond pairwise comparison; study how instructions to labelers shape collected signals.
  - Preference pluralism:
    - Condition models on different groupsâ€™ preferences; develop accountable processes for whose values are encoded and how they are represented (Section 5.2).
  - Technique improvements:
    - Explore alternative policyâ€‘optimization methods (expert iteration, constrained optimization), better KL regularization, and more principled pretrainingâ€‘mix strategies.
  - Evaluation:
    - Broaden multilingual and domainâ€‘expert tests; develop measures for honesty beyond truthfulness and calibrate hedging behavior.

> Core takeaway: Fineâ€‘tuning with human feedback substantially improves instruction following, truthfulness, and perceived helpfulnessâ€”even allowing a 1.3B model to beat a 175B one in human preferenceâ€”while a simple training modification (`PPO-ptx`) keeps most general NLP capabilities intact (Figures 1, 3, 4, 6, 28â€“29, 33).

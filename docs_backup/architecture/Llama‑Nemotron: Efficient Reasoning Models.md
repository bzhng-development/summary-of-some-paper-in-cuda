# Llamaâ€‘Nemotron: Efficient Reasoning Models

**ArXiv:** [2505.00949](https://arxiv.org/abs/2505.00949)
**Authors:** Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran Elâ€‘Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, YianÂ Zhang, TugrulÂ Konuk, GeraldÂ Shen, AmeyaÂ SunilÂ Mahabaleshwarkar, BilalÂ Kartal, YoshiÂ Suhara, OlivierÂ Delalleau, ZijiaÂ Chen, ZhilinÂ Wang, DavidÂ Mosallanezhad, AdiÂ Renduchintala, HaifengÂ Qian, DimaÂ Rekesh, FeiÂ Jia, SomshubraÂ Majumdar, VahidÂ Noroozi, WasiÂ UddinÂ Ahmad, SeanÂ Narenthiran, AleksanderÂ Ficek, MehrzadÂ Samadi, JocelynÂ Huang, SiddharthaÂ Jain, IgorÂ Gitman, IvanÂ Moshkov, WeiÂ Du, ShubhamÂ Toshniwal, GeorgeÂ Armstrong, BranislavÂ Kisacanin, MatveiÂ Novikov, DariaÂ Gitman, EvelinaÂ Bakhturina, JaneÂ PolakÂ Scowcroft, JohnÂ Kamalu, DanÂ Su, KezhiÂ Kong, MarkusÂ Kliegl, RabeehÂ Karimi, YingÂ Lin, SanjeevÂ Satheesh, JupinderÂ Parmar, PritamÂ Gundecha, BrandonÂ Norick, JosephÂ Jennings, ShrimaiÂ Prabhumoye, SyedaÂ NahidaÂ Akter, MostofaÂ Patwary, AbhinavÂ Khattar, DeepakÂ Narayanan, RogerÂ Waleffe, JimmyÂ Zhang, Borâ€‘YiingÂ Su, GuyueÂ Huang, TerryÂ Kong, ParthÂ Chadha, SahilÂ Jain, ChristineÂ Harvey, EladÂ Segal, JiningÂ Huang, SergeyÂ Kashirsky, RobertÂ McQueen, IzzyÂ Putterman, GeorgeÂ Lam, ArunÂ Venkatesan, SherryÂ Wu, VinhÂ Nguyen, ManojÂ Kilaru, AndrewÂ Wang, AnnaÂ Warno, AbhilashÂ Somasamudramath, SandipÂ Bhaskar, MakaÂ Dong, NaveÂ Assaf, ShaharÂ Mor, OmerÂ UllmanÂ Argov, ScotÂ Junkin, OleksandrÂ Romanenko, PedroÂ Larroy, MonikaÂ Katariya, MarcoÂ Rovinelli, VijiÂ Balas, NicholasÂ Edelman, AnahitaÂ Bhiwandiwalla, MuthuÂ Subramaniam, SmitaÂ Ithape, KarthikÂ Ramamoorthy, YutingÂ Wu, SugunaÂ VarshiniÂ Velury, OmriÂ Almog, JoyjitÂ Daw, DenysÂ Fridman, ErickÂ Galinkin, MichaelÂ Evans, KatherineÂ Luna, LeonÂ Derczynski, NikkiÂ Pope, EileenÂ Long, SethÂ Schneider, GuillermoÂ Siman, TomaszÂ Grzegorzek, PabloÂ Ribalta, MonikaÂ Katariya, JoeyÂ Conway, TrishaÂ Saar, AnnÂ Guan, KrzysztofÂ Pawelec, ShyamalaÂ Prayaga, OleksiiÂ Kuchaiev, BorisÂ Ginsburg, OluwatobiÂ Olabiyi, KariÂ Briski, JonathanÂ Cohen, BryanÂ Catanzaro, JonahÂ Alben, YonatanÂ Geifman, EricÂ Chung
**Institutions:** 

## ğŸ¯ Pitch

Introducing the Llama-Nemotron series, this research establishes novel open-weight reasoning models with a unique toggle for user-controlled reasoning depth, maximizing inference efficiency and reasoning quality. These models, employing hardware-aware architecture search and vertical compression, significantly enhance real-world deployability and throughput, crucial for handling dynamic workloads in high-concurrency environments, thus paving the way for more accessible and efficient reasoning-driven applications.

---

## 1. Executive Summary
This paper introduces the Llamaâ€‘Nemotron family (`LNâ€‘Nano` 8B, `LNâ€‘Super` 49B, `LNâ€‘Ultra` 253B), openâ€‘weight reasoning models that pair stateâ€‘ofâ€‘theâ€‘art reasoning quality with high inference efficiency and a userâ€‘controlled reasoning toggle (`"detailed thinking on/off"`). The technical core is a hardwareâ€‘aware neural architecture search plus vertical compression that make very large models fast to serve, followed by supervised reasoning fineâ€‘tuning and largeâ€‘scale reinforcement learning (RL) that lets the top model surpass its teacher on scientific reasoning benchmarks.

## 2. Context and Motivation
- Problem addressed
  - Reasoningâ€‘optimized LLMs (e.g., OpenAI o1, DeepSeekâ€‘R1) achieve strong results by generating long chains of thought, but they are expensive to serve and often require specific, highâ€‘end hardware; they also lack simple user control over when to reason versus answer tersely. Section 1 frames inference efficiency as a new bottleneck for â€œoverall model intelligenceâ€ because modern systems scale at inference time, not only at training time.
- Why it matters
  - Real deployments need to handle many concurrent users, tools, and agentic pipelines. Without high throughput and memory efficiency, multiâ€‘step reasoning becomes impractical. Moreover, not every query benefits from long reasoning; wasted reasoning costs money and time and can reduce usability (Section 1).
- Where prior approaches fall short
  - Stateâ€‘ofâ€‘theâ€‘art open reasoning models like DeepSeekâ€‘R1 run best on 8Ã—H200 and do not give the end user a builtâ€‘in way to switch between terse and chainâ€‘ofâ€‘thought modes. Traditional architecture compression is usually uniform and can degrade performance; prior NAS approaches rarely optimize for realâ€‘world deployment constraints with perâ€‘layer heterogeneity (Section 2).
- Positioning
  - This work combines: (i) deploymentâ€‘constrained NAS (â€œPuzzleâ€) that creates a heterogeneous transformer with blockâ€‘level attention removals and FFN compression, (ii) a new vertical compression (â€œFFN Fusionâ€), and (iii) a twoâ€‘stage postâ€‘training program (reasoning SFT + largeâ€‘scale RL). It releases models, data, and code under permissive licenses (Abstract; release bullets).

## 3. Technical Approach
Stepâ€‘byâ€‘step pipeline across five stages (Sections 2â€“6):

1) Making the base models inferenceâ€‘efficient with Puzzle NAS (Section 2; Figure 3)
- What Puzzle is
  - `Puzzle` is a neural architecture search framework that builds a â€œlibraryâ€ of alternative transformer blocks and then assembles one variant per layer subject to deployment constraints.
  - Each candidate block is trained locally to mimic its parent block (blockâ€‘wise distillation) and is profiled for quality vs. cost.
- What block variants are used here
  - Attention removal in selected layers to reduce both compute and KVâ€‘cache memory.
  - Variable FFN intermediate sizes (e.g., 87%, 75%, 50%, down to 10%) to trade accuracy for speed/memory (Section 2; bullet list).
- How the final architecture is chosen
  - A mixedâ€‘integer programming (MIP) solver selects one variant per layer to optimize quality under constraints such as throughput, latency, memory, or batchÃ—sequence (â€œcached tokensâ€) (Figure 3 and Section 2).
- Deployment targets and measured efficiency
  - `LNâ€‘Super` (49B): optimized for a single H100 at tensor parallel 1 (TP1). It achieves â€œ5Ã— throughput speedup over Llamaâ€‘3.3â€‘70Bâ€‘Instruct at batch 256 and TP1,â€ and still â‰¥2.17Ã— over Llamaâ€‘3.3â€‘70B run at its optimal TP4 (Section 2.1).
  - `LNâ€‘Ultra` (253B): optimized for one 8Ã—H100 node; NAS enforces at least 1.5Ã— latency reduction vs. Llamaâ€‘3.1â€‘405Bâ€‘Instruct, and the final model realizes 1.71Ã— after FFN Fusion (Section 2.1).

2) Vertical compression with FFN Fusion (Section 2; â€œVertical Compression with FFN Fusionâ€)
- Idea
  - After some attention layers are removed by Puzzle, multiple FFN blocks can become consecutive. `FFN Fusion` replaces sequences of consecutive FFNs with fewer, wider FFNs that can be executed in parallel. This reduces the number of sequential steps (model depth along the compute graph), which lowers latency without reducing expressivity.
- Why it matters
  - Lower sequential depth improves utilization, especially in multiâ€‘GPU pipelines where interâ€‘layer communication is costly.

3) Recovery training: knowledge distillation and continued pretraining (Section 2.2; Table 1)
- Purpose
  - NAS changes layer internals; this stage recovers quality and improves interâ€‘block compatibility.
- Details
  - `LNâ€‘Super`: 40B tokens of distillation on the Distillation Mix dataset from Bercovich et al. (2024).
  - `LNâ€‘Ultra`: 65B tokens of distillation + 88B tokens of continued pretraining (CPT) on Nemotronâ€‘H Phase 4 (NVIDIA et al., 2025).
- Effect before SFT/RL (Table 1)
  - `LNâ€‘Ultraâ€‘CPT` exceeds Llamaâ€‘3.1â€‘405Bâ€‘Instruct on MATH500 (80.4 vs. 69.6) and RULERâ€‘128K (83.2 vs. 73.7) and roughly ties on MMLU (88.1 vs. 88.6), showing that aggressive architecture changes can be reconciled with strong base quality via short CPT.

4) Reasoningâ€‘focused supervised fineâ€‘tuning (SFT) with a reasoning toggle (Sections 3â€“4)
- Reasoning toggle
  - A simple system instruction `â€œdetailed thinking on/offâ€` teaches the model to emit chainâ€‘ofâ€‘thought between `<think>...</think>` tags or to answer tersely. Paired data is created so every prompt has both a reasoning and a nonâ€‘reasoning response (Section 3.2).
- Data construction (Section 3; Table 2)
  - Math: Problems from AoPS; remove proofs/MCQ/binary/invalid; generate many candidate solutions (DeepSeekâ€‘R1 for reasoning, Qwen2.5â€‘Mathâ€‘7B for nonâ€‘reasoning), filter by answer match using Qwen2.5â€‘32B judge; perform benchmark decontamination (Section 3.1.1).
  - Code: 28,904 competitive programming problems from TACO, APPS, CodeContests, CodeForces; decontamination and deduplication; multiâ€‘sample solutions with explicit reasoning in `<think>`; syntax checks via Treeâ€‘Sitter; ~488K Python samples; scaling experiments show more, harder data keeps improving results (Section 3.1.2).
  - Science and general: synthetic MCQs and openâ€‘ended prompts with decontamination; responses by strong teachers (DeepSeekâ€‘R1) plus rejection sampling with `Llamaâ€‘3.1â€‘Nemotronâ€‘70Bâ€‘Reward`; also a Feedbackâ€‘Edit inferenceâ€‘time scaling pipeline for highâ€‘quality general responses (Section 3.1.3â€“3.1.4 and 3.2.1).
  - Overall size: 33,011,757 samples; 66.8% math, 30.6% code, 2.1% science, small chat/instruction/safety (Table 2).
- SFT training recipes (Section 4.2)
  - `LNâ€‘Nano`: three stages; start only on reasoning to avoid degenerate repetition; later mix in nonâ€‘reasoning; final blend adds chat/instruction/tool use.
  - `LNâ€‘Super`: one epoch over the full SFT set; sequence length 16k; fixed LR 5eâ€‘6 (smaller runs suggest up to 3â€“4 epochs and higher LR can help).
  - `LNâ€‘Ultra`: sequence packing to 24k effective length; larger LR helps but caused instabilities; they use linear warmup to 1eâ€‘5 then cosine to 1eâ€‘6; training suffered gradient explosions after the first epoch and required optimizer reâ€‘init to continue.

5) Largeâ€‘scale RL to go beyond the teacher (Sections 5â€“6)
- Why RL is needed
  - Distillation is bounded by the teacherâ€™s quality; to surpass DeepSeekâ€‘R1, `LNâ€‘Ultra` uses RL (Section 5).
- Algorithm and rollout setup (Section 5.1; Figure 5)
  - GRPO (`Group Relative Policy Optimization`): a policyâ€‘gradient method using groupwise baselines.
  - Rollout prompt size 72, 16 samples per prompt at temperature=1, top_p=1; global batch 576; 2 gradient updates per rollout. Training consumes ~140k H100 hours.
- Rewards (Section 5.1)
  - Accuracy reward: a served `Llamaâ€‘3.3â€‘70Bâ€‘Instruct` judges whether the prediction matches the ground truth answer (numbers, sentences, or paragraphs).
  - Format reward: enforces `<think>` tags when reasoning is on and their absence when off.
- Data difficulty and curriculum (Section 5.1; Figure 6)
  - Preâ€‘filter prompts using `LNâ€‘Super` passâ€‘rates; drop those with passâ€‘rate â‰¥0.75.
  - Curriculum: batches gradually shift from easy (high passâ€‘rate) to hard (low passâ€‘rate) using a Gaussian target distribution per batch.
- Infrastructure to make RL feasible (Section 5.2)
  - Coâ€‘locate generation (vLLM) and training (Megatronâ€‘LM) on the same GPUs; maintain separate weight copies and synchronize each step.
  - Parallelism: tensor=8 with sequence parallel, context=2, pipeline=18, data=2 for training; tensor=8, data=72 for generation across 72Ã—(8Ã—H100) nodes.
  - FP8 online generation path in vLLM with custom loaders and metaâ€‘initialization to avoid materializing BF16 engines; delivers â€œ32 tokens/s/GPU/prompt,â€ a â€œ1.8Ã— generation speedupâ€ and enables cudagraph thanks to lower memory (Section 5.2.3).
  - Careful memory profiling for GPU/CPU and `/dev/shm` to avoid OOMs; identity layers inserted to balance heterogeneous pipelines (Section 5.2.2).

6) Final alignment via preference optimization (Section 6)
- Instruction following: short RL using `RLOO` (Leaveâ€‘Oneâ€‘Out variant for RL from feedback) on synthetic multiâ€‘constraint prompts, which boosts IFEval and also helps reasoning benchmarks (Section 6.1).
- RLHF with `RPO` (Rewardâ€‘aware Preference Optimization): iterative online RPO against `Llamaâ€‘3.1â€‘Nemotronâ€‘70Bâ€‘Reward` on HelpSteer2. For `LNâ€‘Super`, two iterations raise Arenaâ€‘Hard from 69.1 to 88.1 and also improve most other benchmarks (Section 6.2). `LNâ€‘Ultra` uses GRPO for this stage with 8 samples per prompt for 30 steps.

Definitions of select terms used above
- `KVâ€‘cache`: the saved key/value tensors used by attention to avoid recomputing past context.
- `Sequence packing`: packing multiple shorter training samples into contiguous segments of a long sequence to improve hardware utilization.
- `Context parallel`/`pipeline parallel`/`tensor parallel`: ways to split model computation across GPUs along sequence length, layers, and weight tensors, respectively.
- `FP8`: 8â€‘bit floating point precision; faster and lower memory than BF16/FP16 for GEMMs.

## 4. Key Insights and Innovations
- Hardwareâ€‘constrained, heterogeneous NAS for LLM inference
  - Whatâ€™s new: Instead of uniformly shrinking the whole model, Puzzle builds a perâ€‘layer menu with options like â€œremove attentionâ€ or â€œuse a smaller FFN,â€ then solves a constrained selection problem with a MIP solver (Figure 3). This lets the final architecture sit precisely on a desired throughput/latency/memory point (Section 2).
  - Why it matters: Concrete efficiency wins under real serving constraints. `LNâ€‘Super` yields up to 5Ã— throughput on a single H100 vs. Llamaâ€‘3.3â€‘70Bâ€‘Instruct at TP1 (Section 2.1).
- FFN Fusion: vertical compression that speeds multiâ€‘GPU pipelines
  - Whatâ€™s new: Detect runs of FFNâ€‘only layers that appear after some attention removal and fuse them into fewer, wider FFNs that execute in parallel (Section 2; â€œVertical Compression with FFN Fusionâ€).
  - Why it matters: Lowers sequential critical path; the `LNâ€‘Ultra` model achieves a 1.71Ã— latency improvement vs. Llamaâ€‘3.1â€‘405Bâ€‘Instruct after applying Fusion (Section 2.1).
- A simple, effective reasoning toggle with formatâ€‘aware rewards
  - Whatâ€™s new: The same model can switch between terse and chainâ€‘ofâ€‘thought styles via a 1â€‘line system prompt. Training uses paired data and a format reward to make the control reliable (Sections 3.2 and 5.1).
  - Why it matters: Users spend compute only when they want reasoning; deployment teams can mix workloads without separate models.
- RL at scale with FP8 online generation to surpass the teacher
  - Whatâ€™s new: A GRPO training pipeline that coâ€‘locates vLLM generation and Megatron training, adds an FP8 decoding path with custom weight loaders and cudagraph support, and uses passâ€‘rateâ€‘based curriculum (Sections 5.1â€“5.2).
  - Why it matters: Enables `LNâ€‘Ultra` to exceed DeepSeekâ€‘R1 on GPQAâ€‘Diamond while running on 8Ã—H100 instead of 8Ã—H200 (Figure 4; Table 5).

## 5. Experimental Analysis
- Evaluation setup (Section 7.1)
  - Benchmarks
    - Reasoning: `AIME24`, `AIME25` (competition math), `GPQAâ€‘Diamond` (graduateâ€‘level science MCQ), `MATH500` (stepâ€‘byâ€‘step math), `LiveCodeBench` (coding).
    - Nonâ€‘reasoning: `IFEval` (strict instruction following), `BFCL V2 Live` (tool/function calling), `Arenaâ€‘Hard` (pairwise conversational preference).
  - Decoding and context
    - All results use 32k context at eval time (even though SFT used 16k/24k), because longer context avoids truncating long reasoning (Section 7.1).
    - Reasoningâ€‘on uses temperature 0.6, topâ€‘p 0.95; reasoningâ€‘off is greedy; up to 16 completions; report pass@1 (Section 7.1). AIME has high variance; numbers can vary with sampling.
  - Decontamination and data quality controls are described in Section 3 for math, code, and science.
- Main results
  - Topâ€‘line accuracy and efficiency
    - Figure 4 plots GPQAâ€‘Diamond accuracy vs. throughput (tokens/s) in two concurrency settings with FP8 serving; `LNâ€‘Ultra` dominates both DeepSeekâ€‘R1 and Llamaâ€‘3.1â€‘405B on the Pareto curve. Quoted points: improvements of â€œ1.9Ã—â€ and â€œ4Ã—â€ throughput depending on the setting.
  - `LNâ€‘Ultra` vs open SOTA (Table 5, reasoningâ€‘on)
    - GPQAâ€‘Diamond: 76.0 vs DeepSeekâ€‘R1 71.5; vs Llamaâ€‘4 Maverick 69.8; vs Llamaâ€‘3.1â€‘405B 43.4.
    - AIME24: 80.8 vs DeepSeekâ€‘R1 79.8.
    - AIME25: 72.5 vs DeepSeekâ€‘R1 70.0.
    - MATH500: 97.0 vs DeepSeekâ€‘R1 97.3 (essentially tied).
    - LiveCodeBench (2408â€“2502): 66.3 vs DeepSeekâ€‘R1 65.9.
    - IFEval: 88.9 vs DeepSeekâ€‘R1 88.8 (parity).
    - Arenaâ€‘Hard: 87.0 (DeepSeekâ€‘R1 at 92.0 is higher here).
  - Effect of RL (Table 5)
    - `LNâ€‘Ultraâ€‘SFT` scores 66.4 on GPQAâ€‘D; RL lifts it to 76.0, crossing the teacherâ€™s 71.5. This directly supports the claim that RL is necessary to surpass the teacher on scientific reasoning.
  - `LNâ€‘Super` (49B) tradeâ€‘offs (Table 4)
    - Reasoningâ€‘on GPQAâ€‘D: 66.7 vs DeepSeekâ€‘R1â€‘Distilledâ€‘Llamaâ€‘70B at 65.2; AIME25: 60.0 vs 55.0; MATH500: 96.6 vs 94.5.
    - Instruction following and chat: After a dedicated IFEval RL run and subsequent preference optimization, IFEval reaches 89.2 (on/off similar); Arenaâ€‘Hard hits 88.3, beating several larger proprietary and open models listed in Section 6.2.
    - Coding: LCB (2408â€“2502) 45.5; the paper attributes underperformance to training on an earlier dataset version and plans a refresh (Section 7.3).
  - `LNâ€‘Nano` (8B) (Table 3)
    - Outperforms comparable 7â€“8B baselines on many reasoning tasks, e.g., MATH500 95.4 vs Qwenâ€‘7B 92.8, LiveCodeBench 46.6 vs Llamaâ€‘3.1â€‘8Bâ€‘Instruct 37.6; function calling (BFCL V2 Live) ~64, far ahead of Qwenâ€‘7Bâ€™s 39.2.
- Additional evaluations: LLMâ€‘asâ€‘aâ€‘judge (Table 6)
  - On JudgeBench, `LNâ€‘Ultra` overall 79.14 surpasses DeepSeekâ€‘R1 73.14 and trails only `o3â€‘mini(high)` 80.86. `LNâ€‘Super` 69.71 exceeds `o1â€‘mini` 65.71. This suggests generalization to judgment tasks outside the training targets.
- Ablations and diagnostics
  - Curriculum helps: Figure 6 shows curriculumâ€‘driven batching yields higher GPQAâ€‘D than random sampling across training steps.
  - Training stability: Section 4.2 notes gradient explosions for `LNâ€‘Ultra` during SFT; resuming with reinitialized optimizer states was necessary.
  - Data scaling for code: Section 3.1.2 reports continued benefits up to ~736k samples and especially from focusing first on harder CodeContests problems.
- Do the experiments support the claims?
  - Yes on three axes:
    - Accuracy: Tables 4â€“5 show strong or SOTA openâ€‘model results in reasoning, with RL pushing `LNâ€‘Ultra` beyond DeepSeekâ€‘R1 on GPQA.
    - Efficiency: Section 2.1 and Figure 4 quantify large throughput/latency gains under realistic serving constraints on H100s.
    - Control: Tables split results by reasoning on/off, and Section 5.1 includes a format reward that ensures cleanly separated modes.

## 6. Limitations and Trade-offs
- Heavy reliance on synthetic/teacherâ€‘generated data
  - Many SFT samples are distilled from strong closed/open models (DeepSeekâ€‘R1, Qwen2.5). Although decontamination is performed (Section 3), this can import biases or errors from teachers. The accuracy reward during RL is judged by a `Llamaâ€‘3.3â€‘70Bâ€‘Instruct` model rather than solely by programmatic verification, which can be imperfect for openâ€‘ended answers (Section 5.1).
- RL only for the largest model
  - The paper finds smaller models benefit less from RL and therefore applies reasoning RL only to `LNâ€‘Ultra` (Section 5). This leaves open whether improved or cheaper RL variants could help `LNâ€‘Super` or `LNâ€‘Nano`.
- Compute and system complexity
  - The reasoning RL run consumed about 140k H100 hours (Section 5.1). The heterogeneous architecture, identity layers for balancing, and FP8 generation path add engineering complexity (Section 5.2).
- Hardwareâ€‘specific gains
  - Efficiency results are measured with FP8 serving on NVIDIA H100 nodes and specific parallelism settings (Figure 4; Section 5.2.3). Gains may not transfer directly to other accelerators or software stacks.
- Tradeâ€‘offs between skills
  - Section 7.3 reports a tension between instruction following (IFEval) and conversational preference (Arenaâ€‘Hard). Optimizing one can degrade the other; model merging was required to find a Pareto point for `LNâ€‘Super`.
- Coding lag for `LNâ€‘Super`
  - LiveCodeBench performance trails some contemporaries due to training on an earlier dataset version (Section 7.3), highlighting sensitivity to upâ€‘toâ€‘date code data.
- Mode control via prompting
  - The reasoning toggle depends on a system prompt string. Although a format reward reinforces behavior, misâ€‘prompting or adversarial inputs could still elicit unintended reasoning traces; the paper does not present a robustness audit of the toggle.

## 7. Implications and Future Directions
- How this changes the landscape
  - It demonstrates that openâ€‘weight reasoning models can be both fast and strong, with a simple userâ€‘visible switch for reasoning style. By releasing models, postâ€‘training data, and training code (Abstract bullets), it lowers the barrier for research into efficiencyâ€‘aware reasoning systems and for enterprises that need commercial terms.
- Followâ€‘up research enabled
  - Extending GRPOâ€‘based reasoning RL to smaller models or multiâ€‘modal models; exploring cheaper verifiable rewards (e.g., programmatic checkers beyond science/math); improving reward models to reduce judge bias in openâ€‘ended tasks; principled scheduling between SFT and RL checkpoints to maximize RL success (Section 7.4 hints earlier SFT checkpoints may be better RL initializations).
  - Automated or learned â€œreasoning policyâ€: rather than a manual `on/off` prompt, the model could decide when depth is warranted based on budget, latency targets, or uncertainty.
  - NAS beyond attention/FFN: integrate groupedâ€‘query attention and linear attention blocks more aggressively, or search over routing and mixtureâ€‘ofâ€‘experts under deployment constraints (Section 2 notes Puzzle supports additional operations not used here).
  - Broader efficiency features: generalize the FP8 online generation path to other inference engines and accelerators; study accuracyâ€‘latency tradeâ€‘offs of FP8 across domains.
- Practical applications
  - Production assistants that keep most turns terse but switch to chainâ€‘ofâ€‘thought when asked to justify or solve hard problems; highâ€‘concurrency agentic systems that need predictable latency and memory; enterprise functionâ€‘calling and workflow orchestration (`BFCL V2 Live` scores in Tables 3â€“5); education/tutoring where stepâ€‘byâ€‘step thinking is valuable but controllable.

> Representative headline result: â€œLNâ€‘Ultra ... 76.0% on GPQAâ€‘Diamond while offering higher throughput than DeepSeekâ€‘R1 and fitting on a single 8Ã—H100 nodeâ€ (Figure 4; Table 5).

Overall, the paperâ€™s main technical message is that deploymentâ€‘aware architectural search plus targeted vertical compression, followed by carefully staged reasoning SFT and scalable RL with efficient FP8 generation, yields open models that are both strong reasoners and practical to serveâ€”while giving users explicit control over when to spend the extra compute on long chains of thought.

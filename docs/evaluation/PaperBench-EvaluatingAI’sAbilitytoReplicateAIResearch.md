# PaperBench: Evaluating AIâ€™s Ability to Replicate AI Research

**ArXiv:** [2504.01848](https://arxiv.org/abs/2504.01848)

## ðŸŽ¯ Pitch

PaperBench introduces a rigorous new benchmark that tests whether AI agents can autonomously replicate cutting-edge machine learning researchâ€”from reading the paper and building a codebase to running experiments and reproducing results, entirely from scratch. By formalizing replication as a hierarchical, weighted rubric and employing an LLM-based judge, PaperBench exposes the current limitations of even the best frontier models, highlighting a crucial gap between human and AI research capabilities and setting a new standard for assessing AI autonomy in real-world scientific R&D.

---

## 1. Executive Summary (2â€“3 sentences)
PaperBench is a benchmark for testing whether AI agents can replicate modern machineâ€‘learning (ML) research papers from scratch: read a paper, build a working codebase, run the experiments, and reproduce the reported results. It formalizes replication as a weighted, hierarchical rubric and uses a validated LLMâ€‘based judge to grade results, revealing that current frontier agents achieve only partial success (best average score 21.0% with Claude 3.5 Sonnet; Table 4), well below a strong human baseline (Figure 3).

## 2. Context and Motivation
- Problem addressed
  - There is no rigorous, scalable way to measure whether an autonomous AI agent can carry out endâ€‘toâ€‘end ML research replication: understanding a paper, implementing the methods, running experiments, and verifying results without relying on the original authorsâ€™ code (Section 2; Section 2.5, Rule 1).
- Why this matters
  - Practical impact: Agents that can replicate research could accelerate scientific progress, but also raise safety questions about increasingly autonomous ML R&D (Introduction).
  - Evaluation gap: Traditional benchmarks emphasize smaller tasks (e.g., code writing, Kaggle competitions) or rely on the existence of an official codebase, which sidestep the hardest parts of real ML research: ambiguous specifications, complex pipelines, and longâ€‘horizon execution (Related Work, Section 6).
- Prior approaches and their gaps
  - Using authorsâ€™ repositories to reproduce results (e.g., COREâ€‘Bench) tests debugging and environment setup but not full reâ€‘implementation from scratch (Section 2, citing Siegel et al., 2024).
  - Kaggleâ€‘style ML agent benchmarks (MLEâ€‘bench, MLAgentBench, DSBench) focus on narrower or dated tasks and provide clear scoring functions, avoiding the ambiguity of modern research reproduction (Section 6).
  - REâ€‘Bench offers challenging engineering tasks but many include a builtâ€‘in scoring function; PaperBench must evaluate broad, openâ€‘ended replication where no single score can capture correctness (Section 6).
- How this work positions itself
  - PaperBench evaluates 20 ICML 2024 Spotlight/Oral papers spanning 12 topics (Table 2) using authorâ€‘approved, hierarchical rubrics (Section 3.1). Replication is judged by executing a submissionâ€™s `reproduce.sh` in a clean environment (Section 2.2) and scoring granular outcomes, with results aggregated into a single Replication Score (Section 2.3; Figure 2).

## 3. Technical Approach
This section explains the benchmark as a pipeline: task specification â†’ reproduction execution â†’ rubricâ€‘based grading with an LLM judge.

- Task setup (Section 2.1; Figure 1)
  - Input to the agent (the â€œcandidateâ€):
    - The paper (PDF and Markdown) and a clarifying `addendum` (Section 3.2).
    - The rubric exists but is hidden from the agent during attempts to prevent overfitting (Section 2.1).
  - Required output (â€œsubmissionâ€):
    - A repository containing all code to reproduce the paperâ€™s empirical contributions and an entrypoint script `reproduce.sh` at the repo root (Section 2.1).
    - The script should orchestrate the entire replication, producing outputs (tables/plots) and a `reproduce.log`.

- Reproduction phase: enforcing clean execution (Section 2.2)
  - The submission is copied to a fresh Ubuntu 24.04 VM with a single NVIDIA A10 GPU and executed by running `reproduce.sh`.
  - Outputs produced during this run (results files and `reproduce.log`) are treated as the only evidence of reproduction.
  - Rationale: This separation prevents agents from â€œbaking inâ€ results at task time and increases credibility of claimed reproductions.

- Rubric design and scoring (Sections 2.3â€“2.4; Figure 2; Table 1; Section 3.1)
  - Hierarchical rubric
    - A rubric is a tree that decomposes â€œreplicate the paperâ€™s main contributionsâ€ into increasingly specific requirements.
    - The leaves (â€œleaf nodesâ€) are binary criteriaâ€”each is marked pass/fail based on evidence.
    - Every node has an importance weight; parent scores are the weighted average of children (Figure 2).
    - The final Replication Score is the root score (a weighted proportion of satisfied requirements).
  - Requirement types (Table 1; Section 2.4)
    - `Code Development`: Is the code for a specific component correctly implemented? Evidence: source code and docs.
    - `Execution`: Did running `reproduce.sh` actually execute the relevant pipeline step? Evidence: code, `reproduce.sh`, and `reproduce.log`.
    - `Result Match`: Do the reproduced outputs match the reported findings (within allowed tolerance)? Evidence: `reproduce.sh`, `reproduce.log`, and files created during reproduction.
  - Why three types?
    - A pure â€œresultsâ€‘onlyâ€ rubric would miss partial progress; adding `Execution` and `Code Development` credits incremental steps (Section 2.4).
    - A pure â€œcodeâ€‘onlyâ€ rubric is insufficient because code correctness is hard to establish without running it (Section 2.4).

- Rules to ensure fair, fromâ€‘scratch replication (Section 2.5)
  - Web browsing is allowed, but any blacklisted sourcesâ€”especially the original authorsâ€™ codeâ€”are prohibited (Section 2.5, Rule 1).
  - A simple monitor flags blacklisted URLs in agent logs; confirmed violations are disqualified (10 of 646 runs; Section 2.5).

- LLMâ€‘based grading (â€œSimpleJudgeâ€) and judge validation (â€œJudgeEvalâ€) (Section 4; Table 3; Appendix D)
  - How the judge works (Section 4.1; Appendix D)
    - For each leaf criterion, the judge receives: the paper, the rubric, the criterion, and a filtered view of the submission (topâ€‘k most relevant files selected via a fileâ€‘ranking prompt; Figure 7).
    - The judge inspects the relevant artifacts depending on criterion type (Table 1) and returns 0/1 with rationale (Figures 8â€“9).
  - Backend model and cost
    - Default judge: `o3-mini-2025-01-31` with high reasoning effort (Section 4.1).
    - Average cost â‰ˆ $66 per paper; far cheaper than expert human grading (Figure 5; Section 4.1).
  - Validating the judge with `JudgeEval` (Section 4.2)
    - A set of humanâ€‘graded submissions across 5 papers is used as ground truth.
    - Judge performance: F1 = 0.83 for `o3-mini` at â‰ˆ $66/paper; `o1` is similar (0.84) but â‰ˆ $830/paper (Table 3).

- Dataset and rubric creation (Sections 3, 3.1â€“3.2; Table 2; Table 7)
  - Papers: 20 ICML 2024 Spotlight/Oral papers across 12 topics (Table 2).
  - Scale: 8,316 leaf nodes across papers; rubrics coâ€‘developed with a paper author for accuracy, with node weights reflecting importance (Section 3.1; Table 7).
  - Addendums clarify underspecified details; some judgeâ€‘only addendums assist grading (Section 3.2).

- Agent scaffolds and execution environment (Sections 5.1, 5.3; Appendix F)
  - Environment: Ubuntu 24.04 Docker container, single A10 GPU, internet, and API keys for needed services (Section 5.1).
  - `BasicAgent`: a ReActâ€‘style loop with tools (bash, Python executor, web browser, paginated file reader); can choose to end early (Section 5.1; Appendix F.1).
  - `IterativeAgent`: forces fullâ€‘time work by removing the â€œend taskâ€ tool and prompting the model to take one small step at a time (Section 5.3; Appendix F.2).

- Accessibility variant (Section 2.6)
  - `PaperBench Codeâ€‘Dev`: grades only `Code Development` nodes, skipping the reproduction run; cuts grading cost by ~85% and removes the need for GPUs.

Analogy: Think of PaperBench like a driving test for research agents. The paper is the map, the rubric is the checklist of maneuvers, the clean VM is the empty test track, `reproduce.sh` is the route the agent must actually drive, and the judge is the examiner who evaluates each maneuver and the overall drive.

## 4. Key Insights and Innovations
- Hierarchical, authorâ€‘approved rubrics with three complementary requirement types (Sections 2.3â€“2.4; 3.1)
  - Whatâ€™s new: Granular, weighted trees that separate â€œhaving correct code,â€ â€œactually running it,â€ and â€œmatching results.â€
  - Why it matters: Supports partial credit and fineâ€‘grained diagnosis, reflecting the true complexity of research replication (Figure 2; Table 1).

- Separation of taskâ€‘time work from reproductionâ€‘time verification (Section 2.2)
  - Whatâ€™s new: Results only count if the submissionâ€™s `reproduce.sh` produces them in a fresh environment.
  - Why it matters: Reduces the risk of hardâ€‘coded or nonâ€‘reproducible outputs and establishes a credible replication standard.

- A scalable, validated LLM judge with relevanceâ€‘guided context and a groundâ€‘truth benchmark (`JudgeEval`) (Sections 4.1â€“4.2; Figures 7â€“9; Table 3)
  - Whatâ€™s new: Practical automated grading with demonstrated F1â‰ˆ0.83 at low cost; context management via file ranking; rubricâ€‘aware prompts.
  - Why it matters: Makes largeâ€‘scale evaluation feasible; enables regular, repeatable benchmarking as models improve.

- A twoâ€‘tier evaluation strategy for accessibility (`PaperBench` vs `PaperBench Codeâ€‘Dev`) (Section 2.6)
  - Whatâ€™s new: A lowerâ€‘cost pathway that correlates (weakly) with full performance, useful for early iteration.
  - Why it matters: Lowers barriers for the community to test agent codeâ€‘writing capabilities before investing in full reproduction.

- Timeâ€‘horizon and scaffold sensitivity insights (Sections 5.2â€“5.4; Figure 3; Table 5)
  - Whatâ€™s new: Forcing incremental work via `IterativeAgent` significantly improves some models (e.g., `o1` from 13.2% to 24.4%; Table 5), while others degrade (Claude 3.5 Sonnet falls from 21.0% to 16.1%).
  - Why it matters: Highlights that prompting and runtime management materially affect agent success on longâ€‘horizon research tasks.

## 5. Experimental Analysis
- Evaluation protocol (Sections 5.1â€“5.2; 2.2â€“2.4)
  - Setup: Each model runs on all 20 papers, 3 seeds per paper, 12â€‘hour limit per attempt; submission is reâ€‘executed in a clean VM; judged by `o3-mini` SimpleJudge (Section 5.2).
  - Metric: Average Replication Score (weighted satisfaction of rubric leaves; Section 2.3). Violations of blacklists result in disqualification (10 of 646 runs; Section 2.5).

- Main quantitative results
  - Overall performance with `BasicAgent` (Table 4):
    > Claude 3.5 Sonnet (New) achieves 21.0% Â± 0.8; `o1` achieves 13.2% Â± 0.3; DeepSeekâ€‘R1 6.0% Â± 0.3; GPTâ€‘4o 4.1% Â± 0.1; Gemini 2.0 Flash 3.2% Â± 0.2; `o3-mini` 2.6% Â± 0.2.
  - Effect of `IterativeAgent` (Table 5):
    > `o1` improves to 24.4% Â± 0.7 (and to 26.0% Â± 0.3 with 36 hours); `o3-mini` improves to 8.5% Â± 0.8; Claude 3.5 Sonnet drops to 16.1% Â± 0.1.
  - Codeâ€‘only variant (Table 6):
    > On `PaperBench Codeâ€‘Dev`, `o1` reaches 43.4% Â± 0.8.

- Human baseline (Section 5.4; Figure 3)
  - Design: 8 ML PhDs attempted a 4â€‘paper subset; 3 independent attempts per paper; bestâ€‘ofâ€‘3 used as â€œexpertâ€ score. Timeâ€‘tracked submissions graded at multiple checkpoints (Section 5.4).
  - Result:
    > Humans surpass the agent over time: the `o1` curve plateaus early (â‰ˆ first hour) while human scores continue to rise, overtaking by 24â€“48 hours (Figure 3). On a 3â€‘paper subset (excluding one truncated run), bestâ€‘ofâ€‘3 humans achieve 41.4% after 48 hours, vs `o1` at 26.6% on the same subset (Introduction; Section 5.4).

- Where models succeed vs fail (Table 9; stratified analysis)
  - Stratified by requirement type:
    > For `o1 (IterativeAgent)`, Code Development = 43.3% Â± 1.1; Execution = 4.5% Â± 1.5; Result Match = 0.0% (Table 9). Humans: Code Development = 72.4%; Execution = 20.4%; Result Match = 8.9% (Table 9).
  - Interpretation: Models write lots of code quickly but struggle to integrate, execute, and reach matching resultsâ€”indicating toolâ€‘use reliability, experiment orchestration, and debugging are the bottlenecks (Section 5.2; Table 9).

- Judge validation and costâ€‘effectiveness (Table 3; Figure 5)
  - `o3-mini` judge: F1 = 0.83 at â‰ˆ $66/paper; `o1` judge: F1 = 0.84 at â‰ˆ $830/paper. Both are far cheaper than expert human grading, and close enough in F1 for practical use (Table 3; Figure 5).

- Additional observations and checks
  - Cheating safeguards: Disqualifications for blacklist violations (10/646) demonstrate monitoring is necessary (Section 2.5).
  - Sensitivity to scaffolds: Different prompting and the ability to â€œend earlyâ€ strongly affect outcomes (Sections 5.2â€“5.3).
  - Variance across papers and seeds: High variability suggests multiple seeds are advisable for robust evaluation; detailed perâ€‘paper tables are provided (Tables 10â€“18; Appendix I).

- Overall assessment
  - The experiments convincingly show: (a) nonâ€‘trivial capability at code writing; (b) major gaps in longâ€‘horizon execution and reliable reproduction; and (c) meaningful dependence on runtime scaffolding and time limits. The human baseline confirms the remaining performance gap on realistic research workloads (Figure 3; Sections 5.2â€“5.4).

## 6. Limitations and Trade-offs
- Dataset scope and potential contamination (Section 7)
  - Only 20 papers, albeit with 8,316 leaf requirements. Future models may have pretraining exposure to some code or techniques (â€œcontaminationâ€), although recency mitigates this today.
- Rubric creation cost and complexity (Section 7; Appendix C)
  - Rubrics are laborâ€‘intensive and require expert involvement and iterative review; this constrains scaling. Weight choices, while authorâ€‘approved, encode subjective judgments about importance (Section 3.1).
- Automated judge fidelity and determinism (Section 7; Section 4.2)
  - LLM judges are not perfect or deterministic; although `o3-mini` achieves F1 â‰ˆ 0.83 on JudgeEval (Table 3), expert humans remain the gold standard for nuanced cases.
- Compute and cost constraints (Section 7)
  - Full PaperBench runs are expensive: e.g., â‰ˆ $400 in API credits for a 12â€‘hour `o1 IterativeAgent` rollout per paper, plus â‰ˆ $66 for grading with `o3-mini` (Section 7). GPU requirements (A10) limit accessibility.
- Benchmark rules and realism (Section 2.5)
  - Blacklisting original code ensures fromâ€‘scratch replication but differs from how human researchers often work (who do consult official code to save time). The 12â€‘hour reproduction cap in experiments (Section 2.2) may constrain fullâ€‘scale results for some papers.
- Specification gaming risks (Appendix A.3)
  - Any rubricâ€‘based system can be gamed; continued stressâ€‘testing and adversarial submissions will be needed to ensure robustness.

## 7. Implications and Future Directions
- How this work changes the landscape
  - PaperBench reframes â€œreplicating a paperâ€ as a measurable, endâ€‘toâ€‘end agent capability with credible, scalable oversight. It provides a common yardstick for autonomy and engineering competence in ML R&D (Introduction; Section 2).
- Research avenues enabled or suggested
  - Better agent scaffolds for longâ€‘horizon work: The `IterativeAgent` gains hint at the importance of stepâ€‘wise planning, tool reliability, and â€œdonâ€™t end earlyâ€ strategies (Section 5.3; Table 5).
  - Automated rubric creation and critique: Humanâ€‘inâ€‘theâ€‘loop workflows, dependency graphs in rubrics, and improved task decomposition could reduce rubric authoring costs (Appendix A.1).
  - Stronger, cheaper judges: Improving judge prompts, adding chainâ€‘ofâ€‘thought verification, or agentâ€‘asâ€‘judge designs could raise accuracy while lowering cost; `JudgeEval` is a reusable yardstick (Section 4.2; Appendix A.2).
  - Costâ€‘reduction strategies: â€œPruned rubric gradingâ€ shows promise for 10Ã— cheaper grading with minor accuracy loss in a preliminary test (Appendix H; Figure 6).
  - Safety evaluation: PaperBench can serve preparedness and responsible scaling frameworks as a metric of autonomous R&D capability growth (Introduction).
- Practical applications
  - Model evaluation during deployment: Labs and enterprises can track whether new reasoning models meaningfully improve at real research tasks beyond coding snippets.
  - Research operations: Triage which parts of a paper an agent can reliably implement vs which require human oversight; use rubrics to allocate work.
  - Education and training: Use Codeâ€‘Dev variant for coursework and bootcamps to teach endâ€‘toâ€‘end ML engineering with structured feedback.

> Representative headline results to remember:
> - Best agent on full PaperBench: Claude 3.5 Sonnet (New) at 21.0% (Table 4).
> - `o1` improves from 13.2% to 24.4% with `IterativeAgent` (Table 5), and to 26.0% with 36 hours.
> - Human bestâ€‘ofâ€‘3 on a subset: 41.4% after 48 hours; agents plateau early (Figure 3).
> - Judge validation: `o3-mini` F1 = 0.83 at â‰ˆ $66/paper (Table 3).

In sum, PaperBench introduces a rigorous, endâ€‘toâ€‘end testbed for research replication that exposes where current agents excel (rapid code writing) and where they struggle (execution, debugging, and matching results). It provides both a credible baseline of current capability and a clear roadmap for progress on autonomy, evaluation, and research safety.

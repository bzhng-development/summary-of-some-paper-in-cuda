# SAILâ€‘VL2 Technical Report

**ArXiv:** [2509.14033](https://arxiv.org/abs/2509.14033)
**Authors:** Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng
**Institutions:** 

## ğŸ¯ Pitch

SAIL-VL2 is a groundbreaking open-source vision-language model that achieves state-of-the-art performance with significantly smaller models by utilizing an advanced vision encoder, innovative training techniques, and efficient data curation. This approach not only reduces computational demands, enabling broader deployment, but also demonstrates that strategic data and model alignment can rival the capabilities of much larger, costlier models, transforming future practices in multimodal AI.

---

## 1. Executive Summary (2â€“3 sentences)
SAILâ€‘VL2 is an openâ€‘source suite of visionâ€‘language models (LVMs) that aims to deliver â€œsmall model, strong performanceâ€ by combining a progressively trained vision encoder (`SAILâ€‘ViT`), a lean visionâ€‘toâ€‘language adapter, and dense or mixtureâ€‘ofâ€‘experts (MoE) Qwen3 LLM backbones. Through largeâ€‘scale data curation (especially for captions, OCR, charts, and video), staged preâ€‘training, and a thinkingâ€‘fusion SFTâ€“RL pipeline, SAILâ€‘VL2 achieves stateâ€‘ofâ€‘theâ€‘art results among subâ€‘4B and ~8B models across 106 benchmarks, and strong reasoning scores in OpenCompass (Tables 8â€“10, Figure 1).

## 2. Context and Motivation
- Problem/gap addressed
  - Modern visionâ€‘language models (LVMs) achieve strong results by scaling parameters and data, but this can be computationally expensive and inefficient to deploy. SAILâ€‘VL2 targets the question: how to inject multimodal knowledge efficiently so that smaller models can match or exceed larger alternatives (Intro; â€œsmall model, strong performanceâ€).
  - Key weaknesses in prior efforts include noisy multimodal data (especially captions/OCR/video), shallow alignment between visual and language spaces, and limited reasoning ability without heavy reliance on very large LLMs.

- Importance
  - Practical: Smaller models cut inference cost and latency, enabling wider deployment (mobile, enterprise, edge).
  - Scientific: Demonstrates that careful data curation, progressive alignment, and targeted reasoning postâ€‘training can close performance gaps without bruteâ€‘force scale.

- Prior approaches and their limits
  - â€œBigger is betterâ€ scaling (e.g., InternVL, Qwen2.5â€‘VL) brings accuracy but at high compute and cost.
  - Many models treat vision encoders as fixed, leaving modality gaps; others use generic instruction data that underemphasize OCR, charts, and videos; most reasoning pipelines focus on textâ€‘only or require very large backbones.

- Positioning
  - SAILâ€‘VL2 builds on SAILâ€‘VL (Dong et al., 2025a) and contributes three pillars: (1) a rigorous data pipeline (caption quality judges, chart synthesis, video selection), (2) a progressive training framework from `SAILâ€‘ViT` alignment to multiâ€‘task preâ€‘training, then thinkingâ€‘fusion SFTâ€“RL, and (3) architectural coverage of dense and sparse MoE LLMs with infrastructure for efficiency (Figures 2â€“3; Sections 2â€“5).

## 3. Technical Approach
This section unpacks â€œhow it works,â€ from the model architecture to training recipes and infrastructure.

- Overall architecture (Figure 2; Table 1)
  - Vision encoder: `SAILâ€‘ViT` (Section 2.1) encodes images/videos into visual tokens.
  - Adapter: a lightweight 2â€‘layer MLP projects visual embeddings into the LLM token space (Section 2).
  - LLM backbones: dense Qwen3â€‘1.7B/8B and sparse MoE Qwen3â€‘30Bâ€‘A3B (activates ~3B experts per token) jointly process text and projected visual tokens (Section 2; Table 1).
  - Anyâ€‘resolution option: `SAILâ€‘ViTâ€‘AnyRes` preserves native image resolutions via interpolated positional embeddings, improving fineâ€‘grained grounding (Section 2.1.2).

- `SAILâ€‘ViT`: progressive alignment of vision and language (Section 2.1.1; Figure 2)
  - Key idea: donâ€™t freeze the visual backbone; gradually align it to the LLMâ€™s representation with increasing task complexity.
  - Three stages (instructionâ€‘style training throughout):
    1) Warmâ€‘up: freeze ViT and LLM, tune only the adapter on 8M simple samples (captioning + OCR), LR 2eâ€‘4, batch 1920.
    2) Fineâ€‘grained alignment: unfreeze ViT + adapter, expand data (more caption/OCR + videoâ€‘caption), LR 2eâ€‘5, batch 512.
    3) World knowledge injection: unfreeze all (vision, adapter, LLM) on 36.5M diverse data covering captions, OCR, openâ€‘ended QA, math, short QA, and pure text; LR 1eâ€‘5, batch 512.
  - Result: a vision encoder that produces features closer to the LLM space, empirically validated by both classification benchmarks (Table 6) and featureâ€‘space distance metrics (Table 7, Figure 6).

- `SAILâ€‘ViT` family (Section 2.1.2; 6.1 Model Zoo)
  - Base ViT: 448Ã—448 input, 32Ã—32 patch grid (patch size 14 â†’ 1024 tokens). Highâ€‘res images are tiled into 448Ã—448 crops.
  - AnyRes: interpolates positional embeddings to match arbitrary resolution; supports up to 1792Ã—1792 within a 16,384 token budget (Section 6.1).

- Multiâ€‘modal Mixtureâ€‘ofâ€‘Experts (Section 2.2)
  - `MoE` means replacing some dense MLP layers with many parallel â€œexperts,â€ while a gating network activates only a few per token, scaling parameters without proportional compute.
  - SAILâ€‘VL2 uses Qwen3â€‘MoE with:
    - Loadâ€‘balancing auxiliary loss and averaged activation across ranks for stability.
    - â€œDistributionâ€‘awareâ€ calibration: probe data to adjust expert activation entropy so text and multimodal activation patterns remain healthy (prevents expert collapse).

- Data curation at scale (Sections 3.1, 4.1; Figure 3)
  - `SAILâ€‘Caption2`: 300M captions cleaned to 250M using automated quality judges trained on 500K labeled samples. Two dimensions:
    - `VIR` (Visual Information Richness) and `ITA` (Imageâ€‘Text Alignment), each scored 1â€“5. Judge models exceed 90% precision/recall (Table 2).
    - Outcome: retain >99% estimated highâ€‘quality after filtering; add 1.69M chart captions (400K synthetically rendered from code + 1.29M open datasets) (Section 3.1.1).
  - Synthetic VQA (`Caption2QA`): transform ~80M captions into diverse QA pairs with an LLM; scaling to 180M improves smoothly in a log trend (Section 3.1.2; Figure 4).
  - Video (`SAILâ€‘Video`): 5.1M filtered QA samples using three metrics scored by an LVM: alignment (âˆ’1â€“10), content richness (âˆ’1â€“7), difficulty (âˆ’1â€“8). Keep items with alignmentâ‰¥5, contentâ‰¥5, difficultyâ‰¥3 (Section 4.1.1).
  - Instruction (`SAILâ€‘Instruction2`): 20M diverse, highâ€‘quality visual instructions built via latentâ€‘class bucketing (semantic clustering) and reâ€‘annotation for accuracy; includes more longâ€‘answer/reasoning items (Section 4.1.2; Figure 5).

- Preâ€‘training pipeline (Section 3.2; Table 3)
  - Two phases after `SAILâ€‘ViT`:
    1) Basic multimodal preâ€‘training on 64M samples (captions, chart captions, OCR). Uses `AdaLRS` (Adaptive Learning Rate Search) to automatically raise LR from 2eâ€‘4 up to ~6.75eâ€‘4, improving final loss by >0.06 (Section 3.2.1â€“3.2.3).
       - `AdaLRS` uses a backtracking lineâ€‘search on the loss descent slope; if increasing LR improves lossâ€‘reduction velocity, keep it; otherwise roll back and decrease LR. Equation (1) formalizes the LR update with slope estimates v(Â·) and scaling factors Î±â€², Î²â€².
    2) Multiâ€‘task preâ€‘training on 180M samples mixing visual understanding and instructionâ€‘tuning data (no AdaLRS here due to weak lossâ€“performance correlation). Resampling occurs at two levels:
       - Datasetâ€‘level balancing to mix distributions (basic stage).
       - Linguistic nâ€‘gram balancing to fight phrasing homogenization in synthetic data (multiâ€‘task stage) (Section 3.2.2).
  - Scalingâ€‘law: training up to 360B tokens shows monotonic gains on overall, natural VQA, and OCR VQA benchmarks (Figure 4).

- Postâ€‘training for instruction following and reasoning (Section 4; Table 4)
  - Basic SFT: staged curriculumâ€”world knowledge (Infinityâ€‘MM Stage2) â†’ `SAILâ€‘Instruction2` â†’ harder reasoning subsets (LLaVAâ€‘CoT, MMPR, Condor) â†’ mixed 1:1 image:video phase (with `SAILâ€‘Video`). â€œModel soupâ€ (merging homogeneous runs) yields reliable gains; mixing heterogeneous runs degrades performance (Table 5).
  - LongCoT SFT: build a 400K highâ€‘quality multimodal Chainâ€‘ofâ€‘Thought corpus with consistent formatting (`<think> ... </think>`; answer in `\boxed{}`), strict cleaning (redundancy filter by token overlap; answer distillation; CoT length balancing). Train for 1 epoch, batch 1024, cosine LR 1eâ€‘6; objective is nextâ€‘token prediction over thought+answer, L_LongCoT in Equation (2).
  - RL with verifiable rewards: 70K challenging problems curated via pass@4 filters; two binary rewardsâ€”answer correctness (in `\boxed{}`) and format adherence (`<think> ... </think>`). PPOâ€‘based optimizers differ by backbone: DAPO for dense, GSPO for MoE; context 16,384; max generation 4096; 2048 rollouts/episode; 8 PPO updates/episode; LR 1eâ€‘6; dynamic clip 0.20â€“0.28 (Section 4.2.3).
  - Thinkâ€‘Fusion SFT: 1M examples with 90% direct QA and 10% highâ€‘quality CoT traces harvested from the RL stage via rejection sampling; train with a dualâ€‘loss objective (Equation (3)) that conditions loss on different formats (Section 4.2.4).
  - RL with mixed rewards: curate â€œhard casesâ€ (50K) + 50K general samples (LLaVAâ€‘OneVision) to maintain breadth. Mixed reward = weighted combination of answer, thoughtâ€‘quality (judgeâ€‘scored), and format (all binary); same PPO setup as before (Section 4.2.5). Note: the narrative mentions both 100K and 150K samples hereâ€”an inconsistency flagged in Limitations.

- Efficiency infrastructure (Section 5)
  - Stream packing: concatenate variableâ€‘length sequences to minimize padding; maintain correct positions and masks; online packing from perâ€‘node buffers. Visual packing additionally balances visual token counts across devices, which is critical for AnyRes inputs. Gains: nearly 2Ã— SM utilization, ~50% faster training, +0.7% average accuracy on openâ€‘ended QA; visual packing yields a further ~48% efficiency gain (Section 5.1).
  - MoE infra: kernel fusion for expert ops (up to 3Ã— speedup), optimized attention/LayerNorm; distributed strategies differ by hardware (Megatron on NPUs with pipeline+expert parallelism; DeepSpeed ZeROâ€‘2 with CPU offload on NVIDIA GPUs) (Section 5.2).

## 4. Key Insights and Innovations
- Largeâ€‘scale, qualityâ€‘controlled multimodal data curation that targets hard modalities (Section 3.1; Table 2; Figure 3)
  - Novelty: automated `VIR`/`ITA` caption judges trained on balanced labels bring >90% precision/recall (Table 2), enabling economical filtering of 300M captions down to a highâ€‘quality 250M. A codeâ€‘driven chart synthesis engine and consistent video filtering with alignment/content/difficulty scores create focused corpora where LVMs often struggle.
  - Significance: boosts preâ€‘training efficiency and downstream OCR/chart/video performance (Tables 8â€“9).

- Progressive alignment of the vision encoder to the language space (Section 2.1; Table 6; Table 7; Figure 6)
  - Novelty: threeâ€‘stage training that explicitly unfreezes the ViT and LLM at the right times, rather than â€œfrozen vision encoder + adapter only.â€
  - Significance: `SAILâ€‘ViT` features move measurably closer to text embeddings (lower nearestâ€‘neighbor and Wasserstein distances across LLM sizes in Table 7; tighter overlap in Figure 6), and visual classification improves over AIMv2 baselines (average +2.11% for Huge, Table 6).

- AdaLRS: lossâ€‘guided adaptive LR search during basic multimodal preâ€‘training (Section 3.2.3; Eq. 1)
  - Novelty: a simple lineâ€‘searchâ€‘style scheduler that probes LR increases and rolls back if the loss slope worsens.
  - Significance: automatically finds a better LR (from 2eâ€‘4 to ~6.75eâ€‘4), yielding >0.06 finalâ€‘loss improvement without manual sweeps (Section 3.2.1).

- Thinkingâ€‘fusion training (SFTâ€“RL cycle) with formatâ€‘aware, partly verifiable rewards (Sections 4.2.2â€“4.2.5)
  - Novelty: staged LongCoT SFT â†’ verifiableâ€‘reward RL â†’ Thinkâ€‘Fusion SFT mixing 90% direct QA + 10% curated CoT â†’ mixedâ€‘reward RL. The use of `<think>` tags and `\boxed{}` answers standardizes supervision and reward parsing; rejection sampling harvests â€œbestâ€ CoTs from the modelâ€™s own RL rollouts.
  - Significance: strong reasoning at modest scales. The 8Bâ€‘Thinking model reaches 54.4 average on OpenCompass reasoning (Table 10)â€”competitive with GPTâ€‘4oâ€‘latest (54.8) and above many openâ€‘source peers.

- Trainingâ€‘efficiency engineering that also improves quality (Section 5.1)
  - Novelty: joint stream+visual packing explicitly balances both text and visual token loads across devicesâ€”rarely reported with quantified gains in LVMs.
  - Significance: up to ~1.5Ã— faster training + ~0.7% accuracy gains on longâ€‘context QA (Section 5.1).

## 5. Experimental Analysis
- Evaluation protocol and baselines (Section 6.1)
  - Benchmarks: 106 datasets spanning general multimodal understanding, math/reasoning, multiâ€‘image/video, plus OpenCompass (8 datasets) and multiple video sets.
  - Judging and comparability:
    - For â€œbasicâ€ models: custom VLMEvalKit with Doubaoâ€‘1.5â€‘visionâ€‘pro as judge; all baselines reâ€‘evaluated in the same setting.
    - For â€œthinkingâ€ models: official OpenCompass leaderboard except two models (SAILâ€‘VL2â€‘A3Bâ€‘Thinking and Keyeâ€‘VLâ€‘8Bâ€‘Thinking) evaluated with GPTâ€‘4oâ€‘Mini in OpenCompassâ€‘aligned settings (Section 6.1). This is mostly fair but mixes judge modelsâ€”see Limitations.

- Main quantitative results (Figures 1; Tables 8â€“10)
  - 2B scale (Table 8):
    - OpenCompassavg 70.31 vs Qwen2.5â€‘VLâ€‘3B 65.36, InternVL3.5â€‘2B 66.64.
    - OCR/Docs: OCRBench 89.5, DocVQA 93.10 (leading among <4B).
    - Reasoning subsets: MathVistaâ€‘mini 71.10 (strong for size), MMMUâ€‘val 47.67 (competitive).
    - AnyRes improves grounding: RefCOCOavg 57.82 vs 53.28 for fixedâ€‘res 2B.
  - 8B scale (Table 9):
    - OpenCompassavg 75.07 vs InternVL3.5â€‘8B 73.49; OpenSourceavg 57.20.
    - OCR/Docs: DocVQA 95.28; OCRBench 91.30 (top tier).
    - Reasoning: MMMUâ€‘val 55.44; MathVerseâ€‘mini 43.17; MathVistaâ€‘mini 76.40.
    - Multiâ€‘image/video: TempCompassavg 65.66; LongVideoBenchâ€‘val 58.34.
  - Thinking models (Table 10):
    - `SAILâ€‘VL2â€‘8Bâ€‘Thinking` average 54.4 across MathVista, MathVision, MathVerse, DynaMath, WeMath, LogicVistaâ€”best among openâ€‘source models listed, close to GPTâ€‘4oâ€‘latest 54.8 and above Geminiâ€‘2.0â€‘Flash 50.6.
    - MoE thinking with ~3B active parameters (`A3Bâ€‘Thinking`) averages 53.6, surpassing several larger closedâ€‘source models and openâ€‘source thinkers.

- Ablations and diagnostics
  - `SAILâ€‘ViT` vs AIMv2 in zeroâ€‘shot classification shows consistent improvements across ImageNet variants (Table 6).
  - Featureâ€‘space alignment: distances to text embeddings reduced with `SAILâ€‘ViT` across LLM sizes (Table 7; Figure 6).
  - Scaling curves: larger multiâ€‘task preâ€‘training budget monotonically improves metrics (Figure 4).
  - Model soup: merging homogeneous runs boosts performance (AVG 76.60 vs bases ~74.5) while heterogeneous merging can catastrophically fail (AVG 12.86) (Table 5)â€”a cautionary result.

- Convincingness
  - Breadth: 106 datasets and separate video evaluations provide wide coverage.
  - Depth: The reasoning leaderboard results (Table 10) support the efficacy of the SFTâ€“RL pipeline.
  - Causality evidence: The paper ties specific design choices to measurable effects (e.g., AnyRes â†’ better RefCOCO, AdaLRS â†’ lower loss, packing â†’ faster training + small accuracy gains, `SAILâ€‘ViT` â†’ closer feature spaces and higher ImageNet averages).

- Representative quotes of outcomes
  > â€œSAILâ€‘VL2â€‘2B â€¦ achieves stateâ€‘ofâ€‘theâ€‘art average performance on OpenCompass among officially released openâ€‘source models under the 4B scale.â€ (Figure 1a; Table 8)

  > â€œSAILâ€‘VL2â€‘8Bâ€‘Thinking â€¦ establishes a new stateâ€‘ofâ€‘theâ€‘art for openâ€‘source models â€¦ 54.4 averageâ€ (Table 10)

  > â€œData packing nearly doubles SM utilization and accelerates training by 50%, â€¦ visual packing â€¦ further 48% gain â€¦ +0.7% average improvement on openâ€‘ended QAâ€ (Section 5.1)

## 6. Limitations and Tradeâ€‘offs
- Mixed evaluation judges and potential comparability issues
  - Basic models are judged with Doubaoâ€‘1.5â€‘visionâ€‘pro; thinking models mostly with OpenCompass, but two (including SAILâ€‘VL2â€‘A3Bâ€‘Thinking) are judged via GPTâ€‘4oâ€‘Mini (Section 6.1). While settings are â€œaligned,â€ crossâ€‘judge variance may affect fineâ€‘grained comparisons.

- Reward design and transparency
  - RL uses binary rewards for answer/format and judgeâ€‘based think quality for mixedâ€‘reward RL (Sections 4.2.3, 4.2.5). Coefficients for the mixed reward are not disclosed; sensitivity analysis is absent.

- Data scale, compute, and reproducibility
  - The report mentions training on 776B tokens overall (Intro highlights), and tables detail large budgets (Table 3; Table 4), but compute hours, hardware counts, and perâ€‘stage wallâ€‘clock are not reportedâ€”important for practitioners planning replication.

- Minor inconsistencies
  - The mixedâ€‘reward RL dataset is described as 100K (50K hard + 50K general) in â€œData Curation,â€ yet â€œTraining Recipeâ€ refers to 150K samples (Section 4.2.5). Clarification is needed.

- Safety and bias
  - While hallucination is evaluated (HallusionBench, Tables 8â€“9), there is no targeted safety analysis (e.g., bias across languages/layouts in OCR, robustness to adversarial charts). The heavy use of synthetic/Q&Aâ€‘converted data may induce stylistic biases, though the authors add nâ€‘gram resampling to mitigate this (Section 3.2.2).

- Scope and edge cases not fully explored
  - Longâ€‘video comprehension is evaluated with 16 sampled frames (Tables 8â€“9 notes), which may underrepresent models optimized for dense temporal reasoning.
  - The AnyRes path improves RefCOCO, but the computational cost vs. benefit across tasks is not quantified; similarly, MoE training stability is discussed qualitatively with loadâ€‘balancing, but detailed failure rates/mitigations are not reported.

## 7. Implications and Future Directions
- Impact on the field
  - Demonstrates that careful data engineering and progressive alignment can push smallâ€‘toâ€‘midâ€‘size LVMs to the top of openâ€‘source leaderboards (Figure 1; Tables 8â€“10). This challenges the default â€œscale parameters firstâ€ strategy and provides a replicable recipe focused on data quality + staged training.

- What it enables next
  - Research:
    - Understanding which components drive reasoning gains: controlled ablations on 90/10 Thinkâ€‘Fusion mixes, mixedâ€‘reward coefficients, pass@k thresholds, and `<think>`/`\boxed{}` formatting.
    - More principled expertâ€‘activation calibration for MoE under multimodal distributions (extend entropy probing to crossâ€‘modal gates).
    - Robustness studies on OCR/chart/video under distribution shifts and lowâ€‘resource languages.
  - Engineering:
    - Wider adoption of stream+visual packing in multimodal training pipelines.
    - Reusable caption quality judges and chart synthesis tools for community datasets; the `Caption2QA` approach for scalable VQA preâ€‘training.

- Practical applications
  - Highâ€‘fidelity document OCR and layout understanding (DocVQA 95.28 at 8B) for enterprise automation.
  - Chart/table comprehension and dataâ€‘viz QA in BI tools (large chart corpus; strong ChartQA/DocVQA).
  - Efficient visual assistants on edge/cloud with good reasoning (OpenCompass reasoning 54.4 at 8Bâ€‘Thinking; MoEâ€‘Thinking 53.6 with ~3B active parameters).
  - Video QA for surveillance, education, and media indexing, aided by curated video alignment/difficulty filtering and AnyRes support.

In sum, SAILâ€‘VL2 contributes a wellâ€‘engineered pathway to highâ€‘accuracy, costâ€‘efficient multimodal models: clean and diverse data, progressively aligned `SAILâ€‘ViT`, adaptive preâ€‘training (AdaLRS, resampling), and a compact reasoning pipeline (LongCoT â†’ PPO RL â†’ Thinkâ€‘Fusion â†’ mixedâ€‘reward RL). The results across 106 datasets and OpenCompass reasoning are strong, though future work should tighten evaluation comparability, reveal RL reward settings, and deepen robustness/safety analyses.

# Parameter-Efficient Transfer Learning for NLP

**ArXiv:** [1902.00751](https://arxiv.org/abs/1902.00751)

## ğŸ¯ Pitch

This paper introduces 'adapter tuning,' a novel method for transfer learning in NLP where small, trainable adapter modules are inserted into frozen, pre-trained Transformer models like BERT. By training only a tiny fraction of parameters per task, adapter tuning achieves near state-of-the-art results across diverse NLP benchmarks while drastically reducing redundancy and memory overheadâ€”making deep language models truly scalable for real-world, multi-task environments where new tasks frequently arrive.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces adapter tuning: tiny, trainable â€œadapterâ€ layers inserted into a frozen, pre-trained Transformer (e.g., BERT) that let each new NLP task be learned by training only a few additional parameters while reusing the rest. Across 26 text classification tasks (including GLUE) and SQuAD question answering, adapters match or nearly match full fine-tuning while training two orders of magnitude fewer task-specific parameters (e.g., within 0.4 GLUE points while training ~3.6% parameters per task; Table 1, Figure 1).

## 2. Context and Motivation
- Problem addressed:
  - Fine-tuning large pre-trained language models (e.g., BERT) achieves strong transfer but is parameter inefficient when many downstream tasks must be supported. Each new task typically requires a full copy of the model weights.
- Why this matters:
  - Practical settings such as cloud services must host models for many customers/tasks arriving sequentially. Storing and maintaining a full model per task is costly, and re-training risks forgetting earlier tasks.
- Prior approaches and their gaps:
  - Feature-based transfer: pre-compute embeddings (word/sentence) and train a custom downstream model. Limitation: usually lower performance than fine-tuning and still needs a new downstream model per task (Section 1).
  - Full fine-tuning: copy and update all weights for each task. Limitation: 100% of parameters are task-specific; poor storage and maintenance efficiency (Figure 1).
  - Multi-task learning: share parameters by training all tasks jointly. Limitation: requires simultaneous access to all task data; not suited to sequential arrival (Section 1).
  - Continual learning: sequentially fine-tune one model while mitigating catastrophic forgetting. Limitation: memory of past tasks is imperfect, and training is complex (Section 1).
- Positioning of this work:
  - A parameter-efficient, extensible alternative that enables sequential onboarding of tasks without revisiting or degrading earlier ones: add small per-task adapter modules while keeping the large pre-trained backbone fixed (Sections 1â€“2).

Key terms (defined on first use):
- `Adapter module`: a small, task-specific set of layers inserted between layers of a frozen pre-trained model.
- `Near-identity initialization`: initializing an added module so its initial function is approximately the identity mapping, leaving the pre-trained modelâ€™s behavior unchanged at the start of training.
- `BERT`: a widely used Transformer-based language model pre-trained with masked language modeling (Section 1).
- `GLUE`: a standard suite of 9 NLU tasks (e.g., sentiment, entailment) with a composite score (Section 3.2).
- `SQuAD v1.1`: an extractive QA benchmark where the model selects an answer span from context (Section 3.5).

## 3. Technical Approach
At a high level, an existing network function Ï†_w(x) with parameters `w` is adapted to a new task by defining a new function Ïˆ_{w,v}(x) that:
- Leaves `w` frozen (copied from pre-training).
- Introduces small, trainable parameters `v`â€”the adaptersâ€”such that Ïˆ_{w,v0}(x) â‰ˆ Ï†_w(x) at initialization (near-identity).
- Trains only `v` (plus a task head and per-layer normalization parameters), achieving parameter efficiency and perfect â€œmemoryâ€ of prior tasks because shared weights never change (Section 2).

Step-by-step architecture and data flow (Transformers; Section 2.1, Figure 2):
1. Where adapters are inserted
   - A Transformer layer contains two sub-layers: multi-headed attention and a feed-forward network (FFN).
   - For each Transformer layer, two adapters are inserted in series:
     - One right after the attention sub-layerâ€™s output projection back to the model dimension `d`, and before adding the residual (skip connection).
     - One right after the FFNâ€™s output projection back to `d`, and before adding the residual.
   - The adapter outputs feed into the layer normalization (LayerNorm) that follows each sub-layer.
2. What an adapter does (Figure 2, right):
   - Bottleneck structure: down-project dâ†’m, apply nonlinearity, up-project mâ†’d.
   - Internal skip connection (residual) across the adapter block.
   - Parameter count per adapter: 2 m d + d + m (including biases). With m â‰ª d, this is a small fraction of the backboneâ€™s size (Section 2.1).
   - Initialization: projection weights are near zero so the adapter behaves almost like an identity mapping at the start (near-identity). This stabilizes training (Section 2).
3. What is trained per task
   - Train only: adapter parameters `v`, LayerNorm scale and bias (per layer), and the final classification span/label head (Figure 2 caption; Sections 2â€“3.1).
   - Keep the pre-trained BERT weights `w` frozen for all tasks. Each new task adds only its own `v` and head.
4. Why the design choices
   - Bottleneck (dâ†’mâ†’d) ensures few parameters per task and lets `m` tune the trade-off between parameter cost and accuracy (Section 2.1).
   - Near-identity initialization avoids destabilizing the well-trained backbone. Empirically, large initializations hurt accuracy (Figure 6, right).
   - Placement before residual addition and layer normalization ensures the adapter can shape sub-layer outputs without fighting downstream normalization, while remaining easily ignorable (if not needed) because of the residual path (Section 2.1, Figure 2).

Training and evaluation protocol (Section 3.1):
- Base model: public BERT checkpoints (BERTLARGE for GLUE; BERTBASE for the 17 additional tasks; Section 3.2â€“3.3).
- Classification head: a simple linear layer on the [CLS] token embedding (Section 3.1).
- Optimization: Adam with linear warmup for the first 10% of steps and linear decay to zero thereafter; batch size 32; trained on 4 TPUs (Section 3.1).
- Hyperparameters:
  - GLUE: learning rates {3e-5, 3e-4, 3e-3}, epochs {3, 20}; 5 random seeds due to instability; best validation model selected (Section 3.2).
  - Additional tasks: broader LRs {1e-5, 3e-5, 1e-4, 3e-3}; epochs chosen from {20, 50, 100} based on validation curves (Section 3.3; exact epochs in Supplementary Table 4).
  - SQuAD: LRs {3e-5, 5e-5, 1e-4} for fine-tuning, and {3e-5, 1e-4, 3e-4, 1e-3} for adapters; epochs {2, 3, 5} vs {3, 10, 20}, respectively (Section 3.5).

Baselines (Sections 3.2â€“3.4):
- Full fine-tuning: train all BERT parameters per task.
- Variable fine-tuning: train only the top `n` layers (BERTBASE; `n` swept), freezing the rest (Section 3.3).
- LayerNorm-only tuning: train only LayerNorm parameters to test how far scaling/bias alone can adapt a model (Section 3.4).
- Non-BERT AutoML baseline: a large neural architecture search over standard text models with pre-trained embedding modules, to verify BERT-based approaches are competitive (Section 3.3; Supplementary Tables 5â€“7).

## 4. Key Insights and Innovations
1. A simple, effective adapter for Transformers
   - Whatâ€™s new: a minimal bottleneck adapter with a residual path, placed twice per Transformer layer (after attention and after FFN), initialized near-identity (Figure 2; Section 2.1).
   - Why it matters: it preserves the pre-trained modelâ€™s knowledge intact while allowing task-specific modulation with tiny parameter costs. Compared with prior â€œfeatureâ€ reuse that only reads from inner layers, adapters â€œwriteâ€ into inner layers to reconfigure computations for the task (Related Work, Section 4).

2. Strong parameter-efficiency with minimal performance loss
   - Evidence: On GLUE, adapters achieve a mean score of 80.0 vs. 80.4 for full fine-tuning while adding ~3.6% trainable parameters per task, yielding total model size 1.3Ã— vs. 9Ã— for 9 tasks (Table 1). Trade-off curves show adapters are consistently better in the low-parameter regime than tuning top layers (Figure 1; Figure 3, left).
   - Significance: roughly two orders of magnitude fewer trainable parameters per task without sacrificing accuracy (Figure 1).

3. Extensibility and perfect retention across tasks
   - Mechanism: the backbone is frozen, so prior tasksâ€™ performance cannot be forgotten. New tasks add only their own small adapter/LN/head (Sections 1â€“2).
   - Practical impact: enables sequential task onboarding, ideal for multi-tenant cloud services.

4. Empirical analysis revealing where adaptation happens
   - Finding: upper-layer adapters matter more than lower ones. Removing adapters from lower layers barely affects MNLI performance, while removing larger top-layer spans hurts more; removing all adapters collapses accuracy to near majority-class levels (MNLI ~37%, CoLA ~69%; Figure 6, left/center; Section 3.6).
   - Implication: adapters naturally focus adaptation on higher-level representations, aligning with common fine-tuning intuition.

Incremental vs. fundamental: The adapter design is simple and incremental in architecture, but it delivers a fundamental capability: scalable, sequential, parameter-efficient transfer with strong accuracy across many tasks.

## 5. Experimental Analysis
Evaluation methodology
- Datasets and metrics:
  - GLUE (BERTLARGE; Section 3.2): 9 tasks (WNLI omitted), metrics include:
    - CoLA: Matthews correlation
    - MRPC/QQP: F1 score
    - STS-B: Spearman correlation
    - Others: accuracy (Table 1).
  - 17 additional public classification datasets (BERTBASE): variety of sizes (900 to 330k examples), classes (2 to 157), and text lengths (57 to ~1.9k chars). Accuracy is the metric (Section 3.3; Supplementary Table 3).
  - SQuAD v1.1 (Section 3.5): F1 score on validation; span prediction.
- Baselines:
  - Full fine-tuning and variable fine-tuning (Sections 3.2â€“3.4).
  - LayerNorm-only tuning (Section 3.4).
  - Non-BERT AutoML (Section 3.3).
- Setup fairness:
  - Same pre-trained base used.
  - Hyperparameters tuned per method/dataset; best validation chosen.
  - Multiple random seeds for stability (GLUE: 5; others: typically 3; Figures 4â€“5 show error bars).

Main quantitative results
- GLUE (Table 1):
  - Quote: â€œAdapters achieve a mean GLUE score of 80.0, compared to 80.4 achieved by full fine-tuningâ€¦ On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task.â€
  - Parameter efficiency:
    - Total size to solve all tasks: adapters 1.3Ã— vs. fine-tuning 9Ã—.
    - Fixed adapter size 64 still yields 79.6 (close to 80.4) with even fewer parameters (1.2Ã— total).
  - Per-task highlights (Table 1): adapters sometimes win (e.g., MRPC 89.5 vs. 89.3) and sometimes trail slightly (MNLI matched/mismatched 84.9/85.1 vs. 86.7/85.9).
- Additional 17 tasks (Table 2):
  - Average test accuracy:
    - Adapters: 73.3
    - Full fine-tuning: 73.7
    - Variable fine-tuning: 74.0
    - Non-BERT AutoML: 72.7
  - Parameter efficiency to solve all 17 tasks:
    - Adapters: 1.19Ã— total; train 1.14% per task.
    - Full fine-tuning: 17Ã— total; train 100% per task.
    - Variable fine-tuning: 9.9Ã— total; train 52.9% per task on average.
  - Interpretation: even when the best tuning strategy sometimes is â€œfine-tune only top layers,â€ adapters remain far more compact with competitive accuracy (Section 3.3).
- Trade-off analyses (Figures 3â€“4; Section 3.4):
  - Figure 3: Across GLUE and the additional tasks, adapters dominate the low-parameter regime. Tuning only the top-k layers performs markedly worse for the same parameter budget.
  - Figure 4 (MNLI-m, CoLA): for a comparable number of trainable parameters, adapters significantly outperform fine-tuning top layers. Example: on MNLI-m, fine-tuning just the top layer (~9M params) gives ~77.8% validation accuracy, while adapter size 64 (~2M params) yields ~83.7%; full fine-tuning reaches ~84.4%.
  - LayerNorm-only tuning is notably weaker (~3.5â€“4% drops on CoLA and MNLI; Section 3.4; Figure 4, green).
- SQuAD v1.1 (Figure 5; Section 3.5):
  - Quote: â€œAdapters of size 64 (2% parameters) attain a best F1 of 90.4%, while fine-tuning attains 90.7.â€
  - Very small adapters still work well: size 2 (~0.1% parameters) gets 89.9 F1.
- Ablations and robustness (Section 3.6; Figure 6; Supplementary Figure 7):
  - Where adapters matter: removing adapters from higher layers hurts more; removing all adapters collapses to majority-class baselines (MNLI ~37%, CoLA ~69%).
  - Initialization sensitivity: standard deviation â‰¤ 1e-2 is robust; too-large initializations degrade accuracy, especially on CoLA (Figure 6, right).
  - Adapter size robustness: average validation accuracy is stable across sizes 8, 64, 256 (86.2%, 85.8%, 85.7% respectively across GLUE-style tasks; Section 3.6).
  - Learning-rate robustness: adapters degrade less sharply than fine-tuning when LR increases (Supplementary Figure 7).

Assessment of claims
- The core claimâ€”near-finetuning accuracy with orders-of-magnitude fewer trainable parametersâ€”is strongly supported by:
  - GLUE averages and per-task numbers (Table 1), plus trade-off curves (Figure 3).
  - Broad validation on 17 diverse datasets and SQuAD (Table 2; Figure 5).
  - Ablation pinpointing where adaptation occurs and why the design is stable (Figure 6).
- Caveat: adapters are very close but not consistently better than full fine-tuning on every task; the promise is efficiency with parity, not systematic accuracy gains.

## 6. Limitations and Trade-offs
- Dependence on a strong pre-trained backbone:
  - Assumes a high-quality pre-trained model (BERT). If the backbone lacks relevant features (e.g., highly specialized domains or languages not covered), frozen lower layers may limit performance (implicit in Section 2â€™s rationale and Section 3.6â€™s observation that lower layers are shared).
- Capacity ceiling per task:
  - Since the backbone is frozen, all task adaptation must pass through small adapters and LayerNorm parameters. For tasks requiring substantial changes to low-level representations, this might underfit compared to full fine-tuning.
- Linear growth with the number of tasks:
  - Although small, each new task adds its own adapters and head. Storage still grows linearly with the number of tasks (albeit at ~1â€“4% of full model per task for the sizes tested; Table 1 and Table 2).
- Search/hyperparameters and stability:
  - The method can require re-runs across seeds to avoid instabilities (GLUE uses 5 seeds; Section 3.2). Hyperparameter grids are moderate (e.g., three learning rates on GLUE), which may leave some performance on the table for either method.
- Scope of evaluation:
  - Focuses on classification and extractive QA. Not evaluated on generative tasks, structured prediction beyond QA, multilingual settings, or very low-resource scenarios. Transfer to these cases is not guaranteed.

## 7. Implications and Future Directions
- How it changes the landscape:
  - Provides a practical recipe for multi-tenant, sequential adaptation of large language models without catastrophic forgetting, dramatically lowering per-task storage and training costs while preserving accuracy. This shifts transfer-learning design from â€œcopy-then-tune everythingâ€ to â€œfreeze-and-plug-inâ€ modules.
- Follow-up research enabled/suggested:
  - Adapter placement and routing: learn which layers (or tokens) need adapters or how to gate/route through task-specific vs. shared paths.
  - Automatic adapter sizing: adapt `m` per layer/task under a global budget.
  - Cross-task sharing: share adapters across related tasks, or factorize adapters into shared and task-specific components.
  - Beyond classification/QA: evaluate for generation, dialogue, summarization, and multilingual or domain-adaptation settings.
  - Optimization/stability: principled initializations and schedules to reduce seed sensitivity; theoretical analysis of why near-identity initializations stabilize training (motivated by Figure 6, right).
- Practical applications:
  - Cloud services hosting thousands of customized models for different customers where storage and continual updates matter (Section 1).
  - On-device personalization: ship a single backbone and download tiny per-user adapters.
  - Compliance and A/B testing: switch among task versions by swapping small adapter sets without touching the base model.

> Central takeaway, grounded in the paperâ€™s results: â€œAdapter-based tuning attains a similar performance to full fine-tuning with two orders of magnitude fewer trained parametersâ€ (Figure 1), e.g., GLUE 80.0 vs. 80.4 with ~3.6% parameters per task and 1.3Ã— total model size for all tasks (Table 1), and SQuAD F1 90.4 vs. 90.7 with a 2% parameter adapter (Figure 5).

# MiniMax-01: Scaling Foundation Models with Lightning Attention

**ArXiv:** [2501.08313](https://arxiv.org/abs/2501.08313)

## üéØ Pitch

MiniMax-01 presents a hybrid architecture that combines I/O‚Äëaware linear ‚Äúlightning attention‚Äù with intermittent softmax layers and a large Mixture‚Äëof‚ÄëExperts design (456B params, 45.9B active) plus new parallelism and kernel optimizations to enable training with 1M‚Äëtoken contexts and inference up to 4M tokens. This makes practical, single‚Äëmachine long‚Äëcontext LLM/VLMs possible‚Äîunlocking book‚Äëscale, project‚Äëscale, and many‚Äëshot in‚Äëcontext workflows while matching top‚Äëtier benchmark performance.

---

## 1. Executive Summary (2-3 sentences)

MiniMax-01 introduces `MiniMax-Text-01` (text) and `MiniMax-VL-01` (vision-language) with the explicit goal of achieving top-tier benchmark performance while scaling usable context windows from today‚Äôs typical 32K‚Äì256K tokens up to **1M tokens in training** and **4M tokens in inference** (Introduction; Figure 1; Figure 14). The central technical idea is a **hybrid attention stack** that uses mostly `lightning attention` (a practical, I/O-aware linear attention implementation) plus periodic softmax attention for retrieval, combined with a large `Mixture of Experts (MoE)` model (456B total parameters, 45.9B active per token) and extensive distributed systems optimizations to make million-token contexts feasible (Section 2; Section 3).

## 2. Context and Motivation

- **Problem / gap addressed**
  - Modern LLM/VLM context windows are often limited to **32K‚Äì256K tokens**, which is not enough for whole books, large codebases, or many-shot in-context learning (Introduction).
  - Standard Transformer softmax attention has **quadratic** cost in sequence length, so pushing context much beyond current limits becomes computationally prohibitive even as GPUs improve (Introduction; Figure 5 illustrates quadratic vs linear forms).

- **Why it matters**
  - Longer context is positioned as enabling practical workflows: using entire professional documents, full programming projects, or many-shot examples without external retrieval systems (Introduction).
  - The paper also links long context to better in-context learning and long-horizon assistant behaviors, motivating specialized long-context retrieval benchmarks like MR-NIAH (Section 5.7.2.1; Figure 15).

- **Prior approaches and shortcomings (as framed here)**
  - Existing context expansion in deployed models has largely relied on stronger hardware and optimized softmax attention implementations (e.g., I/O-aware attention) (Introduction).
  - Many proposed sub-quadratic alternatives exist (sparse attention, linear attention, SSMs, etc.), but the paper states they have **limited adoption at commercial scale**, and highlights practical issues with linear attention in causal settings (the need for `cumsum` hurting parallelism) (Section 2.2; Section 2.2.1).

- **How the paper positions itself**
  - The paper aims to show the **first successful large-scale deployment** of linear attention (as part of a hybrid stack) while matching leading models on standard and long-context benchmarks, enabled by both algorithmic choices (lightning attention + periodic softmax) and systems work (MoE communication overlap, long-sequence parallelism, inference kernels) (Contributions list in Introduction; Sections 2‚Äì3; Figure 2; Figure 8).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a very large **MoE-based transformer-style language model** that replaces most quadratic softmax attention layers with an efficient **linear-time attention** implementation (`lightning attention`) while keeping some softmax attention for retrieval.
- It solves long-context scaling primarily by changing the **attention compute pattern** (linear-ish instead of quadratic), then redesigning **distributed training + inference** (communication overlap, sequence parallelism, variable-length ring attention, custom CUDA kernels) to make million-token contexts practical (Sections 2‚Äì3).

### 3.2 Big-picture architecture (diagram in words)

- **Input text tokens** ‚Üí **80-layer stack** of blocks:
  - Most blocks are `transnormer` blocks using **lightning attention** (linear attention variant) + **MoE FFN**.
  - Every 8th block uses **softmax attention** (with GQA) + **MoE FFN** (Section 2; Figure 3).
- Inside each MoE layer:
  - A **router/gate** chooses **top-2 experts** among **32 experts** per token; selected experts run FFNs; outputs are combined (Eq. (1); Figure 3).
- Training/inference is enabled by a custom distributed runtime:
  - Specialized parallel groups for experts (`EP`, `ETP`, `EDP`) and long-context parallelism (`CP`) plus overlap strategies (Section 3.1; Figures 9‚Äì10).
  - Long-sequence attention strategies: `varlen ring attention` for softmax layers and `LASP+` for lightning attention layers (Section 3.2; Figures 11‚Äì12).
  - Inference kernel work to make lightning attention fast for variable-length batches and prefix caching (Section 3.3).

### 3.3 Roadmap for the deep dive

- I will explain, in order:
  1. **MoE routing and stabilization** (what MoE does, how routing collapse is mitigated) because it defines the compute/communication structure (Section 2.1).
  2. **Why linear attention is hard in causal LMs** and how **lightning attention** makes it practical (Section 2.2‚Äì2.2.1).
  3. **Hybrid attention design** (why periodic softmax is retained) and the chosen model hyperparameters (Sections 2.2.2‚Äì2.4; Table 5).
  4. **Distributed training + long-context systems** (EP/ETP overlap, varlen ring attention, LASP+) because long context is impossible without systems changes (Section 3; Figures 9‚Äì12).
  5. **Data + training strategy** for 1M-token training contexts and 4M-token extrapolation (Section 4.2; Table 6; Figure 14).
  6. **Post-training alignment** and the **vision-language extension** (`MiniMax-VL-01`) (Section 5; Section 6).

### 3.4 Detailed, sentence-based technical breakdown

This is primarily an **empirical systems + algorithmic architecture paper**: it combines an attention algorithm (`lightning attention`), a large routed model (`MoE`), and substantial distributed/inference engineering to reach million-token contexts while maintaining strong benchmark performance.

#### 3.4.1 Core model: hybrid attention + MoE blocks

- The model follows a Transformer-like block structure with a **channel mixer** (attention) and a **feature mixer** (MLP/FFN), but the FFN is replaced by a **Mixture of Experts** and the attention is mostly **linear attention** (Section 2; Figure 3).
- The final text model, `MiniMax-Text-01`, is a **hybrid stack**:
  - ‚ÄúA transformer block with softmax attention is positioned after every 7 transnormer blocks of linear attention,‚Äù producing **80 layers total** (Section 2).
  - This means **1/8 of layers are softmax attention** and **7/8 are lightning attention** (Section 2; also echoed in Section 7 limitations/future work).
- Attention and embedding hyperparameters are explicitly given:
  - **Heads:** 64 attention heads, **head dimension:** 128 (Section 2).
  - Softmax attention uses **GQA** (grouped-query attention) with **group size 8** (Section 2).
  - **RoPE** (rotary position embeddings) is applied to **half** the attention head dimension, with base frequency **10,000** (Section 2).
- The model‚Äôs width and MoE configuration are:
  - **Hidden size:** 6144 (Section 2).
  - **Experts per MoE layer:** 32 experts, **top-2 routing** per token (Section 2; Eq. (1); Figure 3).
  - **FFN hidden dimension per expert:** 9216 (Section 2).
  - **Total parameters:** 456B; **activated per token:** 45.9B (Abstract; Section 2).

#### 3.4.2 MoE mechanism and ‚Äúglobal router‚Äù stabilization

- In MoE, each token is routed to only a small subset of experts, reducing **active compute** while increasing **total capacity**.
- The per-token MoE output is defined in Eq. (1) as a weighted sum over experts, but only the `TopK` experts contribute because non-top experts are set to \(-\infty\) before softmaxing routing scores (Eq. (1)).
- The paper adopts a **token-drop** MoE training strategy:
  - Each expert has a **capacity limit**; tokens beyond capacity are **dropped** for that expert, improving training efficiency but introducing a stability risk if drop rates are high (Section 2.1).
- To reduce routing collapse and load imbalance, the paper adds:
  - A `GShard`-style **auxiliary loss** encouraging balanced expert usage, \(L_{aux} = \alpha_{aux}\cdot \frac{1}{E}\sum_i f_i m_i\), where \(f_i\) is fraction of tokens assigned to expert \(i\) and \(m_i\) is mean routing probability (Section 2.1).
  - A **global router** mechanism: because micro-batch sizes are constrained by memory, token distributions fluctuate within and across Expert Parallel groups; they add an **allgather** step to synchronize ‚Äútokens waiting per expert‚Äù before dispatching across EP groups, reducing token drop and stabilizing training (Section 2.1).

#### 3.4.3 Linear attention vs softmax attention, and why causal LMs are tricky

- Softmax attention computes \((QK^\top)V\) with a full \(n\times n\) attention matrix, which is quadratic in sequence length \(n\) (Figure 5; Eq. (10)).
- Linear attention rewrites attention to avoid forming the full attention matrix by associating multiplication as \(Q(K^\top V)\), enabling linear-in-\(n\) behavior when \(d \ll n\) (Section 2.2; Eq. (2)‚Üí(3); Figure 5).
- However, for **causal** language modeling, linear attention often needs a sequential `cumsum`-style dependency (the paper cites this as a bottleneck and adoption barrier), which harms parallelism (Section 2.2).

#### 3.4.4 Lightning attention: the key algorithmic implementation idea

- `Lightning attention` is presented as an **I/O-aware** implementation of a TransNormer linear attention variant, designed to avoid the slow `cumsum` dependency in causal settings (Section 2.2.1).
- The core trick is to partition computation into:
  - **Intra-block** causal attention computed via the ‚Äúleft product‚Äù form \([(QK^\top)\odot M]V\) within a small block (Eq. (4); Eq. (7); Algorithm 1).
  - **Inter-block** contributions computed via a running summary \(KV = K^\top V\) (a ‚Äúright product‚Äù form), which can be updated block-by-block (Eq. (5), Eq. (9); Algorithm 1).
- Algorithm 1 makes the dataflow explicit:
  1. Split \(Q,K,V\) into \(T=n/B\) blocks of size \(B\times d\).
  2. Maintain a running \(KV \in \mathbb{R}^{d\times d}\) initialized to zero.
  3. For each block \(t\):
     - Load \(Q_t,K_t,V_t\) into on-chip SRAM.
     - Compute \(O_{intra}=[(Q_tK_t^\top)\odot M]V_t\) on-chip.
     - Compute \(O_{inter}=Q_t(KV)\) on-chip.
     - Update \(KV \leftarrow KV + K_t^\top V_t\).
     - Write \(O_t = O_{intra}+O_{inter}\) back to HBM.
- The claimed time complexity is \(O(nd^2 + nBd)\) where \(B\) is block size, which is intended to be effectively linear in \(n\) in practice when \(B\) is fixed (Section 2.2.1).

**Worked micro-example (small, concrete walkthrough)**  
To make Algorithm 1 intuitive, consider a toy case with sequence length \(n=8\), feature dimension \(d=2\), and block size \(B=4\). Then there are \(T=2\) blocks: positions 1‚Äì4 and 5‚Äì8.
- **Initialize:** \(KV=\mathbf{0}_{2\times 2}\).
- **Block 1 (tokens 1‚Äì4):**
  - Compute `intra-block` causal attention among tokens 1‚Äì4 using mask \(M\) (so token 3 can attend to 1‚Äì3, etc.).
  - Compute `inter-block` term \(Q_1 KV\), which is zero in the first block because \(KV\) starts at 0.
  - Update \(KV \leftarrow KV + K_1^\top V_1\), which summarizes all (key,value) interactions from tokens 1‚Äì4 into a fixed \(2\times 2\) matrix.
- **Block 2 (tokens 5‚Äì8):**
  - Compute `intra-block` causal attention among tokens 5‚Äì8 (mask only within this block).
  - Compute `inter-block` term \(Q_2 KV\), which injects the effect of tokens 1‚Äì4 into tokens 5‚Äì8 using the summary \(KV\) without explicitly recomputing pairwise interactions across the entire prefix.
  - Update \(KV \leftarrow KV + K_2^\top V_2\) (ready for later blocks if \(n\) were larger).
This illustrates the paper‚Äôs practical goal: keep the expensive ‚Äúwithin-block‚Äù quadratic work confined to small blocks while using a cheap recurrent state \(KV\) to account for long-range prefix influence.

#### 3.4.5 Why hybrid attention is used (retrieval weakness of pure linear attention)

- The paper reports an empirical issue: lightning attention ‚Äúdemonstrates limited retrieval capabilities,‚Äù motivating a hybrid strategy that inserts softmax attention periodically (Section 2.2.2.3).
- In scaling/downstream experiments, lightning attention performs comparably on many tasks but underperforms on **Needle-in-a-Haystack** retrieval (NIAH), while `hybrid-lightning` improves retrieval and extrapolation (Section 2.2.2.3; Figure 7; Table 3; Table 4).
- The hybrid design used in the final model is consistent with this: ‚Äúone transformer block with softmax attention follows every seven transnormer blocks with lightning attention‚Äù (Section 2; Figure 3).

#### 3.4.6 Scaling-law experiments that guide model size choices

- The paper fits scaling laws comparing softmax, lightning, and hybrid-lightning architectures (Section 2.2.2; Figure 6; Table 2).
- Experimental setup for these scaling experiments includes:
  - Model sizes: **70M, 160M, 410M, 1B, 3B, 7B** parameters (Section 2.2.2.1).
  - Training tokens: ‚Äúup to **300B tokens**‚Äù with context length **8192** (Section 2.2.2.1).
  - Batch: **4 million tokens** global batch size (Section 2.2.2.1).
  - Optimizer: **Adam**, learning rate **3e-4**, weight decay **0.1**, and a **fixed LR scheduler** due to constrained resources (Section 2.2.2.1).
- The fitted relationships (Table 2) include:
  - `Softmax`: \(L(C)=3.7087 C^{-0.0798}\)
  - `Lightning`: \(L(C)=3.5391 C^{-0.0768}\)
  - `Hybrid-lightning`: \(L(C)=3.4797 C^{-0.0763}\)
  - and corresponding compute-optimal parameter/token scaling \(N_{opt}(C)\), \(D_{opt}(C)\) (Table 2; Figure 6).
- The paper interprets these fits as: for the same compute budget, lightning/hybrid models tend to use **more parameters and tokens** but achieve **lower loss** than pure softmax (Section 2.2.2.2).

#### 3.4.7 Final model specification rationale and constraints

- A key practical constraint is single-node inference with long context:
  - The paper constrains total parameters to **< 500B** so that **>1M tokens** can be processed ‚Äúon a single machine with up to 8 GPUs and 640GB memory using 8-bit quantization‚Äù (Introduction; Section 2.4).
- They formalize a constrained optimization objective (Eq. (13)) over total parameters \(P_{all}\), active parameters \(P_{act}\), and training tokens \(T\) subject to compute budget and \(P_{all} < 500B\) (Section 2.4).
- Because standard scaling laws extrapolated poorly to a 9.3B-activation regime, they introduce an additional fitted loss model (Eq. (14)) including terms in \(P_{act}\), \(T\), and their product, conditioned on number of experts (Section 2.4).
- This leads to the selected configuration: **456B total / 45.9B active** parameters (Section 2.4).

#### 3.4.8 Distributed training and inference optimizations (why they matter)

The paper argues existing frameworks are optimized for softmax attention and do not support the combined needs of **MoE + lightning attention + million-token contexts**, so they ‚Äúreinvent‚Äù their training and inference framework (Introduction; Section 3).

**MoE communication and parallel strategy (Section 3.1):**
- MoE requires heavy **all-to-all** communication for dispatching tokens to experts and returning outputs, which can dominate runtime at scale (Section 3.1).
- They implement a token-grouping overlap scheme (Figure 9) to overlap all-to-all with expert computation across groups, reducing idle time.
- They introduce specialized process groups:
  - `ETP` (Expert Tensor Parallel) for partitioning expert weights,
  - `EDP` (Expert Data Parallel) for data parallelism of identical experts,
  - `EP` (Expert Parallel) for expert placement and dispatch (Section 3.1; Eqs. (15)‚Äì(16)).
- They emphasize decoupling MoE parallelism from non-MoE parallelism (Section 3.1) and implement `EP-ETP` overlap (Figure 10).
- Quantitatively, they claim these optimizations reduce **pure communication overhead of the MoE component by 50%** versus pre-optimization (Section 3.1).

**Long-context training data formatting (Section 3.2):**
- Long contexts make padding wasteful because real samples have variable lengths.
- They use **data-packing**: concatenating multiple samples end-to-end along the sequence dimension to minimize padding waste (Section 3.2).

**Softmax long-context: varlen ring attention (Section 3.2.1):**
- Ring attention partitions attention across devices to scale to long sequences, but existing implementations are not ideal for packed variable-length (‚Äúvarlen‚Äù) sequences.
- They propose `Varlen Ring Attention` that applies ring attention to the packed sequence while tracking per-sample mask offsets, avoiding the constraint that each sequence must be a multiple of \(2\times\text{CP size}\) (Section 3.2.1; Figure 11).

**Lightning attention long-context: LASP+ (Section 3.2.2):**
- Existing `LASP` sequence parallelism has sequential dependencies across context-parallel ranks due to send/recv prefix accumulation of \(KV\) summaries (Figure 12a).
- `LASP+` removes this serial dependency by:
  1. Each rank computes a **local prefix sum** \(KV_L\),
  2. An **AllGather** synchronizes across ranks,
  3. Each rank computes the needed **global prefix sum** \(KV_G\) and then computes inter-block outputs (Section 3.2.2; Figure 12b).
- They claim the LASP+ compute speed can reach up to **\(1/N_{pcn}\)** of the original LASP time (i.e., near-linear speedup with number of parallel compute nodes), with minimal AllGather overhead (Section 3.2.2).  
  - The paper does not provide a full table of measured wall-clock times here; it reports the relationship qualitatively/empirically.

**Lightning attention inference kernels (Section 3.3):**
- They identify inference-specific challenges: variable-length batches and prefix caching.
- Four inference optimizations are listed:
  - Batched kernel fusion (prefill and decoding fusions),
  - Separate prefill vs decoding kernels and CUDA streams for mixed-length batches,
  - Multi-level padding options (32/64/128/256) to reduce wasted matmul compute with prefix caching,
  - Strided batched matmul optimization via cuBLAS and Hopper-specific kernel work (Section 3.3.1‚Äì3.3.4).
- Reported inference efficiency:
  - They claim **>75% MFU** (Model FLOPs Utilization) end-to-end on **Nvidia H20** (Introduction; Section 3.3.4).
  - At sequence length **1,024,000 tokens**, softmax attention would be **95% of latency**, while lightning attention contributes **<12% latency** under the same conditions (Section 3.3.4).

#### 3.4.9 Data pipeline and training recipe (pre-training ‚Üí long-context extension)

**Pre-training data (Section 4.1):**
- The corpus includes academic literature, books, web content, and code (Section 4.1.1).
- They describe:
  - Rule-based cleaning and deduplication, plus a reward-labeling model (a previous-generation MoE with **5B activations / 60B total parameters**) used to score documents across multiple quality dimensions (Section 4.1.1).
  - They converge to three main quality dimensions: **knowledge depth**, **practical helpfulness**, and **categorical distribution** (Section 4.1.1).
  - They avoid ‚Äúheavy formatting‚Äù (e.g., overly templated markdown) for dialogue/QA to preserve diversity, using a ‚Äúnested document format‚Äù instead (Section 4.1.1).
  - Data mixture balancing: do not eliminate low-quality content entirely because it can hurt downstream performance; instead weight toward high-quality while maintaining diversity (Section 4.1.1).

**Tokenizer (Section 4.1.2):**
- Tokenization is **byte-level BPE** with a pre-tokenizer method.
- Vocabulary size is **200K** tokens, with multilingual upsampling to improve compression efficiency (Section 4.1.2).

**Data experimentation framework (Section 4.1.3):**
- They evaluate data changes as hypothesis tests on distributions of a log-normalized accuracy metric `log accnorm2` defined over multiple-choice likelihoods with byte-normalization to reduce tokenizer effects (Section 4.1.3.1).
- Their efficient experimental unit is: train **MoE models with 1B activation / 8B total parameters** on **40B tokens**, with mixture = **20B web + 20B ‚Äúhypothesis‚Äù data** (Section 4.1.3.1).

**Repetition-aware experiments (Section 4.1.3.2):**
- They argue naive repetition studies are misleading because data efficiency changes over training.
- They propose: deduplicate globally first, then downsample to match expected repetition frequencies under the final training schedule.
- They report: low-quality data degrades after >2 epochs; high-quality can be trained up to ~4 epochs (Section 4.1.3.2).

**Pre-training hyperparameters for MiniMax-Text-01 (Section 4.2):**
- Initialization: Xavier initialization (Section 4.2).
- Normalization scaling: DeepNorm factors \(\alpha=(2N)^{0.25}\), \(\beta=(8N)^{-0.25}\) where \(N\) is number of layers (Section 4.2).
- Optimizer: **AdamW** with \(\beta_1=0.9\), \(\beta_2=0.95\), weight decay **0.1** (Section 4.2).
- Sequence length: **8192** initially (Section 4.2).
- Batch size schedule (tokens):
  - **16M** ‚Üí **32M** at **69B tokens**,
  - **64M** at **790B tokens**,
  - **128M** at **4.7T tokens**, then stays at 128M (Section 4.2; Figure 13 shows the ‚Äúcritical batch size‚Äù power-law rationale).
- Learning rate schedule:
  - 500-iteration linear warmup to peak **2√ó10‚Åª‚Å¥**,
  - constant LR for **7.2T tokens**,
  - then reduce LR to **1.3√ó10‚Åª‚Å¥** for the remaining **3.2T tokens** due to gradient norm anomalies,
  - then a ‚Äúfast decay‚Äù phase: train **1T tokens** with exponential decay to **3√ó10‚Åª‚Åµ** (Section 4.2).
- MoE auxiliary loss coefficient: **0.01** (Section 4.2).

**Long-context extension (Section 4.2; Table 6; Figure 14):**
- They incrementally extend training context length to **1M tokens** and report extrapolation to **4M tokens** on a ‚Äúvanilla NIAH‚Äù retrieval pressure test (Section 4.2; Figure 14).
- Three-stage long-context recipe (Table 6):
  - Train length **128K**, RoPE frequency **5M**, tokens **300B**, mixture 30% short / 70% medium / 0% long.
  - Train length **512K**, RoPE frequency **10M**, tokens **32B**, mixture 35% short / 35% medium / 30% long.
  - Train length **1M**, RoPE frequency **10M**, tokens **26B**, mixture 30% short / 30% medium / 40% long.
- They add **10% high-quality long-context QA** during the last 20% of training cycles in each stage and linearly interpolate source weights to reduce distribution-shift instability (Section 4.2).

#### 3.4.10 Post-training alignment (SFT + DPO + RL) with long-context adaptation

- They describe a post-training pipeline with:
  - Prompt collection and tagging across many domains (Section 5.1).
  - Reward modeling across **correctness**, **truthfulness**, **helpfulness**, **harmlessness** (Section 5.2).
  - **SFT** with rejection sampling and diversity filters (Section 5.3).
  - **Offline RL** using **DPO** (Section 5.4.1).
  - **Online RL** using a modified **GRPO** with importance sampling clipping, a modified KL term, and balanced advantage estimation for stability (Section 5.4.2).
- Long-context alignment training is staged explicitly (Section 5.6; Table 7):
  - Stage I: SFT at 8192 tokens.
  - Stage II: SFT at 1,032,192 tokens with 50% long-context prompts.
  - Stage III: DPO at 8192 tokens.
  - Stage IV: DPO at 1,032,192 tokens (all long-context).
  - Stage V: Online RL at 8192 tokens.
- Table 7 provides key optimization hyperparameters for each stage:
  - Example: Stage I max LR **1e-5** min LR **1e-6**, cosine decay; Stage II constant LR **3e-6**; Stage III max LR **5e-7** min **5e-8**, cosine; Stage IV constant **5e-7**; Stage V max **1e-6** min **1e-7**, cosine (Table 7).
  - Batch sizes differ: Stage I 128, Stage II 80, Stage III 64, Stage IV 64, Stage V 512 (Table 7).  
    - The paper does not specify here whether these are sequences, microbatches, or token counts; I interpret them as batch-size units in their training framework because Table 7 labels them simply ‚ÄúBatch Size.‚Äù

#### 3.4.11 Vision-language extension: MiniMax-VL-01

- `MiniMax-VL-01` is built by adding:
  - A **ViT-L/14** image encoder with **303M parameters**,
  - A **two-layer MLP projector** (randomly initialized),
  - The `MiniMax-Text-01` LLM backbone (Section 6.2.1‚Äì6.2.2).
- They use a dynamic resolution strategy:
  - Resize to a grid between **336√ó336** and **2016√ó2016** plus a fixed **336√ó336 thumbnail**.
  - Split resized images into **336√ó336 patches**, encode patches and thumbnail separately, then concatenate features (Section 6.2.1).
  - They explicitly avoid pooling/downsampling because the LLM can handle long sequences, so they feed more ‚Äúraw‚Äù visual tokens (Section 6.2.1).

**Vision encoder pretraining (Section 6.2.2):**
- Caption data: **694M** unique image-caption pairs; **180M** refined captions; sample raw/refined with probability 0.5 (Section 6.1.1).
- They train the ViT from scratch and use contrastive learning inspired by CoCa (contrastive + cross-entropy objectives) (Section 6.2.2).
- Training scale: initial training at **224√ó224** for **37B image-caption pairs**, then fine-tune at **336√ó336** for **1.2B pairs**, captions truncated to **76 tokens** (Section 6.2.2).
- They report ImageNet-1K zero-shot classification accuracy **80.55%** at 336√ó336 (Section 6.2.2).

**VLM training stages (Section 6.3):**
- Stage I modality alignment: update vision encoder + adapter using **80B tokens** from description data; images fixed at 336√ó336 (Section 6.3).
- Stage II instruction tuning (all parameters trainable): **420B multimodal tokens** + text post-training data mixed at **20:1** ratio (Section 6.3).
- Stage III user experience: **44.8B multimodal tokens**, 1 epoch (Section 6.3).
- Stage IV preference tuning: DPO with **40,000 image-text pairs** plus ‚Äúa significant proportion of pure text pairs,‚Äù and early stopping to avoid overfitting (Section 6.3).

**Note on token counts:** The abstract claims ‚Äúcontinued training with **512B vision-language tokens**,‚Äù while Section 6.3 enumerates 80B + 420B + 44.8B = 544.8B multimodal tokens before Stage IV; the paper does not reconcile this discrepancy in the provided text, so I treat 512B as the headline claim and the stage totals as the detailed accounting (Abstract; Section 6.3).

## 4. Key Insights and Innovations

1. **Lightning attention as a practical, I/O-aware linear attention implementation**
   - Novelty: The paper emphasizes not just a linear-attention formula, but an implementation strategy (tiling into intra-/inter-block computation) that avoids the causal `cumsum` bottleneck and is designed around on-chip SRAM vs HBM movement (Section 2.2.1; Algorithm 1).
   - Significance: This is positioned as enabling long contexts ‚Äúspanning millions of tokens‚Äù with feasible training/inference, with inference kernel work achieving >75% MFU on H20 (Introduction; Section 3.3.4).

2. **Hybrid-lightning architecture: mixing sparse softmax layers into a mostly-linear stack**
   - Novelty: The paper explicitly diagnoses retrieval weakness of pure lightning attention and uses periodic softmax to restore retrieval while keeping most layers linear (Section 2.2.2.3; Section 2; Figure 3).
   - Significance: Empirically, hybrid-lightning improves retrieval benchmarks like NIAH and performs strongly on long-context reasoning suites (Figure 7; Tables 9‚Äì10).

3. **MoE scaling with stability mechanisms tuned for real distributed constraints**
   - Novelty: The ‚Äúglobal router‚Äù addresses EP-group token distribution variance due to small microbatches, via an allgather-based global dispatch step (Section 2.1).
   - Significance: This targets a practical failure mode (routing collapse / token drop) in large MoEs and is integrated with custom EP/ETP parallelism (Sections 2.1, 3.1).

4. **End-to-end long-context systems redesign (varlen ring attention + LASP+)**
   - Novelty: The paper adapts long-context parallel algorithms to packed variable-length data and removes serial dependencies in LASP via AllGather-based parallel prefix sums (Section 3.2.1‚Äì3.2.2; Figures 11‚Äì12).
   - Significance: These are the enabling components for training/inference at 1M tokens without extreme padding waste and without serial bottlenecks.

5. **A concrete long-context training curriculum to 1M tokens, with 4M extrapolation evidence**
   - Novelty: The paper provides an explicit staged recipe (length ranges, RoPE frequencies, token counts, mixture percentages) and reports 4M-token NIAH pressure test success despite 1M-token training (Section 4.2; Table 6; Figure 14).
   - Significance: This connects architecture choices (half-RoPE, hybrid attention) to a reproducible long-context training procedure.

## 5. Experimental Analysis

### 5.1 Evaluation methodology: datasets, metrics, baselines

**Architecture/scaling experiments (Section 2.2.2):**
- Benchmarks: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC (easy/challenge), OpenBookQA, NIAH, SCROLLS (Section 2.2.2.1).
- Comparison axes:
  - Loss-vs-compute scaling fits (Figure 6; Table 2).
  - Downstream task performance, notably retrieval (NIAH) (Figure 7).
  - Training speed measured as `TGS` (tokens per GPU per second) (Figure 8; Table 3; Table 4).

**Text model main benchmarks (Section 5.7):**
- Core: MMLU, MMLU-Pro, SimpleQA, C-SimpleQA, IFEval, Arena-Hard, GPQA diamond, DROP, GSM8k, MATH, MBPP+, HumanEval (Section 5.7.1; Table 8).
- Settings: greedy decoding + ‚Äúzero-shot chain-of-thought‚Äù strategy; starred tasks in Table 8 evaluated with 0-shot CoT (Section 5.7.1; Table 8 note).
- Long-context: RULER (13 tasks) up to 1M tokens; LongBench-V2 in short/medium/long regimes, with and without CoT (Section 5.7.2.2; Tables 9‚Äì10).
- Retrieval-focused long-context: MR-NIAH English/Chinese, adjusted recall metric (Section 5.7.2.1; Figure 15).

**Vision-language model benchmarks (Section 6.4):**
- MMMU/MMMU-Pro, ChartQA, DocVQA, OCRBench, AI2D, MathVista, OlympiadBench, MMLongBench-Doc, MEGA-Bench, and an in-house benchmark (Section 6.4; Table 13).

### 5.2 Main quantitative results (with specific numbers)

**Core text benchmarks (Table 8; Figure 1a):**
- `MiniMax-Text-01` reports:
  - MMLU **88.5**
  - MMLU-Pro **75.7**
  - C-SimpleQA **67.4**
  - IFEval **89.1**
  - GPQA diamond **54.4**
  - MATH **77.4**
  - HumanEval **86.9**
  (Table 8; also summarized in Figure 1a‚Äôs bar chart)

**Long-context reasoning/understanding (RULER; Table 9; Figure 1c):**
- `MiniMax-Text-01` on RULER average accuracy across 13 tasks:
  - 64k: **0.943**
  - 128k: **0.947**
  - 256k: **0.945**
  - 512k: **0.928**
  - 1M: **0.910**
  (Table 9)
- Table 9 also shows competing models degrade at longer lengths where reported (e.g., Gemini-1.5-Pro drops to 0.850 at 1M; Gemini-2.0-Flash drops to 0.709 at 512k). The paper uses this to argue robustness at 1M (Section 5.7.2.2).

**LongBench v2 (Table 10):**
- With CoT: `MiniMax-Text-01` overall **56.5**, easy **66.1**, hard **50.5**, short **61.7**, medium **56.7**, long **47.2** (Table 10).
- Without CoT: overall **52.9**, easy **60.9**, hard **47.9**, short **58.9**, medium **52.6**, long **43.5** (Table 10).

**Hybrid-lightning vs alternatives on retrieval/speed (Tables 3‚Äì4; Figure 7):**
- Table 3 (1B parameter benchmarking of hybrid-linear variants):
  - `Hybrid-lightning` achieves `TGS` **33.4K** and `NIAH` **95.7**, outperforming hybrid-cosformer2 (`NIAH` 43.6) and hybrid-hgrn2 (`NIAH` 91.8) under the shown settings (Table 3).
- Table 4 compares `Hybrid-lightning` to `Hybrid-window` baselines:
  - For 1B, `Hybrid-lightning` has `NIAH` **95.7** vs hybrid-window 256/512/1024 giving **46.8 / 25.7 / 53.9** (Table 4).
  - For 3B, `Hybrid-lightning` has `NIAH` **98.0** vs hybrid-window **40.9 / 57.9 / 41.6** (Table 4).
- These results are used to argue hybrid-lightning is both fast and retrieval-strong relative to their tested baselines (Section 2.2.3; Section 2.2.4).

**MoE module ablations (Table 5):**
- In a 28B total / 5B active MoE, swapping many softmax layers for hybrid-lightning improves several metrics (e.g., BBH 28.2‚Üí32.2; DROP 27.4‚Üí29.0; MATH 4.6‚Üí6.8; WG 65.6‚Üí67.5) though CMMLU decreases slightly (47.3‚Üí46.0) (Table 5).
- For normalization in a 60B total / 9.3B active model:
  - Post-LN with DeepNorm outperforms Pre-LN across listed metrics (e.g., MMLU 43.9‚Üí50.2; CMMLU 41.8‚Üí49.2) (Table 5).

**Inference latency and efficiency (Figure 2; Section 3.3.4):**
- Figure 2 compares prefilling latency vs context length. The paper states:
  - `MiniMax-Text-01` and Llama3-70B are tested on **H800** with tensor parallelism 8 and **8-bit weight-only quantization (W8A16)**; others via APIs.
  - Data is fit with a quadratic function after outlier removal (Figure 2 caption).
- Section 3.3.4 claims at 1,024,000 tokens:
  - softmax attention would be 95% of latency, lightning attention <12% (Section 3.3.4).

**Vision-language benchmarks (Table 13; Figure 1b):**
- `MiniMax-VL-01` reports:
  - MMMU **68.5**
  - MMMU-Pro **52.7**
  - ChartQA **91.7**
  - DocVQA **96.4**
  - OCRBench **865**
  - AI2D **83.3**
  - MathVista **68.6**
  - OlympiadBench **24.2**
  - MMLongBench-Doc acc **32.5**
  - MEGA-Bench macro **47.4**
  (Table 13; Figure 1b summarizes a subset)

### 5.3 Do experiments support the claims?

- **Supportive evidence**
  - The paper provides consistent evidence that hybrid-lightning improves long-context retrieval relative to pure linear attention (Section 2.2.2.3; Figure 7; Tables 3‚Äì4).
  - Long-context benchmarks (RULER up to 1M, LongBench-V2) show strong performance at high lengths, with explicit numbers (Tables 9‚Äì10).
  - Systems claims are partially quantified: 50% MoE communication overhead reduction (Section 3.1) and >75% MFU on H20 (Section 3.3.4), plus a detailed algorithmic description (Algorithm 1, LASP+ Figure 12).

- **Where evidence is thinner or hard to verify from the provided content**
  - The paper claims ‚Äúmatch the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet‚Äù (Abstract), but ‚Äúmatching‚Äù depends on which tasks and evaluation settings; Table 8 shows mixed results (e.g., MMLU is close, but GPQA is lower than Claude-3.5-Sonnet; HumanEval is lower than multiple baselines). So the claim is directionally plausible but not uniformly true across all metrics.
  - Many systems optimizations are described qualitatively; outside of a few headline numbers (50% overhead reduction; MFU >75%; latency composition at 1,024,000 tokens), there is limited detailed throughput/latency breakdown for full end-to-end training at 1M tokens in the excerpt.

### 5.4 Ablations, failure cases, robustness checks

- Ablations include:
  - MoE vs dense isoflop comparison (Figure 4).
  - Softmax vs lightning vs hybrid-lightning scaling laws (Table 2; Figure 6).
  - Downstream performance showing retrieval weakness of lightning attention (Figure 7).
  - Hybrid-linear alternatives (hybrid-cosformer2, hybrid-hgrn2) (Table 3).
  - Hybrid-window baselines (Table 4).
  - MoE module ablations: hybrid attention substitution and normalization choice (Table 5).
- Failure/limitation explicitly noted:
  - Lightning attention shows **limited retrieval** capability, which is why pure linear attention is deemed unsuitable alone for LLMs (Section 2.2.2.3; Section 2.2.4).

## 6. Limitations and Trade-offs

- **Residual dependence on softmax attention**
  - The model keeps **1/8 softmax attention layers** (Section 2; Figure 3), and the conclusion explicitly calls out ongoing work to eliminate softmax entirely to enable ‚Äúunlimited context windows without computational overhead‚Äù (Section 7).

- **Retrieval limitations of pure linear attention**
  - The paper finds lightning attention alone is weaker at retrieval tasks like NIAH (Section 2.2.2.3; Figure 7). This necessitates a hybrid design, which reintroduces some quadratic-cost components (though amortized by sparsity in layer placement).

- **MoE training trade-offs**
  - Token-drop MoE improves efficiency but can drop routed tokens when experts hit capacity, which can destabilize training; they mitigate via global routing (Section 2.1), but token-drop inherently introduces an approximation trade-off.

- **System complexity and specialization**
  - Achieving the reported results requires a substantial bespoke stack:
    - New process groups (EP/ETP/EDP), overlap scheduling, custom attention parallelism (varlen ring attention, LASP+), and CUDA inference kernels (Section 3).
  - This increases engineering complexity and may reduce portability to generic frameworks, which the paper itself notes are currently insufficient (Section 3).

- **Evaluation limitations for ‚Äúrealistic‚Äù long-context reasoning**
  - The paper argues existing long-context retrieval evaluations are often artificial/simplified, and long-text reasoning evaluation in practical document analysis is limited; it identifies this as future work (Section 7).

- **Uneven capability profile**
  - The paper acknowledges limitations in ‚Äúcomplex programming tasks‚Äù due to limited coding data in pretraining (Section 7).
  - For VLM, it notes struggles with advanced math reasoning (OlympiadBench) and gaps in some MMLongBench-Doc subsets (Section 6.4 narrative).

## 7. Implications and Future Directions

- **Field-level implication: million-token contexts become a first-class design point**
  - The paper demonstrates an end-to-end recipe‚Äîarchitecture + distributed runtime + training curriculum‚Äîthat targets **1M-token training** and **4M-token inference extrapolation** (Introduction; Section 4.2; Figure 14; Table 6). If reproducible broadly, this shifts long-context modeling from ‚Äúonly quadratic attention with bigger GPUs‚Äù toward hybrid/linear approaches.

- **Architecture direction: reducing or removing softmax attention**
  - The explicit future direction is to eliminate the remaining 1/8 softmax layers while retaining retrieval ability (Section 7). This suggests research into linear attention variants with better retrieval, or alternative mechanisms that restore retrieval without quadratic cost.

- **Systems direction: long-context kernels and parallelism as core research**
  - The paper‚Äôs LASP+ and varlen ring attention suggest that long-context capability is as much about **communication patterns** and **variable-length packing** as it is about attention math (Section 3.2; Figures 11‚Äì12). Follow-up work could generalize these primitives into reusable libraries.

- **Data/training direction: explicit long-context curricula and monitoring**
  - They argue NIAH saturates early and is inadequate for monitoring long-context progress, motivating more demanding long-context tasks and staged curricula (Section 4.2; Section 5.7.2). This implies future benchmarks and training monitoring approaches that better track incremental long-context reasoning gains.

- **Practical applications / downstream use**
  - Based on the paper‚Äôs benchmark focus and in-house evaluations, intended applications include:
    - Long document QA and reasoning (RULER, LongBench-V2; Tables 9‚Äì10),
    - Long dialogue memory retrieval (MR-NIAH; Figure 15),
    - Multi-document and structured long inputs (LongBench-V2 categories; Section 5.7.2.2),
    - Multimodal document understanding and OCR-heavy tasks (DocVQA, OCRBench, MMLongBench-Doc; Table 13).

- **Repro/Integration Guidance (based on what‚Äôs provided here)**
  - When to prefer this approach (within the paper‚Äôs framing):
    - If you need **hundreds of thousands to millions of tokens** of context and want the model itself (not an external retriever) to handle long-range inputs efficiently, the hybrid-lightning design is the central mechanism (Section 2; Section 3; Table 9).
    - If your task is retrieval-heavy, the paper‚Äôs own evidence suggests avoiding pure linear attention and using a hybrid that includes periodic softmax layers (Section 2.2.2.3; Figure 7).
  - What you must be prepared to implement:
    - Long-context efficiency depends on **data packing**, **varlen attention implementations**, and **sequence-parallel prefix-sum strategies** (Section 3.2), not just swapping an attention module in a standard trainer.



# Training Language Models to Self‚ÄëCorrect via Reinforcement Learning

**ArXiv:** [2409.12917](https://arxiv.org/abs/2409.12917)
**Authors:** Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co‚ÄëReyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, Aleksandra Faust
**Institutions:** 

## üéØ Pitch

The paper introduces SCoRe (Self-Correction via Reinforcement Learning), an innovative two-stage RL method that empowers language models to autonomously correct their own outputs without external feedback, addressing the intrinsic self-correction problem. By rewarding progress from initial to revised attempts, SCoRe significantly enhances self-correction in complex tasks like math and code, demonstrating a leap in model autonomy and efficiency, crucial for applications where external validation is elusive.

---

## 1. Executive Summary
This paper introduces SCoRe (Self-Correction via Reinforcement Learning), a two-stage, multi-turn RL method that teaches a single language model to revise its own answers without external feedback. It overcomes well-documented failures of intrinsic self-correction by training on the model‚Äôs own trajectories and by explicitly rewarding ‚Äúprogress‚Äù from a first attempt to a corrected second attempt. On math (MATH) and code (HumanEval), SCoRe yields substantial self-correction gains‚Äîe.g., a +15.6 percentage point improvement in the correction delta on MATH relative to the base model (Table 2).

## 2. Context and Motivation
- Problem addressed
  - Intrinsic self-correction: getting a model to detect and fix its own mistakes at test time without external signals (no test cases, no teacher model, no labels at inference). Defined in the paper as revising a first attempt into a better second attempt purely from the previous text (Section 1; Figure 2 shows examples).
  - Why it‚Äôs hard: prior studies show modern LLMs often do not meaningfully improve their answers on a second try and sometimes make them worse (Section 1; also surveyed in Related Work, Section 2).

- Importance
  - Practical: many real tasks (math proofs, code synthesis) require revising an initial (often wrong) attempt using only internal reasoning cues.
  - Scientific: tests whether LLMs can learn meta-strategies that use their own outputs as informative context, beyond single-pass generation.

- Where prior approaches fall short
  - Prompting-based self-correction often fails or regresses performance in intrinsic settings (Section 2). For instance, using only prompts typically does not beat simply sampling more answers in parallel (Figure 1, right).
  - Supervised fine-tuning on self-generated traces (e.g., STaR; Pair-SFT) struggles with:
    - Distribution shift: models learn to correct errors made by the base model used to create the dataset but fail to correct their own new errors at test time (Figure 5).
    - Behavior collapse: models learn to either barely edit or not edit at all on the second attempt, choosing a ‚Äúbest first attempt‚Äù strategy instead of true revision (Figure 4a; Section 4).
  - Multi-turn RL without special care also collapses to ‚Äúdon‚Äôt change the answer‚Äù (Figure 6), showing the need for additional regularization.

- Positioning
  - SCoRe differs by using on-policy, two-stage RL with progress-based reward shaping to directly train the self-correction behavior on the model‚Äôs own distribution, avoiding both distribution shift and collapse (Sections 5.1‚Äì5.2; Figure 7).

## 3. Technical Approach
At a high level, the model makes two sequential attempts per problem:
- Turn 1: produce an initial answer `y1` to problem `x`.
- Turn 2: read `x` plus `y1` (and a generic self-correction instruction) and produce a revised answer `y2`.

Key terms
- On-policy data: training samples generated by the current policy rather than a fixed dataset.
- KL-divergence penalty: a regularization that keeps the fine-tuned policy close to a reference policy (here, typically the base model).
- Reward shaping: modifying the RL reward to emphasize desired behaviors (here, improvement from `y1` to `y2`).

3.1 Formalization (Section 3)
- Goal: maximize total reward across turns (Equation 1). You can think of reward as ‚Äúcorrect or not‚Äù returned by an automatic checker (an ‚Äúoracle reward‚Äù is available for training but not at test time).
- Base RL building block: REINFORCE with a KL penalty to a reference policy (Equation 2; Ahmadian et al., 2024). This is standard RLHF-style optimization for one-shot outputs.

3.2 Why na√Øve multi-turn RL collapses (Section 5; Figure 6)
- Two degenerate solutions can reach similar training rewards:
  1) Learn to truly improve from turn 1 to turn 2.
  2) Learn to make turn 1 as good as possible and then avoid changing at turn 2.
- Overparameterized models often choose (2), producing tight coupling between turns (Figure 6a) and rarely changing their answers (Figure 6b).

3.3 SCoRe: a two-stage RL procedure (Figure 7)
- Stage I: Train an initialization that decouples the two turns (Section 5.1; Equation 3).
  - Optimize only the second-turn reward, while forcing the first-turn distribution to remain close to the base model via a strong KL penalty (Œ≤2 in Equation 3).
  - Intuition: ensure the model explores varied second-turn edits given imperfect first attempts, rather than converging to ‚Äúdo it all in turn 1.‚Äù Figure 6b shows this increases the frequency of meaningful second-turn changes.
- Stage II: Joint multi-turn RL with progress-based shaping (Section 5.2; Equation 4).
  - Optimize rewards for both turns with a standard KL penalty (Œ≤1), but add a bonus at turn 2 that rewards improvement relative to turn 1:
    - Shaped bonus bÃÇ(y2|y1) = Œ± ¬∑ (r(y2) ‚àí r(y1)); Œ± > 1 (they use Œ± = 10; Table 5).
    - If a wrong first answer becomes correct, the bonus is positive; if a correct first answer becomes wrong, the bonus is strongly negative.
  - Effect: directly incentivizes the self-correction move itself rather than just the final correctness.
- Practicalities (Section 5.3)
  - Sampling: on-policy, with occasional inclusion of first-turn samples from the base model to broaden coverage when data is limited.
  - Training details: Gemini 1.5 Flash for MATH; Gemini 1.0 Pro for code; standard RL hyperparameters (Table 5).

Analogy
- Think of a two-draft writing process. Stage I teaches the model to write a second draft that meaningfully revises the first, while keeping the first draft ‚Äúimperfect‚Äù (close to the base). Stage II then rewards actual improvements from draft 1 to draft 2 so the model learns when and how to revise, rather than trying to write a perfect first draft every time.

## 4. Key Insights and Innovations
- A diagnosis of why SFT-based self-correction fails (Section 4)
  - Novel analysis shows two failure modes on self-generated data: distribution shift and behavior collapse.
  - Evidence: Pair-SFT and STaR either make few edits (Figure 4a), overfit to base-model mistakes (Figure 5), or even degrade second-turn accuracy (Table 1).
  - Significance: clarifies why ‚ÄúSFT on offline traces‚Äù is insufficient for intrinsic self-correction.

- Two-stage RL to decouple attempts and avoid collapse (Sections 5.1‚Äì5.2; Figure 7)
  - Stage I constrains turn-1 to base-like outputs while maximizing turn-2 reward (Equation 3).
  - Stage II adds progress-based shaping to reinforce improvement from turn 1 to 2 (Equation 4).
  - Significance: explicitly trains the meta-strategy ‚Äúrevise your own answer,‚Äù not just ‚Äúproduce a correct answer.‚Äù

- Progress-based reward shaping for multi-turn reasoning (Section 5.2)
  - The bonus Œ±¬∑(r2 ‚àí r1) is simple yet effective: it directly penalizes making a correct answer worse and rewards flipping an incorrect answer to correct.
  - Significance: creates a learning signal targeted at the act of self-correction, which standard multi-turn RL lacks (Figure 6).

- Compute-efficient inference-time scaling via sequential self-correction (Figure 1, right; Section 6.2)
  - For a fixed sample budget, allocating some compute to sequential self-correction (K parallel first attempts, each followed by one correction) beats using all samples in parallel for majority vote.
  - Significance: a practical decoding recipe when compute is capped.

## 5. Experimental Analysis
- Evaluation setup
  - Datasets and splits (Section 6)
    - MATH (Hendrycks et al., 2021): train on an augmented set; evaluate on MATH500 holdout (Section 6; ‚ÄúMATH500‚Äù).
    - Code: train on MBPP; test on HumanEval; also test offline repair on MBPP-R (Section 6; Table 3).
  - Metrics (Section 3)
    - `Accuracy@t1` and `Accuracy@t2`.
    - `Œî(t1, t2)`: net change from first to second turn.
    - `Œî_i‚Üíc`: fraction wrong‚Üíright; `Œî_c‚Üíi`: fraction right‚Üíwrong.
  - Baselines
    - Prompting: Self-Refine (Madaan et al., 2023).
    - SFT: STaR (Zelikman et al., 2022) and a single-model repair-style SFT (‚ÄúPair-SFT‚Äù) (Section 4).
    - RL ablations: single-turn RL; multi-turn RL without Stage I or without shaping (Table 4).
  - Prompts and decoding
    - Zero-shot CoT for MATH; zero-shot for HumanEval; standard 3-shot for MBPP first attempts (Appendix C).
    - Test-time uses greedy decoding unless otherwise specified (Section 6).

- Main results
  - MATH (Table 2; Figure 1, left)
    > SCoRe: Accuracy@t1 = 60.0%, Accuracy@t2 = 64.4%, Œî = +4.4 pp.  
    > Base: Accuracy@t1 = 52.6%, Accuracy@t2 = 41.4%, Œî = ‚àí11.2 pp.
    - Compared to base, SCoRe lifts the self-correction delta by +15.6 points; it also improves both first- and second-turn accuracies.
    - Edit behavior: SCoRe makes meaningful edits (Figure 4a), unlike STaR/Pair-SFT which tend to make minimal changes.
  - HumanEval and MBPP-R (Table 3)
    > SCoRe: HumanEval Accuracy@t2 = 64.6%, Œî = +12.2 pp (vs base Œî = +3.0 pp).  
    > Offline code repair (MBPP-R): SCoRe = 60.6% (vs base 47.3%).
    - Notably, Pair-SFT matches SCoRe on MBPP-R (59.8% vs 60.6%) but harms self-correction on HumanEval (Œî = ‚àí1.8 pp), underscoring the need for on-policy training for intrinsic self-correction.
  - Inference-time compute scaling (Figure 1, right; Section 6.2)
    > With a 32-sample budget, parallel sampling gives +7.4% accuracy, whereas K-parallel + 1 sequential correction gives +10.5%.
    - Sequential self-correction yields better use of compute than ‚Äújust more parallel samples.‚Äù

- Do experiments support the claims?
  - Convincing diagnosis: Table 1 shows STaR and Pair-SFT barely improve Œî or even make it negative; Figures 4‚Äì5 visualize conservative editing and distribution-shift failures.
  - Necessity of SCoRe components: Table 4 ablations
    > Without Stage I: Œî halves from +4.4 to +2.2; without reward shaping: Œî drops to +2.6; single-turn training: Œî becomes ‚àí2.4.
  - RL baseline collapse: Figure 6 demonstrates multi-turn RL without SCoRe‚Äôs design still collapses to ‚Äúdon‚Äôt change the answer.‚Äù
  - Robustness checks
    - Multiple attempts beyond two: SCoRe maintains or slightly improves past two turns, whereas baselines plateau or degrade (Appendix A.1; Figure 8).
    - Alternative RL return discounting (Œ≥ = 0.8) does not fix collapse without SCoRe‚Äôs shaping (Appendix A.2; Figure 9).

- Failure modes and trade-offs observed
  - Even with SCoRe, improvement is concentrated in the second turn; scaling to many turns shows diminishing returns (Appendix A.1).
  - Shaping and Stage I are critical; removing either reduces Œî (Table 4).

## 6. Limitations and Trade-offs
- Reliance on an automatic reward at training time (Section 3)
  - The method assumes access to a binary verifier‚Äîe.g., exact-answer match for math or passing all tests for code. Such verifiers may be unavailable or imperfect in some domains.
- Trained for two turns (Section 7; Appendix A.1)
  - Infrastructure limited training to a single round of correction; benefits for more turns are modest without dedicated training for more rounds.
- Compute and data considerations
  - RL with on-policy sampling is more compute-intensive than SFT. Stage I and II introduce two RL phases.
  - Reward shaping hyperparameters (e.g., Œ±, Œ≤1, Œ≤2; Table 5) require tuning; poor settings could under- or over-couple turns.
- Generality of tasks and verifiers
  - Experiments focus on math and coding with clear verifiers; it remains open how well SCoRe transfers to domains with fuzzy or subjective correctness.
- Degenerate strategies still exist in principle
  - Without the two-stage design and shaping, multi-turn RL drifts to non-correcting behavior (Figure 6). SCoRe mitigates but does not theoretically eliminate all degeneracies.

## 7. Implications and Future Directions
- What changes in the field
  - Self-correction becomes a trainable skill rather than a property that emerges from prompting. SCoRe demonstrates that explicit, on-policy, progress-aware RL is key to teaching models to revise themselves.
  - This reframes multi-turn generation tasks: train the improvement step directly, not just the final output.

- Practical applications
  - Math assistance, code synthesis/repair, and any domain where models must re-evaluate their own outputs without external feedback (e.g., drafting legal or technical documents with internal consistency checks).
  - Inference-time compute allocation: for a fixed budget, ‚Äúparallel + sequential‚Äù decoding can outperform pure parallel majority voting (Figure 1, right).

- Research avenues
  - Multi-round training: extend beyond two attempts to learn longer self-improvement chains (Section 7 suggests this as future work).
  - Unifying stages: a single training scheme that both decouples turns and rewards progress could simplify the pipeline (Section 7).
  - Beyond binary verifiers: develop learned or weak verifiers and adapt SCoRe to noisy/partial feedback.
  - Broader domains and richer progress signals: apply to tool-using agents, retrieval-augmented generation, or planning tasks where ‚Äúprogress‚Äù can be measured along interpretable intermediate states.

In sum, SCoRe‚Äôs core idea‚Äîuse on-policy, two-stage RL with a progress reward to train self-correction explicitly‚Äîboth diagnoses why prior methods fail (Figures 4‚Äì6; Table 1) and delivers clear empirical gains on math and code (Tables 2‚Äì3), while outlining a practical recipe for leveraging self-correction at inference time (Figure 1, right).

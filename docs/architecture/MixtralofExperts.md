# Mixtral of Experts

**ArXiv:** [2401.04088](https://arxiv.org/abs/2401.04088)

## ğŸ¯ Pitch

Mixtral 8x7B is a groundbreaking sparse Mixture-of-Experts (SMoE) language model that routes each token to only 2 out of 8 experts per layer, activating just 13B parameters per token out of 47B total. This innovative architecture achieves or surpasses the performance of much larger dense models like Llama-2-70B and GPT-3.5 across key benchmarksâ€”including code, mathematics, and multilingual tasksâ€”while delivering drastically lower inference costs. By providing state-of-the-art open weights and efficient deployment tools, Mixtral paves the way for more accessible, scalable, and cost-effective advanced language AI.

---

## 1. Executive Summary
Mixtral 8x7B is a sparse Mixtureâ€‘ofâ€‘Experts (SMoE) transformer that replaces each feedâ€‘forward block with 8 experts and routes each token to only 2 of them per layer. By activating 13B parameters per token while holding 47B total (â€œsparseâ€) parameters, it matches or beats much larger dense models (notably Llamaâ€‘2â€‘70B and GPTâ€‘3.5) on many benchmarks, especially code, math, and multilingual tasks (Table 2, Table 3, Table 4). This matters because it delivers nearâ€‘frontier capability at significantly lower inference compute.

## 2. Context and Motivation
- Problem gap
  - Scaling language models has traditionally required activating all parameters for every token, making inference costly. This limits deployability and throughput, especially for long contexts and interactive systems.
  - Conditional computationâ€”only using some parameters per tokenâ€”has been explored in earlier Mixtureâ€‘ofâ€‘Experts systems, but there has not been an openâ€‘weights model that simultaneously delivers competitive general performance, strong math/code/multilingual ability, 32kâ€‘token context, and practical inference tooling.
- Why it matters
  - Realâ€‘world: Lower compute per token translates to faster responses at low batch sizes and higher throughput at large batch sizes (Abstract; â€œSize and Efficiency,â€ p. 4), enabling costâ€‘effective deployment.
  - Scientific: Demonstrates that SMoE can scale capability without scaling perâ€‘token compute, and provides routing analyses that inform future MoE design (Section 5, pp. 6â€“8).
- Prior approaches and shortcomings
  - Dense transformers (e.g., Llamaâ€‘2â€‘70B) activate all parameters per tokenâ€”high quality but expensive.
  - Prior routed/MoE models (e.g., GShard) used conditional computation but did not replace every FFN block with MoE and/or were not available as highâ€‘quality open weights. Mixtral positions itself as a fully open, highâ€‘performing SMoE with efficient kernels and deployment integrations (Abstract; Section 2; â€œCodeâ€ and â€œWebpageâ€ links).
- Positioning
  - Mixtral uses the Mistral 7B architecture as a base but replaces FFNs with MoE and supports a fully dense 32k context (Section 2, Table 1). It aims for bestâ€‘inâ€‘class open performance with only 13B active parameters, demonstrating outsized gains on math, code, and multilingual tasks (Abstract; Table 2; Table 4).

## 3. Technical Approach
At a high level, Mixtral is a decoderâ€‘only transformer where each layerâ€™s feedâ€‘forward network (FFN) is replaced by a Mixtureâ€‘ofâ€‘Experts (MoE) block.

Key components and how they work:
- Sparse Mixtureâ€‘ofâ€‘Experts (SMoE)
  - Definition: An MoE layer contains multiple alternative subâ€‘networks (â€œexpertsâ€). A small â€œrouterâ€ chooses which experts should process each token, so only a subset run per token.
  - In Mixtral, â€œeach layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs.â€ (Abstract; Figure 1)
  - The gating function: compute logits `x Â· Wg`, keep the Topâ€‘K entries (here K=2), apply a Softmax over those K values to get weights, then compute the weighted sum of the selected expertsâ€™ outputs (Section 2.1). In notation (p. 2â€“3):
    - `G(x) := Softmax(TopK(x Â· Wg))`
    - `y = Î£_i Softmax(Top2(x Â· Wg))_i * SwiGLU_i(x)`
  - Why it reduces compute: with 8 experts per layer but K=2, only 2 expert FFNs run per token per layer, so active compute scales with K, not with total experts (Section 2.1).
- Experts
  - Each expert is a standard FFN using `SwiGLU` (a gated MLP activation variant) as in a vanilla transformer (Figure 1; p. 3).
- Router and load balancing
  - The router picks Topâ€‘2 experts per token. Efficient execution uses specialized GPU kernels (Megablocks) that cast MoE FFNs as sparse matrix multiplications and handle variable tokenâ€‘toâ€‘expert assignments (Section 2.1, p. 2â€“3).
  - Distribution across GPUs uses Expert Parallelism (EP): tokens assigned to an expert are shipped to that expertâ€™s device and returned after computation (Section 2.1). EP raises loadâ€‘balancing challenges because some experts may receive many more tokens; Mixtral discusses this explicitly (p. 2â€“3).
- Model size and context
  - Architecture hyperparameters are in Table 1: `dim=4096`, `n_layers=32`, `n_heads=32`, `n_kv_heads=8`, `hidden_dim=14336`, `num_experts=8`, `top_k_experts=2`, `context_len=32768`, `vocab_size=32000`.
  - â€œMixtral supports a fully dense context length of 32k tokensâ€ (p. 2) and successfully retrieves information anywhere in that window (Figure 4, left).
- Parameter accounting
  - â€œSparse parameter countâ€ = total parameters across all experts (â‰ˆ47B in Mixtral).
  - â€œActive parameter countâ€ = parameters used per token (â‰ˆ13B with K=2). This is proportional to inference compute (Section 2.1; â€œSize and Efficiency,â€ p. 4).
- Training and instruction tuning
  - Pretraining: multilingual data; 32k context (Abstract; Section 3.1).
  - Instruction tuning: supervised fineâ€‘tuning (SFT) followed by Direct Preference Optimization (`DPO`, a preferenceâ€‘learning method that aligns outputs to preferred responses) (Section 4).
- Inference stack
  - Optimized with `Megablocks` CUDA kernels and integrated into `vLLM`; deployable via `Skypilot` (Abstract).

Why these design choices:
- K=2 over 8 experts per FFN
  - Strong compute/quality tradeâ€‘off: increases total capacity without increasing perâ€‘token compute beyond two experts (Section 2.1).
- SwiGLU experts
  - A wellâ€‘performing FFN variant used in Mistral 7B; keeps expert definition simple while focusing novelty on routing (Section 2; [18]).
- Dense 32k context
  - Enables longâ€‘document use cases and is validated by retrieval accuracy and perplexity improvements with longer context (Figure 4).

Analogy:
- Imagine each token entering a â€œcommittee meetingâ€ at each layer. There are 8 specialists (experts), but the token only consults the top 2, as chosen by a quick vote (router logits + Topâ€‘2 + Softmax). The final advice is a weighted average of those two specialistsâ€™ recommendations.

## 4. Key Insights and Innovations
- Highâ€‘performing open SMoE at low active compute (fundamental)
  - Novelty: Fully openâ€‘weights SMoE where â€œeach token has access to 47B parameters, but only uses 13B active parameters during inferenceâ€ (Abstract) and yet outperforms or matches much larger dense models (Table 2, Table 3).
  - Significance: Demonstrates stateâ€‘ofâ€‘theâ€‘art open performance with much lower perâ€‘token compute, showing that routed capacity can deliver outsized gains in code and math (Table 2; Figure 3).
- 32k dense context with strong retrieval and better perplexity (incremental but impactful)
  - Evidence: Figure 4 (left) shows â€œ100% retrieval accuracyâ€ on the Passkey task across positions and lengths; Figure 4 (right) shows perplexity on â€œproofâ€‘pileâ€ decreases as context increases.
  - Significance: Validates longâ€‘range reasoning/retrieval ability at large context.
- Strong multilingual capability through scaled capacity and data upsampling (incremental)
  - Evidence: Table 4 shows Mixtral 8x7B beating Llamaâ€‘2â€‘70B on ARCâ€‘C, HellaSwag, and MMLU in French, German, Spanish, and Italian (e.g., French MMLU 70.9% vs 64.3%).
  - Significance: Improved nonâ€‘English utility without sacrificing English performance.
- Router behavior is more syntactic than topical; high temporal locality (new analysis)
  - Evidence: Figure 7 shows expert assignment distributions look similar across varied domains (ArXiv, PubMed, PhilPapers), with only slight divergence for DM Mathematics. Figure 8 highlights tokens like â€œselfâ€ (Python) and â€œQuestionâ€ (English) being routed consistently, suggesting syntactic cues. Table 5 quantifies locality: e.g., â€œfirst choiceâ€ expert repetition between consecutive tokens at layer 15 is 27.9% on ArXiv vs 12.5% random expectation; â€œfirst or second choiceâ€ repetitions reach 62â€“67% across sources (Table 5).
  - Significance: Guides systems design (caching, scheduling) and suggests that expert specialization may emerge around syntax/structure more than topic.

## 5. Experimental Analysis
- Evaluation protocol
  - Benchmarks span commonsense, world knowledge, reading comprehension, math, code, and popular aggregates (Section 3; Figure 2; Figure 3). Datasets include HellaSwag, WinoGrande, PIQA, SIQA, OpenBookQA, ARCâ€‘Easy/Challenge, CommonsenseQA; NaturalQuestions, TriviaQA; BoolQ, QuAC; GSM8K, MATH; HumanEval, MBPP; MMLU, BBH, AGIâ€‘Eval.
  - They reâ€‘evaluate all baselines with a unified pipeline (Figure 2 caption). Noted differences: (1) MBPP uses the handâ€‘verified subset; (2) TriviaQA is evaluated without Wikipedia contexts (p. 5, â€œEvaluation Differencesâ€).
- Main results (selected highlights)
  - Across broad benchmarks vs Llamaâ€‘2â€‘70B (Table 2):
    - `MMLU`: Mixtral 8x7B 70.6% vs 69.9%.
    - `MBPP (pass@1)`: 60.7% vs 49.8%.
    - `GSM8K (8â€‘shot maj@8)`: 74.4% vs 69.6%.
    - `HumanEval`: 40.2% vs 29.3%.
    - Mixtral trails slightly on some commonsense metrics: `HellaSwag` 84.4% vs 85.4%; `WinoGrande` 77.2% vs 80.4%.
  - Against GPTâ€‘3.5 and Llamaâ€‘2â€‘70B (Table 3; different prompt shots than Table 2):
    - `MMLU`: 70.6% (Mixtral) vs 70.0% (GPTâ€‘3.5) vs 69.9% (Llamaâ€‘2â€‘70B).
    - `HellaSwag (10â€‘shot)`: 86.7% vs 85.5% vs 87.1%.
    - `ARCâ€‘Challenge (25â€‘shot)`: 85.8% vs 85.2% vs 85.1%.
    - `MBPP (pass@1)`: 60.7% vs 52.2% vs 49.8%.
    - `GSMâ€‘8K (5â€‘shot)`: 58.4% vs 57.1% vs 53.6%.
    - For the instructionâ€‘tuned variants, MTâ€‘Bench: 8.30 (Mixtralâ€‘Instruct) vs 8.32 (GPTâ€‘3.5â€‘Turboâ€‘1106) (Table 3), and the LMSys Arena shows Elo 1121, beating Claudeâ€‘2.1, Gemini Pro, GPTâ€‘3.5 versions, and Llamaâ€‘2â€‘70Bâ€‘chat (Figure 6).
  - Multilingual (Table 4):
    - Example (Spanish): ARCâ€‘C 55.4% vs 50.5%; HellaSwag 77.6% vs 74.5%; MMLU 72.5% vs 66.0%.
  - Long context (Figure 4):
    - â€œ100% retrieval accuracy regardless of the context length or the positionâ€ on Passkey; perplexity on proofâ€‘pile decreases monotonically with more context.
  - Bias (Section 3.3; Figure 5):
    - â€œBBQ accuracyâ€ 56.0% (Mixtral) vs 51.5% (Llamaâ€‘2â€‘70B).
    - BOLD shows higher average (more positive sentiment) and similar or lower variance (less intraâ€‘group bias).
- Efficiency analysis (p. 4, â€œSize and Efficiencyâ€)
  - Active parameters (13B) drive compute; memory is set by sparse parameters (47B), still below Llamaâ€‘2â€‘70B but higher than a 13B dense model.
  - SMoE introduces extra overhead from routing and memory loads; best for batched workloads where arithmetic intensity is higher.
- Do the experiments support the claims?
  - For code and math, the gains are substantial and consistent across both table settings (Tables 2â€“3; Figure 3). For general knowledge (MMLU), Mixtral is competitive or slightly better. Commonsense tasks are mixed: it wins some but trails on WinoGrande and sometimes HellaSwag depending on shot settings.
  - Long context tests convincingly demonstrate retrieval capability (Figure 4), though realâ€‘task longâ€‘context benchmarks are not reported here.
  - The bias analysis indicates a favorable trend (Figure 5) but is not exhaustive.
- Ablations and diagnostics
  - Section 5 provides a routing analysis rather than classic ablations. Key findings:
    - Expert selection is not strongly domainâ€‘specialized (Figure 7), with limited divergence on DM Mathematics.
    - Strong temporal locality: repeated expert selections across consecutive tokens much higher than randomâ€”e.g., â€œfirst choiceâ€ repetitions at layer 15: 27â€“28% across domains vs 12.5% random; â€œfirst or second choiceâ€ 61â€“67% vs â‰ˆ46% random (Table 5; Figure 10).
  - Implications: locality may cause expert overâ€‘subscription under EP but also enables caching (p. 7).

## 6. Limitations and Trade-offs
- Compute vs memory and overhead
  - While only 13B parameters are activated per token, all 47B parameters must be stored in memory; SMoE routing also introduces overhead and increased memory loads, so utilization is best at larger batches (p. 4, â€œSize and Efficiencyâ€).
- Load balancing in Expert Parallelism
  - Routerâ€‘induced token clustering can overload some experts, creating bottlenecks; the paper flags this as a key systems challenge (p. 2â€“3). The routing analysis (Table 5) shows temporal locality that can exacerbate overâ€‘subscription.
- Mixed performance on certain benchmarks
  - On some commonsense tasks (e.g., WinoGrande) Mixtral lags Llamaâ€‘2â€‘70B (Table 2). Readingâ€‘comprehension plots (Figure 3) suggest Mixtral is not best in that category.
- Evaluation caveats
  - Differences in evaluation protocolâ€”MBPP handâ€‘verified subset; TriviaQA without Wikipedia contextâ€”may affect comparability (p. 5).
- Limited details on training data and techniques
  - Pretraining is described broadly as multilingual with 32k context; granular dataset composition and training schedule are not detailed here.
- Expert specialization
  - Analyses suggest experts do not strongly specialize by topic (Figure 7). While not necessarily negative, it raises questions about how to intentionally induce useful specialization.

## 7. Implications and Future Directions
- Field impact
  - Mixtral shows that routed capacity can rival or exceed large dense models at a fraction of active compute. This is a compelling blueprint for future open models to scale via SMoE rather than pure density.
- Practical applications
  - Costâ€‘effective deployment for longâ€‘context assistants, code generation, math problem solving, and multilingual agentsâ€”helped by vLLM integration and cloud deployment via Skypilot (Abstract).
- Research directions
  - Routing and specialization
    - Explore alternative gating (e.g., expertâ€‘choice routing, loadâ€‘aware gates) to improve balance, reduce overhead, and possibly promote meaningful specialization (Section 2.1; [35]).
    - Leverage temporal locality for expertâ€‘output caching and scheduling (p. 7; [11]).
  - Compute policies
    - Dynamic K (vary number of experts by token or layer) to trade quality/latency adaptively.
  - Longâ€‘context capabilities
    - Move beyond synthetic passkey retrieval to evaluate and train for realâ€‘task longâ€‘context reasoning (document QA, multiâ€‘document retrieval).
  - Safety and bias
    - Extend BBQ/BOLD analysis, combine with preferenceâ€‘modeling (DPO) to steer behavior, and broaden multilingual fairness evaluation (Section 3.3; Section 4).
  - Systems
    - Better EP strategies and kernel optimizations to reduce router overhead and memory traffic; further integration into highâ€‘throughput inference stacks.

Key citations from the paperâ€™s content:
- â€œEach layer is composed of 8 feedforward blocksâ€¦ a router network selects two expertsâ€¦â€ (Abstract; Figure 1).
- â€œEach token has access to 47B parameters, but only uses 13B active parameters during inference.â€ (Abstract).
- â€œMixtral supports a fully dense context length of 32k tokens.â€ (p. 2; Table 1).
- â€œ100% retrieval accuracyâ€ for passkey across positions/lengths (Figure 4 left); decreasing perplexity with longer context (Figure 4 right).
- Strong code/math gains and competitive MMLU (Table 2; Table 3; Figure 3).
- Multilingual gains across FR/DE/ES/IT (Table 4).
- Bias metrics: BBQ 56.0% vs 51.5% (Figure 5).
- Routing locality and lack of strong domain specialization (Figure 7; Figure 8; Table 5; Figure 10).

# LLaVA-CoT: Let Vision Language Models Reason Step-by-Step

**ArXiv:** [2411.10440](https://arxiv.org/abs/2411.10440)

## üéØ Pitch

LLaVA-CoT introduces a novel, stage-structured approach for vision-language models, breaking reasoning down into explicit Summary, Caption, Reasoning, and Conclusion stages with self-aware transitions. This systematic pipeline, reinforced by a new 100k-sample dataset and a stage-wise retracing test-time search, dramatically boosts multimodal reasoning accuracy‚Äîoutperforming even much larger or closed-source models‚Äîand sets a new bar for reliability in tasks like visual math, science QA, and complex chart interpretation.

---

## 1. Executive Summary
LLaVA-CoT proposes a way for vision-language models (VLMs) to reason in an orderly sequence of stages‚ÄîSummary, Caption, Reasoning, and Conclusion‚Äîrather than jumping straight to an answer or producing an unstructured chain of thought. Trained on a 100k-sample dataset with stage-annotated explanations and paired with a stage-wise retracing search at test time, it substantially improves reasoning-heavy multimodal tasks, outperforming larger open models and even some closed models (see Table 5).

## 2. Context and Motivation
- The gap this paper addresses
  - Many VLMs either answer directly or use free-form chain-of-thought, which often drifts, hallucinates, or locks into early mistakes because generation proceeds token-by-token (see Section 1 and Appendix A). The paper distinguishes two missing properties:
    - Systematic: conducting reasoning through distinct stages rather than a single linear chain.
    - Structured: the model explicitly knows which stage it is in and what task to accomplish there.
- Why this matters
  - Visual reasoning underpins practical applications such as chart reading, science QA on diagrams, math with figures, and robust question answering under visual illusions. Better reasoning reduces hallucinations and improves reliability in these domains (Introduction; benchmarks in Section 4).
- Shortcomings of prior approaches
  - Direct-response VLMs underperform on tasks needing multi-step logic [26, 33, 35].
  - CoT prompting helps but remains brittle: models can insert wrong intermediate claims and rationalize them (Introduction; [29, 34, 53]). The ‚Äúchain‚Äù is not organized into semantically coherent stages tied to the task.
  - Test-time search methods (best-of-N, standard beam search) operate at coarse or arbitrary token intervals, not aligned with reasoning stages; they select among full responses or partial token sequences and can get stuck in local optima (Section 2.3).
- Positioning of this work
  - Method: a four-stage, tag-delimited reasoning protocol trained end-to-end; plus a stage-wise retracing search (SWIRES) that can backtrack between stages during inference (Sections 3.1 and 3.2).
  - Data: LLaVA-CoT-100k, a curated 99k multimodal QA set where each sample contains the four structured stages, generated with GPT-4o and filtered for quality (Figure 3, Table 1, Appendix B).
  - Claims: improved accuracy on diverse reasoning benchmarks; visible scaling with more test-time compute; gains concentrate on reasoning-heavy skills (Tables 2‚Äì5; Figure 5).

## 3. Technical Approach
This section explains how LLaVA-CoT is built and how it performs inference.

- Core idea: four tagged reasoning stages
  - The model is trained to emit its thinking in four explicit stages, each wrapped in tags that denote intent:
    - `<SUMMARY>‚Ä¶</SUMMARY>`: outline of how to tackle the problem.
    - `<CAPTION>‚Ä¶</CAPTION>`: a focused description of relevant visual content for the question.
    - `<REASONING>‚Ä¶</REASONING>`: the step-by-step derivation toward an answer.
    - `<CONCLUSION>‚Ä¶</CONCLUSION>`: a concise final answer (Section 3.1.1).
  - Rationale: As the model generates, the tags act like a scaffold. The model ‚Äúknows‚Äù what to do in each stage (organize the task, extract pertinent visual facts, reason, then answer), reducing drift and premature conclusions noted in Appendix A.

- Training data and pipeline (Figure 3; Table 1; Appendix B)
  - Source data: 99k QA pairs drawn from general VQA and science/math-oriented datasets (e.g., ShareGPT4V, ChartQA, A-OKVQA, AI2D, GeoQA+, ScienceQA, DocVQA, CLEVR, CLEVR-Math; Table 1 lists counts).
  - Stage annotation: Each sample is augmented by prompting GPT-4o to produce the four tagged sections; outputs are validated for format and answer correctness, filtering out refusals or mismatches (Appendix B‚Äôs prompts).
  - Why GPT-4o? No existing multimodal model provided this structured staged reasoning out-of-the-box; GPT-4o is used as an annotator to bootstrap the training set (Figure 3).

- Model and supervised fine-tuning (SFT)
  - Base: `Llama-3.2-11B-Vision-Instruct` (Meta, 2024) (Section 3.1.2).
  - Training: full-parameter SFT on the 100k staged data; setup: single node, 8√óH100 GPUs; key hyperparameters in Table 6 (e.g., learning rate 1e-5, 3 epochs, context length 4096).

- Inference-time search: from stage-wise beam to stage-wise retracing (SWIRES) (Section 3.2; Figure 4; Appendix D)
  - Why search at the stage boundary? Traditional majority vote or best-of-N selects from whole responses, and standard beam search checkpoints at token intervals that ignore semantic stages. Here, the search step is the end of a reasoning stage, matching the model‚Äôs internal structure (Section 3.2.1).
  - Stage-wise beam search (Figure 4, middle)
    - At a stage, generate M candidates; score them with a multimodal reward model (`InternLM-XComposer2.5-Reward`, [64]); keep top N to expand in the next stage, generating M/N candidates per survivor so the total remains M.
    - Limitation: local optima‚Äîif, say, the caption is slightly wrong, no amount of downstream reasoning fixes it (Section 3.2.1).
  - Stage-wise retracing search (SWIRES) (Figure 4, right; Algorithm 1 in Appendix D)
    - Mechanism:
      1) At the current stage, generate M candidates and score them.
      2) If at least one candidate exceeds a quality threshold, choose the top N and proceed.
      3) If none clears the bar, ‚Äúretrace‚Äù to the previous stage, regenerate its outputs, and use those to produce fresh candidates for the current stage; merge and repeat up to C retraces.
      4) After the final stage, select the answer with highest reward.
    - Thresholding: a z-scored cutoff computed from reward statistics collected on validation:
      - `backtrack_cutoff = reward_mean + Z * reward_std` (Appendix D).
      - Paper‚Äôs values: `reward_mean = -0.77`, `reward_std = 2.08`, `Z = 0.2533` (Table 8). The Z is chosen so that ‚Äúpass‚Äù roughly targets the top ~40% under a normal approximation.
    - Default search hyperparameters (Table 8): `M=4`, `N=2`, `C=3`. Retracing is applied starting from the caption stage, since summaries are generally reliable (Section 3.2.2).

- A concrete analogy
  - Think of solving a word problem with a diagram:
    - Stage 1 (Summary) writes a mini plan (‚ÄúI‚Äôll list knowns, identify unknowns.‚Äù).
    - Stage 2 (Caption) notes only the relevant diagram facts (‚Äúthe red bar is 40mm tall; the width is 100mm‚Äù).
    - Stage 3 (Reasoning) performs the math or logic.
    - Stage 4 (Conclusion) states the final numeric or option answer.
  - SWIRES is like recognizing ‚Äúmy summary/caption was off; redo that step‚Äù before recomputing the solution.

## 4. Key Insights and Innovations
- Structured, stage-tagged reasoning instead of free-form CoT
  - What‚Äôs new: fixed, semantically meaningful stages with explicit tags produced end-to-end in one pass (Section 3.1.1).
  - Why it matters: encourages correct task decomposition and prevents premature conclusions. Removing tags hurts performance (Table 2: average drops from 62.4 to 60.9 when tags are removed), showing the tags themselves help the model stay organized.

- LLaVA-CoT-100k: a compact, diverse staged-reasoning dataset
  - What‚Äôs new: 99k multimodal QA pairs with full stage annotations covering both general and science/math domains (Figure 3; Table 1).
  - Why it matters: well-structured supervision teaches the model ‚Äúhow to think,‚Äù not just ‚Äúwhat to answer.‚Äù Directly training on original Q&A (without staging) improves less and can even degrade on detail-heavy tasks (Table 2: ‚ÄúDirect Training‚Äù average 59.0 vs. 62.4 for full LLaVA-CoT; MMVet notably 49.9 vs. 60.3).

- Stage-wise retracing search (SWIRES) for test-time scaling
  - What‚Äôs new: a search procedure aligned to semantic stages with the ability to backtrack one stage when current proposals are uniformly low quality (Figure 4; Algorithm 1).
  - Why it matters: better use of test-time compute and stronger error correction than best-of-N and stage-wise beam search. On MMStar, SWIRES continues to improve with more time while the others plateau (Figure 5).

- Evidence that improvements come from structured CoT, not ‚Äúdense GPT supervision‚Äù alone
  - Multi-task decomposition without CoT (training separate captioning/summarizing tasks) performs worse (Table 7: ‚Äúmulti-task‚Äù average 57.7).
  - Scrambling the stage order also underperforms (Table 7: ‚Äúreorder‚Äù 58.2). Proper stage ordering is essential.
  - Prompting the base models with the same structure helps GPT-4o but not Llama-3.2-Vision (Table 7), indicating SFT on staged data is key for weaker models.

## 5. Experimental Analysis
- Setup and benchmarks (Section 4.1)
  - Evaluated with VLMEvalKit to ensure consistency.
  - Six benchmarks:
    - General VQA: MMStar, MMBench V1.1, MMVet.
    - Reasoning-centric: MathVista (math in visual contexts), AI2D (diagram understanding), HallusionBench (language hallucination & visual illusion).
  - A ‚Äúreasoning-only‚Äù subset is also reported (Table 5) by filtering out pure perception/OCR: MMStar-R, MMBench-R, MMVet-R (Appendix E describes selection criteria).

- Main quantitative results
  - Post-training gains vs base model (Table 2):
    - Average across six benchmarks: 62.4 for LLaVA-CoT vs 56.6 for the base model (‚àÜ +5.8 points).
    - Notable per-benchmark improvements: MMStar 49.8 ‚Üí 57.6; MMVet 57.6 ‚Üí 60.3; MathVista 48.6 ‚Üí 54.8; HallusionBench 40.3 ‚Üí 47.8.
  - Where the gains come from (Table 3; MMStar skill breakdown):
    - Reasoning-heavy skills show the largest deltas:
      - Instance reasoning: 57.6 ‚Üí 63.2.
      - Logical reasoning: 50.8 ‚Üí 58.0.
      - Math: 45.2 ‚Üí 64.0.
      - Science & technology: 32.8 ‚Üí 44.8.
    - Perception gains are smaller (coarse 66.0 ‚Üí 68.8; fine-grained 46.4 ‚Üí 46.8), aligning with the method‚Äôs focus on reasoning.
  - Test-time scaling with SWIRES (Figure 5; Table 4):
    - Scaling curve (Figure 5): accuracy on MMStar increases most with SWIRES and keeps improving beyond ~10k seconds, whereas best-of-N and stage-wise beam plateau.
    - With scaling (Table 4): average across six benchmarks rises from 62.4 to 65.5; on MMStar 57.6 ‚Üí 62.5; on MMVet 60.3 ‚Üí 64.9.
  - Comparison to state of the art (Table 5; reasoning-centric sets):
    - `LLaVA-CoT (w/ scaling)` average 66.3, outperforming several larger open-source VLMs (e.g., `Llama-3.2-90B-Vision-Instruct` average 62.3; `Deepseek-VL2` 66.0) and some closed models (`Gemini-1.5-Pro` 63.6; `GPT-4o-mini` 63.8).
    - It remains below top-tier closed models (`GPT-4o` 71.8; `GLM-4v-Plus` 72.5).

- Ablations and diagnostics
  - Structured tags matter: removing tags reduces average from 62.4 to 60.9 (Table 2).
  - Direct training (no stages) helps less and can harm on MMVet (49.9 vs base 57.6; Table 2).
  - CoT design validity:
    - For the teacher GPT-4o, using the structured CoT prompt improves its own results (71.8 ‚Üí 74.1; Table 7), showing the design is effective, not merely mimicking GPT-4o‚Äôs defaults.
    - For Llama-3.2-Vision, prompting alone doesn‚Äôt help (56.9 ‚Üí 56.9 in Table 7), emphasizing the need for SFT on CoT-structured data.
  - Case illustrations: Figure 2 contrasts flawed base-model reasoning with LLaVA-CoT‚Äôs staged solution; Appendix Figure 7 shows SWIRES correcting a wrong numeric conclusion by selecting a better reasoning path.

- Do the experiments support the claims?
  - Yes, on three dimensions:
    - Post-training structured reasoning improves accuracy, especially where reasoning is required (Tables 2‚Äì3).
    - Stage-wise test-time search with retracing yields stronger, sustained scaling (Figure 5; Table 4).
    - Competitiveness vs larger models indicates the method adds capability beyond parameter count (Table 5).

## 6. Limitations and Trade-offs
- Dependence on GPT-4o-generated supervision
  - The 100k staged dataset is produced by a closed model; quality is filtered (Appendix B), but biases and style from GPT-4o may imprint on LLaVA-CoT.
- Reward model reliance
  - SWIRES needs a multimodal reward model (`InternLM-XComposer2.5-Reward`) to score candidates; misalignment or domain gaps in this reward could steer search poorly (Appendix D).
- Compute at inference
  - Stage-wise search increases latency; Figure 5 quantifies time-accuracy trade-offs (log-scale time axis). While SWIRES scales better, it still requires more compute than a single-pass answer.
- Failure modes
  - On very complex images, the model can ‚Äúget lost‚Äù during retracing or hallucinate toward an answer (Appendix J). Retracing helps but is not a guarantee.
- Scope and assumptions
  - The four-stage template assumes problems that benefit from a caption-like step and explicit reasoning. Tasks dominated by pure perception or OCR may not gain much and could be slowed by the overhead.
- Generality across domains
  - The dataset spans many VQA types (Table 1), but specialized domains (medical, remote sensing) are not included; transfer is untested.

## 7. Implications and Future Directions
- How this changes the landscape
  - It demonstrates that explicit, train-time ‚Äúreasoning stage‚Äù supervision plus stage-aware test-time search can close much of the gap to larger or closed models on reasoning-heavy multimodal tasks (Table 5). This reframes progress from ‚Äúbigger models‚Äù to ‚Äúbetter thought structure and search.‚Äù
- Follow-up research enabled
  - Reinforcement learning for staged reasoning: The paper suggests applying RL to further improve complex multimodal reasoning (Conclusion). This could replace static reward thresholds with learned policies that decide when to retrace or stop early.
  - Better reward models: Domain-specific or instruction-tuned multimodal reward models might yield stronger SWIRES performance and more robust thresholds.
  - Adaptive stage controllers: Dynamically skipping or expanding stages based on uncertainty; e.g., omitting caption when the question is text-only, or inserting verification sub-stages for math.
  - Data generation at scale and diversity: Extending the staged dataset to more domains and languages; exploring human-in-the-loop corrections for higher fidelity than GPT-only supervision.
- Practical applications
  - Chart and document understanding, visual math tutors (MathVista-like tasks), science education on diagrams (AI2D), and safety-critical scenarios where hallucination control matters (HallusionBench). The stage tags also improve interpretability by making the model‚Äôs plan, perception, and logic explicit‚Äîuseful for auditing and teaching.

> Headline result: ‚ÄúLLaVA-CoT ‚Ä¶ outperforms its base model by 9.4% on a wide range of multimodal reasoning benchmarks‚Äù (Abstract; also see Table 4 averages: 56.6 ‚Üí 65.5 with SWIRES), and ‚Äúeven surpasses ‚Ä¶ Gemini-1.5-Pro and GPT-4o-mini‚Äù on reasoning-focused suites (Table 5).

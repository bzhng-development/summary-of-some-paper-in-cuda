# Why Do Multi-Agent LLM Systems Fail?

**ArXiv:** [2503.13657](https://arxiv.org/abs/2503.13657)

## ğŸ¯ Pitch

This paper delivers the first systematic investigation into why multi-agent large language model (LLM) systems frequently underperform, introducing MASTâ€”a 14-mode, empirically grounded taxonomy of failure typesâ€”and accompanying it with MAST-Data, a publicly available dataset of 1,642 annotated execution traces. By releasing both the taxonomy and dataset alongside a scalable LLM-as-a-judge annotation pipeline, the authors provide the entire community with practical tools for diagnosing, comparing, and mitigating MAS failures, paving the way for more robust multi-agent AI systems and accelerating research progress in collaborative intelligence.

---

## 1. Executive Summary (2-3 sentences)
This work explains why Multiâ€‘Agent Large Language Model Systems (MAS) so often fail and provides concrete tools to diagnose and reduce those failures. It introduces (i) MAST, a 14â€‘mode taxonomy of MAS failures mapped to conversation stages (Figure 1), (ii) MASTâ€‘Data, a public corpus of 1,642 annotated MAS execution traces across 7 frameworks and multiple tasks (Table 1), and (iii) an LLMâ€‘asâ€‘aâ€‘Judge annotator that scales failure labeling with high agreement to human experts (Table 2).

## 2. Context and Motivation
- Problem/gap addressed
  - MASâ€”systems where multiple LLM â€œagentsâ€ interact to solve tasksâ€”are increasingly popular for coding, math, web/desktop tasks, and general assistants. Yet their gains over singleâ€‘agent systems are often marginal, and failure rates remain high. The paper documents failures between 41% and 86.7% across six popular systems (Figure 5).
  - There is no shared, fineâ€‘grained framework for what â€œfailureâ€ means in MAS or how to diagnose root causes. Without common definitions, teams cannot compare systems, reproduce findings, or direct engineering effort.

- Why it matters
  - Practical: MAS are being deployed in software engineering, web automation, and research assistance. Failures create wasted compute, unreliable behavior, and user risk (Sections 1, 2.1).
  - Scientific: Understanding failure patterns in multiâ€‘agent coordination, memory, and verification reveals where architectural or training changes are most needed (Sections 1, 4).

- Prior approaches and their limits
  - Singleâ€‘system design advice (e.g., modularity, keep frameworks simple) offers principles but not a crossâ€‘system error taxonomy (Related Work 2.2).
  - Benchmarks target overall performance (e.g., SWEâ€‘Bench, GAIA, AppWorld) but do not explain why failures happen (Related Work 2.1, 2.3).
  - Debugging tools exist (e.g., interactive agent debuggers) but lack a standardized failure vocabulary to aggregate insights across systems (Related Work 2.3).

- Positioning
  - This work takes a bottomâ€‘up, empirical approach: derive a general failure taxonomy from real execution traces (Grounded Theory in Â§3.1), validate it through interâ€‘annotator agreement (IAA) (Â§3.2), then scale annotation with an LLMâ€‘based judge (Â§3.3) to build a large, public dataset (Â§3.4). The taxonomy is then used to analyze failure distributions across models, frameworks, and tasks (Figures 4, 8, 9) and to guide concrete system interventions (Appendix H, Table 5).

## 3. Technical Approach
At a high level: derive a taxonomy from data, validate it with humans, scale with an LLM annotator, and apply it across many MAS to analyze and improve systems. Below is the stepâ€‘byâ€‘step pipeline (also depicted in Figure 2).

- Core concepts (defined for clarity)
  - `Agent`: an LLMâ€‘powered component with a role prompt (initial state), a conversation history (state), and the ability to act (e.g., tool calls); see Introduction, p.1.
  - `MAS` (Multiâ€‘Agent System): a set of agents interacting via an orchestration/workflow to solve tasks (p.1).
  - `Trace`: the full, ordered record of agent messages, tool calls, outputs, and orchestrator decisions for one task run.
  - `Failure mode`: a recurring pattern describing how/why a run fails its task objective.
  - `Grounded Theory`: a qualitative method that codes raw data to surface recurring phenomena without preâ€‘set labels; here: open coding, constant comparison, memoing, theorizing (Â§3.1).
  - `Interâ€‘Annotator Agreement (IAA)` and `Cohenâ€™s Îº`: a statistic (âˆ’1 to 1) that measures agreement beyond chance between annotators; Îºâ‰ˆ0.8â€“1.0 is considered strong.

Step 1 â€” Derive a failure taxonomy from traces (Grounded Theory; Â§3.1, Â§4)
- Data: 150 traces from 5 MAS (HyperAgent, AppWorld, AG2, ChatDev, MetaGPT) spanning programming and math tasks; each trace averages >15,000 lines of text (Â§3.1).
- Method: six experts independently performed open coding; through constant comparison and theorizing, they converged on recurring failure patterns; analysis proceeded until â€œtheoretical saturation,â€ i.e., no new failure patterns emerged with new traces (Â§3.1).
- Outcome: MAST (Multiâ€‘Agent System Failure Taxonomy), 14 failure modes grouped into 3 categories and mapped to conversation stages (Figure 1; details in Appendix A).

Step 2 â€” Standardize definitions and validate human agreement (Â§3.2)
- Three annotators labeled 15 traces in three rounds, refining definitions each round through discussion to resolve disagreements; final IAA achieved Îº = 0.88 (Â§3.2).
- A visual example shows how a snippet is labeled, e.g., â€œInformation Withholdingâ€ when a Phone Agent fails to share required API username format, leading to repeated failed logins (Figure 3).

Step 3 â€” Build an LLMâ€‘asâ€‘aâ€‘Judge annotator to scale labels (Â§3.3)
- Approach: prompt OpenAI `o1` with (a) a trace, (b) MAST definitions, and (c) fewâ€‘shot examples from the humanâ€‘labeled set to predict which failure modes occurred (Appendix N describes examples).
- Calibration: on heldâ€‘out humanâ€‘labeled traces, the fewâ€‘shot `o1` annotator achieves:
  > Accuracy 0.94, Recall 0.77, Precision 0.833, F1 0.80, Cohenâ€™s Îº 0.77 (Table 2).
- Generalization: on two unseen MAS (OpenManus, Magenticâ€‘One) and two new benchmarks (MMLU, GAIA), human IAA with the finalized MAST reached Îº = 0.79 (Â§3.4), indicating definitions transfer.

Step 4 â€” Construct the dataset (Â§3.4; Table 1)
- `MASTâ€‘Data`: 1,642 annotated traces from 7 MAS frameworks across coding, math, and general tasks (ChatDev, MetaGPT, HyperAgent, AppWorldâ€‘derived multiâ€‘agent, AG2 MathChat, Magenticâ€‘One, OpenManus).
- Tasks/Models: e.g., ProgramDev/ProgramDevâ€‘v2 (coding), SWEâ€‘Bench Lite (code maintenance), GSMâ€‘Plus & OlympiadBench (math), MMLU (knowledge), GAIA (general agents) with models GPTâ€‘4/4o, Claude 3.7 Sonnet, Qwen2.5â€‘Coderâ€‘32B, and CodeLlamaâ€‘7B (Table 1).
- `MASTâ€‘Dataâ€‘human`: a smaller subset with triple human annotations and rationales (used for IAA).
- Cost to annotate with LLMâ€‘asâ€‘Judge averages $1.8 per trace; perâ€‘framework costs in Table 9.

Step 5 â€” Use MAST to analyze failures and guide improvements
- Failure categories and modes (Figure 1; Appendix A):
  - FC1 System Design Issues (44.2% of failures in Figure 1): task/role nonâ€‘adherence (FMâ€‘1.1, FMâ€‘1.2), step repetition (FMâ€‘1.3), history loss (FMâ€‘1.4), unaware of termination (FMâ€‘1.5).
  - FC2 Interâ€‘Agent Misalignment (32.3%): conversation reset (FMâ€‘2.1), failure to ask clarification (FMâ€‘2.2), derailment (FMâ€‘2.3), information withholding (FMâ€‘2.4), ignoring others (FMâ€‘2.5), reasoningâ€‘action mismatch (FMâ€‘2.6).
  - FC3 Task Verification (23.5%): premature termination (FMâ€‘3.1), no/incomplete verification (FMâ€‘3.2), incorrect verification (FMâ€‘3.3).
- The taxonomy maps each mode to conversation stages (preâ€‘execution, execution, postâ€‘execution) to indicate where detection/mitigation fits (Figure 1).

Step 6 â€” Release a developer tool
- A Python library `agentdash` exposes the annotator and taxonomy for practitioners (Appendix C shows usage).

Design choices and why they matter
- Grounded, dataâ€‘first taxonomy: avoids importing preâ€‘conceived categories; ensures modes reflect real system behavior.
- Stage mapping (Figure 1): helps engineering teams place checks where they can be most effective (e.g., preâ€‘execution role prompts vs. postâ€‘execution verifiers).
- LLMâ€‘asâ€‘Judge: manual labeling at scale is infeasible; the calibrated annotator maintains high agreement with humans (Table 2) and keeps costs reasonable (Table 9).

## 4. Key Insights and Innovations
1) A unified, fineâ€‘grained taxonomy for MAS failures (MAST)
- Novelty: First empirically derived taxonomy specific to multiâ€‘agent LLM systems, with 14 distinct modes organized by system stage (Figure 1; Appendix A).
- Why it matters: Distinguishes visually similar symptoms with different roots (e.g., â€œmissing informationâ€ can be FMâ€‘2.4 withholding, FMâ€‘2.5 ignoring, or FMâ€‘1.4 history loss). This precision enables targeted fixes rather than generic â€œmake prompts better.â€

2) A large, publicly annotated dataset of real multiâ€‘agent executions (MASTâ€‘Data)
- Novelty: 1,642 traces from 7 frameworks and diverse tasks/models (Table 1).
- Why it matters: Enables comparative analysis across systems and benchmarks (Figures 4, 8, 9), correlation studies (Figures 6â€“7), and failureâ€‘success relationships (Table 7). Prior work lacked such breadth and standardized labels.

3) Scalable annotation via an LLMâ€‘asâ€‘aâ€‘Judge with strong human agreement
- Novelty: A practical pipeline using `o1` with MAST definitions and fewâ€‘shot examples achieves Îº = 0.77 (Table 2) and generalizes to unseen systems/benchmarks with Îº = 0.79 (Â§3.4).
- Why it matters: Makes largeâ€‘scale, fineâ€‘grained failure analysis feasible for industry teams that cannot afford extensive human coding.

4) Three actionable systemâ€‘design insights grounded in data (Â§4)
- FC1: Failures are often architectural/specification issues, not just â€œLLM canâ€™t follow instructions.â€ Example: in ChatDev, adjusting hierarchy so the CEO has final say raised success by +9.4% (Â§4, Appendix H).
- FC2: Communication protocol alone is insufficient; agents often lack â€œtheoryâ€‘ofâ€‘mindâ€â€‘like modeling of othersâ€™ information needs (discussion near Figure 3). This calls for structural message content changes and/or modelâ€‘level training for social reasoning.
- FC3: Verification must be multiâ€‘level and taskâ€‘aware. Superficial checks (e.g., code compiles) miss deeper errors; adding highâ€‘level objective verification improved ChatDev by +15.6% on ProgramDev (Appendix H).

These are fundamental innovations (taxonomy, dataset, scalable annotator) plus concrete, validated design principles.

## 5. Experimental Analysis
Evaluation setup
- Datasets/benchmarks and systems: Table 1 lists the MASâ€“benchmarkâ€“LLM combinations that contribute to MASTâ€‘Data, covering coding (ProgramDev, SWEâ€‘Bench Lite), math (GSMâ€‘Plus, OlympiadBench), general agent tasks (GAIA), and knowledge (MMLU).
- Metrics:
  - For annotation quality: Accuracy/F1 and Cohenâ€™s Îº vs. human experts (Table 2).
  - For system behavior: (a) task success rates (humanâ€‘evaluated for several configs in Table 1 and Appendix B), (b) failure counts per mode/category (Figures 1, 4, 8, 9; Tables 6â€“8), (c) correlations across modes/categories (Figures 6â€“7).
- Experimental design highlights:
  - Human IAA to validate taxonomy (Îº = 0.88; Â§3.2).
  - Calibrated LLM annotator, including fewâ€‘shot prompting; accuracy 0.94 and Îº = 0.77 (Table 2).
  - Generalization checks on new systems/benchmarks (Â§3.4).
  - Caseâ€‘study interventions (Appendix H; Table 5; Figures 10â€“11).

Main quantitative results
- High failure rates across MAS: for six systems run on their respective benchmarks, failures ranged from 41.0% to 86.7% (Figure 5). This motivates the need for failureâ€‘aware design.
- Distinct failure profiles by system: Figure 4 (210 traces) shows, for example, AppWorld often ends too early (FMâ€‘3.1 premature termination), OpenManus repeats steps (FMâ€‘1.3), and HyperAgent shows both step repetition (FMâ€‘1.3) and incorrect verification (FMâ€‘3.3).
- Category distinctiveness: correlations among the three categories are low (0.17â€“0.32; Figure 6), suggesting they capture different aspects (design vs. coordination vs. verification). Some modeâ€‘level correlations exist but remain moderate (max 0.63; Figure 7).
- Model choice matters (Figure 8):
  - Within MetaGPT on ProgramDevâ€‘v2, GPTâ€‘4o exhibits substantially fewer FC1/FC2 failures than Claude 3.7 Sonnet (the text notes a 39% reduction in FC1; Â§5.1/Figure 8), though FC3 remains high for bothâ€”verification remains a bottleneck.
- MAS architecture matters (Figure 9):
  - With GPTâ€‘4o held fixed, MetaGPT shows far fewer FC1/FC2 failures than ChatDev but considerably more FC3 failures. Interpretation in Â§F: MetaGPTâ€™s strong role SOPs reduce specification/coordination errors but ChatDevâ€™s explicit testing/review phases catch more verification issues.
- Case studies show targeted gains (Appendix H; Table 5):
  - AG2 MathChat on GSMâ€‘Plus: â€œImproved prompt (with verification)â€ increased accuracy from 84.75%Â±1.94 to 89.75%Â±1.44 with GPTâ€‘4; with GPTâ€‘4o, from 84.25%Â±1.86 to 89.00%Â±1.38. A topology redesign yielded smaller or modelâ€‘dependent gains.
  - ChatDev: On ProgramDevâ€‘v0, success improved from 25.0% to 34.4% (prompt changes) and to 40.6% (topology changes). On HumanEval, the gains were modest (89.6%â†’91.5% with topology).
  - Failureâ€‘mode reductions from these interventions are visualized in Figures 10â€“11, showing broad decreases, with topology generally outperforming prompt tweaks.
- Openâ€‘source model comparison (Table 6):
  - Qwen2.5â€‘Coderâ€‘32B shows substantially fewer failures than CodeLlamaâ€‘7B in both ChatDev and MetaGPT. Both lag GPTâ€‘4o/Claude overall, indicating a remaining performance gap for open models in MAS settings.
- Failureâ€“success relationships (Table 7):
  - Some modes are â€œfatalâ€: FMâ€‘1.5 (Unaware of termination conditions) and FMâ€‘2.4 (Information withholding) appear almost exclusively in failed runs.
  - Verification failures (FMâ€‘3.2 No/Incomplete, FMâ€‘3.3 Incorrect) also appear in successful runs, indicating superficial checks can pass with latent defectsâ€”a critical engineering warning.
- Benchmark difficulty shifts distributions (Table 8):
  - With AG2+GPTâ€‘4o fixed, harder benchmarks (e.g., Olympiad) show higher overall failure rates and different mixes across categories than GSM.

Do the experiments support the claims?
- The taxonomyâ€™s reliability is supported by strong human IAA (Îº=0.88) and annotator agreement (Îº=0.77) with generalization to new systems (Îº=0.79) (Sections 3.2â€“3.4; Table 2).
- The analysis across systems/models/tasks (Figures 4, 8, 9; Tables 6â€“8) substantiates that (a) failures are common, (b) failure profiles depend on architecture and model, and (c) verification is a persistent bottleneckâ€”directly supporting the three design insights in Â§4.
- Interventions yield measurable, modeâ€‘specific improvements (Table 5; Figures 10â€“11), demonstrating practical value and that MAST can guide debugging.

Caveats the paper notes
- Crossâ€‘system bar charts (e.g., Figure 4) are diagnostic rather than leaderboard comparisons because tasks differ (Â§5 caption).
- LLMâ€‘asâ€‘Judge remains an automated proxy for human annotations; however, agreement is high and costs are transparent (Table 9).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - The taxonomy emphasizes failures amenable to system design and verification improvements. It does not attempt to catalog all underlying model limitations (e.g., factual hallucination) except where they manifest as MAS failures (Â§4 note).
  - Failure â€œstageâ€ mapping (pre/execution/post) is a heuristic; some modes span stages (Figure 1).

- Annotation dependencies
  - The scalable annotator depends on a specific closedâ€‘source model (`o1`) and fewâ€‘shot prompts (Table 2; Appendix N). Replicating Îº may require similar models; domain shifts could change agreement.

- Comparability constraints
  - Many results are across different tasks/benchmarks per system (Figure 4), so not strict applesâ€‘toâ€‘apples comparisons. The paper uses them for profile discovery, not ranking.

- Generality and exhaustiveness
  - MAST is empirical and may not cover rare or domainâ€‘specific failures (explicitly acknowledged in Â§4). New domains (robotics, safetyâ€‘critical control) could introduce additional modes.

- Ground truth for â€œroot causeâ€
  - Even with fineâ€‘grained labels, causal chains in MAS can be intertwined (e.g., a verifier miss might be precipitated by earlier misalignment). The taxonomy captures observed modes, not formal causal proofs.

- Compute/cost tradeâ€‘offs
  - LLMâ€‘based annotation adds cost (avg $1.8/trace; Table 9) and latency. Stronger fewâ€‘shot calibrations can raise accuracy but also context length and price.

## 7. Implications and Future Directions
- How this changes the landscape
  - Provides a shared language (MAST) and public data (MASTâ€‘Data) to move MAS development from adâ€‘hoc debugging to systematic diagnosis. Teams can now measure not only â€œhow oftenâ€ systems fail but â€œhowâ€ they fail, and test whether interventions reduce the right modes (Figures 10â€“11).

- Research enabled
  - Learning for coordination: Train agents to model other agentsâ€™ information needs (addressing FMâ€‘2.2/2.4/2.5), possibly via supervised traces from MASTâ€‘Data or multiâ€‘agent RL.
  - Verification research: Develop multiâ€‘level, domainâ€‘aware verifiers that combine static analysis, test generation, external knowledge, and symbolic checks (addresses FC3; Â§4 and Appendix G).
  - Organizational design for agents: Explore workflows/topologies that reduce FC1 and FC2 (e.g., role hierarchies, turnâ€‘taking protocols, â€œfinal authorityâ€ agents), supported by ablations like those in Appendix H.
  - Memory/state management: Reduce FMâ€‘1.4 (history loss) with structured memory (e.g., workflow memory, OSâ€‘style context management) and robust state machines (Appendix G).
  - Standardized communication protocols: Formal message schemas to reduce misalignment (Appendix G) and enable automated coherence checks.

- Practical applications
  - MAS engineering dashboards: Integrate `agentdash` to track failure modes over time, identify regressions, and target mitigations.
  - Model/architecture selection: Use failure profiles (Figures 8â€“9) to pick LLMs and workflows suited to a domainâ€™s risk tolerance (e.g., choose architectures strong on FC3 for safetyâ€‘critical coding).
  - Guardrail design: Detect â€œfatalâ€ modes early (Table 7 shows FMâ€‘1.5, FMâ€‘2.4 correlate with failures) and trigger fallbacks: ask for clarification, escalate to a verifier, or hand off to a human.

Key takeaway
- MAS reliability is not primarily a matter of â€œmore agentsâ€ or â€œmore samples.â€ The data show that design choices (roles, workflows, memory) and multiâ€‘level verification dominate outcomes, and that different architectures and models fail in systematically different ways. MAST and MASTâ€‘Data give the community the vocabulary, data, and tooling to turn those insights into durable engineering practices.

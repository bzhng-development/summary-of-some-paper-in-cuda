# DeepSeek LLM: Scaling Open-Source Language Models with Longtermism

**ArXiv:** [2401.02954](https://arxiv.org/abs/2401.02954)

## ðŸŽ¯ Pitch

DeepSeek LLM redefines how open-source language models are scaled by empirically deriving new, practical scaling laws for allocating compute between model size and data size, as well as for key hyperparameters like batch size and learning rate. By introducing a more accurate measure of model scale (non-embedding FLOPs per token) and systematically validating these guidelines at billion-scale across 2 trillion tokens, DeepSeek produces bilingual models that not only surpass LLaMA-2 70B on multiple benchmarks but also approach or exceed GPT-3.5â€™s performance in open-ended tasks. This work equips the open-source AI community with robust, reproducible strategies to maximize performance and efficiency for future large-scale model training.

---

## 1. Executive Summary
DeepSeek LLM revisits how to scale openâ€‘source large language models by (i) deriving empirical scaling laws for critical hyperparameters and for the tradeâ€‘off between model size and data size, and (ii) validating those laws by training bilingual 7B and 67B models on 2T tokens with strong reasoning, math, code, and Chinese capabilities. The central advance is a more accurate way to allocate compute between model and data using a new measure of model scale and an â€œIsoFLOPâ€ methodology, leading to models that outperform LLaMAâ€‘2 70B on many benchmarks and rival GPTâ€‘3.5 in openâ€‘ended dialogue (Sections 3, 5; Tables 5â€“8; Figures 4â€“5).

## 2. Context and Motivation
- Problem addressed
  - Openâ€‘source LLMs typically train a few fixed parameter sizes (7Bâ€“70B) without principled guidance on how to allocate compute between model size and data, and without clear tuning rules for batch size and learning rate (Introduction; Section 3). Prior scaling laws disagree on the optimal model/data split: Kaplan et al. favor larger models (coefficients â‰ˆ 0.73 model / 0.27 data), while Chinchilla favors more data (â‰ˆ 0.49 / 0.51).
- Why this matters
  - Compute budgets are finite. Misallocating compute (too many parameters, too little dataâ€”or vice versa) wastes resources and caps performance. Better scaling guidance directly improves the effectiveness and cost of training large openâ€‘source models (Section 3).
- Shortcomings of prior approaches
  - Conflicting model/data exponents (Table 4) and incomplete reporting of hyperparameters make it unclear whether earlier experiments reached optimal settings at each compute budget (Section 3).
- Positioning of this work
  - DeepSeek LLM contributes:
    - Practical scaling rules for batch size and learning rate across budgets (Eq. (1); Figures 2â€“3).
    - A new, more faithful measure of â€œmodel scaleâ€ for decoderâ€‘only Transformersâ€”`nonâ€‘embedding FLOPs per token` (`M`)â€”and a compute identity `C = M Â· D` that improves the fit of scaling curves (Eq. (2); Table 3).
    - Evidence that the optimal model/data split depends on data qualityâ€”higher-quality data should be paired with larger models (Table 4).
    - Two large bilingual models (7B, 67B) trained to 2T tokens and aligned via supervised fineâ€‘tuning (SFT) and direct preference optimization (DPO), with broad evaluation (Sections 2, 4, 5).

## 3. Technical Approach
This work comprises a data pipeline, model design, training setup, a scalingâ€‘law program, and an alignment pipeline.

- Data pipeline (Section 2.1; Table 1)
  - Stages: deduplication â†’ filtering â†’ remixing.
  - Aggressive web deduplication across 91 Common Crawl dumps (not just dumpâ€‘internal) removes 89.8% of documents, vs. 22.2% within a single dump (Table 1). Goal: reduce repeated text that can distort learning signals.
  - Filtering uses linguistic and semantic heuristics to improve â€œinformation densityâ€ (Section 2.1).
  - Tokenizer: byteâ€‘level BPE with preâ€‘tokenization to avoid merging across newlines, punctuation, and CJK symbols; numbers split into digits. Vocabulary trained to 100,015 tokens (plus special tokens), but the model uses a padded size of 102,400 for compute efficiency (Section 2.1).

- Model architecture (Section 2.2; Table 2)
  - Base design follows LLaMA: Preâ€‘Norm with `RMSNorm`, `SwiGLU` feedâ€‘forward, `Rotary Embedding` for positions.
  - 67B uses `Groupedâ€‘Query Attention (GQA)` to reduce key/value cache cost at inference; unlike typical GQA deployments that widen layers, DeepSeek scales depth (67B has 95 layers; 7B has 30) to match parameter budgets while easing pipeline partitioning (Section 2.2; Table 2).
  - Context length is 4096; batch sizes are 2304 (7B) and 4608 (67B) tokens per step (Table 2).

- Training setup (Sections 2.3â€“2.4; Figure 1; Table 2)
  - Optimizer: AdamW with Î²1=0.9, Î²2=0.95, weight decay=0.1; gradient clipping=1.0; bf16 compute with fp32 gradient accumulation.
  - Learningâ€‘rate schedule: multiâ€‘step rather than cosine for better continualâ€‘training reuse. Warmup = 2000 steps; then LR drops to 31.6% at 80% of tokens and to 10% at 90% (Section 2.3).
    - Figure 1a shows multiâ€‘step and cosine reach similar final loss; Figure 1b shows the chosen 80/10/10 split balances reuse and performance.
  - Infrastructure: custom `HAIâ€‘LLM` framework with data/tensor/sequence/pipeline parallelism, FlashAttention, ZeROâ€‘1 optimizer partitioning, fused kernels, overlap of compute/communication, and â€œinâ€‘placeâ€ crossâ€‘entropy to cut memory. Asynchronous checkpoints every 5 minutes limit worstâ€‘case loss from failures (Section 2.4).

- Scalingâ€‘law program (Section 3)
  - Goal: ensure that, at each compute budget `C`, training uses nearâ€‘optimal batch size and learning rate and finds the best model/data split.
  - Hyperparameter scaling (Sections 3.1; Figures 2â€“3; Eq. (1)):
    - Run many smallâ€‘toâ€‘medium experiments (C from 1e17 to 2e19 FLOPs) over grids of batch sizes and learning rates. Treat any setting within 0.25% of the best validation loss as â€œnearâ€‘optimal.â€
    - Fit power laws:
      - `Î·_opt = 0.3118 Â· C^(-0.1250)`
      - `B_opt = 0.2920 Â· C^(0.3271)`
    - Figures 2a/2b show broad â€œvalleysâ€ of good hyperparameters; Figures 3a/3b show the fitted curves. Larger compute favors larger batches and lower learning rates.
  - Model/data scaling (Sections 3.2; Eq. (2); Table 3; Figures 4â€“5):
    - Define model scale as `M = nonâ€‘embedding FLOPs/token` to include attention compute but exclude vocabulary embedding compute. Prior proxiesâ€”`6Â·N1` (nonâ€‘embedding params) and `6Â·N2` (all params)â€”systematically misestimate compute, especially in small models and long contexts (Table 3).
    - Use the `IsoFLOP` profile: for each compute budget `C = MÂ·D`, sweep multiple `(M, D)` allocations and identify the allocation with the lowest validation loss. Budgets span 1e17â€“3e20 FLOPs; ~10 allocations per budget; validation set is 100M tokens with the same distribution as training (Figure 4a).
    - Fit the optimal allocations (Figures 4bâ€“4c) to obtain:
      - `M_opt = 0.1715 Â· C^(0.5243)`
      - `D_opt = 5.8316 Â· C^(0.4757)` (Eq. (4))
    - Fit a performance curve of bitsâ€‘perâ€‘byte vs. `C` to predict largeâ€‘model performance. Figure 5 shows the smallâ€‘scale fit accurately predicts the 7B and 67B models (a ~1000Ã— extrapolation).
  - Role of data quality (Section 3.3; Table 4):
    - Repeat the `IsoFLOP` fitting across three datasets: early inâ€‘house, improved inâ€‘house, and OpenWebText2. As data quality improves, the exponent on model scale increases while the exponent on data decreases:
      - Early data: `a=0.450, b=0.550`
      - Current data: `a=0.524, b=0.476`
      - OpenWebText2: `a=0.578, b=0.422`
    - Insight: higherâ€‘quality data can productively feed larger models before data saturation.

- Alignment pipeline (Section 4)
  - Supervised Fineâ€‘Tuning (SFT): ~1.5M instruction examples (31.2% general tasks; 46.6% math; 22.2% code) plus 300k safety prompts. 7B trains 4 epochs (LR 1eâ€‘5), 67B trains 2 epochs (LR 5eâ€‘6). They monitor a â€œrepetition ratioâ€ (share of neverâ€‘terminating, looping outputs) and observe it rises with heavy math SFT (Section 4).
  - Twoâ€‘stage SFT (Section 5.5, Table 12): for 7B, a second stage without math/code keeps benchmark scores while reducing repetition from 2.0% to 1.4%.
  - Direct Preference Optimization (DPO): a preferenceâ€‘based alignment method that adjusts the model to prefer higherâ€‘rated responses without training a separate reward model. Train 1 epoch at LR 5eâ€‘6, batch size 512, using modelâ€‘generated responses as candidates for both helpfulness and harmlessness. It improves openâ€‘ended quality with little impact on standard benchmarks (Table 17).

## 4. Key Insights and Innovations
- A more faithful compute model for scaling curves (Fundamental)
  - Using `M = nonâ€‘embedding FLOPs/token` as the modelâ€‘scale variable yields better predictions than parameter counts, because it includes attention cost (which grows with sequence length) and ignores vocabulary compute (which contributes less to model capacity). Table 3 quantifies large errors when using `6Â·N1` or `6Â·N2`, especially for small models; Appendix A.2 shows that fits using `6Â·N1` overestimate largeâ€‘model performance, while `6Â·N2` underestimates it (Figure 6).
- Empirical hyperparameter scaling laws (Practical, immediately usable)
  - Powerâ€‘law formulas for batch size and learning rate vs. compute budget (Eq. (1); Figures 2â€“3) give a recipe for picking nearâ€‘optimal settings across budgets. This removes guesswork when changing `C`.
- Data quality shifts the optimal model/data tradeâ€‘off (Conceptual)
  - Table 4 shows that better data pushes the optimal allocation toward larger models (higher `a`, lower `b`). This reconciles prior disagreements (Kaplan vs. Chinchilla) by attributing differences to data quality and composition.
- Depthâ€‘heavy 67B with GQA and multiâ€‘step LR (Incremental but effective)
  - The 67B model opts for more layers (95) rather than wider layers, easing pipeline parallelism and showing strong results (Section 2.2; Table 2). The multiâ€‘step LR schedule enables efficient continual training without losing final quality (Figure 1).
- Alignment practices that balance capability and stability (Practical)
  - The twoâ€‘stage SFT and DPO reduce repetition and improve openâ€‘ended dialogue without overfitting to multipleâ€‘choice formats (Section 5.5; Tables 12â€“13, 17).

## 5. Experimental Analysis
- Evaluation setup (Section 5; Appendix A.6)
  - Benchmarks span language understanding (HellaSwag, PIQA, ARC, OpenBookQA, BBH), knowledge QA (TriviaQA, NaturalQuestions), reading comprehension (RACE, DROP, C3), Chinese tasks (CHID, Câ€‘Eval, CMMLU, CMath, CCPM), math (GSM8K, MATH), coding (HumanEval, MBPP), and language modeling (Pileâ€‘test).
  - Protocols:
    - Multipleâ€‘choice: perplexity scoring over answer options, with length normalization (and unconditional normalization for ARC/OpenBookQA).
    - Generation tasks: greedy decoding and programmatic answer parsing.
    - Language modeling: bitsâ€‘perâ€‘byte (lower is better).
- Main quantitative results (Base models; Table 5)
  - Relative to LLaMAâ€‘2 70B, `DeepSeekâ€‘67B` improves substantially on math and code:
    - MATH: 18.7 vs. 13.5 (+5.2)
    - GSM8K (8â€‘shot): 63.4 vs. 58.4 (+5.0)
    - HumanEval (0â€‘shot): 42.7 vs. 28.7 (+14.0)
    - MBPP (3â€‘shot): 57.4 vs. 45.6 (+11.8)
    - BBH: 68.7 vs. 62.9 (+5.8)
  - Chinese tasks: large gains over LLaMAâ€‘2 70B
    - Câ€‘Eval: 66.1 vs. 51.4 (+14.7)
    - CMMLU: 70.8 vs. 53.1 (+17.7)
    - CHID: 92.1 vs. 55.5 (+36.6)
  - English understanding is comparable:
    - HellaSwag: 84.0 (tie)
    - MMLU (5â€‘shot): 71.3 vs. 69.0 (+2.3)
    - Pileâ€‘test BPB: 0.642 vs. 0.649 (lower is better)
- Effects of tuning (Chat models; Table 6)
  - SFT/DPO massively boost math and code:
    - GSM8K (0â€‘shot): 67B rises from 63.4 (base, 8â€‘shot) to 84.1 (chat, 0â€‘shot).
    - HumanEval: 67B from 42.7 to 73.8.
    - MATH: 67B from 18.7 to 32.6.
  - Some multiâ€‘choice style tasks drop after alignment (e.g., HellaSwag and WinoGrande), which the paper attributes to pure LM perplexity scoring favoring base models (Section 5.1.2).
- Openâ€‘ended evaluations (Section 5.2)
  - Chinese AlignBench (GPTâ€‘4 judged; Table 7):
    - `DeepSeekâ€‘67Bâ€‘Chatâ€‘DPO`: 6.69 overall, outscoring ChatGPT (6.08) and most openâ€‘source peers; behind GPTâ€‘4 variants.
    - DPO improves nearly all categories over SFTâ€‘only.
  - English MTâ€‘Bench (GPTâ€‘4 judged; Table 8):
    - `67Bâ€‘Chat`: 8.35 (â‰ˆ GPTâ€‘3.5 turbo at 8.39), improving to 8.76 with DPO; GPTâ€‘4 is 9.26.
- Heldâ€‘out evaluations (Section 5.3; Table 9)
  - LeetCode (recent contests): `67Bâ€‘Chat` achieves 17.5 pass@1 (vs. Qwenâ€‘72B 12.7).
  - Hungarian National Highâ€‘School Exam (math): 58 (vs. Qwenâ€‘72B 52; GPTâ€‘4 68).
  - Instruction Following (IFEval): 55.5 (vs. Qwenâ€‘72B 50.8).
- Safety (Section 5.4; Tables 10â€“11)
  - Humanâ€‘built taxonomy with 2400 prompts shows high safeâ€‘answer rates across categories (Table 10).
  - On â€œDoâ€‘Notâ€‘Answerâ€ (939 prompts), `67Bâ€‘Chat` scores 97.8, slightly above ChatGPT (97.7) and above GPTâ€‘4 (96.5) in that metric (Table 11).
- Ablations and diagnostics (Section 5.5)
  - Twoâ€‘stage SFT reduces repetition without hurting code/math (Table 12).
  - Adding 20M Chinese multipleâ€‘choice questions inflates MC benchmarks in both languages (e.g., 7B MMLU +11.5 points; Câ€‘Eval +24.3) but does not help generative QA (TriviaQA unchanged; ChineseQA slightly down)â€”evidence of formatâ€‘specific overfitting (Table 13).
  - Instruction data late in preâ€‘training vs. in SFT yields similar final capability; they choose not to include instruction data in preâ€‘training (Section 5.5).
  - System prompts help large models but can slightly hurt small ones: 67B MTâ€‘Bench improves 8.35 â†’ 8.58; 7B drops 7.15 â†’ 7.11 (Table 14).

Assessment: The breadth of benchmarks, inclusion of heldâ€‘out datasets, ablations on MC data, and safety checks make a credible case that the models are strong, especially for math/code and Chinese. Some evaluations rely on LLMâ€‘asâ€‘judge (GPTâ€‘4), which is standard but can bias rankings; the paper mitigates this with many programmatic benchmarks and humanâ€‘graded tests (e.g., Hungarian exam).

## 6. Limitations and Trade-offs
- Dependence on data quality without a formal metric
  - The claim that â€œhigherâ€‘quality data â†’ more model scalingâ€ (Table 4) is supported by experiments but lacks a quantitative, reusable quality measure. Applying these exponents to other corpora may require reâ€‘estimation (Section 3.3).
- Scope of the compute model
  - `M = nonâ€‘embedding FLOPs/token` fits decoderâ€‘only Transformers with standard attention and feedâ€‘forward shapes (Eq. (2)). Variants with very long contexts, flashâ€‘decoding tricks, or architectural changes (e.g., Mixtureâ€‘ofâ€‘Experts, retrievalâ€‘augmented models) may alter the mapping between parameters and compute, limiting direct transfer of the fitted exponents (Appendix A.2 highlights representation sensitivity at low budgets).
- Hyperparameter scaling coverage
  - Batch size and LR formulas (Eq. (1)) are fitted over 1e17â€“2e19 FLOPs with reuse of the first training stage. The formulas center in the optimal region (Figure 2b) but do not model effects beyond `C` (e.g., exact `(M, D)` mix also nudges the optimum; Section 3.1 notes this).
- Evaluation tradeâ€‘offs
  - Alignment can reduce scores on MC tasks like HellaSwag (Table 6); choosing between conversational helpfulness and raw MC performance is a design tradeâ€‘off.
  - Some openâ€‘ended evaluations are judged by GPTâ€‘4 (Tables 7â€“8), which introduces potential bias; however, many standard, autoâ€‘graded datasets offset this.
- Compute and resource cost
  - Training to 2T tokens with 67B parameters is expensive; while the paper details efficiency techniques (Section 2.4), reproducing the full setup requires substantial hardware.
- Transparency of data
  - The 2T bilingual corpus is described at a high level (Section 2.1) but not fully released in this paper; replicability of exact scaling results across unseen data distributions remains an open question.

## 7. Implications and Future Directions
- Field impact
  - The compute identity `C = M Â· D` with `M` as nonâ€‘embedding FLOPs/token and the fitted exponents (Eq. (4)) provide a practical playbook for future openâ€‘source LLM training. Researchers can plan budgets and predict performance (Figure 5) rather than guess. The observation that better data favors larger models helps reconcile Kaplanâ€‘ vs. Chinchillaâ€‘style guidance (Table 4).
- Followâ€‘up research
  - Generalize the compute model to other architectures (e.g., Mixtureâ€‘ofâ€‘Experts, retrievalâ€‘augmented, longâ€‘context attention) and verify whether analogous `M` definitions lead to stable exponents.
  - Formalize â€œdata qualityâ€ with measurable proxies (e.g., perplexity filtering, diversity/novelty metrics) and test how each aspect shifts the `a/b` exponents.
  - Extend hyperparameter scaling to include weight decay, warmup schedules, and optimizer variants; analyze sensitivity to `(M, D)` composition (Section 3.1 notes residual dependence).
  - Explore reinforcement learningâ€“based alignment to further improve complex reasoning (Section 6 â€œConclusionâ€ hints at positive early results).
- Practical applications
  - The 67B chat modelâ€™s strength on math/code (Tables 6, 8, 9, A.4) suggests immediate use in programming assistants, math tutoring, and enterprise Q&A in bilingual (Chinese/English) settings.
  - The strong safety performance (Tables 10â€“11) and twoâ€‘stage SFT recipe (Table 12) offer concrete guidelines for deploying helpful, lowâ€‘repetition chat systems without overfitting to multipleâ€‘choice formats (Table 13).

> In short, DeepSeek LLM contributes a tested methodology for computeâ€‘optimal scalingâ€”hyperparameters, model/data allocation, and alignment choicesâ€”and validates it with competitive 7B/67B bilingual models trained on 2T tokens. The work offers both actionable recipes (Eq. (1), Eq. (4)) and conceptual insights (data qualityâ€™s role) that future openâ€‘source LLM projects can adopt and extend.

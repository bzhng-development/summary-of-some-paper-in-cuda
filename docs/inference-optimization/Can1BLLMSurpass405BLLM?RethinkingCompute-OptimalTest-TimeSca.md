# Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling

**ArXiv:** [2502.06703](https://arxiv.org/abs/2502.06703)

## ğŸ¯ Pitch

This paper demonstrates that with carefully chosen, reward-aware test-time scaling (TTS) strategies, small language models (as small as 1Bâ€“3B parameters) can outperform much larger modelsâ€”even those over 100 times biggerâ€”on advanced mathematical reasoning tasks. By systematically analyzing how the optimal use of inference-time computation depends on the interplay between model size, verifier (PRM), and problem difficulty, the authors show that efficient, tailored TTS can dramatically boost reasoning accuracy while reducing computational cost by up to 1000Ã—, reshaping the path to cost-effective and scalable AI systems.

---

## 1. Executive Summary
This paper asks when extra test-time compute can let a small language model outperform a much larger one on hard reasoning tasks. It builds a rewardâ€‘aware framework for â€œcomputeâ€‘optimalâ€ testâ€‘time scaling (TTS) and shows, with extensive experiments on math benchmarks, that the best TTS strategy depends strongly on the policy model, the process reward model (PRM), and problem difficulty. Notably, with the right strategy a 1Bâ€“3B model can surpass models 100Ã— larger on MATHâ€‘500, and a 7B model beats frontier â€œlongâ€‘thinkingâ€ systems on AIME24 (Figure 1, Table 3).

## 2. Context and Motivation
- Problem addressed
  - How to allocate extra computation at inference time to boost reasoning accuracy most efficiently (â€œtestâ€‘time scaling,â€ TTS).
  - Existing TTS work does not systematically analyze how three interacting factorsâ€”policy model, verifier (PRM), and problem difficultyâ€”determine the computeâ€‘optimal strategy (Introduction; Â§1).

- Why this matters
  - Practical: If small models can match or exceed large ones via smarter inference, we can reduce total FLOPS by 100â€“1000Ã— while retaining accuracy (Table 4).
  - Scientific: Clarifies when and why verifierâ€‘guided search helps or hurts different models and tasks, providing principled guidance rather than oneâ€‘sizeâ€‘fitsâ€‘all recipes.

- Prior approaches and gaps
  - Internal TTS (â€œlong CoTâ€): train the model to think longer (e.g., o1; DeepSeekâ€‘R1) but requires costly training and can be inefficient on easy problems (Introduction).
  - External TTS: sampling and search guided by a verifier/PRM (e.g., Bestâ€‘ofâ€‘N, beam search, verifier tree search) but prior studies pick one verifier and do not analyze crossâ€‘model generalization or difficultyâ€‘aware scaling (Figures 2, 4â€“5; Â§2.2).
  - Computeâ€‘optimal TTS (Snell et al., 2024) formalizes perâ€‘prompt optimality but ignores that different PRMs change the search distributionâ€”this paper argues optimality must be rewardâ€‘aware (Â§3.1, Eq. 3).

- Positioning
  - Provides the first broad, controlled study of TTS across multiple policy families (0.5Bâ€“72B), many PRMs (1.5Bâ€“72B), several scaling strategies (BoN, beam, DVTS), and difficulty strata on two math benchmarks (MATHâ€‘500, AIME24) with unified setups (Â§4.1; Figures 4â€“11).
  - Introduces two methodological refinements: rewardâ€‘aware computeâ€‘optimality (Â§3.1) and absolute difficulty thresholds rather than dataset quantiles (Â§3.2, Figure 3).

## 3. Technical Approach
This section explains the framework, the TTS algorithms, and how the study is run.

- Problem formalization (Â§2.1; Eq. 1)
  - Each reasoning instance is an episode in an MDP: the policy model (LLM) generates a sequence of actions (tokens/steps) given state (prompt + prior steps). A PRM provides stepâ€‘level rewards â„›(s, a) (a scalar score) for process supervision.
  - â€œPolicy modelâ€ is the LLM that proposes steps; â€œprocess reward model (PRM)â€ is a separate model scoring the quality of intermediate steps (different from outcomeâ€‘only verifiers).

- Testâ€‘time scaling methods (Â§2.2; Figure 2)
  - Bestâ€‘ofâ€‘N (BoN): Sample N complete solutions from the policy, then choose an answer using a vote or PRMâ€‘based selection.
  - Beam search with PRM guidance: At each depth, expand candidates; the PRM selects the top N/M partial steps to continue (beam width N, branching M).
  - Diverse Verifier Tree Search (DVTS): Run multiple independent PRMâ€‘guided subtrees (N/M of them), increasing diversity compared to a single beam (Figure 2).

- Scoring and voting (Â§4.1)
  - For a trajectory of length H, compute a PRM score per step; aggregate by:
    - `PRM-Min`: minimum step score; `PRM-Last`: last step score; `PRM-Avg`: mean score.
  - Final answer selection across candidates uses:
    - `Majority Vote`: most frequent answer.
    - `PRM-Max`: single candidate with highest score.
    - `PRM-Vote`: sum scores over identical answers, pick the highest total.

- Computeâ€‘optimality and reward awareness
  - Classical perâ€‘prompt computeâ€‘optimality (Eq. 2): for budget N, choose TTS hyperparameters Î¸ that maximize the probability the output equals the correct answer y*(x).
  - Key addition (Â§3.1; Eq. 3): make the â€œtargetâ€ distribution explicitly depend on the reward function â„› because searchâ€‘based methods change the generation distribution via PRM scores:
    - Target(Î¸, N, x, â„›): distribution over outputs produced when the PRM guides search or is used for selection. This turns the optimization into a rewardâ€‘aware selection of strategy and hyperparameters per prompt.

- Difficultyâ€‘aware design (Â§3.2)
  - Instead of difficulty quantiles tied to a specific model (which shift as models improve), define absolute bins by Pass@1 accuracy on a strong reference policy: easy (50â€“100%), medium (10â€“50%), hard (0â€“10%) (Figure 3).

- Experimental setup (Â§4.1)
  - Datasets: MATHâ€‘500 (500 representative math problems) and AIME24 (hard Olympiadâ€‘style problems).
  - Policy models: Llamaâ€‘3.x and Qwen2.5 families from 0.5B to 72B, instruct variants.
  - PRMs: Mathâ€‘Shepherdâ€‘7B; RLHFlow PRMs (Mistralâ€‘8B base; DeepSeekâ€‘8B base); Skywork PRMs (1.5B, 7B); Qwen2.5â€‘Math PRMs (7B, 72B). Qwen2.5â€‘Mathâ€‘PRMâ€‘72B is the strongest openâ€‘source PRM tested (Â§4.1).
  - Search budgets: N in {4, 16, 64, 256}; one 1B case uses N=512 (Table 3 note).
  - Beam/DVTS beam width=4; max new tokens per response 8192; perâ€‘step token cap 2048; temperature 0.7 for search and 0.0 for vanilla CoT (Â§4.1).
  - Code: OpenR2 reasoning framework.

- How PRM choice changes search behavior (mechanism)
  - Because the PRM ranks partial steps, it decides which branches survive; different PRMs can prefer shorter or longer steps and thereby change both accuracy and token usage.
  - A toy example (Figure 12) shows two PRMs scoring different partial steps; one leads to a short but wrong solution (660), the other explores longer branches and finds the correct answer (2220), producing nearly 3Ã— more tokens.

## 4. Key Insights and Innovations
1) Rewardâ€‘aware computeâ€‘optimal TTS (Â§3.1; Eq. 3)
- Whatâ€™s new: Treat the verifierâ€™s reward as part of the generative processâ€”optimal hyperparameters depend on both the policy and the PRM because the PRM changes the search distribution.
- Why it matters: Explains why the best strategy varies across PRMs and why onâ€‘policy vs offâ€‘policy PRMs behave differently. Empirically, the same policy with different PRMs yields different accuracyâ€‘vsâ€‘compute curves (Figures 4â€“5).

2) Absolute difficulty thresholds (Â§3.2; Figure 3)
- Whatâ€™s new: Replace â€œquantileâ€‘basedâ€ difficulty splits (which shift with model strength) with fixed Pass@1 ranges: easy (â‰¥50%), medium (10â€“50%), hard (<10%).
- Why it matters: Avoids misleading conclusions when a strong model makes most problems â€œeasyâ€ under quantiles (Figure 3 shows Qwen2.5â€‘72B solves >80% of MATHâ€‘500 at Pass@1 on 76.2% of problems).

3) Empirical mapping from policy/PRM/difficulty to optimal TTS method (Â§4.2â€“Â§4.3; Figures 4â€“9)
- Novelty: A comprehensive matrix of results that yields actionable rules of thumb:
  - Small policies (<7B): searchâ€‘based methods > BoN (Figure 7).
  - Large policies (â‰¥32B): BoN > search (Figure 7).
  - With strong PRMs (Skyworkâ€‘7B, Qwen2.5â€‘Mathâ€‘7B/72B): search scales well; with weaker or OOD PRMs (Mathâ€‘Shepherd, RLHFlow for some settings): BoN often wins (Figure 4).
  - By difficulty: for small policies, BoN works best on easy items, beam search on hard ones; for 7Bâ€“32B, DVTS is strong on easy/medium and beam on hard; at 72B, BoN is best across all levels (Figure 8; Figure 9).

4) Diagnosing PRM biases and sensitivities (Â§4.4; Table 1â€“2; Figures 13â€“18)
- Findings:
  - Length bias: PRMs trained on longer steps favor longer generations and consume more tokens at the same budget (Table 1; narrative around RLHFlowâ€‘DeepSeek vs RLHFlowâ€‘Mistral).
  - Voting sensitivity: Skyworkâ€‘PRMâ€‘7B benefits from `PRMâ€‘Vote` over `PRMâ€‘Max`, while Qwen2.5â€‘Mathâ€‘PRMâ€‘7B is relatively insensitive (Table 2).
  - Error modes with case studies: Overâ€‘criticism (penalizing correct steps), error neglect (missing clear errors), error localization bias (penalizing the wrong step), and scoring bias by token length (Figures 13â€“18). These explain search failures and suggest training/labeling issues.

These insights go beyond incremental tweaks: they shift the recommended practice from â€œpick a search method and PRMâ€ to â€œtune TTS jointly with the policy, PRM, and difficulty, and make PRMs part of the computeâ€‘optimal formulation.â€

## 5. Experimental Analysis
- Evaluation methodology (Â§4.1)
  - Datasets: MATHâ€‘500 and AIME24 (harder Olympiadâ€‘style).
  - Metrics: accuracy (Pass@k plots show upper bounds when one of k samples is correct), final answer accuracy under each selection method, and compute budgets N.
  - Baselines: Vanilla CoT for many models; frontier systems (GPTâ€‘4o, o1 family, DeepSeekâ€‘R1); openâ€‘source longâ€‘CoT/RL methods (rStarâ€‘Math, Eurusâ€‘2, SimpleRL, Satori) in Â§5.3.
  - Fairness notes: Small models use external TTS with compute budgets up to N=256 (N=512 for one 1B case). Large proprietary baselines are evaluated with their standard longâ€‘CoT (no external search). FLOPS accounting separates preâ€‘training and inference (Table 4).

- Main quantitative results
  - Small beating large (Figure 1; Table 3):
    - 3B vs 405B: â€œLlamaâ€‘3.2â€‘3Bâ€‘Instruct (TTS) = 75.6 on MATHâ€‘500, 30.0 on AIME24â€ vs â€œLlamaâ€‘3.1â€‘405Bâ€‘Instruct (CoT) = 71.4 and 23.3.â€
    - 1B vs 405B: with larger budget N=512, â€œLlamaâ€‘3.2â€‘1Bâ€‘Instruct (TTS) = 72.2 on MATHâ€‘500,â€ exceeding 405Bâ€™s 71.4 (but falls short on AIME24: 10.0 vs 23.3) (Table 3).
    - 0.5B/3B vs GPTâ€‘4o: â€œQwen2.5â€‘0.5Bâ€‘Instruct (TTS) = 76.4 on MATHâ€‘500, 10.0 on AIME24â€ and â€œLlamaâ€‘3.2â€‘3Bâ€‘Instruct (TTS) = 75.6 / 30.0,â€ both surpass GPTâ€‘4oâ€™s 74.6 / 9.3 (Table 3).
    - 7B vs frontier longâ€‘thinkers: â€œDeepSeekâ€‘R1â€‘Distillâ€‘Qwenâ€‘7B (TTS) = 95.2 on MATHâ€‘500, 83.3 on AIME24,â€ beating â€œo1 (94.8 / 79.2)â€ and beating â€œDeepSeekâ€‘R1 (79.8 on AIME24)â€ while slightly below it on MATHâ€‘500 (97.3) (Table 3, Figure 1c,f).

  - FLOPS savings (Table 4):
    - â€œLlamaâ€‘3.2â€‘3B (TTS) total FLOPS â‰ˆ 1.62Ã—10^23â€ vs â€œLlamaâ€‘3.1â€‘405B (CoT) â‰ˆ 3.65Ã—10^25.â€ This is â‰ˆ225Ã— smaller.
    - â€œDeepSeekâ€‘R1â€‘Distillâ€‘7B (TTS) total FLOPS â‰ˆ 7.56Ã—10^23â€ vs â€œDeepSeekâ€‘R1 (CoT) â‰ˆ 5.96Ã—10^25â€ (â‰ˆ79Ã— smaller).

  - How PRMs change outcomes (Figures 4â€“5):
    - For Llamaâ€‘3.1â€‘8B, search with Skywork and Qwen2.5â€‘Math PRMs improves steadily with budget, but search with Mathâ€‘Shepherd and RLHFlow PRMs performs poorlyâ€”even worse than Majority Vote in some regimes (Figure 4).
    - For Qwen2.5â€‘7B, Skyworkâ€‘7B and Qwen2.5â€‘Math PRMs scale well; others lag (Figure 4). On AIME24, gains are smaller across the board (Figure 5).

  - Which TTS method is best (Figure 7; Â§4.2):
    - Small policies (0.5Bâ€“3B): beam/DVTS > BoN.
    - Larger policies (32Bâ€“72B): BoN > search methods.

  - Difficultyâ€‘aware findings (Figures 8â€“9; Â§4.3):
    - For small policies: BoN best on easy; beam best on hard.
    - For 7Bâ€“32B: DVTS shines on easy/medium; beam on hard.
    - At 72B: BoN dominates across difficulty levels.

  - Efficiency and effectiveness vs CoT and Majority (Â§5.2; Table 5):
    - Example: â€œLlamaâ€‘3.2â€‘1Bâ€‘Instruct: CoT=26.0, Majority=39.0, TTS=66.2,â€ a 155% improvement over CoT and >256Ã— efficiency gain vs Majority under their budget metric.
    - Gains shrink as the policy gets stronger (e.g., Qwen2.5â€‘72B: CoT 83.8 â†’ TTS 91.8, +9.5%).

  - TTS vs longâ€‘CoT training (Â§5.3; Table 6):
    - With Qwen2.5â€‘7B policy and strong PRM, TTS gets â€œ91.0/36.7,â€ outperforming rStarâ€‘Math, Eurusâ€‘2, SimpleRL, and Satori on both datasets.
    - Distilled DeepSeekâ€‘R1â€‘7B reaches â€œ92.4/63.3â€ with CoT aloneâ€”higher on AIME24 than TTS using a plain 7B policy without distillationâ€”indicating specialized longâ€‘CoT distillation still wins on the hardest problems.

- Robustness and diagnostics
  - Correlation between PRM process quality and TTS accuracy: fitted curve Y = 7.66 log(X) + 44.31, where X is ProcessBench score and Y is TTS performance (Figure 6).
  - Failure analyses exposing PRM error modes (Figures 13â€“18) substantiate why some searches derail.
  - Tokenâ€‘length bias traced to PRM training data (Table 1) explains different compute consumption.

- Do the experiments support the claims?
  - Yes for the conditional headline: with rewardâ€‘aware, computeâ€‘optimal TTS, small models can beat larger CoTâ€‘only models on MATHâ€‘500, and a 7B model can beat o1 and DeepSeekâ€‘R1 on AIME24 (Figure 1; Table 3).
  - The support is strongest when PRMs are wellâ€‘matched and budgets are tuned; it is weaker on AIME24 for plain small policies without distillation (Table 3, 6).

## 6. Limitations and Trade-offs
- Dependence on PRM quality and match
  - PRMs generalize poorly across policy families or tasks; offâ€‘policy PRMs can push search into local optima (Figures 4â€“5; Â§4.2). Rewardâ€‘aware optimality addresses this conceptually but does not remove the underlying brittleness.
- Sensitivity to hyperparameters and voting
  - Best scoring/voting combos differ by PRM (Table 2). This increases tuning burden and may complicate deployment.
- Compute budgets vs fairness
  - Comparisons pit â€œsmall+TTSâ€ vs â€œlarge+CoTâ€; both are realistic usage modes but not identical compute regimes. The FLOPS analysis (Table 4) argues in favor of small+TTS on total compute, yet wallâ€‘clock latency and memory havenâ€™t been exhaustively profiled.
- Task scope
  - Evaluation is limited to mathematical reasoning; coding, science, and multiâ€‘modal tasks remain open (Limitations, Â§7 Discussion).
- Failure modes in PRMs
  - Overâ€‘criticism and error neglect (Figures 13â€“15) can invalidate search even when the policy proposes a correct path. These indicate data/labeling issues and limit reliability.
- Diminishing returns with strong policies
  - As base models get better, TTS gains shrink (Table 5), so the costâ€‘benefit must be reconsidered for very strong LLMs.

## 7. Implications and Future Directions
- How this changes the landscape
  - Moves the field from â€œTTS is good in generalâ€ to â€œTTS must be rewardâ€‘aware and tailored to the policy, the PRM, and problem difficulty.â€ It demonstrates that careful inference strategy selection can flip performance orderings across 100Ã— parameter gaps (Figure 1; Table 3).
  - Establishes empirical rules: use search for small policies (with strong PRMs), favor BoN for large policies, and adapt the method by difficulty (Figures 7â€“9).

- Followâ€‘up research enabled
  - PRM research:
    - Training data curation to reduce overâ€‘criticism/errorâ€‘neglect and length biases (Figures 13â€“18; Table 1).
    - Weakâ€‘toâ€‘strong supervision: the paper shows a 7B PRM effectively supervises a 72B policy (Â§7 Conclusion), motivating scalable verifiers rather than everâ€‘larger PRMs.
    - More robust scoring/voting designs that are less sensitive across policy families (Table 2).
  - TTS algorithms:
    - Adaptive budget allocation â€œmidâ€‘generationâ€ based on selfâ€‘predicted uncertainty (related to Â§6 Related Work; could be combined with rewardâ€‘aware search).
    - Difficulty predictors to route problems to BoN vs beam vs DVTS automatically, following the empirical mapping in Figures 8â€“9.
  - Beyond math:
    - Apply the rewardâ€‘aware framework to coding (unit tests as rewards), scientific question answering (symbolic checkers), or multimodal reasoning (vision value models; see Â§6 Related Work).

- Practical applications
  - Costâ€‘effective deployment: small onâ€‘device or edge models augmented with PRMâ€‘guided TTS for highâ€‘accuracy math tutoring, homework checking, or exam prep.
  - Cloud inference optimization: dynamically choose BoN vs search based on prompt difficulty and model size to minimize latency/compute for a target accuracy.

---

Key citations to ground claims:
- Rewardâ€‘aware computeâ€‘optimal formulation: Â§3.1, Eq. (3).
- Difficulty thresholds: Â§3.2, Figure 3.
- TTS methods: Â§2.2, Figure 2.
- Crossâ€‘matrix results: Figures 4â€“11.
- Small vs large comparisons: Figure 1; Table 3.
- FLOPS comparisons: Table 4.
- Gains vs CoT and Majority: Â§5.2, Table 5.
- TTS vs longâ€‘CoT training: Â§5.3, Table 6.
- PRM biases and failures: Â§4.4, Table 1â€“2, Figures 12â€“18.

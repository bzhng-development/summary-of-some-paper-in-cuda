# FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores

**ArXiv:** [2311.05908](https://arxiv.org/abs/2311.05908)
**Authors:** Daniel Y.â€¯Fu, Hermannâ€¯Kumbong, Ericâ€¯Nguyen, Christopherâ€¯RÃ©
**Institutions:** Stanford University (Hazy Research)

## ðŸŽ¯ Pitch

FlashFFTConv revolutionizes long-sequence convolutions by optimizing the Fast Fourier Transform (FFT) for modern GPUs, leveraging tensor cores and kernel fusion to enhance speed and efficiency. This innovation bridges the performance gap with Transformer models, enabling competitive wall-clock speeds and scalability in domains like language, vision, and genomics, ultimately maximizing throughput and context length capabilities.

---

## 1. Executive Summary (2-3 sentences)
FlashFFTConv is a system that makes long-sequence convolutions fast and memoryâ€‘efficient on modern GPUs by rewriting the Fast Fourier Transform (FFT) as a series of matrix multiplications that run on tensor cores and by aggressively fusing kernels to cut data movement. This matters because long convolutions power stateâ€‘ofâ€‘theâ€‘art nonâ€‘Transformer sequence models, yet standard FFT implementations underutilize hardware and spend most time on memory I/O; FlashFFTConv closes that gap and enables longer contexts, higher throughput, and in several cases better accuracy for the same compute.

## 2. Context and Motivation
- Problem addressed
  - Long-sequence convolution models use filters as long as the input; naive timeâ€‘domain convolution is quadratic in sequence length. Using the FFT makes the operation O(N log N), but offâ€‘theâ€‘shelf FFTs run poorly on GPUs for long sequences: they donâ€™t use tensor cores and incur heavy memory traffic between GPU memory levels (Section 1; Figure 1 middle right).
- Why itâ€™s important
  - Convolutional sequence models (e.g., Hyena, S4, M2) deliver competitive or stateâ€‘ofâ€‘theâ€‘art quality in language, time series, vision, and genomics, and can be more stable and betterâ€‘scaling in context length than attention (Section 1). Yet they lag Transformers in wallâ€‘clock speed, limiting adoption and maximum context.
- Where prior approaches fall short
  - Classical FFT algorithms broadcast matrix operations across batch and channels, not sequence (Figure 3, top left), so fusing the endâ€‘toâ€‘end pipeline becomes infeasible once sequences exceed the capacity of onâ€‘chip memory (SRAM). Tensor cores remain largely idle because the FFT is dominated by small transforms and permutations, which are memoryâ€‘bound (Sections 1â€“2.2).
- Positioning relative to existing work
  - FlashFFTConv builds on the idea that systems advances can unlock better models (cf. FlashAttention). It leverages a structured factorization of FFTsâ€”the orderâ€‘`p` Monarch decomposition (Section 2.1; Figure 2)â€”and reorients it to exploit tensor cores and to enable kernel fusion at long lengths, adding domainâ€‘specific optimizations for real signals and causal padding (Section 3.1). It also introduces convolutional analogues of attention sparsityâ€”partial and frequencyâ€‘sparse convolutions (Section 3.3).

## 3. Technical Approach
Step-by-step, how FlashFFTConv turns a slow FFTâ€‘based convolution into a tensorâ€‘coreâ€‘friendly, IOâ€‘efficient pipeline.

- What the computation is
  - A long convolution `y = u * k` (sequence length `N`) is computed via the FFT as:
    - Conceptually: convolve in time by multiplying in frequency.
    - Formally (Equation 1): `u * k = F^{-1}(F u âŠ™ F k)`, where `âŠ™` is elementwise multiply.
  - In practice, convolution layers reuse the kernel FFT `k_f = F k` across a batch, leaving just an FFT of each input, a pointwise multiply by `k_f`, and an inverse FFT (Section 1).

- Why naive FFTs are slow on GPUs
  - Two bottlenecks arise at long `N` (Section 1):
    - Poor use of specialized matrix units (tensor cores): FFTs are implemented as many tiny operations and permutations rather than large matrix multiplies.
    - Costly I/O across the memory hierarchy (HBM â†’ SRAM â†’ registers): as sequences get large, intermediate tensors can no longer be kept onâ€‘chip, so kernels canâ€™t be fused; padding for causality and realâ†”complex conversions add extra traffic (Figure 1 left and middle right).

- Core idea 1: Monarch FFT decomposition mapped to matrix multiplies
  - Monarch decomposition expresses an `N Ã— N` FFT matrix `F_N` as a product of `p` blockâ€‘structured transforms (Section 2.1; Figure 2). For orderâ€‘2 (twoâ€‘way):
    - `F_N = P (I_{N2} âŠ— F_{N1}) D P^{-1} (I_{N1} âŠ— F_{N2}) P`, with:
      - `âŠ—` Kronecker product (applies many small FFTs in parallel),
      - `P` permutations (reshape/transpose bookkeeping),
      - `D` diagonal twiddle factors (phase corrections).
  - Higher order `p` recursively applies this factorization, trading more, smaller matrix multiplications for additional permutations (Section 2.1). Crucially, each factor can be executed as matrix multiplications sized to tensor cores.

- Core idea 2: Broadcast along the sequence to enable fusion
  - Classical FFT variants broadcast the small FFTs over batch and channels (Figure 3 top left), which forces loading many sequences concurrently to fill tensor coresâ€”unsuitable for long sequences.
  - FlashFFTConv flips the broadcast dimension to the sequence itself (Figure 3 top right):
    - Each small transform multiplies a block along the length dimension, and the algorithm runs in parallel across batch (`B`) and channels (`H`).
    - The expensive global permutations become fast onâ€‘chip matrix transposes (Figure 3 bottom).
    - Result: only a single sequence needs to live in SRAM per SM, allowing kernel fusion up to 32K tokens on A100/H100 and fused innermost steps even beyond (Section 3.1; Algorithm 1).

- Core idea 3: Kernel fusion and recomputation
  - For long sequences, inner matrix multiplications and elementwise ops are fused and kept onâ€‘chip; only the outermost steps touch HBM (Section 3.1).
  - Backward pass uses recomputation rather than storing large intermediates (e.g., reâ€‘do `F u` instead of saving it), cutting memory footprint and I/O (Section 3.1; Tables 16â€“17).

- Domainâ€‘specific optimizations for sequence learning
  - Realâ€‘toâ€‘real FFT: since inputs and kernels are real, a standard trick computes a sizeâ€‘`N` real FFT via a sizeâ€‘`N/2` complex FFT (Appendix A.1). FlashFFTConv implements this â€œdecimation in timeâ€ method to halve FFT cost (Section 3.1, â€œDomainâ€‘Specific Optimizationsâ€).
  - Implicit causal padding: causal convolutions zeroâ€‘pad inputs; FlashFFTConv recognizes these zeros and skips half of the outermost matmuls in the FFT/iFFT (Section 3.1).
  - Fuse common gating: many longâ€‘conv blocks use multiplicative gating `y = v âŠ™ ((u âŠ™ w) * k)`; FlashFFTConv fuses the two elementwise multiplications into the FFT pipeline to avoid extra HBM reads/writes (Section 3.1 and Table 4).

- Orderâ€‘`p` cost model: choosing how much to factorize
  - Intuition: larger `p` yields smaller matmuls (fewer FLOPs) but introduces more intermediate results (more I/O). There is an optimal `p` that depends on sequence length and hardware (Section 3.2).
  - Plain language version of Equation 2: total cost = compute time of the `p` matmul stages + I/O time to move the `p` intermediate results through memory. It accounts for whether each matmul meets the minimal size to run on tensor cores.
  - Formal (Equation 2): `C = B H âˆ‘_{i=1}^p [ 16 N N_i / Î³(N_i) + 4 N / Ï‰(i) ]`, where:
    - `N = Î _i N_i` is the factorization, `Î¼` is the tensorâ€‘core tile size (e.g., 16 on A100/H100),
    - `Î³(N_i)` equals tensorâ€‘core FLOPs `Ï„_M` if `N_i â‰¥ Î¼`, else general FLOPs `Ï„_G`,
    - `Ï‰(i)` is the bandwidth of the memory level used at stage `i` (HBM vs SRAM),
    - Empirical constants for A100 are given in Appendix C/Table 19.
  - Figure 4 shows perâ€‘token cost vs sequence length for `p âˆˆ {2,3,4}` on A100:
    - At short lengths, higher `p` hurts (matrices fall below tensorâ€‘core size; â€œMatrices Too Smallâ€).
    - At mid/long lengths, higher `p` helps until SRAM becomes the bottleneck (the bump for `p=3` near 32Kâ€“64K is from exhausting SRAM; `p=4` regains ground by further factoring).

- Architectural extensions: sparsity mapped to skipped matmuls
  - Partial convolutions: learn shorter kernels (like local attention). Implementation: skip parts of the FFT pipeline corresponding to trailing zeros in time domain; reduces memory and enables slidingâ€‘window extension to longer contexts (Section 3.3; Section 4.3).
  - Frequencyâ€‘sparse convolutions: zero out portions of `k_f` (frequency response). Implementation: skip specific blocks inside the Monarch matmuls that would multiply by zero, yielding actual compute savings without changing outputs (Section 3.3; Appendix A.4 explains which blocks can be skipped in 4â€‘way decomposition).

- Lowâ€‘level CUDA execution (Appendix A.2â€“A.5)
  - Uses the WMMA API to run 16Ã—16Ã—16 fp16/bf16 matmuls on tensor cores; carefully aligns data layouts so accumulator fragments can be reused as inputs to avoid SRAM roundâ€‘trips (Algorithm 2).
  - Doubleâ€‘buffered I/O across memory levels, vectorized loads/stores, warpâ€‘level tiling across `B` and `H`.
  - Hardware support currently targets A100/H100; V100 not supported due to different tensorâ€‘core tile sizes (Appendix A.5).

## 4. Key Insights and Innovations
- Turning FFTs into tensorâ€‘core matmuls at long sequence lengths
  - Novelty: Adapts the Monarch factorization specifically to broadcast over the sequence dimension, not batch/channels (Figure 3), so the FFTâ€™s heavy lifting becomes wellâ€‘sized matmuls for tensor cores.
  - Significance: Converts a traditionally memoryâ€‘bound primitive into computeâ€‘efficient steps with high FLOP utilization; enables fusion and reduces HBM traffic (Section 3.1).
- IOâ€‘aware orderâ€‘`p` factorization with a simple rooflineâ€‘style cost model
  - Novelty: Equation 2 blends compute throughput (tensorâ€‘core vs general) with memory bandwidth at each factorization stage; selects `p` based on sequence length and perâ€‘GPU constants (Figure 4; Appendix C/Table 19).
  - Significance: Explains when and why to change decomposition order as `N` grows; predicts the â€œbumpsâ€ seen in practice (e.g., `p=3` bump near 32Kâ€“64K).
- Domainâ€‘specific fusion for real, causal, gated longâ€‘conv blocks
  - Novelty: Integrates â€œreal FFT via N/2 complex FFTâ€ (Appendix A.1), implicit causal padding, and common gating `y = v âŠ™ ((u âŠ™ w) * k)` into the fused pipeline (Algorithm 2).
  - Significance: Delivers the largest measured speedupsâ€”up to 7.93Ã— vs PyTorch for gated convolutions (Table 4)â€”and the biggest memory savings in endâ€‘toâ€‘end models (Tables 16â€“17).
- Convolutional analogues of sparse/approximate attention
  - Novelty: Defines partial (timeâ€‘domain) and frequencyâ€‘sparse (frequencyâ€‘domain) convolution schemes that map cleanly to â€œskipping blocksâ€ in the Monarch matmuls (Section 3.3; Appendix A.4).
  - Significance: Enables longerâ€‘sequence modeling (first singleâ€‘nucleotide embeddings of the longest human genes at 2.3M bp; Table 8) and further runtime savings at the same or better quality (Table 9).

## 5. Experimental Analysis
- Evaluation setup
  - Benchmarks span synthetic kernels and full models across modalities and sequence lengths from 256 to 4M (Tables 3â€“4, 11â€“17).
  - Models include M2â€‘BERTâ€‘base (masked LM), Hyena small (GPTâ€‘style), a longâ€‘conv model on Long Range Arena Pathâ€‘X/Pathâ€‘512, SaShiMi (audio), and HyenaDNA (genomics) (Sections 4.1â€“4.2; Table 5).
  - Metrics: wallâ€‘clock time, sequences/tokens per second, memory footprint, FLOP utilization, perplexity (PPL), GLUE score, and task accuracy.

- Main results (quantitative, with citations)
  - Convolution kernels
    - Forward speedups vs PyTorch up to 6.54Ã— for plain conv (Table 3, 1K seq), and up to 7.93Ã— for gated conv (Table 4, 1K seq).
    - Memory savings up to 8.21Ã— for conv (Table 3/16, 256 seq) and 6.65Ã— for gated conv (Table 4/17, 256 seq), still 2.6â€“2.8Ã— at millionâ€‘token scale (Tables 16â€“17).
    - Backward pass is also faster: 1.45â€“6.43Ã— over PyTorch depending on length (Table 15).
  - Endâ€‘toâ€‘end model throughput (Table 5)
    - M2â€‘BERTâ€‘base (128 tokens): 1.9Ã— sequences/s.
    - Hyenaâ€‘sâ€‘4K: 1.7Ã— sequences/s.
    - Pathâ€‘X conv model (16K): 2.4Ã— images/s.
    - SaShiMi (64K audio): 1.3Ã— clips/s (convolutions are a smaller fraction of endâ€‘toâ€‘end time here).
    - HyenaDNAâ€‘1M: 4.4Ã— sequences/s by enabling a 4Ã— larger batch than PyTorch.
  - Quality at fixed compute (Table 1)
    - â€œMore training for the same budgetâ€ effect: higher throughput lets models see more tokens.
    - Reported gains: 
      > M2â€‘BERTâ€‘base average GLUE: 77.6 â†’ 80.9  
      > Hyenaâ€‘s perplexity on The Pile: 13.4 â†’ 11.1
  - Longâ€‘context capability (Table 2)
    - Pathâ€‘512 (sequence 256K): prior convolutional setups OOM, yet
      > FlashFFTConv achieves 96.1% accuracy.  
      Pathâ€‘X (16K) remains at 96.9% (no regression).
  - Transformer comparison (Table 6, same 2.7B params)
    - Tokens/s: Hyena with FlashFFTConv is faster at 2K, 8K, 16K (1.1Ã—, 1.3Ã—, 1.5Ã—).
    - FLOP utilization endâ€‘toâ€‘end: FlashFFTConv ~56â€“62% vs FlashAttentionâ€‘v2 66â€“79%. Despite lower utilization, convolution has fewer FLOPs, so wallâ€‘clock wins.
  - Partial convolutions (Tables 7â€“8)
    - Training memory reduction with little/no loss in quality for Hyenaâ€‘sâ€‘8K: convolution kernel can be shortened down to 2K with essentially unchanged PPL (Table 7).
    - Extending HyenaDNA to longer sequences via sliding window over short filters:
      > At 4M length, PPL matches or slightly improves (2.91 â†’ 2.90) while enabling embedding of the longest human genes (Table 8; Appendix Figure 5).
  - Frequencyâ€‘sparse convolutions (Table 9; Appendix A.4)
    - Zeroing 50â€“79% of frequency coefficients leaves PPL unchanged (2.91â€“2.90), and
      > yields up to 1.4Ã— extra speedup in the convolution.  
      Quality starts to degrade beyond ~84% sparsity.

- Do the experiments support the claims?
  - Speed and memory: Yes. Multiple sequence lengths, forward/backward breakdowns, and endâ€‘toâ€‘end models show consistent gains. Tables 3â€“4 and 11â€“15 confirm that the claimed â€œup to 7.93Ã—â€ kernel speedup is achieved in realistic guarded/gated convs; memory reductions match the recomputation/fusion story (Tables 16â€“17).
  - Quality at fixed compute: Plausible and quantified. Throughput gains translate to more tokens seen; the measured improvements (Table 1) are in line with scalingâ€‘law expectations and are calibrated against larger baselines (Appendix B.2, Table 18).
  - Longâ€‘context and sparsity: Convincing. Pathâ€‘512 success (Table 2) demonstrates concrete new capability; partial/frequencyâ€‘sparse results include both quality and speed metrics (Tables 7â€“9) and explainability via skipâ€‘patterns (Appendix A.4).

- Ablations and robustness
  - Fusionâ€‘only (no tensor cores) baselines show that tensorâ€‘core matmuls are necessary for longâ€‘sequence performance; otherwise the kernel becomes computeâ€‘bound on general ALUs and runs out of SRAM beyond 32K (Table 3, â€œFusionâ€‘Only/cuFFTdxâ€).
  - Domainâ€‘specific fusions (gating, causal) provide additional measurable gains (Table 4; Appendix B.1 Tables 13â€“14).
  - Costâ€‘model sanity: Figure 4 explains where each `p` dominates and matches observed performance transitions (e.g., SRAM limit bump for `p=3` near 32â€“64K).

## 6. Limitations and Trade-offs
- Hardware specificity
  - Current implementation is optimized for NVIDIA A100/H100 tensorâ€‘core tile sizes and memory characteristics (Appendix A.5; Appendix C/Table 19). Older GPUs (e.g., V100) are not supported; portability to nonâ€‘GPU accelerators is future work.
- Diminishing speedups at extreme lengths
  - At multiâ€‘million tokens, speedups over PyTorch narrow (e.g., 1.3â€“1.8Ã— at 2â€“4M; Tables 3â€“4, 11â€“14) as SRAM and HBM I/O dominate and only outermost stages remain unfused.
- Orderâ€‘`p` selection depends on hardware and `N`
  - The optimal factorization changes with sequence length and memory limits (Figure 4). Misâ€‘tuned `p` (e.g., matrices too small for tensor cores) can lose much of the benefit.
- Precision and numerical considerations
  - The implementation leans on fp16/bf16 tensor cores (Appendix A.2). Although standard in DL, some applications may require higher precision or careful scaling to avoid numerical artifacts in frequency space when using sparsity.
- Applicability scope
  - The strongest benefits show up for long convolutions with shared kernels across batch (standard in sequence models). Workloads with tiny kernels (e.g., typical 2D convs in vision) arenâ€™t the target; those already use different fast paths.
- Sparsity patterns and learned kernels
  - Frequencyâ€‘sparse zeroing is applied postâ€‘pretraining for HyenaDNA; while small to moderate sparsification preserves quality (Table 9), the optimal pattern may be modelâ€‘ and taskâ€‘dependent (Appendix A.4). Learning sparsity during training remains open.

## 7. Implications and Future Directions
- How this changes the landscape
  - By making longâ€‘sequence convolutions competitive in wallâ€‘clock time and memory with highly optimized attention, FlashFFTConv removes a key systems barrier. This strengthens the case for convolutionâ€‘ and stateâ€‘spaceâ€‘based architectures in domains where long context, stability, or linearâ€‘time scaling is advantageous (Section 4.2; Table 6).
- Research directions enabled
  - Hardwareâ€‘aware algorithm design for other fast transforms (e.g., wavelets, Chebyshev) via Monarchâ€‘style factorizations and broadcastâ€‘alongâ€‘sequence patterns.
  - Learning partial/frequencyâ€‘sparse structure endâ€‘toâ€‘end (jointly selecting skipâ€‘blocks in the Monarch matmuls) as a principled analogue to sparse attention patterns (Section 3.3; Appendix A.4).
  - Automated `p`â€‘selection and tiling using the cost model (Equation 2) plus online profiling; extending to heterogeneous clusters (HBM tiers, NVLink/PCIe effects).
  - Portability beyond NVIDIA: reâ€‘implement using platformâ€‘agnostic libraries (e.g., CUTLASS variants) and adapt to emerging accelerators with different tensorâ€‘core shapes (Appendix A.5).
- Practical applications
  - Longâ€‘context language models that maintain high throughput beyond 8â€“16K tokens (Table 6).
  - Highâ€‘resolution vision or spatial reasoning tasks requiring hundreds of thousands of tokens (Table 2).
  - Genomics at singleâ€‘nucleotide resolution over multiâ€‘megabase regions, including embedding the longest human genes (Table 8; Appendix Figure 5).
  - Timeâ€‘series and audio models operating at native sampling rates over long windows with efficient training and inference (Table 5, SaShiMi).

> In sum, FlashFFTConv reframes the FFT for long convolutions as a sequenceâ€‘broadcasted, tensorâ€‘coreâ€‘friendly pipeline with IOâ€‘aware factorization and fusion (Algorithms 1â€“4; Figures 2â€“4). The measured kernelâ€‘level and endâ€‘toâ€‘end gains (Tables 3â€“6, 11â€“17), the quality improvements at fixed compute (Table 1), and the new longâ€‘context capabilities (Tables 2, 7â€“9) collectively support its central claim: efficient FFT convolutions at long sequence lengths are achievable on todayâ€™s accelerators, and doing so unlocks both better models and new applications.

# A Survey of Reinforcement Learning for Large Reasoning Models

**ArXiv:** [2509.08827](https://arxiv.org/abs/2509.08827)
**Authors:** Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, +17 more authors
**Institutions:** Tsinghua University (TsinghuaC3I)

## ğŸ¯ Pitch

This paper introduces a unified framework for leveraging reinforcement learning in large reasoning models by systematically organizing the field into three critical components: reward design, policy optimization, and sampling strategies. By transforming dispersed methodologies into a coherent blueprint, it highlights how reinforcement learning can provide reasoning advantages over traditional supervised fine-tuning, enabling scalable, automated feedback particularly in domains like math and code. This new understanding paves the way for more effective AI models with enhanced problem-solving capabilities and opens avenues for practical applications in fields requiring complex reasoning.

---

## 1. Executive Summary (2-3 sentences)
This survey systematizes the fastâ€‘moving area of â€œreinforcement learning for large reasoning models (LRMs)â€ by organizing it into three interacting pillarsâ€”reward design, policy optimization, and samplingâ€”plus training resources and applications. Its significance is to turn scattered recipes from recent model releases (e.g., OpenAI o1, DeepSeekâ€‘R1) into a coherent blueprint for scaling reasoning with reinforcement learning (RL), clarifying when and how RL yields genuine reasoning gains beyond standard supervised fineâ€‘tuning (SFT).

## 2. Context and Motivation
- Problem/gap
  - Reasoningâ€‘capable language models (LMs) have surged after systems like o1 and DeepSeekâ€‘R1 used RL with automatically checkable (â€œverifiableâ€) rewards. Yet the field lacks a unifying map: What types of rewards actually scale? Which RL algorithms matter? How should we sample trajectories? What infrastructure and datasets are required? Where does RL help more than SFTâ€”and why?
  - Section 1 and Figure 2 frame the shift from alignmentâ€‘oriented RLHF/DPO to â€œRL with Verifiable Rewardsâ€ (RLVR) that directly incentivizes task solving (math, code). The survey targets foundational components, open problems, resources, and applications (Figure 1).
- Importance
  - Practical: Verifiable domains (math, code, some scientific tasks) allow scalable automated feedback, unlocking training signals not bounded by human labeling (Sections 3.1.1 and 3.1.4).
  - Theoretical: RL introduces a new scaling axisâ€”â€œtrainâ€‘time RLâ€ and â€œtestâ€‘time compute (thinking time)â€â€”orthogonal to model size and preâ€‘training data (Section 1).
- Prior approaches and shortcomings
  - Alignment methods (RLHF, DPO) optimize preferences and safety but do not directly teach problem solving; they also depend on noisy learned reward models and humans (Section 1; Figure 2).
  - Pure SFT often memorizes solutions and can harm generalization to outâ€‘ofâ€‘distribution tasks (Section 4.2).
- Positioning
  - The paper formalizes RL for LLMs (Section 2.1; Figure 3; Eq. (1)), then builds a taxonomy:
    - Reward design (verifiable, generative, dense/process, unsupervised, shaping) in Section 3.1.
    - Policy optimization (criticâ€‘based vs. criticâ€‘free, offâ€‘policy, regularization) in Section 3.2.
    - Sampling strategies (dynamic/structured and hyperâ€‘parameters) in Section 3.3.
  - It then examines five foundational controversies (Section 4), catalogs training resources (Tables 4â€“6), and synthesizes application domains (Figure 6).

## 3. Technical Approach
This is a survey, but it provides a precise conceptual and algorithmic framework that explains how RL for LRMs actually works.

- Formal problem setup (Section 2.1; Figure 3; Eq. (1))
  - Map language generation to an RL Markov Decision Process (MDP):
    - State `s_t`: the prompt plus tokens generated so far.
    - Action `a_t`: the next token (or segment/response).
    - Transition: deterministically concatenates `a_t` to the context.
    - Reward `R(x, y)`: may be given at sequenceâ€‘level (sparse), tokenâ€‘level, stepâ€‘level (a â€œstepâ€ can be a sentence, a reasoning step, or a turn in an agent loop).
  - Objective: maximize expected return over prompts (Eq. (1)) with optional regularization toward a reference policy.
  - Granularity matters (Table 2): trajectoryâ€‘level rewards support simple banditâ€‘style updates; token/step/turnâ€‘level rewards enable denser credit assignment.

- Pillar 1 â€” Reward design (Section 3.1)
  - Verifiable rewards (Section 3.1.1)
    - How it works: For math, enforce a parseable answer format (e.g., `\boxed{â€¦}`) and compare against ground truth using programmatic checkers; for code, compile or run unit tests. Format constraints (special `<think>` and `<answer>` fields) ensure reliable parsing at scale.
    - â€œVerifierâ€™s Lawâ€: tasks become easy to train once reliable automated verification exists (Section 3.1.1).
  - Generative rewards (Section 3.1.2)
    - Modelâ€‘based verifiers reduce brittleness of rule systems (they judge semantic equivalence, not exact strings).
    - Reasoning reward models (RMs) â€œthink before judgingâ€: they generate critiques/rationales and then output a scalar preference or score; some are themselves trained with RL using verifiable metaâ€‘rewards.
    - Rubricâ€‘based rewards structure subjective evaluation into checklists (e.g., writing quality), enabling RL beyond pure correctness signals.
    - Coâ€‘evolving systems unify policy and rewardâ€”either a single model selfâ€‘rewards (â€œselfâ€‘judgesâ€) or the policy and RM are coâ€‘optimized in one loop.
  - Dense rewards (Section 3.1.3; Table 2)
    - Tokenâ€‘level signals: implicit PRMs (process reward models) induce tokenâ€‘wise rewards from outcomes or learned oracles.
    - Stepâ€‘level signals: two strategies:
      - Modelâ€‘based PRMs to score intermediate steps (risk: reward hacking).
      - Samplingâ€‘based Monte Carloâ€”branch the reasoning tree, evaluate outcomes of partial steps, backâ€‘propagate credit; includes tree search (TreeRL, TreeRPO) and â€œforce stoppingâ€ at intermediate points to estimate step values.
    - Turnâ€‘level signals for multiâ€‘turn agents: either explicitly reward each actionâ€‘result turn or decompose sessionâ€‘level rewards back to turns (credit attribution).
  - Unsupervised rewards (Section 3.1.4)
    - Modelâ€‘specific (no external labels): majority/consensus voting across samples; internal confidence (entropy/probability/attention); selfâ€‘rewarding/selfâ€‘instruction curricula.
    - Modelâ€‘agnostic: heuristic rules (format/length) and dataâ€‘centric RL that reframes nextâ€‘token prediction as an RL problem (RPT).
  - Reward shaping (Section 3.1.5)
    - Ruleâ€‘based mixturesâ€”combine verifiers with RMs to avoid 0/1 rewards and improve gradients.
    - Structureâ€‘based shapingâ€”groupâ€‘wise baselines over a set of candidates for the same prompt (e.g., GRPO), or transform rewards to align with Pass@K metrics.

- Pillar 2 â€” Policy optimization (Section 3.2)
  - Policy gradient objective (Section 3.2.1; Eq. (5))
    - Intuition: increase the probability of aboveâ€‘average actions (`advantage > 0`) and decrease belowâ€‘average ones; PPO uses a clipped ratio to stabilize updates.
  - Criticâ€‘based algorithms (Section 3.2.2)
    - PPO with a value function (â€œcriticâ€) supplies tokenâ€‘wise advantages via GAE (Eqs. (8)â€“(9)). Scales when you can reliably train a critic/reward model, but incurs extra compute and risk of reward hacking.
  - Criticâ€‘free algorithms (Section 3.2.3)
    - REINFORCE and relatives: treat the whole sequence as one action; stabilize via baselines (e.g., greedy baseline in ReMax or leaveâ€‘oneâ€‘out in RLOO).
    - GRPO (Eq. (11)â€“(12)): compute a groupâ€‘relative advantage by normalizing each responseâ€™s reward by the mean/std over G candidates for the same prompt; apply PPOâ€‘style clipping but no learned critic. This is favored in RLVR because rewards are reliable and the method is simpler and cheaper.
    - Enhancements: DAPO (decoupled clipping and dynamic sampling), CISPO (importance weighting), GSPO (sequenceâ€‘level clipping), VinePPO (Monteâ€‘Carlo advantages), FlowRL (optimize reward distributions to avoid mode collapse).
    - Importance sampling: needed because rollouts lag parameter updates; most methods approximate tokenâ€‘wise ratios; newer variants explore sequenceâ€‘level ratios (GSPO) or geometric means (GMPO) to reduce variance.
  - Offâ€‘policy optimization (Section 3.2.4)
    - Learn from â€œolderâ€ trajectories or offline datasets; use replay buffers and truncated importance sampling to limit bias; combine SFTâ€‘style losses with RL (UFT, SRFT, mixedâ€‘policy training).
  - Regularization (Section 3.2.5)
    - KL regularization: toward a reference model or prior policy; opinions divergeâ€”some remove KL entirely to let exploration diverge (beneficial in reasoning RLVR), others retain adaptive KL for stability.
    - Entropy regularization: maintain exploration but can destabilize sparseâ€‘reward training; practical recipes include emphasizing highâ€‘entropy tokens or constraining covariance between probabilities and advantages.
    - Length penalties: encourage concise reasoning, sometimes conditioned on difficulty.

- Pillar 3 â€” Sampling strategies (Section 3.3)
  - Dynamic sampling (Section 3.3.1)
    - Efficiencyâ€‘oriented: oversample â€œmedium difficultyâ€ questions that still yield nonâ€‘zero advantages; prioritize failureâ€‘prone items; curriculum by category or difficulty; reuse rollouts via replay.
    - Explorationâ€‘oriented: branch at highâ€‘attention or highâ€‘uncertainty steps; add guided prefixes or rubrics; keep â€œallâ€‘wrongâ€ items but inject intermediate guidance to bootstrap.
  - Structured sampling (Section 3.3.1)
    - Treeâ€‘structured rollouts with Monte Carlo Tree Search (MCTS) and nodeâ€‘level rewards; sharedâ€‘prefix/segment sampling to reuse KV caches and cut compute.
  - Hyperâ€‘parameters (Section 3.3.2)
    - Temperature schedules to control exploration; staged contextâ€‘length curricula (e.g., 8kâ†’16kâ†’24kâ†’32k) to teach efficient short reasoning before enabling long chains; mixed strategies for overâ€‘length responses (masking vs. soft penalties).

- Training resources and system plumbing
  - Static corpora (Section 5.1; Table 4): curated math/code/STEM/agent datasets with verifiable outcomes and, increasingly, process traces.
  - Dynamic environments (Section 5.2; Table 5): programmatic logic/maths/code gyms, GUI/web agents, and modelâ€‘based arenas that provide interactive, dense feedback.
  - Infrastructure (Section 5.3; Table 6): open RL runtimes (TRL, OpenRLHF, Verl, AReaL, ROLL, slime, RLinf) built atop vLLM/SGLang for serving and FSDP/Megatron/DeepSpeed for training; many support asynchronous rollouts and agentic RL.

## 4. Key Insights and Innovations
- A unifying, mechanismâ€‘level taxonomy of RL for LRMs (Section 3; Figure 5)
  - Novel because it ties concrete design choices (reward granularity, algorithm family, sampling topology) to failure modes (reward hacking, entropy collapse) and scaling levers (group baselines, tree sampling). This moves beyond mere lists of techniques to â€œhow the pieces interact.â€
- Centering â€œVerifierâ€™s Lawâ€ for scalable reasoning (Section 3.1.1)
  - Fundamental insight: if a domain admits fast, reliable automated checks, RL can scale without humans. This explains why math and code are the leading edges (ruleâ€‘based checkers and unit tests) and why openâ€‘ended writing still struggles (subjective rewards).
- A clear articulation of the RL vs. SFT boundary (Section 4.2)
  - Synthesizes evidence that â€œSFT memorizes, RL generalizesâ€ under distribution shift, but also explains conditions where SFT plus careful weighting or warmâ€‘up is beneficial and when RL is not a panacea.
- Reconciling the â€œSharpening vs. Discoveryâ€ debate (Section 4.1)
  - New perspective: RL can both concentrate probability on latent correct modes (sharpening via reverseâ€‘KL dynamics) and, given time and exploration, compose skills into new behaviors (discovery). The survey identifies metrics (Pass@K vs. CoTâ€‘Pass@k) and training recipes that push one or the other.
- Endâ€‘toâ€‘end view of resources and systems (Sections 5â€“6; Figure 6)
  - The compilation of static corpora (Table 4), dynamic environments (Table 5), and RL infrastructure (Table 6) provides a practical, reproducible path from research insight to deployed agentic systems.

## 5. Experimental Analysis
While this is a survey, it aggregates quantitative evidence, recipes, and ablations from many studies and provides structured comparisons.

- Evaluation methodology and scope
  - Models and timelines (Figure 4; Table 1): catalogs public and proprietary LRMs trained with RL (e.g., DeepSeekâ€‘R1 671B MoE; QwQâ€‘32B; Internâ€‘S1 241B; Minimaxâ€‘M1 456B), along with algorithms such as GRPO, MPO, CISPO, and GSPO.
  - Algorithm comparison (Table 3): contrasts PPO/GRPO variants by advantage estimate, importance sampling, and loss aggregation level (token vs. sequence).
  - Reward/action granularity (Table 2): clarifies how returns are computed at trajectory, token, step, and turn levels.
- Representative quantitative findings cited in the survey
  - Length and efficiency:
    - â€œSâ€‘GRPO â€¦ shortens sequence length by 35â€“61% across multiple benchmarks, with slight improvements in accuracyâ€ (Section 4.4).
  - Generalization and data efficiency:
    - Oneâ€‘shot RLVR â€œmore than doubled MATH500 accuracy for a 1.5B modelâ€ and improved averages across multiple math benchmarks (Section 4.3).
  - Exploration and stability:
    - Dynamic sampling that filters allâ€‘correct and allâ€‘wrong batches (DAPO) yields â€œstateâ€‘ofâ€‘theâ€‘artâ€ AIME24 performance with reproducible recipes (Section 4.4).
  - Pass@K alignment:
    - Setâ€‘level objectives and reward transformations (Walder & Karkhanis; Chen et al.) derive unbiased/lowâ€‘variance estimators to optimize Pass@K directly (Section 3.1.5).
- Resources and scale indicators
  - Static corpora contain up to millions of verifiable reasoning tracesâ€”for example, OpenMathReasoning at 5.5M (Table 4), AMâ€‘DeepSeekâ€‘R1â€‘0528â€‘Distilled at 2.6M, and MegaScience at 2.25M; code datasets like OpenCodeReasoning at 735K and rStarâ€‘Coder at 592K.
  - Dynamic environments span logic puzzles (AutoLogi: 2,458/6,739 puzzles), GUI agents (AgentCPMâ€‘GUI: 55K trajectories), and modelâ€‘based TextArena with 99 adversarial games (Table 5).
- Do experiments support the claims?
  - The compiled results consistently show that RL with verifiable rewards (RLVR) improves Pass@1 and toolâ€‘use reliability in math and code and increasingly in agents (Sections 2.2 and 6). Where claims are mixed (e.g., whether RL â€œdiscoversâ€ skills), the survey presents both counterâ€‘evidence (e.g., â€œLimitâ€‘ofâ€‘RLVRâ€ observing worse largeâ€‘K Pass@K; Section 4.1) and techniques that address it (selfâ€‘play synthesis; Pass@Kâ€‘aligned objectives).
- Ablations and robustness
  - The survey highlights ablations such as:
    - Removing KL vs. adaptive KL: many RLVR pipelines now omit KL for freer exploration, but several works use adaptive or tokenâ€‘dependent KL to preserve knowledge (Section 3.2.5).
    - Entropyâ€‘control ablations: highâ€‘entropy token emphasis vs. explicit entropy loss vs. covariance clipping strategies to avoid collapse (Section 3.2.5).
    - Sampling ablations: mediumâ€‘difficulty filtering and replay significantly stabilize GRPO (Section 3.3.1; DAPO, PRIME).
- Failure cases and conditions
  - Reward hacking appears when modelâ€‘based PRMs are used without strong verifiers (Sections 3.1.3 and 3.1.2).
  - RL does not always beat SFT under severe overfitting or abrupt distribution shifts (Section 4.2).
  - Entropy collapse and length sprawl are recurring issues if not explicitly managed (Sections 3.2.5 and 3.3.2).

> â€œOutcome rewards provide scalable goal alignment with automated verification, while process rewards offer interpretable dense guidanceâ€ (Section 4.5). This duality explains both the successes (math/code) and the remaining brittleness (openâ€‘ended writing, subjective judgments).

## 6. Limitations and Trade-offs
- Assumptions and prerequisites
  - Reliable verifiers or reward proxies exist (Verifierâ€™s Law). Without them, generative/rubricâ€‘based rewards are noisy and prone to gaming (Sections 3.1.1â€“3.1.2).
  - Groupâ€‘based training expects multiple rollouts per prompt (GRPO), increasing inference cost (Section 3.2.3).
- Scope gaps
  - Openâ€‘ended subjective tasks remain hard; rubricâ€‘based rewards help but are not as scalable or robust as ruleâ€‘based checks (Sections 3.1.2 and 4.5).
  - Credit assignment for very long chains is still expensive; tree search and step stopping help but raise compute (Section 3.1.3).
- Computational constraints
  - RLVR requires repeated sampling with high temperatures, long contexts, and often multiple candidates per prompt; compute and latency are bottlenecks (Sections 3.3 and 5.3).
  - Asynchronous actors/learners and replay improve utilization but create offâ€‘policy drift that must be controlled (Section 3.2.4).
- Algorithmic tradeâ€‘offs
  - Removing KL improves exploration but risks knowledge drift; adding KL can overâ€‘constrain progress (Section 3.2.5).
  - Process rewards increase stability but invite reward hacking if PRMs are weak; outcomeâ€‘only rewards are scalable but suffer from credit assignment sparsity (Sections 3.1.3 and 4.5).
- Open questions
  - When does RL truly â€œdiscoverâ€ vs. â€œsharpenâ€? The survey provides hypotheses and metrics but no definitive boundary (Section 4.1).
  - How to generalize RL beyond verifiable domains without heavy human oversight (Section 7).

## 7. Implications and Future Directions
- How this changes the landscape
  - RL is becoming a core mechanism for scaling reasoning, not merely alignment. The field is coalescing around RLVR with criticâ€‘free updates (GRPO family), dynamic sampling, and lengthâ€‘aware trainingâ€”all supported by standardized verifiers, open corpora, and asynchronous RL infrastructures (Sections 2.2, 3, and 5).
- Followâ€‘up research avenues (Section 7)
  - Continual RL (Section 7.1): Lifelong, multiâ€‘stage RL that preserves past skills while learning new tasks; needs replay, policy reuse, and reward shaping tailored to LRMs.
  - Memoryâ€‘based RL (Section 7.2): Turn taskâ€‘specific memories into general experience repositories; learn memory operations via RL to reuse strategies across tasks.
  - Modelâ€‘based RL (Section 7.3): Integrate world models (including videoâ€‘trained ones) to provide rich state and synthetic rewards for agents in GUI/web/robotics domains.
  - Efficient reasoning (Section 7.4): Learn computeâ€‘allocation policies (adaptive halting, difficultyâ€‘aware budgets) to minimize overthinking and underthinking.
  - Latentâ€‘space reasoning (Section 7.5): Move from tokenâ€‘space CoT to continuous latent thought optimized with RL; requires new reward estimators for latent trajectories.
  - RL in preâ€‘training (Section 7.6): RL as a scalable preâ€‘training objective (e.g., RPT), potentially reducing dependence on nextâ€‘token prediction alone.
  - RL for diffusion LLMs (Section 7.7): Solve ELBO variance and guide multiâ€‘step denoising with intermediate rewards; mixed ODE/SDE sampling for explorationâ€“efficiency balance.
  - Scientific discovery (Section 7.8): Use soft verifiers (biological models, simulations) to scale verifiable rewards beyond math/code; couple labâ€‘inâ€‘theâ€‘loop agents with inâ€‘silico training.
  - Architectureâ€“algorithm coâ€‘design (Section 7.9): Treat routing/sparsity/expert activation as RL actions to optimize both accuracy and hardware efficiency.

- Practical applications (Figure 6; Section 6)
  - Code generation and repositoryâ€‘level engineering (unit tests and CI as rewards).
  - Agentic search/deep research on the web (browser and tool use with outcome/process rewards).
  - GUI/computerâ€‘use agents with environmentâ€‘derived success signals.
  - Multimodal reasoning and generation (vision/video/3D) with verifiable attributes.
  - Robotics with Visionâ€‘Languageâ€‘Action models trained via sparse success signals and GRPO/PPO.
  - Medical reasoning with ruleâ€‘based (verifiable) and rubricâ€‘based (nonâ€‘verifiable) rewards.

In short, this survey provides a mechanismâ€‘centric map that practitioners can follow endâ€‘toâ€‘end: pick a verifiable task (or build a surrogate verifier), choose criticâ€‘free GRPOâ€‘style optimization with dynamic sampling and length control, reuse open datasets/environments, and scale via asynchronous RL infrastructureâ€”while being mindful of reward hacking, entropy collapse, and generalization tradeâ€‘offs.

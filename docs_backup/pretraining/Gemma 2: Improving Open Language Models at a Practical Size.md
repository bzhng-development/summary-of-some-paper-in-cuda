# Gemma 2: Improving Open Language Models at a Practical Size

**ArXiv:** [2408.00118](https://arxiv.org/abs/2408.00118)
**Authors:** Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, LÃ©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre RamÃ©, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk
**Institutions:** Google DeepMind

## ğŸ¯ Pitch

Gemma 2 presents a breakthrough in small-to-mid-size language models by leveraging an innovative approach where knowledge distillation replaces traditional next-token training. This technique, coupled with architectural optimizations, significantly enhances model efficiency and performance, enabling smaller models to rival far larger counterparts, thus making them ideal for resource-constrained applications without compromising quality.

---

## 1. Executive Summary
Gemma 2 introduces three open, decoderâ€‘only language models (`2B`, `9B`, `27B` parameters) that pair architectural tweaks with an aggressive use of knowledge distillation to push smallâ€‘toâ€‘midâ€‘size models to nearâ€“stateâ€‘ofâ€‘theâ€‘art quality. The standout idea is to replace standard nextâ€‘token training with distillation from a stronger â€œteacherâ€ model and to train far beyond computeâ€‘optimal token counts, yielding models that outperform peers of the same size and rival models 2â€“3Ã— larger on many benchmarks.

## 2. Context and Motivation
- Problem addressed
  - Small open models have improved mostly by more training data (longer training), but such gains scale poorly. The paper notes that recent small models consume â€œup to 15T tokens to improve the state of the art by less than 1â€“2%â€ (Section 1, citing AI@Meta 2024). This suggests small models are underâ€‘trained in an information sense, not just a tokenâ€‘count sense.
- Why it matters
  - Practical deployment often needs compact models (resourceâ€‘constrained devices, lower inference cost). If small models can be trained to â€œthinkâ€ more effectively without bruteâ€‘force scaling, they could deliver high quality at practical sizes.
- Prior approaches and gaps
  - Prior small models largely extended token budgets and adopted standard objectives (nextâ€‘token prediction). Scaling laws (Hoffmann et al., 2022) predict diminishing returns from just adding tokens.
  - Knowledge distillation exists but is often used to speed up training rather than as the primary longâ€‘horizon training objective.
- Positioning of this work
  - Gemma 2 reframes distillation as the main objective for small models and couples it with architectural choices aimed at throughput and stability. The paper trains the `2B` and `9B` models â€œon a quantity of tokens that is more than 50Ã— the computeâ€‘optimal quantityâ€ (Section 1), using a teacherâ€™s full probability distribution to provide richer learning signals per token.

## 3. Technical Approach
This section explains the architecture, training pipeline, and postâ€‘training recipe.

- Model architecture (Section 2; Table 1, Table 2)
  - Base: decoderâ€‘only Transformer with `8192` token context, RoPE positional encoding, and `GeGLU` activations.
  - Interleaved localâ€“global attention:
    - A â€œlocal sliding windowâ€ attention layer alternates with a â€œglobal attentionâ€ layer every other layer.
    - Local layers attend over a sliding window of `4096` tokens; global layers attend over the full `8192` context (Section 2: â€œWe alternate between a local sliding window attention â€¦ and global attention â€¦ The sliding window size â€¦ is set to 4096 tokens, while the span of the global attention layers is set to 8192 tokens.â€).
    - Why: local attention reduces compute, while periodic global layers restore longâ€‘range information flow.
  - `GQA` (Groupedâ€‘Query Attention) with `num_groups = 2`
    - Definition: in GQA, multiple attention heads share the same Key/Value projections (grouped K/V), reducing memory and speed costs while keeping separate Query projections for expressivity.
    - Chosen due to â€œincreased speed at inference time while maintaining downstream performanceâ€ (Section 2). Table 8 shows nearâ€‘parity between standard Multiâ€‘Head Attention (`MHA`) and `GQA`, favoring GQA for efficiency.
  - Normalization and stability
    - `RMSNorm` is used both preâ€‘norm and postâ€‘norm for attention and feedâ€‘forward blocks (Section 2). Preâ€‘norm stabilizes gradients in deep nets; postâ€‘norm helps stabilize outputs.
  - Logit softâ€‘capping
    - To prevent excessively large logits (which can destabilize training and inference), they cap logits using `logits â† soft_cap * tanh(logits / soft_cap)`, with `soft_cap = 50.0` for selfâ€‘attention layers and `30.0` for the final layer (Section 2).
  - Sizes and layouts (Table 1)
    - `2B`: 26 layers, `d_model=2304`, `8` heads (`4` KV heads), `head_size=256`, `FFN dim=18432`.
    - `9B`: 42 layers, `d_model=3584`, `16` heads (`8` KV), `head_size=256`, `FFN dim=28672`.
    - `27B`: 46 layers, `d_model=4608`, `32` heads (`16` KV), `head_size=128`, `FFN dim=73728`.
    - Large shared vocabulary (`256,128` entries) increases embedding parameters (Table 2).
- Preâ€‘training data and objective (Section 3)
  - Token budgets: `27B` trained on `13T` tokens (from scratch), `9B` on `8T`, `2B` on `2T` (Section 3.1).
  - Data mixture: primarily English, drawn from web, code, and science sources; filtered to remove unsafe content and to decontaminate evaluation sets (Section 3.1).
  - Tokenizer: SentencePiece with byteâ€‘level encodings and digit splitting, `256k` vocab (Section 3.1).
- Knowledge distillation as the main objective for `2B` and `9B` (Section 3.2)
  - Definition: train a smaller â€œstudentâ€ to match a larger â€œteacherâ€ modelâ€™s probability distribution over the next token, not just the oneâ€‘hot target. Formally:
    > minimize over PS:  Î£x [ âˆ’ PT(x | xc) Â· log PS(x | xc) ]  (Section 3.2)
  - Intuition: the teacherâ€™s full distribution conveys â€œdark knowledgeâ€ (relative probabilities among plausible tokens), delivering richer gradients, especially helpful when training on massive token counts beyond computeâ€‘optimal limits.
- Training infrastructure and software (Section 3.3; Table 3)
  - Hardware: TPUv5e (`2B`), TPUv4 (`9B`), TPUv5p (`27B`), scaling up to `6144` chips for `27B`.
  - Parallelism: data replication, model sharding, ZeROâ€‘3â€‘like optimizer sharding, Pathways for crossâ€‘pod reduction, `JAX` singleâ€‘controller programming, `GSPMD` partitioner, MegaScale XLA compiler.
  - Carbon footprint: estimated `1247.61 tCO2eq`, with data centers operating under Googleâ€™s carbonâ€‘neutral policy (Section 3.4).
- Postâ€‘training for instruction tuning (Section 4; Table 4, Table 5)
  - SFT (supervised fineâ€‘tuning): on a mix of human and synthetic promptâ€“response data, heavily leveraging teacherâ€‘generated responses; also distillation on the studentâ€™s distribution during SFT (Section 4).
  - RLHF (reinforcement learning from human feedback): reward model â€œan order of magnitude larger than the policy,â€ oriented toward multiâ€‘turn conversation (Section 4).
  - Model merging: weightâ€‘space averaging of multiple runs to improve overall performance (Section 4).
  - Safetyâ€‘aware data filtering and formatting: standardized control tokens for multiâ€‘turn chat; updated schema ends model outputs with `<end_of_turn><eos>` (Table 4, Table 5).

Analogy for the core idea: Instead of teaching a student only the single correct answer per question (nextâ€‘token), the student watches the teacherâ€™s full answer key showing partial credit for close answers (full probability distribution). Practiced across far more â€œquestionsâ€ than usual, the student learns deeper patterns with similar study time.

## 4. Key Insights and Innovations
- Distillation as a longâ€‘horizon training objective for small models
  - Novelty: treat knowledge distillation not just as a compression or speedâ€‘up technique but as the primary training objective for small models, over very large token counts.
  - Evidence: 
    > Table 6: `2B` trained 500B tokens â€œfrom scratchâ€ vs â€œdistilledâ€ shows â€œAverage (3 bench.) 60.3 â†’ 67.7â€.  
    > Table 7: Distillation lowers perplexity across 200M, 400M, and 1B models (e.g., at 1B: `from scratch 17` vs `distilled 15`).
  - Significance: This reframing helps small models close the gap to much larger models without prohibitive parameter growth.
- Interleaved localâ€“global attention for long contexts with efficiency
  - Difference: alternates local sliding window (`4096`) with full `8192` global attention per layer (Section 2).
  - Why it matters: retains longâ€‘range modeling while reducing attention compute on many layers. Also enables an inferenceâ€‘time speed/quality tradeâ€‘off (Table 10 shows minimal perplexity change when shrinking the local window from 4096 to 1024).
- Adoption of `GQA` to cut inference cost with negligible quality loss
  - Evidence: 
    > Table 8: `MHA 50.3` vs `GQA 50.8` (average across 4 benches).  
  - Benefit: memory and speed benefits of grouped K/V outweigh tiny differences in average scores.
- Deeper vs wider preference at fixed parameter budget
  - Evidence:
    > Table 9: â€œWide 50.8â€ vs â€œDeep 52.0â€ (average across 4 benches).
  - Insight: additional depth can be more useful than width for 9Bâ€‘scale models under these training regimes.
- Format robustness tracking
  - Observation: sensitivity to prompting/evaluation format measured by stdâ€‘dev on MMLU across 12 formats (Table 11). 
    > `Gemma 2 2B`: 2.1; `Gemma 2 9B`: 0.9; `Gemma 2 27B`: 1.0; `Mistral 7B`: 6.9.
  - Significance: lower variance suggests more stable performance under reasonable formatting variations.

## 5. Experimental Analysis
- Evaluation methodology
  - Preâ€‘training quality: standard academic benchmarks such as MMLU, GSM8K, ARCâ€‘c, HellaSwag, Winogrande (Table 12). 
  - Comparative baselines: models of similar or larger sizes (e.g., Qwen1.5 32B, LLaMAâ€‘3 70B; Table 12).
  - Postâ€‘training quality: human preference studies, Chatbot Arena Elo (Table 14), instruction following and safety SxS vs GPTâ€‘4o (Table 15), multiâ€‘turn conversations (Table 16), and fewâ€‘shot performance changes from IT (Table 17).
  - Ablations: distillation vs scratch (Table 6â€“7), `GQA` vs `MHA` (Table 8), deep vs wide (Table 9), sliding window change at inference (Table 10), format robustness (Table 11).
  - Safety and memorization: toxicity/bias/factuality suites (Table 18), memorization analysis (Figure 1), and assurance studies (offensive cybersecurity, code vulnerabilities, selfâ€‘proliferation, persuasion; Tables 19â€“25).
- Main quantitative results
  - Distillation gains for small models
    > Table 6: `2B` @500B tokens: average across 3 benchmarks improves from `60.3` (scratch) to `67.7` (distilled).  
    > Table 7: perplexity reductions across model sizes with a 7B teacher (e.g., 1B: `17` â†’ `15`).
  - Preâ€‘trained 27B vs larger baselines (Table 12)
    > `Gemmaâ€‘2 27B` MMLU `75.2`, GSM8K `74.0`, ARCâ€‘c `71.4`, HellaSwag `86.4`, Winogrande `83.7`.  
    > Outperforms Qwen1.5 32B on most metrics, and is â€œonly a few percent below LLaMAâ€‘3 70B despite being 2.5Ã— smaller and trained on 2/3rds less data.â€
  - Preâ€‘trained `2B`/`9B` vs prior open models (Table 13)
    - Average across all benchmarks:  
      > `Gemmaâ€‘1 2B 44.2` â†’ `Gemmaâ€‘2 2B 48.7` (+4.5 points);  
      > `Gemmaâ€‘1 7B 57.9` â†’ `Gemmaâ€‘2 9B 64.9` (+7.0);  
      > `Gemmaâ€‘2 27B 69.4`.
    - Perâ€‘task highlights (Gemmaâ€‘2 9B): MMLU `71.3`, GSM8K `68.6`, BBH `68.2`, MBPP `52.4`.
  - Postâ€‘training human preference: Chatbot Arena Elo (Table 14)
    > `gemmaâ€‘2â€‘27bâ€‘it`: Elo `1218`, ranked above `llamaâ€‘3â€‘70bâ€‘instruct 1206`.  
    > `gemmaâ€‘2â€‘9bâ€‘it`: Elo `1187`, comparable to `gptâ€‘4â€‘0314 1186`.  
    > `gemmaâ€‘2â€‘2bâ€‘it`: Elo `1126`, above `gptâ€‘3.5â€‘turboâ€‘0613 1116`.
  - Instruction following & safety (Table 15)
    > Instruction following (singleâ€‘sided): `Gemmaâ€‘2 9B 34.1% Â± 3.0%` (vs `Gemmaâ€‘1.1 7B 24.3% Â± 1.9%`).  
    > Safety (Win/Tie/Loss vs GPTâ€‘4o): `Gemmaâ€‘2 9B 48.2% / 19.2% / 28.3%`; `Gemmaâ€‘2 2B 53% / 9% / 38%`.
  - Multiâ€‘turn conversations (Table 16; 500 scenarios; 1â€“5 scale)
    > User satisfaction: `Gemmaâ€‘1.1 7B 3.32` â†’ `Gemmaâ€‘2 27B 4.20`.  
    > Goal achievement: `3.36` â†’ `4.24`.
  - IT vs PT on fewâ€‘shot (Table 17)
    > MMLU (2B/9B/27B): `52.2â†’56.1`, `71.3â†’72.3`, `75.2â†’76.2`.  
    > MBPP: `30.2â†’36.6`, `52.4â†’59.2`, `62.6â†’67.4`.
  - Memorization (Figure 1)
    > â€œSignificantly lower memorization rates acrossâ€‘theâ€‘board,â€ with exact memorization below `0.1%` overall. Approximate memorization increases are small relative to prior models.
    > Personal data analysis: â€œno instances of highâ€‘severity,â€ and only `0.00026%` of memorized data contained lowerâ€‘severity PII (Section 7).
  - Safety benchmarks (Table 18)
    - The table lists RealToxicity, CrowSâ€‘Pairs, BBQ, Winogender/Winobias, TruthfulQA, etc. Trends vary by metric; larger Gemma 2 models often match or improve over Gemma 1.1, with some metrics favoring specific sizes. The table notes which direction (higher/lower) is better when bolded.
  - Assurance results (Tables 19â€“21)
    - Offensive cybersecurity (CTFs; Table 19): `Gemmaâ€‘2 27B` solves `34/76` InterCode tasks (45% success on that subset), `1/13` internal CTF, `0/13` Hack the Box; well below `Gemini 1.5 Pro 62/76` but above older open models (e.g., CodeGemma 7B).
    - Code vulnerability detection (Table 20): nearâ€‘chance accuracy on several datasets; roughly on par with Gemini 1.5 Pro on some suites (e.g., SecretPatch `72%` vs `67%`).
    - Selfâ€‘proliferation (Table 21): `0/10` endâ€‘toâ€‘end tasks; `1/10` â€œall milestonesâ€ scenarios; `22/45` milestones overall; indicates limited autonomous capability without human intervention.
  - Persuasion studies (Tables 22â€“25)
    - Rapport (â€œCharm Offensiveâ€, Table 22): high perceived traits (e.g., `Personal connection 80%`, `Trustworthy 87%` at 27B), comparable to strong proprietary models.
    - Hidden agenda (Table 23): persuades a fraction of participants to click links/find info/run code (`34%/9%/11%`), within the range of Gemini models.
    - Donations (â€œMoney Talksâ€, Table 24): no significant increase vs baseline.
    - â€œWeb of Liesâ€ (Table 25): shifts beliefs toward correct facts (`18% Â± 5%`) but minimal shift toward incorrect (`1% Â± 4%`), weaker than human confederates at inducing false beliefs.
- Do the experiments support the claims?
  - Yes for core claims:
    - Distillation increases smallâ€‘model quality (Tables 6â€“7).
    - The `27B` preâ€‘trained model competes with larger baselines (Table 12).
    - Instructionâ€‘tuned Gemma 2 models achieve top openâ€‘weights standings on Chatbot Arena (Table 14).
  - Robustness and ablations are present (Tables 6â€“11), and assurance/safety sections probe potential harms and capabilities (Tables 18â€“25, Figure 1).
- Nuances and tradeâ€‘offs seen in results
  - Format sensitivity improves with size but the `2B` model is less robust (Table 11).
  - Safety benchmarks show mixed patterns across datasets (Table 18), suggesting gains depend on the metric and domain.
  - Assurance results show capability increases on some cyber tasks but remain far below cuttingâ€‘edge proprietary systems and fail endâ€‘toâ€‘end autonomy tests (Tables 19â€“21).

## 6. Limitations and Trade-offs
- Reliance on a highâ€‘quality teacher
  - Distillation quality depends on the teacher. Any biases or errors in the teacher distribution may be inherited. The paper does not detail the exact teacher identity for the final models, limiting replicability of the precise effect size.
- Training compute and data
  - Although parameter counts are small, token budgets are very large (e.g., `8T` for `9B`, `2T` for `2B`, `13T` for `27B`; Section 3.1). The approach shifts cost from parameters to data/compute for long training runs. Carbon impact is reported (`1247.61 tCO2eq`; Section 3.4), but the training remains resourceâ€‘intensive.
- Limited multilingual and multimodal scope
  - The models are â€œnot trained specifically for stateâ€‘ofâ€‘theâ€‘art multilingual capabilitiesâ€ and are textâ€‘only (Section 3.1). Embedding size is large due to the `256k` multilingualâ€‘friendly vocabulary (Table 2), which increases memory footprint without providing full multilingual or multimodal coverage.
- Formatting sensitivity at small scale
  - The `2B` model shows higher variance to formatting on MMLU (Table 11), which can affect reliability in real applications if prompts vary.
- Safety strengths but residual risks
  - Assurance tests show persuasion and some offensive cybersecurity capabilities (Tables 22â€“23, 19), even if far from frontier models. Developers still need systemâ€‘level safeguards.
- Inference tradeâ€‘offs
  - Reducing sliding window size can speed inference but slightly raises perplexity (Table 10), implying a qualityâ€“latency tradeâ€‘off that downstream users must tune.

## 7. Implications and Future Directions
- How this work shifts the landscape
  - It validates a path to highâ€‘quality small models by maximizing information per token via distillation, rather than only scaling parameters or raw token counts. This changes the optimization target for practical models: invest in a better training signal (teacher distributions) over longer runs.
- Followâ€‘up research
  - Distillation design: choice of teacher(s), temperature scaling, curriculum schedules, and domainâ€‘specific teachers for code, math, or reasoning.
  - Architecture: further exploration of localâ€“global layer patterns, dynamic window sizes, or learned schedules; combining with Mixtureâ€‘ofâ€‘Experts while retaining small active parameter counts.
  - Data and safety: better methods to measure and reduce approximate memorization; more transparent reporting of data mixtures; domainâ€‘targeted safety RLHF to reduce persuasion on harmful tasks without hurting helpfulness.
  - Robustness: reduce formatting sensitivity for very small models; evaluate crossâ€‘lingual generalization given the large vocabulary and partially multilingual data.
- Practical applications
  - Edge and onâ€‘prem deployment: `2B` and `9B` models with strong Chatbot Arena rankings (Table 14) are attractive for private or resourceâ€‘constrained settings.
  - Multiâ€‘turn assistants: improved satisfaction and goal achievement (Table 16) suggest suitability for customer support, tutoring, and planning tools, with attention to safety guardrails.
  - Coding and reasoning helpers: solid gains on MBPP, HumanEval, GSM8K, and BBH (Table 13) make these models broadly useful as coding/math copilots, though further fineâ€‘tuning or tool integration may be required for highâ€‘stakes tasks.

Overall, Gemma 2 demonstrates that reâ€‘thinking the training objectiveâ€”feeding small models dense, distributional supervision from a strong teacher over very long token budgetsâ€”can deliver outsized quality at practical parameter counts. The comprehensive evaluations, ablations, and safety analyses ground the claim that such models can be both capable and responsibly deployable, while highlighting remaining gaps in multilinguality, autonomy, and robustness that future work can address.

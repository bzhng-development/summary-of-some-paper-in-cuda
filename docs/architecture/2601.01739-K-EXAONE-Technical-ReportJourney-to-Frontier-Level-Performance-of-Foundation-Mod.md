# K-EXAONE Technical Report
Journey to Frontier-Level Performance of Foundation Models

**ArXiv:** [2601.01739](https://arxiv.org/abs/2601.01739)

## üéØ Pitch

K-EXAONE is a 236B-parameter multilingual foundation model that uses a sparse Mixture-of-Experts design (activating ~23B params) combined with hybrid global + sliding-window attention and an MTP auxiliary module to enable efficient inference and a 256K-token context window. This architecture and its post-training (SFT, verifiable-reward RL, preference learning) deliver frontier-level reasoning, agentic, long-context, multilingual (including Korean) and safety performance‚Äîmaking K-EXAONE practical for industrial-scale, high-context applications while lowering deployment compute costs.

---

## 1. Executive Summary (2-3 sentences)

K-EXAONE is a 236B-parameter multilingual foundation language model that uses a `Mixture-of-Experts (MoE)` design to keep inference compute closer to a 23B-parameter model while retaining large total capacity, and it extends usable context length up to `256K tokens`. It combines sparse MoE feed-forward blocks, a hybrid attention stack (global + sliding-window attention), and an auxiliary `Multi-Token Prediction (MTP)` module, then applies post-training (SFT + verifiable-reward RL + preference learning) to produce both ‚Äúreasoning‚Äù and ‚Äúnon-reasoning‚Äù modes. Across a broad evaluation suite (reasoning, agentic tool use, Korean, multilinguality, long-context, and safety), it performs competitively with other open-weight reasoning models of similar scale (Tables 3‚Äì4, Figure 1).

---

## 2. Context and Motivation

- **Problem/gap addressed**
  - The report targets the challenge of building a *frontier-level* foundation model under **resource constraints**, specifically highlighting South Korea‚Äôs relative shortages in AI data centers and AI chips (Section 1).
  - It also targets two practical capability gaps for industrial deployment:
    - **Long-context processing** (up to hundreds of thousands of tokens).
    - **Multilingual robustness**, especially for Korean plus additional languages beyond prior EXAONE coverage.

- **Why this matters**
  - The report frames global LLM progress as driven by **scaling** (hundreds of billions to near-trillion parameters), which has narrowed the gap between closed-source and open-weight models (Section 1).
  - For Korea‚Äôs AI transformation, the report argues that achieving globally competitive model performance is foundational, and a government program providing GPUs helps close infrastructure gaps (Section 1).

- **Prior approaches and limitations (as positioned here)**
  - Earlier efforts in the region focused on **cost-effective smaller models** (tens of billions of parameters) due to limited compute (Section 1).
  - Within LG‚Äôs own lineage:
    - `EXAONE 4.0` is described as a **dense** model family with hybrid reasoning/non-reasoning capabilities and a hybrid attention mechanism for long context (Section 1‚Äì2).
    - K-EXAONE departs from dense modeling by moving to **MoE** for compute-efficient scaling (Section 2.1).

- **How this work positions itself**
  - Architecturally: ‚Äúfrontier-style‚Äù scaling via `MoE` (236B total, 23B activated) plus long-context hybrid attention (Section 2.1, Table 1).
  - Linguistically: expands from Korean/English/Spanish to six languages by tokenizer redesign and added corpora (Section 1, 2.2, 3.1).
  - Empirically: evaluated against several other reasoning-capable open models with a standardized internal setup (Section 4.1, Tables 3‚Äì4), aiming for ‚Äúcomparable‚Äù performance rather than claiming undisputed state-of-the-art.

---

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **large multilingual language model** designed for both strong reasoning behavior and practical deployment features like long-context processing.  
- The solution shape is a **compute-efficient `MoE` transformer** with hybrid attention for long contexts, plus a multi-stage training and post-training pipeline that produces both reasoning and non-reasoning operating modes.

### 3.2 Big-picture architecture (diagram in words)

- **Tokenizer ‚Üí Transformer backbone ‚Üí Output head**, with training-time and inference-time augmentations:
  1. A redesigned **150K-vocabulary tokenizer** (with ‚Äúsuperword‚Äù tokens) converts text in six languages (and code/STEM text) into tokens (Section 2.2, Figure 3).
  2. A **48-layer transformer stack** mixes:
     - **Global Attention (GA)** layers for full-sequence interaction.
     - **Sliding Window Attention (SWA)** layers for efficient long-context scaling (Section 2.1, Table 1, Figure 2).
  3. The feed-forward sublayers are mostly **Sparse MoE** blocks with 128 experts and top-8 routing plus a shared expert (Section 2.1, Figure 2).
  4. An auxiliary **MTP block** trains an extra future-token prediction objective and can be used for **self-drafting** speedups (Section 2.1, Figure 2).
  5. Post-training adds **instruction following (SFT)**, **verifiable-reward RL**, and **preference learning (GROUPER)** (Sections 3.3‚Äì3.5).

### 3.3 Roadmap for the deep dive

- I will explain:
  1. The **core backbone** (MoE + hybrid attention) because it determines compute/memory behavior (Section 2.1, Table 1).
  2. The **tokenizer redesign** because it changes multilingual/code efficiency and affects downstream performance (Section 2.2, Figure 3).
  3. The **pre-training curriculum** and data synthesis because they motivate reasoning/multilingual behavior before alignment (Section 3.1).
  4. The **context-length extension procedure** because 256K context is a key deployment feature with stability risks (Section 3.2).
  5. The **post-training stack (SFT ‚Üí RL ‚Üí preference learning)** because it produces the reported ‚Äúreasoning/non-reasoning‚Äù modes and agentic behavior (Sections 3.3‚Äì3.5).
  6. The **evaluation protocol/results** to connect design choices to measured outcomes (Section 4, Tables 3‚Äì4).

### 3.4 Detailed, sentence-based technical breakdown

This is an **empirical systems-and-training** paper/report: it combines a specific large model design (MoE + long-context attention) with a multi-stage training recipe and evaluates the resulting model on a wide benchmark suite.

#### 3.4.1 Backbone: Sparse MoE for compute-efficient scale

- K-EXAONE uses a `Mixture-of-Experts (MoE)` feed-forward design, meaning each token is processed by only a subset of expert sub-networks rather than a single dense feed-forward layer (Section 2.1).
- Concretely, each MoE block has:
  - `128` total experts.
  - **Top-8 routed experts per token**, plus **1 shared expert**, so **9 experts run per routing decision** (Section 2.1, Figure 2).
- This yields:
  - **Total parameters:** `236B`.
  - **Activated parameters per inference step:** ~`23B` (Section 2.1, Table 1).
  - The practical intent is to get representational diversity of a very large model while keeping per-token compute closer to a much smaller model.

**Routing stability and utilization controls**
- The report uses **sequence-level load balancing** in routing to stabilize expert usage and improve utilization efficiency (Section 2.1).
- It adopts **dropless routing** so tokens are not dropped due to expert capacity limits, which the report links to stabilized gradient flow and improved convergence for large MoE training (Section 2.1).

**A stability-oriented first layer**
- The architecture makes the **first layer‚Äôs FFN dense** rather than MoE, explicitly for training stability (Figure 2 caption: ‚Äúonly the first layer implemented as a dense layer for training stability‚Äù).

#### 3.4.2 Long-context efficiency: Hybrid attention (GA + SWA) and small windows

- K-EXAONE supports a **maximum context length of `256K tokens`** (Sections 2.1, 3.2).
- Instead of using full global attention everywhere (which scales poorly in memory/compute with long sequences), it uses a **hybrid attention** stack:
  - Some layers use **Global Attention (GA)** to allow any token to attend to any other token.
  - More layers use **Sliding Window Attention (SWA)** where tokens attend only to a local window (Section 2.1, Figure 2, Table 1).
- The specific layer split is (Table 1):
  - **Total layers:** `48`
  - **SWA layers:** `36`
  - **GA layers:** `12`
- The report reduces the sliding window size from `4,096` to `128` to minimize KV-cache usage during long-context inference while ‚Äúpreserving modeling capacity‚Äù (Section 2.1).
  - This is a direct long-context deployment trade-off: much lower memory per token, but the model must rely on GA layers (and other mechanisms) for long-range interactions.

**Attention and head configuration (given)**
- Attention heads: `64` query heads and `8` key/value heads (Table 1).
- Head dimension: `128` (Table 1).

> Important missing details: the excerpt provides head counts and head dimension, but does **not** provide the model‚Äôs overall embedding/hidden size, MLP expansion ratios (beyond the first dense FFN hidden size), or per-layer attention dimensions. I therefore cannot reconstruct total FLOPs/token from architecture alone.

#### 3.4.3 Training stabilizers for deep / long-context transformers

- `QK Norm` is used: layer normalization is applied to query and key vectors before attention, intended to prevent attention logit explosion and stabilize training (Section 2.1).
- `RoPE (Rotary Positional Embeddings)` are applied **only** to SWA layers (‚ÄúSWA-only RoPE‚Äù), with the stated motivation of preventing interference with global token interactions and improving robustness to long-sequence extrapolation (Section 2.1).

#### 3.4.4 MTP module: auxiliary future-token objective and optional self-drafting

- The model includes a dense `Multi-Token Prediction (MTP)` module (Section 2.1, Figure 2).
  - The training objective supervises prediction of **an additional +1 future token** (Figure 2 caption).
- The report claims that during inference the MTP block can be leveraged for **self-drafting** to achieve approximately **`1.5√ó` decoding throughput** versus standard autoregressive decoding (Section 2.1).
- However, in the evaluation setup they explicitly **disable MTP at inference time** (Section 4.1).
  - Practically, the benchmark numbers in Tables 3‚Äì4 reflect performance without MTP-enabled decoding.

MTP configuration (given)
- MTP block parameters: `0.52B` (Table 1).
- MTP training loss weight: `0.05` (Section 3.1, Training Setup).

#### 3.4.5 Tokenizer redesign for multilingual + code/STEM efficiency

- Vocabulary size increases from `100K` (EXAONE 4.0 tokenizer) to `150K` (Section 2.2).
- They retain the **top 70% high-frequency** portion of the prior vocabulary, reallocating capacity to:
  - Additional languages (German, Japanese, Vietnamese),
  - `STEM`,
  - `code` (Section 2.2).
- They use `SuperBPE` with **superword tokens** that compress common multi-token sequences into a single token (Section 2.2).
  - Superword tokens are ~`20%` of the vocabulary.
  - Allocation ratio across English:Korean:multilingual is `2:3:1` (Section 2.2).
- Pre-tokenization and normalization changes:
  - Regex updated for superword boundaries, line breaks, and multilingual Unicode (Section 2.2).
  - Unicode normalization changes from `NFKC` to `NFC` to preserve semantic distinctions in symbol-rich text (superscripts/subscripts), common in code/STEM (Section 2.2).

Tokenizer efficiency result (Figure 3)
- Efficiency is measured as **bytes per token** (higher is better).
- Reported improvements of K-EXAONE vs EXAONE 4.0 tokenizer (Figure 3):
  - English: `+19.6%`
  - Korean: `+29.0%`
  - Multilingual: `+49.8%`
  - STEM: `+20.1%`
  - Code: `+26.7%`
  - The report summarizes this as ~`30%` improvement on average.

#### 3.4.6 Pre-training curriculum, data synthesis, and compute

**Data/compute scale**
- Total pre-training data: `11T tokens` (Table 2).
- Total computation: `1.52 √ó 10^24 FLOPs` (Table 2).
- The excerpt does **not** include hardware type/count, wall-clock time, or batch size.

**Three-stage curriculum (high level)**
- The report describes a ‚Äústrategic three-stage pre-training curriculum‚Äù to progressively build:
  1. Foundational knowledge,
  2. Domain expertise,
  3. Reasoning capability (Section 3.1).
- It inherits EXAONE 4.0‚Äôs data pipeline but applies ‚Äúmulti-faceted data filtering‚Äù for quality (Section 3.1), without detailing specific filters/deduplication in the excerpt.

**Multilingual extension and balancing**
- Adds high-quality web text in German, Japanese, Vietnamese (Section 3.1).
- Addresses language imbalance via **targeted synthesis**:
  - It generates synthetic corpora to propagate specialized knowledge and reasoning patterns across languages, aiming for balanced knowledge distribution and consistent performance across supported languages (Section 3.1).

**Thinking-augmented synthesis**
- It generates ‚Äúdocument-grounded thinking trajectories‚Äù and combines them with source content into unified samples encoding step-by-step inference (Section 3.1).
  - This is positioned as preconditioning the model for post-training reasoning.

**Training setup (optimizer/schedule/precision and MoE regularization)**
- Precision: trained ‚Äúnatively‚Äù with `FP8`, with loss curves comparable to `BF16` (Section 3.1).
- Optimizer: `Muon` (Section 3.1).
- Learning rate schedule: `Warmup‚ÄìStable‚ÄìDecay (WSD)` (Section 3.1).
  - Max learning rate: `3.0 √ó 10^-4` (Section 3.1).
- MoE regularization parameters (fixed throughout training):
  - Sequence auxiliary loss coefficient: `1.0 √ó 10^-4` (Section 3.1).
  - Expert bias update factor: `1.0 √ó 10^-4` (Section 3.1).
- MTP loss weight: `0.05` (Section 3.1).

> Missing hyperparameters relative to the requested checklist: batch size, weight decay, gradient clipping, exact warmup/stable/decay durations, tokenizer training details beyond high-level, and context length during base pre-training beyond ‚Äúmax 8K‚Äù.

#### 3.4.7 Two-stage context length extension to 256K

- Base model pre-trains at `8K` max context, then extends:
  - Stage 1: `8K ‚Üí 32K`
  - Stage 2: `32K ‚Üí 256K` (Section 3.2).
- Across both stages, they keep the same high-level mixture components but adjust sampling ratios:
  1. **Rehearsal dataset** to prevent degradation on short-context tasks (Section 3.2).
  2. **Synthetic reasoning dataset** to strengthen multi-step reasoning robustness (Section 3.2).
  3. **End-to-end long-document dataset** with full-document sequences consumed in a single training instance (no truncation) to teach long-range dependencies (Section 3.2).

**Monitoring and stopping criterion**
- They run:
  - Short-context evaluations (same protocol as pre-training),
  - `Needle-In-A-Haystack (NIAH)` tests for long-context retrieval ability (Section 3.2).
- Training is repeated until ‚Äúnear-perfect NIAH performance‚Äù across target ranges for each stage (‚Äúgreen light‚Äù) (Section 3.2).
  - The excerpt does not give the exact NIAH scoring thresholds.

#### 3.4.8 Post-training: SFT, agentic tool-use data, RL, and preference learning

**Stage (i): Supervised fine-tuning (SFT)**
- SFT trains instruction-following across domains using generation pipelines largely inherited from EXAONE 4.0 (Section 3.3).
- Korean capability enhancement uses public/institutional data from `K-DATA` (Korea Data Industry Promotion Agency), filtered and converted into datasets like DocQA and translation (Section 3.3).

**Stage (ii): Training agentic tool use**
- Rather than collecting real tool environments (costly), they synthesize tool-use scenarios and pass criteria using LLMs, then filter unrealistic/unsolvable cases by evaluating models in those environments (Section 3.3).
- Output is ‚Äúhundreds of verifiable, realistic tool-use tasks‚Äù plus evaluation environments (Section 3.3).

**Web search agent design with sub-agents**
- When doing web search, K-EXAONE (as primary agent) is augmented with:
  - A **summarizer** sub-agent to distill fetched pages, reducing long/noisy input processing.
  - A **trajectory compressor** sub-agent that, after a predefined number of tool steps, compresses the interaction history into a single JSON record of key facts and open questions (Section 3.3).
- Both sub-agents are implemented using the same underlying model as K-EXAONE at inference time (Section 3.3).

**Stage (iii): Reinforcement learning (RL) with verifiable rewards**
- RL is multi-task across math, code, STEM, and instruction-following (Section 3.4).
- Verification combines:
  - Rule-based verifiers,
  - An `LLM-as-a-judge` (Section 3.4).
- Optimization uses `AGAPO` (off-policy policy gradient) with **truncated importance sampling** (Section 3.4, Eq. (1)‚Äì(2)).
- Efficiency/stability tricks during RL:
  - **Zero-variance filtering**: drop prompts where all sampled rollouts get identical rewards (zero advantage) (Section 3.4).
  - **Group-level advantage computation** and **global advantage normalization** (Section 3.4, Eq. (2)).
  - **No KL penalty** (explicitly excluded) to improve performance and reduce compute (Section 3.4).
  - **Freeze the MoE router** during RL (Section 3.4), which avoids destabilizing expert allocation during policy updates.

**Explaining the RL math in plain language**
- For each question `q`, sample `G` candidate responses.
- Each response gets a verifiable reward `r_i ‚àà [0,1]`.
- Compute a ‚Äúhow much better than peers‚Äù advantage:
  - `A_group,i` is response `i`‚Äôs reward minus the average reward of the other responses (Eq. (2)).
  - `A_global,i` normalizes these group advantages across the batch (Eq. (2)).
- Update the policy to increase probability of tokens that appear in higher-advantage responses, with an importance-sampling correction between the rollout policy and the updated policy (Eq. (1)).

*Toy micro-example (illustrative, not from the report):*  
If `G=4` and rewards are `[1.0, 0.5, 0.5, 0.0]`, then response 1 has positive `A_group` (better than peers) and response 4 has strongly negative `A_group`, so the gradient update will push the model toward producing tokens like response 1 and away from tokens like response 4, after normalization.

**Preference learning: GROUPER**
- After RL, they apply preference learning to improve alignment on general domains: chat, safety, instruction following, agentic tool use, creative writing (Section 3.5).
- They introduce `GROUPER (Group-wise SimPER)`, a variant of SimPER that samples multiple responses per prompt and trains using **group-wise advantages** inspired by GRPO (Section 3.5).
- Reward signal is a combination of:
  - Rule-based rewards,
  - Rubric-based generative rewards that score multiple dimensions (Section 3.5).
- The method standardizes group scores and scales them into `[-1, 1]` (Eq. (4)), then weights updates by that advantage (Eq. (3)).

#### 3.4.9 Evaluation configuration (inference settings)

- Temperature: `1.0`
- Top-p: `0.95` (Section 4.1).
- Context length during eval:
  - `160K` for long-context benchmarks,
  - `128K` for others (Section 4.1).
- `MTP disabled` at inference time during evaluation (Section 4.1).
- Some baselines are evaluated internally when official scores are unavailable; official baseline scores are marked with `*` in Tables 3‚Äì4 (Section 4.1, Tables 3‚Äì4).

---

## 4. Key Insights and Innovations

1. **Fine-grained MoE at 236B total / 23B activated with dropless routing**
   - Novelty here is not ‚ÄúMoE exists,‚Äù but the specific package: `128` experts, `top-8` routing + `shared expert`, **dropless routing**, and **sequence-level load balancing** (Section 2.1, Figure 2).
   - Significance: targets frontier-scale capacity while controlling inference compute and stabilizing large MoE training.

2. **Hybrid attention stack designed explicitly for very long contexts with deployment-friendly primitives**
   - The design mixes `SWA` and `GA` layers (48 total; 36 SWA / 12 GA) and shrinks SWA window to `128` to reduce KV-cache usage (Table 1, Section 2.1).
   - Significance: aims for practical long-context inference costs using mechanisms ‚Äúnatively supported by modern LLM inference engines‚Äù (Section 2.1), which is an integration-driven design choice.

3. **MTP module used for resource-efficient auxiliary training and optional inference acceleration**
   - Adds a dense `MTP` block trained to predict an extra +1 token (Figure 2), intended to improve predictive capability while avoiding MoE routing overhead.
   - The reported decoding benefit is ~`1.5√ó` throughput via self-drafting (Section 2.1), though not used in benchmark eval (Section 4.1).

4. **Two-stage context extension with rehearsal + synthetic reasoning + long-document training and explicit NIAH gating**
   - The training recipe explicitly addresses the known risk that long-context specialization can hurt short-context performance via rehearsal data (Section 3.2).
   - The ‚Äúgreen light‚Äù criterion based on near-perfect NIAH is an operationally concrete stopping heuristic for long-context readiness (Section 3.2), even though exact thresholds are not disclosed.

5. **Safety evaluation tailored to Korean sociocultural context via K-AUT and KGC-SAFETY**
   - Introduces `K-AUT` with 4 domains and `226` risk areas, and builds `KGC-SAFETY` with `2,260` instances (10 per risk area) including multilingual, adversarial, and multi-turn variations (Appendix F, Tables 7‚Äì8).
   - Significance: attempts to measure safety beyond predominantly Western-centric taxonomies by incorporating Korean legal/historical sensitivities (Appendix F.1).

---

## 5. Experimental Analysis

### 5.1 Evaluation methodology (benchmarks, metrics, setup)

- **Benchmark categories (9)** (Section 4.1):
  - World knowledge: `MMLU-PRO`, `GPQA-DIAMOND`, `HUMANITY‚ÄôS LAST EXAM (text-only)`
  - Math: `IMO-ANSWERBENCH`, `AIME 2025`, `HMMT NOV 2025`
  - Coding/agentic coding: `LIVECODEBENCH PRO`, `LIVECODEBENCH V6`, `TERMINAL-BENCH 2.0`, `SWE-BENCH VERIFIED`
  - Agentic tool use: `œÑ¬≤-BENCH`, `BROWSECOMP`
  - Instruction following: `IFBENCH`, `IFEVAL`
  - Long context: `AA-LCR`, `OPENAI-MRCR`
  - Korean: `KMMLU-PRO`, `KOBALT`, `CLICK`, `HRM8K`, `KO-LONGBENCH (in-house)`
  - Multilingual: `MMMLU`, `WMT24++` (evaluated on 5 non-English supported languages: ko,de,es,ja,vi; Section 4.1 footnote)
  - Safety: `WILDJAILBREAK`, `KGC-SAFETY (in-house)`
- **Decoding settings**: temperature `1.0`, top-p `0.95` (Section 4.1).
- **Inference context**: `160K` for long-context benchmarks; `128K` for others (Section 4.1).
- **MTP disabled** in evaluation (Section 4.1).
- **Prompting templates** are given for multiple-choice and math categories in Appendix C (Figures 4‚Äì7).

**Judge-model dependencies**
- Several evaluations use LLM-as-a-judge:
  - HUMANITY‚ÄôS LAST EXAM judging uses `gpt-5-mini-2025-08-07` (Appendix C.3).
  - WMT24++ judging uses `gpt-5-mini-2025-08-07` (Appendix C.9, Figure 8).
  - KGC-SAFETY judging uses `gpt-4.1-mini-2025-04-14` (Appendix F.2).
  - CODEUTILITYBENCH uses `gpt-5-2025-08-07` (Appendix D.1).

This matters because judge choice and prompting can materially affect measured scores; the report is transparent about which judge models it uses, but that also means some results are not purely automatic/ground-truth metrics.

### 5.2 Main quantitative results (with numbers)

Below I focus on representative headline results; Tables 3‚Äì4 contain the full matrix.

#### Reasoning mode (Table 3, Figure 1)

- **World knowledge**
  - `MMLU-PRO`: `83.8`
  - `GPQA-DIAMOND`: `79.1`
  - `HUMANITY‚ÄôS LAST EXAM (text-only)`: `13.6`
- **Math**
  - `AIME 2025`: `92.8`
  - `IMO-ANSWERBENCH`: `76.3`
  - `HMMT NOV 2025`: `86.8`
- **Coding**
  - `LIVECODEBENCH V6`: `80.7`
  - `LIVECODEBENCH PRO 25Q2 (MEDIUM)`: `25.9`
- **Agentic coding / software engineering**
  - `TERMINAL-BENCH 2.0`: `29.0`
  - `SWE-BENCH VERIFIED`: `49.4`
- **Agentic tool use**
  - `œÑ¬≤-BENCH (weighted avg noted in Figure 1 caption)`: category breakdown includes
    - Retail `78.6`, Airline `60.4`, Telecom `73.5` (Table 3)
  - `BROWSECOMP`: `31.4` (marked ‚Ä° Non-reasoning in Table 3 note; Table 3 row also shows it under reasoning table with that marker)
- **Instruction following**
  - `IFBENCH`: `67.3`
  - `IFEVAL`: `89.7`
- **Long context**
  - `AA-LCR`: `53.5`
  - `OPENAI-MRCR`: `52.3`
- **Korean**
  - `KOBALT`: `61.8`
  - `CLICK`: `83.9`
  - `HRM8K`: `90.9`
  - `KO-LONGBENCH (in-house)`: `86.8`
- **Multilingual**
  - `MMMLU (ko,de,es,ja)`: `85.7`
  - `WMT24++ (ko,de,es,ja,vi)`: `90.5`
- **Safety**
  - `WILDJAILBREAK`: `89.9` safe rate
  - `KGC-SAFETY (in-house)`: `96.1` safe rate

#### Non-reasoning mode (Table 4)

- `MMLU-PRO`: `81.0`
- `AIME 2025`: `44.6` (notably much lower than reasoning mode, consistent with ‚Äúmode‚Äù affecting reasoning-style tasks)
- `OPENAI-MRCR`: `60.9` (higher than its reasoning-mode MRCR score of 52.3 in Table 3)
- `KGC-SAFETY`: `88.4` safe rate (lower than reasoning-mode 96.1)

### 5.3 Do experiments support the claims?

- **Claim: competitive performance vs open-weight models of similar size**
  - Tables 3‚Äì4 provide direct comparisons against several large models (e.g., `Qwen3-235B-A22B`, `DeepSeek-V3.2`, `gpt-oss-120b`) with their parameter counts and activated parameters listed.
  - The results are mixed rather than uniformly dominant:
    - Example: On `MMLU-PRO` (reasoning), K-EXAONE `83.8` is close to Qwen `84.4*` and DeepSeek `85.0*` (Table 3).
    - On `AIME 2025`, K-EXAONE `92.8` is close to DeepSeek `93.1*` and above Qwen `92.3*` (Table 3).
    - On `SWE-BENCH VERIFIED`, K-EXAONE `49.4` is below gpt-oss `62.4*` and DeepSeek `73.1*` (Table 3).
  - This pattern matches the report‚Äôs conservative framing (‚Äúcomparable‚Äù, not ‚Äúbest everywhere‚Äù).

- **Claim: long-context capability up to 256K**
  - The training procedure explicitly targets 256K (Section 3.2), and evaluation uses up to 160K/128K contexts (Section 4.1).
  - Long-context benchmark scores (AA-LCR, OPENAI-MRCR) are reported (Tables 3‚Äì4), which supports that the model functions in long-context regimes, though it does not by itself prove perfect performance at the full 256K limit.

- **Claim: multilingual coverage and balanced gains**
  - Appendix E reports per-language MMMLU gains (Table 5) and WMT24++ directions (Table 6).
  - Example (Table 5, reasoning): German increases from `80.3` (EXAONE-4.0-32B) to `85.1` (K-EXAONE), suggesting broad gains rather than a single-language improvement.

### 5.4 Ablations / robustness / failure cases

- The excerpt does **not** include classical ablations (e.g., removing MTP, changing routing, changing window size) with quantified impacts.
- Robustness checks present include:
  - Long-context monitoring with NIAH during training (Section 3.2), but without published numeric curves/thresholds here.
  - Safety evaluation spanning naive/adversarial/multi-turn/multilingual variations in KGC-SAFETY (Appendix F.2, Table 8).
- Failure cases are not deeply enumerated beyond the general limitations list (Section 5).

---

## 6. Limitations and Trade-offs

- **General model limitations (explicitly stated in Section 5)**
  - The model may generate inappropriate, biased, or incorrect responses, including harmful/personal information, despite filtering efforts.
  - Outputs may be outdated or contradictory because the model does not reflect the latest information.
  - Responses can be semantically/syntactically incorrect due to statistical learning artifacts.

- **MoE-specific trade-offs (implied by design; partially addressed)**
  - MoE introduces routing complexity; the report mitigates this via sequence-level load balancing and dropless routing (Section 2.1), but does not quantify residual routing overheads or expert collapse metrics in the excerpt.
  - Freezing the router during RL (Section 3.4) stabilizes training but may limit the ability of post-training to adapt expert specialization.

- **Long-context trade-offs**
  - Reducing SWA window size to `128` saves KV-cache memory (Section 2.1) but can reduce purely local attention bandwidth; the model relies on GA layers and training to maintain long-range coherence.
  - The evaluation uses up to `160K` contexts (Section 4.1), so benchmark evidence at the full `256K` maximum is indirect in this excerpt.

- **Evaluation and reproducibility constraints**
  - Some baseline results are pulled from ‚Äúofficial technical reports/blog/leaderboards‚Äù (`*` in Tables 3‚Äì4), while others are internally evaluated, creating potential setup mismatch despite best-effort configuration alignment (Section 4.1).
  - Multiple key benchmarks rely on LLM judges (Appendix C.3, C.9; Appendix D.1; Appendix F.2), which can introduce judge/model bias and reduce comparability unless the same judge protocol is reused.

- **Missing training details (from the excerpt)**
  - Hardware, distributed training strategy, batch sizes, weight decay, dropout, gradient clipping, and detailed data filtering/deduplication are not specified here, limiting full replication from this text alone.

---

## 7. Implications and Future Directions

- **How this changes the landscape (as supported here)**
  - The report demonstrates a concrete recipe for building a **frontier-scale Korean-centered multilingual MoE model** with a very long context window (`256K`) and a broad capability evaluation suite (Sections 2‚Äì4).
  - It also contributes a Korea-contextualized safety framework (`K-AUT`) and benchmark (`KGC-SAFETY`) that could influence how ‚Äúsovereign AI‚Äù efforts evaluate cultural/legal sensitivity (Appendix F).

- **Follow-up research enabled/suggested**
  - **Quantitative ablations**: measure the isolated impact of dropless routing, SWA window size reduction, SWA-only RoPE, QK Norm, and MTP on both quality and throughput.
  - **Long-context validation at full 256K**: publish benchmark protocols/results explicitly at or near 256K to substantiate max-context claims beyond training-time NIAH gating.
  - **Agentic evaluation standardization**: since tool-agent pipelines vary (notably BROWSECOMP baselines are not reproduced; Appendix C.6), more standardized agent setups would improve comparability.

- **Practical applications / downstream use cases implied by the design**
  - **Industrial long-document workflows**: summarization, QA over large internal documents, and multi-step analysis where 128K‚Äì256K context helps.
  - **Multilingual assistants** across Korean/English/Spanish/German/Japanese/Vietnamese, including translation (WMT24++ results in Tables 3‚Äì4 and Appendix E.1).
  - **Agentic tool use** where the summarizer/trajectory compressor pattern improves context efficiency over long tool-call traces (Section 3.3), which is relevant for search agents and operational automation.

- **Repro/Integration Guidance (when to prefer this approach)**
  - Prefer an MoE foundation model like K-EXAONE when you need **high capacity** but want to keep **per-token compute** closer to a smaller dense model (236B total vs 23B activated; Table 1).
  - Prefer this long-context design when your serving stack supports `SWA` and `GA` natively and you need memory-efficient inference at long contexts (Section 2.1).
  - If deploying agentic web search, the report‚Äôs design suggests using **internal summarization** plus **structured trajectory compression** to prevent tool-history bloat (Section 3.3), especially when tool steps can reach hundreds (Appendix C.6 uses up to 500 steps and compresses every 50 steps for BROWSECOMP evaluation).



# Transformers without Normalization

**ArXiv:** [2503.10622](https://arxiv.org/abs/2503.10622)
**Authors:** Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu
**Institutions:** Meta (Fair), New York University, MIT, Princeton University

## ğŸ¯ Pitch

The paper presents Dynamic Tanh (DyT), an innovative element-wise operation that replaces normalization layers in Transformers, maintaining or slightly enhancing model performance across diverse domains. By eliminating the need for computationally heavy normalization processes, DyT simplifies architectures and improves portability, particularly benefiting hardware where reduction operations are costly, thereby redefining the essential mechanisms behind training stability and model efficacy.

---

## 1. Executive Summary (2â€“3 sentences)
This paper introduces Dynamic Tanh (`DyT`), a single elementâ€‘wise operation `DyT(x) = Î³ Â· tanh(Î±x) + Î²` that can replace normalization layers (LayerNorm/RMSNorm) inside Transformers. Across vision, speech, diffusion, DNA sequence modeling, and large language models (LLMs), models with DyT match or slightly exceed the performance and stability of their normalized counterparts while requiring little or no hyperparameter tuning (Tables 1â€“6, Figures 5â€“6).

## 2. Context and Motivation
- Gap being addressed
  - Modern deep networks nearly always include normalization layers like LayerNorm (LN) and RMSNorm to stabilize training and improve generalization. The paper asks whether such normalization is truly indispensable in Transformers and, if not, what minimal mechanism is actually needed.
- Why it matters
  - Practical: Removing normalization removes reductions over dimensions (means/variances), potentially simplifying kernels and improving portability to hardware where reductions are costly. It also simplifies architectural design and may ease integration with fused kernels.
  - Scientific: Understanding what normalization layers â€œdoâ€ clarifies their role in optimization and representation. The paper empirically characterizes LNâ€™s behavior and proposes a simpler primitive that appears to capture the essential effect.
- Prior approaches and shortcomings
  - Initialization-only strategies (e.g., Fixup; SkipInit) and weight reparameterization or spectral control (e.g., ÏƒReparam) can train without normalization but often require delicate learning-rate tuning and still underperform on strong baselines (Table 9 shows 4â€“10+ percentageâ€‘point gaps on ImageNet-1K for ViT and MAE).
  - Other work removes or relocates normalization but keeps some form of normalization or requires extensive fineâ€‘tuning.
- Positioning
  - The paper starts from an empirical analysis of LNâ€™s inputâ€“output behavior (Figures 2â€“4), observes it is Sâ€‘shaped and tanhâ€‘like, and introduces a dropâ€‘in elementâ€‘wise layer that reproduces this behavior without computing statistics. The claim is not that normalization is unnecessary in principle, but that Transformers can achieve parity when the â€œsquash extreme values while stay nearly linear near zeroâ€ behavior is provided by a much simpler mechanism.

## 3. Technical Approach
Step 1: Examine what normalization layers compute
- General normalization form (Equation 1): `normalize(x) = Î³ * (x - Î¼) / sqrt(Ïƒ^2 + Îµ) + Î²`. LN computes `Î¼, Ïƒ` per token; RMSNorm drops mean-centering but scales by the RMS of features. These operations are linear for a single tokenâ€™s features but vary tokenâ€‘byâ€‘token.

Step 2: Empirical probe of LNâ€™s inputâ€“output mapping
- Method: For trained ViT, wav2vec 2.0, and DiT models, collect tensors immediately before and after LN (preâ€‘affine) and plot output vs. input elementwise (Figure 2).
- Observation: Deeper LNs produce Sâ€‘shaped curves closely resembling a scaled `tanh` (Figure 3). Early layers look nearly linear; deeper layers show a clear â€œsquash the extremesâ€ effect.
- Why LN appears nonâ€‘linear overall despite being linear per token: When points are colored by token, each tokenâ€™s mapping is a straight line with a different slope (due to different perâ€‘token standard deviations). Overlaying all tokens yields an aggregate Sâ€‘shape (Figure 4, left). When colored by channel, a few channels have extreme ranges and get squashed most (Figure 4, right).

Step 3: Hypothesis distilled
- The â€œessentialâ€ effect of LN in Transformers is:
  - Keep small/typical activations roughly linear (center of the curve).
  - Disproportionately squash extreme activations so they do not dominate downstream computation.
- This is analogous to a smooth saturating nonlinearity with learnable scale.

Step 4: Replace LN with a direct elementâ€‘wise squasher
- Dynamic Tanh (Equation 2): `DyT(x) = Î³ Â· tanh(Î±x) + Î²`.
  - `tanh(Â·)` provides the saturating Sâ€‘curve.
  - `Î±` is a learnable scalar shared across the whole layer that adjusts how â€œwideâ€ the linear regime is.
  - `Î³, Î²` are perâ€‘channel scale and shift (same shapes as in normalization layers). They preserve representational flexibility of the downstream layer.
- Implementation: Replace every LN/RMSNorm in attention blocks, feedâ€‘forward blocks, and the final pre-output normalization with one DyT (Figure 1). Pseudocode is given in Algorithm 1; it is a few lines of elementâ€‘wise math.

Step 5: Initialization and minimal architectural additions
- Default initialization works broadly: `Î³ = 1`, `Î² = 0`, and `Î±0 = 0.5` for nonâ€‘LLM models (Section 4; Section 7.1 and Figure 9).
- For LLMs, training improves if `Î±0` is tuned and a single scalable scalar is inserted after the token embedding to set a reasonable activation scale at the start of training (Appendix A). Optimal `Î±0` tends to be higher in attention blocks than in FFN blocks (Table 10; Figure 11), and smaller for wider models (Table 11).

Why this design over alternatives
- Squashing function choice: ablations show the squasher is essential; replacing `tanh` with identity causes divergence; `hardtanh` and `sigmoid` train but underperform `tanh` (Table 7).
- Learnable `Î±` is needed: removing `Î±` degrades accuracy for all squashers (Table 8).
- Mechanistic support: During and after training, learned `Î±` tracks the inverse standard deviation of the preâ€‘DyT activations (Figure 8), approximating the scaleâ€‘setting role of normalization while using a single scalar rather than perâ€‘token statistics.

## 4. Key Insights and Innovations
- Empirical decoding of LayerNormâ€™s behavior into a tanhâ€‘like mapping
  - Novelty: Rather than assuming LN is â€œjust linear standardization,â€ the paper shows its global elementâ€‘wise mapping across tokens is Sâ€‘shaped and closely matches `tanh` (Figures 2â€“4). This reframes normalization as a smooth â€œsquashâ€‘extremesâ€ mechanism with a large linear center.
  - Significance: Provides an interpretable and testable target behavior for normalization substitutes.
- Dynamic Tanh (`DyT`) as a dropâ€‘in, statisticsâ€‘free replacement
  - Novelty: `DyT(x) = Î³ Â· tanh(Î±x) + Î²` replaces the entire normalization computation (no means/variances, no reductions) with an elementâ€‘wise function plus learnable scale/shift (Equation 2, Algorithm 1).
  - Significance: In practice, DyT achieves parity or small gains across diverse Transformer applications and sizes (Tables 1â€“6), challenging the view that perâ€‘token normalization statistics are essential.
- Mechanistic link between learned `Î±` and activation scale
  - Novelty: `Î±` learns to track `1 / std(preactivations)` throughout training (Figure 8).
  - Significance: Explains why DyT stabilizes trainingâ€”`Î±` maintains activations in the wide linear region of `tanh` and lets the tails saturate, mimicking LNâ€™s outlier control.
- Practical recipe for LLMs: roleâ€‘specific and widthâ€‘aware `Î±0`
  - Novelty: Tuning `Î±0` differently for attention vs. other blocks materially improves LLM pretraining, and optimal values shrink with model width (Tables 10â€“11; Figure 11).
  - Significance: Converts DyT from a general proofâ€‘ofâ€‘concept into a practical normâ€‘free alternative for billionâ€‘parameter language models.

## 5. Experimental Analysis
Evaluation setup
- Models and domains (Section 5)
  - Supervised vision: ViTâ€‘B/L and ConvNeXtâ€‘B/L on ImageNetâ€‘1K.
  - Selfâ€‘supervised vision: MAE and DINO with ViT backbones, evaluated by fineâ€‘tuning on ImageNetâ€‘1K.
  - Diffusion: DiTâ€‘B/L/XL on ImageNetâ€‘1K; metric is FID (lower is better).
  - Speech: wav2vec 2.0 Base/Large pretraining on LibriSpeech; report validation loss.
  - DNA sequence modeling: HyenaDNA and Caduceus; accuracy on GenomicBenchmarks.
  - LLMs: LLaMA 7B/13B/34B/70B trained on The Pile for 200B tokens; report training loss and average zeroâ€‘shot accuracy on 15 lmâ€‘eval tasks (Appendix A for dataset and protocol details).
- Baselines and hyperparameters
  - Wherever possible, use the exact training recipe of the normalized baseline; only replace LN/RMSNorm with DyT (Section 5). For DiT, a brief LR search was applied to both LN and DyT; for LLMs, a single learnable scalar is added after embeddings and `Î±0` is tuned per Section 7.

Main quantitative results
- Supervised vision (Table 1; Figure 5)
  - > ViTâ€‘B: 82.3% (LN) â†’ 82.5% (DyT); ViTâ€‘L: 83.1% â†’ 83.6%.
  - > ConvNeXtâ€‘B: 83.7% â†’ 83.7%; ConvNeXtâ€‘L: 84.3% â†’ 84.4%.
  - Training curves are nearly identical (Figure 5), suggesting comparable optimization dynamics.
- Selfâ€‘supervised vision (Table 2)
  - > MAE ViTâ€‘B: 83.2% (LN) vs 83.2% (DyT); MAE ViTâ€‘L: 85.5% vs 85.4%.
  - > DINO ViTâ€‘B p16: 83.2% vs 83.4%; p8: 84.1% vs 84.5%.
- Diffusion (Table 3)
  - > FIDâ†“: DiTâ€‘B 64.9 (LN) â†’ 63.9 (DyT); DiTâ€‘L 45.9 â†’ 45.7; DiTâ€‘XL 19.9 â†’ 20.8 (slightly worse).
- Speech pretraining (Table 5)
  - > wav2vec 2.0 Base: loss 1.95 (LN) vs 1.95 (DyT); Large: 1.92 vs 1.91.
- DNA sequence modeling (Table 6)
  - > HyenaDNA: 85.2% (LN) vs 85.2% (DyT); Caduceus: 86.9% vs 86.9%.
- LLMs (Table 4; Figure 6; Section 7.2)
  - > Zeroâ€‘shot average: LLaMA 7B/13B/34B/70B all identical to three decimals (0.513/0.529/0.536/0.549).
  - > Final losses within 0.01 of RMSNorm for 7B and 13B and identical for 34B/70B.
  - Loss curves overlap closely during pretraining (Figure 6).
- Comparison to other normâ€‘free training methods (Table 9)
  - > On ImageNetâ€‘1K with ViTâ€‘B/L and MAE ViTâ€‘B/L, DyT consistently outperforms Fixup and SkipInit by large margins and slightly edges ÏƒReparam.

Ablations, diagnostics, and robustness
- Squasher necessity and choice (Table 7; Figure 7)
  - Identity in place of `tanh` causes divergence; `hardtanh`/`sigmoid` are stable but worse than `tanh`. This confirms the centrality of â€œbounded squashing.â€
- Role of `Î±` (Table 8)
  - Removing `Î±` drops ViTâ€‘B accuracy from 82.5% to ~81% even with squashing, showing scale control is critical.
- Dynamics and values of `Î±` (Figure 8)
  - `Î±` evolves in tandem with `1/std` of inputs during training and correlates with it at convergence, explaining stability and suggesting an approximate normalization effect without statistics.
- Sensitivity of `Î±0` (Section 7; Figures 9â€“11; Tables 10â€“11)
  - Nonâ€‘LLM tasks: broad plateau of good performance for `Î±0` in [0.5, 1.2] (Figure 9). Very large ViTâ€‘L with high LR can diverge if `Î±0` is too big; reducing LR or `Î±0` restores stability (Figure 10).
  - LLMs: Best `Î±0` is higher in attention than in FFN/last layers and shrinks with model widthâ€”e.g., 7B: 0.8(attn)/0.2(other); 70B: 0.2/0.05 (Table 10). Heatmaps (Figure 11) visualize loss improvements with these settings. Width dictates `Î±0` more than depth (Table 11).

Do the experiments support the claims?
- Breadth: The study spans recognition and generation, supervised and selfâ€‘supervised learning, and multiple modalitiesâ€”strong evidence of generality.
- Strength: Parity with RMSNorm on multiâ€‘billionâ€‘parameter LLMs (loss curves in Figure 6 and Table 4) is particularly convincing.
- Nuance: Slight regressions exist (e.g., DiTâ€‘XL FID 20.8 vs 19.9), but overall parity holds. The added embedding scalar for LLMs is a small but real architectural tweak (Appendix A).

Efficiency observations (Appendix C)
- On uncompiled Hugging Face LLaMAâ€‘7B, DyT layer time is lower than RMSNorm and modestly improves endâ€‘toâ€‘end runtime (Table 14), but with `torch.compile` the advantage disappears (Table 15). DyT is elementâ€‘wise (no reductions), which could be beneficial on reductionâ€‘limited hardware, but benefits are not guaranteed on wellâ€‘optimized GPU stacks.

## 6. Limitations and Trade-offs
- Not a dropâ€‘in for all normalization types
  - Replacing BatchNorm in ConvNets (ResNetâ€‘50, VGG19) degrades accuracy notably (Table 16). DyT appears most suitable where LN/RMSNorm are standard (i.e., Transformers).
- LLM recipe is not â€œzeroâ€‘changeâ€
  - Successful LLM training uses one extra learnable scalar after embeddings and tuned `Î±0` per block type and width (Appendix A; Section 7.2). This is still simple, but not a literal 1:1 swap.
- Performance parity, not consistent superiority
  - Most tasks show parity or small gains; some settings regress slightly (e.g., DiTâ€‘XL FID). There is no universal win across all scales and domains.
- Efficiency is contextâ€‘dependent
  - Despite being elementâ€‘wise, DyT did not yield speedups once layers were compiled (Table 15). Gains may depend on hardware, kernel fusion, and compiler maturity.
- Statistical adaptivity vs. global scale
  - LN adapts per token and per sample. DyT uses a single `Î±` per layer, so it cannot rescale different tokens differently within a batch. The â€œperâ€‘token linear, globally Sâ€‘shapedâ€ effect emerges from tanh saturation rather than perâ€‘token statistics; edge cases with highly heterogeneous token statistics might stress DyT.
- Assumptions
  - The central hypothesis is that â€œsquash extremes + learned global scaleâ€ captures what matters about LN in Transformers. While well supported empirically, it remains a modeling assumption rather than a proof.

## 7. Implications and Future Directions
- Conceptual shift
  - Normalizationâ€™s indispensability is questioned: a simple, learnable saturating nonlinearity paired with perâ€‘channel affine parameters appears sufficient for Transformer stability and performance. This reframes normalization as â€œrobust activation shapingâ€ rather than â€œperâ€‘token standardization.â€
- Practical applications
  - Model simplification: Elementâ€‘wise DyT can simplify kernels and may be easier to fuse with adjacent matrix multiplies. It could benefit accelerators where reductions are expensive.
  - Deployment: For inferenceâ€‘only deployments, removing reductions may reduce latency variance and improve portability, especially in smallâ€‘batch or sequential regimes.
- Research directions
  - Extend beyond Transformers: Can DyT be augmented to handle BatchNormâ€‘style roles (Appendix D shows current shortcomings)? Hybrids that combine DyT with light-weight perâ€‘token or perâ€‘channel statistics are promising.
  - Tokenâ€‘aware variants: Introduce a small number of learnable `Î±`s (e.g., per head, per group of channels, or conditional on token type) to reintroduce some adaptivity without full statistics.
  - Alternative squashers: While `tanh` wins among tested options (Table 7), other smooth bounded functions or learnable parametric families could provide better tradeâ€‘offs between linear range and saturation.
  - Theory: Formalize why the global Sâ€‘shape arises in LN (Figures 2â€“4) and when an elementâ€‘wise saturating nonlinearity suffices for stable gradient flow in deep residual Transformers.
  - Scaling laws for `Î±0`: The widthâ€‘driven schedule for `Î±0` (Tables 10â€“11) hints at predictable scaling rules; deriving and validating these across architectures could make DyT fully â€œplugâ€‘andâ€‘play.â€

Overall, the work provides a clear mechanistic story, an extremely simple replacement layer, and broad empirical evidence that Transformers can train stably and perform competitively without normalization statistics. The strongest signals are the LLM results (Table 4, Figure 6) and the mechanistic tracking of `Î±` to activation scale (Figure 8), which together make a persuasive case that â€œsquash extremes + learn a scaleâ€ captures the essential role of LayerNorm in these architectures.

# Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging

**ArXiv:** [2410.12937](https://arxiv.org/abs/2410.12937)
**Authors:** Jacob Morrison, Noah A. Smith, Hannaneh Hajishirzi, Pang Wei Koh, Jesse Dodge, Pradeep Dasigi
**Institutions:** 

## ğŸ¯ Pitch

This paper introduces Parallel Train Then Merge (PTM), a method to integrate new skills into instruction-tuned language models without retraining, preserving existing knowledge and using just a fraction of the compute. By enabling efficient skill expansion and robust safety improvements, PTM supports continuous model evolution, making it a significant advancement for cost-effective, targeted model enhancement in rapidly changing environments. 

---

## 1. Executive Summary
This paper shows a practical way to add new skills to an already instructionâ€‘tuned language model without retraining it from scratch and without erasing what it already knows. The core ideaâ€”â€œparallel train then mergeâ€ (PTM)â€”trains a small skillâ€‘specific model and then combines its weights with a general model using a â€œtask vectorâ€; across science, safety, and coding skills, PTM often matches retraining on the new task while preserving general abilities and using a fraction of the compute (e.g., ~4% of training steps vs. retraining in Table 2).

## 2. Context and Motivation
- Problem addressed
  - How to add new capabilities (e.g., science literature understanding, coding, safety refusals) to an existing instructionâ€‘tuned LM while preserving its general skills and keeping costs low (Section 1, Section 2).
- Why this matters
  - Instruction data is evolving; repeatedly retraining general models is expensive and sometimes infeasible because the original training mix is unavailable for many public models (e.g., Llama 3, Mistral 7B, Gemma; Section 2.2).
  - Continued finetuning (CFT) on a new skill often causes catastrophic forgettingâ€”loss of earlier capabilities (Section 1; quantified in Table 3 as 10â€“40% drops on â€œGeneralâ€).
  - Safety is a special case: you want stronger refusals to harmful prompts without overâ€‘refusing benign requests (â€œexaggerated refusals,â€ defined in Section 3.1).
- Prior approaches and their gaps
  - CFT: inexpensive but degrades general ability (Section 2.1; Table 3 shows âˆ’32.5% to âˆ’40.1% on â€œGeneralâ€ when optimized for new skills).
  - Retraining from scratch (RT) with a combined data mix: preserves general ability better but is expensive and sometimes impossible due to missing base mixes (Section 2.2).
  - Model merging literature exists (task vectors, linear interpolation, WiSEâ€‘FT; Section 2.3; Related Work Section 5), but had not been systematically tested for instructionâ€‘tuned LMs and multiâ€‘skill addition.
- Positioning
  - The paper formulates and evaluates a PTM pipeline for instruction tuning, comparing it headâ€‘toâ€‘head with CFT and RT on cost, generalâ€‘skill retention, and newâ€‘skill gains across three domains (Section 3).

## 3. Technical Approach
At a high level, PTM follows a simple, repeatable pipeline:

1) Train a skillâ€‘specific model in isolation
- Start from a pretrained base `Î¸_pre` and fully finetune on new skill data `D` for two epochs (Section 3.2), producing `Î¸_D`.
- This isolates skill learning from the general instruction mix, avoiding interference during training.

2) Build a task vector
- A â€œtask vectorâ€ is the parameter difference between the skillâ€‘specific model and its starting point:
  - Equation (1): `Ï„_D = Î¸_D âˆ’ Î¸_pre` (Section 2.3, â€œTask Arithmeticâ€).
- Intuition: `Ï„_D` encodes what changed to learn the skill.

3) Merge the skill into the general model
- Combine the task vector with an existing general instructionâ€‘tuned model `Î¸_G`:
  - Equation (2): `Î¸_final = Î¸_G + Ï‰ Â· Ï„_D`, where `Ï‰` is a scalar weight controlling how much of the new skill to add (Section 2.3).
- Model selection for `Ï‰`
  - If heldâ€‘out validation exists, tune `Ï‰` to balance general vs. newâ€‘skill performance.
  - When no heldâ€‘out data is available, use a simple heuristic (Equation (7) in Section 4.2): `Ï‰ = |D| / |G|`, i.e., the ratio of skillâ€‘data steps to generalâ€‘data steps. Figure 1 highlights that this often lands near a good tradeâ€‘off across domains.

Why task arithmetic over alternatives?
- The paper evaluates three PTM instantiations (Section 2.3):
  - Task arithmetic (above).
  - Linear interpolation: treat both general and skill vectors relative to `Î¸_pre`, then linearly mix them (Equations (3)â€“(4)).
  - WiSEâ€‘FT: continue finetuning the general model on the skill (`Î¸_CFT` from `Î¸_G`), then merge back with a weight (Equations (5)â€“(6)).
- Empirically, task arithmetic best preserves general skills for a given skill gain (Figure 3), and it also has the lowest training cost because it trains a single skill model once and then explores multiple `Ï‰` values at negligible extra cost.

Experimental design (Section 3)
- Base and infrastructure
  - Backbone: `Llama 2 7B` (Touvron et al., 2023).
  - Training hyperparameters (Appendix A.2): full finetuning, 2 epochs, batch size 128, max length 4096, learning rate 2eâˆ’5, warmup 3%.
  - Compute: Google TPU v3; training steps are the cost unit (Section 2).
- Data (Table 1; Section 3.1)
  - General training: modified `TÃ¼lu V2` mix (275k instances) with science, code, and refusals removed to create room for â€œnew skillâ€ gains.
  - Skill datasets:
    - `SciRIFF` (61k): multiâ€‘task scientific literature understanding; has validation and test (Section 3.1 â€œScienceâ€).
    - `Safety` (66k): internally built harmful prompts with GPTâ€‘4 refusals; covers malicious uses, toxicity, misinformation (Section 3.1 â€œSafetyâ€).
    - `CodeFeedback` singleâ€‘turn subset (156k) (Section 3.1 â€œCodingâ€).
- Evaluations (Sections 3.1; Tables 7â€“16 for perâ€‘benchmark detail)
  - General: average over MMLU, GSM8K, BBH, TruthfulQA, AlpacaEval.
  - Science: SciRIFFâ€™s nine validation/test tasks.
  - Safety: average of ToxiGen, HarmBench, and XSTest Unsafe, plus a separate â€œExaggerated Refusalsâ€ metric from XSTest Safe.
  - Coding: HumanEval+ and MBPP+, pass@10 with temperature 0.8.

Cost accounting (Section 2)
- CFT: train multiple runs on various subsampled `D_i`; cost is sum of `|D_i|`.
- RT: for each mixing ratio, train on all `G` plus some `D_i`; cost `n Â· |G| + Î£_i |D_i|`â€”expensive because `|G|` is large.
- PTM: train once on all `D` to get a single task vector; selection over `Ï‰` is essentially free; cost `|D|`.

## 4. Key Insights and Innovations
- Efficient â€œskills as vectorsâ€ works for instructionâ€‘tuned LMs
  - Novelty: A systematic, multiâ€‘domain evaluation of taskâ€‘vector merging for instruction tuning (Section 4). Prior merging/editing work focused on other settings (Section 5).
  - Significance: PTM matches or approaches retraining on new skills while preserving the general skill set and cutting compute by 50â€“95% (Section 4.1; Table 2).
- Simple weight heuristic generalizes
  - Insight: Setting `Ï‰ = |D| / |G|` balances newâ€‘skill gains with generalâ€‘skill preservation when no validation data exists (Equation (7)).
  - Evidence: Across science, safety, and coding, the â€œheuristicâ€ points in Figure 1 lie near the knee of the tradeâ€‘off curves.
- Safety gains without overâ€‘refusal
  - Insight: A â€œsafety vectorâ€ dramatically improves refusal of unsafe prompts while avoiding exaggerated refusalsâ€”i.e., refusing safe but superficially risky prompts (Section 4.2 â€œPTM Mitigates Exaggerated Refusalsâ€).
  - Evidence:
    - Table 3 shows â€œBest PTM (Safety)â€ changes general by âˆ’0.13% and improves safety by +88.9%, while â€œBest CFT (Safety)â€ loses 40.1% on general.
    - With all three skills merged, â€œExaggerated Refusalsâ€ improve to 93.2 vs. 16.0 (CFT) and 37.2 (RT) in Table 4.
- Diagnosis of multiâ€‘skill interference
  - Insight: Merging multiple skills sometimes creates negative interactions, especially between coding and safety vectors for science tasks (Section 4.3).
  - Evidence: Pairwise merges in Table 5 show the â€œSafety and Codingâ€ pair drops Science to 18.8 vs. 32.1 (â€œScience and Codingâ€) and 31.6 (â€œScience and Safetyâ€).
- Why some PTM variants hurt general skills
  - Observation: WiSEâ€‘FT and linear interpolation can reach strong newâ€‘skill results but degrade general ability more than task arithmetic (Figure 3).
  - Mechanism: WiSEâ€‘FT finetunes from `Î¸_G` using only `D`, shifting the distribution; mixing a matching amount of general data with `D` during the WiSEâ€‘FT step restores general skill and improves science (Figure 2).

## 5. Experimental Analysis
- Setup recap (Sections 3.1â€“3.2; Table 1; Appendix A)
  - Models: Llama 2 7B variants; all finetuned for two epochs.
  - Skill domains: Science (SciRIFF), Safety (internal), Coding (CodeFeedback).
  - Metrics: Composite general score; skillâ€‘specific aggregates; separate â€œExaggerated Refusalsâ€ from XSTest Safe.
  - Baseline â€œTÃ¼lu Onlyâ€ general score: 49.9 (Table 2).

- Main quantitative results
  - Science, compute vs. performance (Section 4.1; Table 2)
    - PTM reaches Science 38.2 and General 47.1 with only 479 steps.
    - Retraining (RT) reaches Science 37.8 and General 50.6 but costs 11,766 steps.
    - Continued finetuning (CFT) reaches Science 40.6 but General falls to 33.7 (1,005 steps).
    - Quote the tradeâ€‘off:
      > Table 2: â€œPTM shows equivalent science performance to the best RT model â€¦ while taking about 4% as many training steps.â€
  - Crossâ€‘domain deltas (Section 4.2; Table 3)
    - PTM preserves general ability compared to CFT in all domains.
      - Example: â€œBest PTM (Coding)â€ improves General by +1.43% and Coding by +33.3%, whereas â€œBest CFT (Coding)â€ drops General by âˆ’7.73% even though Coding is +51.6%.
    - Safety stands out:
      - â€œBest PTM (Safety)â€ changes General by âˆ’0.13% and improves Safety by +88.9%; â€œBest RT (Safety)â€ is +0.66% General and +89.6% Safety but ~24Ã— the training steps (12,311 vs. 517).
    - Exaggerated refusals:
      - â€œBest PTM (Ex. Ref.)â€ improves exaggeratedâ€‘refusal compliance by +72.6 (percentage points; see Table 3 note) at modest general loss (âˆ’6.45%).
  - Multiâ€‘skill merging (Section 4.3; Table 4)
    - Merge all three vectors (â€œPTM (All 3)â€):
      - General improves to 51.1 (vs. 49.9 baseline).
      - Coding: 45.3 (up from 37.6).
      - Safety: 84.0 (up from 50.3).
      - Exaggerated Refusals: 93.2 (huge gain vs. CFT 16.0 and RT 37.2).
      - Science drops slightly to 26.6 (vs. 27.8).
      - Cost: zero extra training steps once the three skill vectors exist (â€œAdditional Training Stepsâ€ column).
    - Interference diagnosis (Table 5):
      > Science collapses most when merging â€œSafety and Codingâ€ (Science 18.8) compared to â€œScience and Safetyâ€ (31.6) and â€œScience and Codingâ€ (32.1).
  - PTM variants (Section 4.4; Figure 3; Figure 2)
    - Figure 3: task arithmetic best preserves general ability; linear interpolation and WiSEâ€‘FT show larger general drops for similar newâ€‘skill gains.
    - Figure 2: WiSEâ€‘FT trained on SciRIFF plus a matched amount of TÃ¼lu data retains more general skill and improves science vs. WiSEâ€‘FT on SciRIFF alone.
  - Other merge algorithms (Appendix B; Table 6)
    - TIES and DARE, which aim to reduce interference, do not outperform simple weighted averaging in this setup and do not fix the science drop in the 3â€‘skill merge.

- Robustness, ablations, and diagnostics
  - Tradeâ€‘off curves: The paper plots full curves over Ï‰ (Figure 1; Figure 3; Figure 6), not just single points, showing PTMâ€™s controllable tradeâ€‘off between general and skillâ€‘specific performance.
  - Heuristic selection: The `Ï‰ = |D| / |G|` marker is consistently near a good operating point (Figure 1; Figure 5).
  - Safety nuance: Direct comparison of â€œGeneral vs. Exaggerated Refusalsâ€ demonstrates PTMâ€™s advantage at any given general score (Figure 4).

- Do the experiments support the claims?
  - Yes for efficiency and retention: Across domains, PTM achieves skill gains comparable to RT with far fewer training steps, while CFTâ€™s general performance consistently drops (Tables 2â€“4).
  - Yes for safety behavior: PTM boosts refusals and reduces exaggerated refusals substantially (Tables 3â€“4; Tables 12 and 16).
  - Mixed for multiâ€‘skill composition: Merging multiple skills is feasible but can create interference; the paper both exposes and diagnoses this with pairwise analyses (Table 5), which is candid and useful.

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Focus on one backbone (`Llama 2 7B`) and supervised instruction tuning; no RLHF or postâ€‘training alignment layers are studied (Limitations section).
  - Coding evaluation uses singleâ€‘turn data and pass@10; multiâ€‘turn code refinement is outside scope (Section 3.1).
- When PTM may underperform
  - Multiâ€‘skill interference can hurt some domains (science in 3â€‘skill merge), and common antiâ€‘interference mergers (TIES/DARE) did not resolve it here (Table 6).
  - WiSEâ€‘FT and linear interpolation variants can degrade general skills if the finetuning distribution differs from the general mix (Figure 3); adding matched general data helps but increases cost (Figure 2).
- Data and selection constraints
  - Many instruction datasets lack validation splits; the paper offers a heuristic for `Ï‰`, but selection without heldâ€‘out data will still be approximate (Section 3.2; Section 4.2).
- Compute reporting
  - Cost is measured in â€œtraining stepsâ€ at fixed batch size; hardware, FLOPs, and wallâ€‘clock time may vary across setups (Section 2).
- External validity
  - Results are on open datasets and one internal safety dataset; broader generalization to other families, sizes, or proprietary corpora remains to be demonstrated (Limitations; Section 3.1).

## 7. Implications and Future Directions
- Practical impact
  - â€œSkills as plugâ€‘in vectorsâ€ makes continual capability growth feasible without retraining large general models or needing access to their original training mix.
  - Organizations can add or update safety behaviors rapidly while avoiding excessive overâ€‘refusal, which is critical for user experience and compliance (Tables 3â€“4, 12, 16).
  - Produces a zeroâ€‘inferenceâ€‘overhead solution: merging is weightâ€‘space, so runtime cost is unchanged compared to the base model (Section 2.3).
- New workflows this enables
  - A â€œskill libraryâ€ ecosystem: curate and share task vectors (science, safety facets, coding languages, domains), then compose them as needed using simple weighted addition.
  - Onâ€‘device or privacyâ€‘sensitive settings: PTM avoids retraining on potentially sensitive original data; only the new skill data is needed.
- Research directions
  - Interferenceâ€‘aware composition: develop principled methods to detect and mitigate crossâ€‘skill clashes during merge time; the failure case in Table 5 is a concrete target.
  - Dataâ€‘aware WiSEâ€‘FT: quantify â€œhow much general dataâ€ is needed during CFT to preserve general ability (two explicit questions raised in Section 4.4).
  - Automatic `Ï‰` selection: metaâ€‘learning or Bayesian selection of mixture weights using small, representative probes when validations are absent.
  - Beyond supervised instruction tuning: test PTM with models that underwent RLHF, toolâ€‘use training, or multiâ€‘turn interactive fineâ€‘tuning.
  - Evaluation breadth: expand generalâ€‘reasoning and multiâ€‘turn safety/coding tests to probe whether task vectors transfer to more complex interaction patterns.

Overall, this work reframes model adaptation as an efficient, modular merge problem. The evidence across sections and tables shows PTM as a strong default for adding skills: it usually matches retraining on the target skill, better preserves what the model already knows than continued finetuning, and is especially effective and controllable for safety behaviors.

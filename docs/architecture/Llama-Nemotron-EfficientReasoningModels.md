# Llama-Nemotron: Efficient Reasoning Models

**ArXiv:** [2505.00949](https://arxiv.org/abs/2505.00949)

## ğŸ¯ Pitch

Llama-Nemotron debuts a family of open, efficient 'reasoning' language modelsâ€”spanning 8B to 253B parametersâ€”combining state-of-the-art multi-step reasoning with remarkably fast, memory-efficient inference and a user-controlled reasoning switch. By leveraging neural architecture search tailored for hardware, advanced distillation, targeted pretraining, and large-scale reinforcement learning, Llama-Nemotron not only outperforms previous open models but also democratizes scalable reasoning for enterprises and researchers, enabling accessible deployment and dynamic control of answer depth across diverse real-world applications.

---

## 1. Executive Summary (2-3 sentences)
Llamaâ€‘Nemotron introduces an open family of â€œreasoningâ€ language modelsâ€”`LNâ€‘Nano` (8B), `LNâ€‘Super` (49B), and `LNâ€‘Ultra` (253B)â€”that combine strong multiâ€‘step problem solving with high inference efficiency and a userâ€‘controllable reasoning toggle (â€œdetailed thinking on/offâ€). Using a hardwareâ€‘aware neural architecture search, targeted distillation/pretraining, supervised reasoning traces, and largeâ€‘scale reinforcement learning (RL), the flagship `LNâ€‘Ultra` matches or exceeds stateâ€‘ofâ€‘theâ€‘art open models on reasoning while running faster and in less memory, especially on 8Ã—H100 GPUs (Figures 2 and 4; Tables 1 and 5).

## 2. Context and Motivation
- Problem addressed
  - Recent â€œreasoning LLMsâ€ achieve high accuracy by generating long chains of thought, but they are expensive to run and often slow at inference because they expand many tokens per answer (Section 1). This constrains both user experience and the feasibility of large agent systems.
  - Users also need control: many tasks do not benefit from verbose multiâ€‘step reasoning, so a model should switch style without swapping models (Section 1).

- Why it matters
  - Inference cost and latency have become a limiting factor for systemâ€‘level intelligence and realâ€‘world deployment (Section 1). Efficient reasoning enables broader, cheaper, and faster use of advanced capabilities in applications like STEM tutoring, code assistants, and agent pipelines.

- Prior approaches and gaps
  - Strong closed and open reasoning models (e.g., OpenAIâ€™s o1, DeepSeekâ€‘R1) achieve accuracy but typically require heavy hardware (e.g., 8Ã—H200 for DeepSeekâ€‘R1) and lack explicit, open, hardwareâ€‘optimized architectures with controllable reasoning modes (Sections 1â€“2; Figure 4 caption).
  - Open instructionâ€‘tuned models (e.g., Llama 3.x) are efficient but generally weaker on multiâ€‘step reasoning without additional postâ€‘training, and they do not provide a dynamic reasoning toggle (Sections 1, 3).

- Positioning
  - Llamaâ€‘Nemotron bridges the gap by: (1) redesigning base Llama models for inference efficiency through a NAS framework (`Puzzle`) and FFN Fusion (Section 2), (2) recovering and enhancing quality via distillation and continued pretraining (Section 2.2), then (3) adding reasoning via supervised traces and RL (Sections 4â€“5), all while exposing a simple runtime switch for reasoning behavior (Sections 1, 3).

## 3. Technical Approach
Stepâ€‘byâ€‘step pipeline spanning five stages (Sections 1â€“2, 4â€“6):

1) Creating inferenceâ€‘optimized backbones with `Puzzle` NAS (Section 2; Figure 3)
- What `Puzzle` does:
  - Builds a â€œlibraryâ€ of alternative transformer â€œblocksâ€ for each layer using blockâ€‘wise local distillation: each candidate block is trained to mimic the original layerâ€™s behavior while trading off accuracy vs. speed/memory (Section 2; Figure 3 Step 1).
  - Available variants include:
    - Attention removal: omit attention in some layers to reduce compute and KVâ€‘cache memory (Section 2).
    - Variable FFN width: shrink or widen the feedâ€‘forward networkâ€™s hidden size at different ratios (Section 2).
    - Other options are supported (e.g., groupedâ€‘query attention, linear attention, noâ€‘ops), but the two above dominated the LN modelsâ€™ efficiency/quality tradeâ€‘off (Section 2).
  - Assembling the final model:
    - A mixedâ€‘integer programming (MIP) solver selects one candidate block per layer to optimize accuracy under deployment constraints (throughput, latency, memory, hardware) (Section 2; Figure 3 Step 2).
- Why this design:
  - It directly searches the accuracyâ€‘efficiency Pareto frontier per layer and per hardware target instead of naively compressing the whole network, allowing heterogeneous layers specialized for throughput or memory (Section 2.1).

2) Vertical depth reduction with `FFN Fusion` (LNâ€‘Ultra only) (Section 2; â€œVertical Compression with FFN Fusionâ€)
- Mechanism:
  - After `Puzzle` removes attention from some layers, adjacent FFN blocks can appear. `FFN Fusion` replaces consecutive FFNs with fewer but wider FFNs that execute in parallel, reducing sequential depth and interâ€‘GPU communication (Section 2; FFN Fusion).
- Benefit:
  - Improves latency and utilizationâ€”crucial on multiâ€‘GPU nodes (Section 2).

3) Recovery training: knowledge distillation and continued pretraining (CPT) (Section 2.2; Table 1)
- Purpose:
  - Restores/boosts quality after architectural changes and improves interâ€‘block compatibility.
- Setup:
  - `LNâ€‘Super`: 40B tokens of distillation on Distillation Mix (Section 2.2).
  - `LNâ€‘Ultra`: 65B distillation + 88B CPT on Nemotronâ€‘H phase 4 (Section 2.2).
- Outcome before any reasoning SFT/RL (Table 1):
  - `LNâ€‘Ultraâ€‘CPT` meets or exceeds strong baselines on challenging tasks, e.g. MATH500 80.4 vs. 69.6 for Llamaâ€‘3.1â€‘405B, and RULERâ€‘128K 83.2 vs. 73.7 (Table 1).

4) Reasoningâ€‘focused supervised fineâ€‘tuning (SFT) with a runtime reasoning toggle (Section 4; Section 3; Table 2)
- Reasoning toggle:
  - A lightweight system instructionâ€”`"detailed thinking on"` or `"detailed thinking off"`â€”conditions the model to include or hide chains of thought. Format rewards later enforce `<think> ... </think>` tags when â€œonâ€ and their absence when â€œoffâ€ (Sections 1, 3, 5.1).
- Synthetic data curation (Section 3; Table 2):
  - Math: harvested problems from AoPS forums; invalid/problematic items filtered; final answers extracted for automatic checking; decontamination against benchmarks; multiâ€‘solution generation with DeepSeekâ€‘R1 (16 samples) and Qwen2.5â€‘Mathâ€‘7B (64); wrong answers removed via LLMâ€‘based equivalence checking (Section 3.1.1).
  - Code: 28,904 programming tasks aggregated (TACO, APPS, CodeContests, CodeForces) with strict decontamination; DeepSeekâ€‘R1 produces multiâ€‘sample solutions with explicit reasoning in `<think>` tags; code segments validated (Treeâ€‘Sitter); ~488K Python samples (Section 3.1.2). Scaling study indicates larger, harder datasets keep improving coding performanceâ€”no early plateau (Section 3.1.2 â€œData Scaling Insightsâ€).
  - Science: mixture of real and synthetic MCQs across physics/biology/chemistry; decontaminated against GPQA/MMLU/MMLUâ€‘Pro; DeepSeekâ€‘R1 generates multiâ€‘trace solutions; majority voting when gold answers are missing (Section 3.1.3).
  - General: openâ€‘domain instructions/prompts with multiple responses filtered by a 70B reward model; augmented with safety/functionâ€‘calling datasets (Sections 3.1.4, 3.2).
  - Paired â€œreasoning on/offâ€ responses to teach the toggle, with offâ€‘mode responses generated by Llamaâ€‘3.1â€‘Nemotronâ€‘70Bâ€‘Instruct or Llamaâ€‘3.3â€‘70Bâ€‘Instruct (Section 3.2).
  - Overall blend: 33,011,757 samples; notably math is 66.8% and code 30.6% of the corpus (Table 2).
- Modelâ€‘specific SFT (Section 4.2):
  - `LNâ€‘Nano`: threeâ€‘stage SFT, starting with reasoningâ€‘only to avoid degenerate repetition, then mixing in nonâ€‘reasoning, finishing with chat/instruction/toolâ€‘calling; effective sequence length 32k, global batch 256 (Section 4.2).
  - `LNâ€‘Super`: one epoch over full SFT dataset; seq length 16k; global batch 256; fixed LR 5eâ€‘6 (Section 4.2).
  - `LNâ€‘Ultra`: sequence packing to ~24k effective length; global batch 256; cautious LR schedule (warmup to 1eâ€‘5 then cosine to 1eâ€‘6); training required restarts due to gradient instabilities after the first epoch (Section 4.2).

5) Reinforcement learning for scientific reasoning (LNâ€‘Ultra) (Section 5; Figures 5â€“6)
- Why RL: SFT distilled from a teacher caps performance at the teacherâ€™s level; RL enables surpassing it (Section 5).
- Algorithm:
  - `GRPO` (Group Relative Policy Optimization): sample groups of responses per prompt and update toward those that score better relative to the group (Section 5.1).
- Rewards (Section 5.1):
  - Accuracy reward: an LLM judge (Llamaâ€‘3.3â€‘70Bâ€‘Instruct) checks if the final answer matches ground truth (numeric, sentence, paragraph).
  - Format reward: enforce `<think>` tags when â€œonâ€ and their absence when â€œoffâ€, similar to DeepSeekâ€‘R1 (Section 5.1).
- Hardâ€‘example curriculum (Section 5.1; Figure 6):
  - Precompute pass rates using `LNâ€‘Super`; discard easy items (pass rate â‰¥ 0.75); progressively shift batches from easier to harder using a Gaussian target distribution over pass rates. Improves stability and accuracy (Figure 6).
- Scale and settings (Section 5.1, 5.2):
  - 72 nodes Ã— 8Ã—H100; generation with `vLLM`, training with `Megatronâ€‘LM`; FP8 decoding; ~140k H100 hours; sampling 16 responses/prompt at temperature=1, top_p=1; global batch 576, 2 optimizer steps per rollout (Sections 5.1â€“5.2).

6) Instruction following and RLHF (Section 6)
- Short `RLOO` run (leaveâ€‘oneâ€‘out style RL) trains compliance with multiâ€‘step instructions using a verifier reward (Section 6.1).
- RLHF via Rewardâ€‘aware Preference Optimization (`RPO`) improves helpfulness/chat quality on HelpSteer2 prompts using the 70B reward model; two online RPO iterations lift `LNâ€‘Super` Arenaâ€‘Hard from 69.1 to 88.1 (Section 6.2). `LNâ€‘Ultra` uses GRPO for a brief RLHF run (Section 6.2). `LNâ€‘Nano` uses offline RPO for reasoning control then instruction following (Section 6.2).

7) System and memory engineering to make RL feasible (Section 5.2)
- Coâ€‘locate generation and training on the same GPUs, hotâ€‘swapping weights via shared memory; finely tuned parallelism: tensor=8 with sequence parallel, context=2, pipeline=18, data=2; `vLLM` tensor=8, data=72 (Section 5.2.1).
- Memory profiling (GPU/CPU/dev/shm), reshaping heavyweight tensors (one had 13B elements â‰ˆ 26 GB in BF16), and balancing pipeline with identity layers to avoid OOM while keeping >90% utilization (Section 5.2.2).
- FP8 generation path in `vLLM` with perâ€‘token activation scaling and perâ€‘tensor weight scaling; metaâ€‘tensor init avoids materializing BF16 engines; enables cudagraphs and yields ~1.8Ã— generation speedup, peaking at 32 tokens/s/GPU/prompt (Section 5.2.3).

8) Deployment targets (Section 2.1; Figure 4)
- `LNâ€‘Super`: optimized for a single H100 at tensorâ€‘parallel=1 with â‰¥2.17Ã— throughput over Llamaâ€‘3.3â€‘70Bâ€‘Instruct even when the latter uses TP=4; supports ~300k cached tokens at FP8 (Section 2.1).
- `LNâ€‘Ultra`: optimized for a single 8Ã—H100 node; 1.71Ã— latency reduction versus Llamaâ€‘3.1â€‘405Bâ€‘Instruct; up to 3M cached tokens in FP8 and 600k in BF16 on an H100 node (Section 2.1); better accuracyâ€‘throughput tradeâ€‘off than Llamaâ€‘3.1â€‘405B and DeepSeekâ€‘R1 (Figure 4).

## 4. Key Insights and Innovations
- Hardwareâ€‘constrained heterogeneous architecture search that removes attention where safe and compresses FFNs (Section 2; Figure 3)
  - Whatâ€™s new: perâ€‘layer block replacement chosen by a MIP solver under real deployment constraints (latency/throughput/memory), not just uniform pruning/quantization.
  - Why it matters: achieves large throughput gains without large quality loss; enables â€œrightâ€‘sizingâ€ each layer for specific hardware budgets (Section 2.1; Figure 4).

- `FFN Fusion` to reduce sequential depth (Section 2; â€œVertical Compression with FFN Fusionâ€)
  - Whatâ€™s new: exploit attentionâ€‘removed stretches to merge adjacent FFNs into fewer, wider parallel FFNs.
  - Why it matters: reduces interâ€‘layer hops and improves GPU utilizationâ€”key for multiâ€‘GPU latency (Section 2).

- A practical reasoning toggle learned endâ€‘toâ€‘end and enforced by rewards (Sections 3â€“5)
  - Whatâ€™s new: a simple, lightweight system prompt (â€œdetailed thinking on/offâ€) paired with data and formatâ€‘rewards that reliably add or hide chainâ€‘ofâ€‘thought spans (`<think>` tags).
  - Why it matters: users can pick concise answers for routine questions or deep reasoning for hard tasks without switching models (Sections 1, 3, 5.1).

- Curriculumâ€‘driven RL with FP8 generation inside the training loop (Sections 5.1â€“5.2)
  - Whatâ€™s new: combine RL on hardâ€‘filtered questions, a passâ€‘rate curriculum, and an FP8 `vLLM` path that speeds decoding 1.8Ã— and reâ€‘enables cudagraphs (Section 5.2.3).
  - Why it matters: makes very largeâ€‘scale reasoning RL feasible on commodity 8Ã—H100 nodes and enables surpassing the teacher (Figures 5â€“6; Table 5).

- Open, enterpriseâ€‘permissive release with full postâ€‘training dataset and code (Abstract; Section 1)
  - Whatâ€™s new: open weights (`LNâ€‘Nano`, `LNâ€‘Super`, `LNâ€‘Ultra`, plus `LNâ€‘Ultraâ€‘CPT`), the `Llamaâ€‘Nemotronâ€‘Postâ€‘Trainingâ€‘Dataset`, and training codebases (NeMo, NeMoâ€‘Aligner, Megatronâ€‘LM).
  - Why it matters: enables reproducibility and downstream research on reasoning efficiency and RL at scale.

## 5. Experimental Analysis
- Evaluation methodology (Section 7.1)
  - Reasoning: AIME24/25 (competition math), GPQAâ€‘Diamond (graduateâ€‘level science), MATH500 (math proofâ€‘style problems), LiveCodeBench (fresh, contaminationâ€‘controlled coding).
  - Nonâ€‘reasoning: IFEval for instruction following, BFCL V2 Live for tool/function calling, Arenaâ€‘Hard for conversational preference.
  - Settings: 32k evaluation context (even if trained at 16â€“24k, extended context improves long reasoning completion); reasoningâ€‘on uses temperature 0.6, topâ€‘p 0.95; reasoningâ€‘off uses greedy decoding; up to 16 completions prompt, report average pass@1 (Section 7.1). Checkpoints selected on a reasoning subset; small reasoning benchmarks can have high variance (Section 7.1).

- Efficiency vs. accuracy (Figure 4; Section 2.1)
  - Quote:
    > LNâ€‘Ultra â€œconsistently outperforms DeepSeekâ€‘R1 and Llamaâ€‘3.1â€‘405B in both accuracy and efficiencyâ€ on GPQAâ€‘Diamond across two throughput settings; it runs on 8Ã—H100 while R1 requires 8Ã—H200 (Figure 4).

- Quality after CPT (before SFT/RL) (Table 1)
  - Quote:
    > `LNâ€‘Ultraâ€‘CPT` vs. Llama baselines: MMLU 88.1 (vs. 88.6 for 405B), MATH500 80.4 (vs. 69.6 for 405B), HumanEval 88.4 (vs. 86.0 for 405B), RULERâ€‘128K 83.2 (vs. 73.7 for 405B).
  - Interpretation: architectural optimization plus short distillation/CPT can meet or exceed the 405B baseline on several tasks even before reasoning SFT (Section 2.2).

- LNâ€‘Nano results (Table 3)
  - Strong for its size on math and code:
    - GPQAâ€‘Diamond reasoningâ€‘on 54.1% vs. 49.0% for DeepSeekâ€‘R1â€‘Distilledâ€‘Llamaâ€‘8B; MATH500 95.4% vs. 89.1% (Table 3).
    - LiveCodeBench (2408â€“2502) 46.6% pass@1, beating 8B baselines (Table 3).
  - Reasoning toggle control: offâ€‘mode reduces verbosity and sometimes accuracy (expected), but tool calling (BFCL) stays nearly unchanged (63.9/63.6) (Table 3).

- LNâ€‘Super results (Table 4)
  - Reasoningâ€‘on performance competitive with larger/distilled peers:
    - GPQAâ€‘Diamond 66.7% (vs. 65.2% DeepSeekâ€‘R1â€‘Distilledâ€‘Llamaâ€‘70B; 58.8% QwQâ€‘32B) (Table 4).
    - AIME25 60.0% (vs. 55.0% DeepSeekâ€‘R1â€‘Distilledâ€‘70B) (Table 4).
    - MATH500 96.6% (on par with QwQâ€‘32Bâ€™s 96.2) (Table 4).
  - Nonâ€‘reasoning alignment:
    - IFEval up to 89.2 after targeted instructionâ€‘following RL; Arenaâ€‘Hard 88.3 after RLHF (Table 4; Section 6.2).
  - Mixed results:
    - LiveCodeBench lags (45.5%) because SFT used an older dataset version (Section 7.3).

- LNâ€‘Ultra results and surpassing the teacher (Table 5; Figures 5â€“6)
  - Quote:
    > GPQAâ€‘Diamond reasoningâ€‘on 76.0% for `LNâ€‘Ultra`, surpassing DeepSeekâ€‘R1 at 71.5% and also exceeding Llamaâ€‘4 Maverick (69.8) and Llamaâ€‘3.1â€‘405B (43.4) (Table 5).
    > AIME24 80.8% vs. 79.8% R1; AIME25 72.5% vs. 70.0% R1 (Table 5).
    > LiveCodeBench (2410â€“2502) 68.1%, well above Llamaâ€‘4 Behemoth 49.4 and Maverick 43.4 (Table 5).
  - RL effect:
    - `LNâ€‘Ultraâ€‘SFT` approaches R1, but RL is â€œcritical for surpassing DeepSeekâ€‘R1, particularly on GPQAâ€ (Table 5 and Section 7.4). Figure 5 shows steady GPQAâ€‘D improvements during RL; Figure 6 shows the curriculum beats random batching.

- LLMâ€‘asâ€‘aâ€‘Judge generalization (Table 6)
  - Quote:
    > On JudgeBench, `LNâ€‘Ultra` attains 79.14 overall, above DeepSeekâ€‘R1 at 73.14, second only to o3â€‘mini(high) 80.86. `LNâ€‘Super` (69.71) surpasses o1â€‘mini (65.71) (Table 6).

- Do the experiments support the claims?
  - Yes for both axes:
    - Efficiency: `Puzzle` + `FFN Fusion` meet specific deployment targets and yield better accuracyâ€‘throughput than strong baselines (Figure 4; Section 2.1).
    - Quality: Across diverse reasoning and nonâ€‘reasoning benchmarks, the models are competitive or leading; RL lifts `LNâ€‘Ultra` beyond the teacher on GPQA, aligning with the stated motivation for RL (Table 5; Figures 5â€“6).
  - Tradeâ€‘offs are candidly reported: instruction following vs. conversationality (IFEval vs. Arenaâ€‘Hard) and a coding shortfall for `LNâ€‘Super` due to dataset versioning (Section 7.3).

## 6. Limitations and Trade-offs
- Reliance on powerful teacher models and LLMâ€‘based rewards
  - SFT uses DeepSeekâ€‘R1 traces extensively; RLâ€™s accuracy reward depends on a 70B LLM judge (Section 5.1). This can propagate teacher biases and judge errors; there is no humanâ€‘verified gold for every sample.

- RL at scale is resourceâ€‘intensive
  - ~140k H100 hours, 72 nodes, complex parallelism, and custom FP8 decoding (Sections 5.1â€“5.2). This limits accessibility for many labs.

- Instabilities and engineering complexity
  - `LNâ€‘Ultra` SFT required restarts due to gradient explosions (Section 4.2). The RL stack needed weight hotâ€‘swapping, sharedâ€‘memory staging, and careful pipeline balancing to avoid OOM (Section 5.2.2).

- Modeâ€‘control vs. content leakage
  - The toggle and format reward enforce presence/absence of `<think>` tags (Section 5.1), but the paper does not quantify rare failures to comply or cases where implicit reasoning still leaks into the answer in offâ€‘mode.

- Evaluation scope and variance
  - Reasoning benchmarks like AIME have small test sizes and high variance (Section 7.1). Although decontamination is applied in data pipelines (Sections 3.1.1â€“3.1.3), residual contamination risk always exists in largeâ€‘scale webâ€‘sourced corpora.

- Coverage gaps
  - RL for reasoning is applied only to `LNâ€‘Ultra`; smaller models may benefit less from RL per preliminary observations (Section 5). Some capabilities (e.g., longâ€‘context beyond 32k) are not evaluated despite 128k context support (Section 1; Section 7.1).

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that hardwareâ€‘aware heterogeneous architectures plus targeted distillation can yield large, fast reasoning models without sacrificing quality, and that RL can push them past their teachers at acceptable cost on 8Ã—H100 hardware (Figure 4; Tables 1 and 5).
  - Establishes a practical, userâ€‘visible reasoning toggle that lets one model serve both concise assistant use and deep stepâ€‘byâ€‘step problem solving (Sections 1, 3, 5).

- Enabled research directions
  - Open dataset and code allow:
    - Replicating largeâ€‘scale reasoning RL with FP8 decoding.
    - Studying perâ€‘layer NAS choices and FFN Fusion effects on different hardware (Section 2; code releases).
    - Investigating curriculum schedules based on difficulty estimates beyond pass rates (Section 5.1; Figure 6).
  - Robustness work: auditing LLMâ€‘judge reward accuracy, calibrating the toggleâ€™s reliability, and measuring leakage of reasoning in offâ€‘mode.

- Practical applications
  - Highâ€‘throughput assistants that switch to deep reasoning only when needed (customer support, tutoring).
  - Scientific and engineering copilots where GPQAâ€‘Diamondâ€‘level reasoning matters; `LNâ€‘Ultra` leads open models there (Table 5).
  - Coding agents leveraging strong LiveCodeBench performance with controllable verbosity.
  - Enterprise deployment on common H100 nodes, aided by throughput and memory improvements (Section 2.1; Figure 4).

- Next steps suggested by the paperâ€™s findings
  - Extend RL to smaller models if computeâ€‘efficient curricula/rewards can close the gap (Section 5).
  - Refresh SFT coding data for `LNâ€‘Super` to match `LNâ€‘Ultra`â€™s coding gains (Section 7.3).
  - Explore broader multiâ€‘objective alignment to better balance IFEval compliance and conversational preference (Section 7.3; Section 6.2).
  - Generalize FFN Fusion and NAS policies to longer contexts and different transformer variants (e.g., hybrid architectures; Section 2).

Overall, Llamaâ€‘Nemotron provides a full, open blueprintâ€”from hardwareâ€‘aware architecture search through reasoning RLâ€”for building fast, controllable reasoning LLMs. The empirical results (Figures 2, 4â€“6; Tables 1, 3â€“6) substantiate both the efficiency gains and the reasoning quality, with `LNâ€‘Ultra` setting a new open baseline on difficult scientific reasoning while remaining deployable on standard H100 nodes.

# Training Language Models to Self-Correct via Reinforcement Learning

**ArXiv:** [2409.12917](https://arxiv.org/abs/2409.12917)

## ğŸ¯ Pitch

This paper introduces SCoRe, a novel two-stage multi-turn reinforcement learning approach that enables a single large language model to intrinsically self-correctâ€”detect and fix its own reasoning or coding mistakesâ€”using only self-generated data, without external feedback or auxiliary models. By tackling the key obstacles of distribution shift and behavior collapse that hampered previous methods, SCoRe achieves the first robust, significant improvements in self-correction on challenging benchmarks like MATH and HumanEval. This unlocks practical gains in reliability and autonomy for language models, marking a vital step towards models that can audit and refine their own outputsâ€”an essential meta-reasoning ability for advanced AI.

---

## 1. Executive Summary
This paper introduces SCoRe, a twoâ€‘stage, multiâ€‘turn reinforcement learning (RL) method that trains a single language model to detect and fix its own mistakes (â€œintrinsic selfâ€‘correctionâ€) using only selfâ€‘generated data. On MATH and HumanEval, SCoRe delivers the first clearly positive selfâ€‘correction gains without external feedback or auxiliary modelsâ€”e.g., on MATH it raises finalâ€‘turn accuracy to 64.4% with a +4.4% improvement from turn 1 to turn 2 (Table 2), and on HumanEval it achieves a +12.2% improvement (Table 3).

## 2. Context and Motivation
- Problem addressed
  - Intrinsic selfâ€‘correction: improving an answer in a second pass without any external feedback (no labels, tools, or teacher model at test time). Section 1 and Figure 2 show that contemporary LLMs rarely improve their own answers; they often either make superficial changes or turn correct answers into incorrect ones.

- Why it matters
  - Practical: Many math and coding tasks require catching an earlier mistake and revising reasoning or code. Enabling a single model to reliably improve its own output can reduce the need for multiple model calls, human feedback, or external verifiers at inference time.
  - Scientific: Selfâ€‘correction is a concrete instance of metaâ€‘reasoningâ€”using testâ€‘time computation to implement strategies like â€œaudit and revise.â€ Section 1 argues current models often contain the needed knowledge but fail to elicit it during revision.

- Prior approaches and their limits (Sections 2 and 4)
  - Prompting for selfâ€‘correction often fails in the intrinsic setting (no oracle signals), and can even hurt performance.
  - Supervised fineâ€‘tuning (SFT) on revision tracesâ€”e.g., STaR or pairing incorrect with correct answersâ€”improves firstâ€‘turn accuracy but yields little or negative improvement from turn 1 to turn 2. Table 1 shows small or negative deltas `Î”(t1, t2)` on MATH even after fineâ€‘tuning.
  - Multiâ€‘model systems (separate â€œrefinerâ€ or â€œteacherâ€ models) work but complicate deployment. This paper aims for a singleâ€‘model solution trained only on its own rollouts.

- The core gaps identified (Sections 4 and 5)
  - Distribution shift: Offline SFT learns to fix baseâ€‘model mistakes but fails when faced with its own different mistakes at test time (Figure 5).
  - Behavior collapse: The learner converges to â€œgive best first answer and donâ€™t change it,â€ producing little or no improvement on the second attempt (Figures 4 and 6).

## 3. Technical Approach
SCoRe (Selfâ€‘Correction via Reinforcement Learning) trains a single policy over two sequential attempts per problem. Notation:
- `Accuracy@t1` and `Accuracy@t2`: correctness at the first and second attempt, respectively (Section 3).
- `Î”(t1, t2)`: improvement from attempt 1 to attempt 2.
- `Î”_iâ†’c`: fraction of problems that go from incorrect to correct across attempts; `Î”_câ†’i`: fraction that flip from correct to incorrect (Section 3).

Highâ€‘level objective (Eq. 1)
- The model produces two responses per input. The training goal is to maximize the sum of rewards across both attempts, where reward is given only by an â€œoracle checkerâ€ during training (e.g., exact answer on MATH or pass/fail test cases for coding). No checker is used at inference.

Base RL machinery (Eq. 2)
- Policy gradient (REINFORCE) with a KL penalty to a fixed reference model (`Ï€_ref`) to prevent the policy from drifting too far from a good prior. â€œOnâ€‘policyâ€ means the model is trained on rollouts generated by the current policy, avoiding the offline distribution shift seen in SFT.

Twoâ€‘stage training to avoid collapse (Figure 7)
1) Stage I: Decouple the two attempts
   - Train only the second attempt to maximize reward while constraining the first attempt to remain close to the base model via a strong KL penalty on turn 1 (Eq. 3, `Î²2` term).
   - Intuition: If the model is free to make its first attempt as good as possible, it will learn to â€œsolve in one shot,â€ leaving nothing to correct. By â€œfreezingâ€ firstâ€‘turn behavior near the base model, Stage I forces learning how to improve given imperfect first answers. Figure 6b shows this increases the frequency of proposing a different second answer (i.e., prevents â€œdonâ€™t changeâ€ collapse).

2) Stage II: Joint optimization with progressâ€‘based reward shaping
   - Now train both attempts with standard RL (Eq. 4) plus a â€œprogress bonusâ€ on turn 2: `bÌ‚(y2|y1, y*) = Î± Â· (rÌ‚(y2) âˆ’ rÌ‚(y1))`.
   - Intuition: Reward shaping explicitly favors transitions that fix mistakes and heavily penalizes turning a correct answer into an incorrect one. This biases learning towards the selfâ€‘correction strategy rather than â€œsolve once and copy.â€

Additional implementation details (Section 5.3; Appendix B)
- Onâ€‘policy sampling is primary, with optional inclusion of baseâ€‘model first attempts to broaden state coverage.
- Hyperparameters include KL weights (`Î²1`, `Î²2`) and the progress multiplier `Î±` (Table 5).
- REINFORCE with KL to a reference model; greedy decoding for evaluation except in inferenceâ€‘compute scaling (Section 6.2).

Why this design over alternatives (Sections 4â€“5)
- Offline SFT fails due to distribution shift and collapse (Table 1, Figures 4â€“5). Onâ€‘policy RL fixes the shift but still collapses (Figure 6) unless the two stages are used.
- Reward discounting alone (`Î³ > 0`) does not prevent collapse (Appendix A.2, Figure 9). The explicit progress bonus is needed.

A simple example to build intuition
- Suppose turn 1 answers â€œ21â€ to a math question whose true answer is â€œ24.â€ Stage I teaches the model to notice and fix such near misses while keeping turnâ€‘1 behavior close to the base model. Stage II then reinforces the specific transition â€œ21 â†’ 24,â€ granting extra reward for correcting and penalizing â€œ24 â†’ 21.â€

## 4. Key Insights and Innovations
- Twoâ€‘stage RL to learn the selfâ€‘correction â€œmetaâ€‘strategyâ€ (Sections 5.1â€“5.2; Figure 7)
  - Novelty: Trains the model to improve over its own prior attempt by explicitly decoupling attempts (Stage I) and then jointly optimizing with a progressâ€‘based bonus (Stage II).
  - Significance: Avoids the twin pitfalls identified in Section 4â€”distribution shift and behavior collapseâ€”where both SFT and naÃ¯ve multiâ€‘turn RL fail (Figures 5â€“6; Table 1).

- Progressâ€‘based reward shaping that directly encodes â€œfix mistakesâ€ (Section 5.2)
  - Different from prior RLHF style objectives that only reward final correctness. The bonus `Î±Â·(r2 âˆ’ r1)` encourages â€œincorrect â†’ correctâ€ flips and discourages â€œcorrect â†’ incorrect.â€
  - Impact: Improves `Î”(t1, t2)` substantially, as seen in ablations (Table 4, â€œw/o reward shapingâ€).

- Evidenceâ€‘backed diagnosis of SFT failure modes for selfâ€‘correction (Section 4)
  - New analyses show SFT models learn to make minimal edits (Figure 4a) and generalize poorly from fixed offline traces to selfâ€‘generated first attempts (Figure 5).

- Computeâ€‘efficient inference strategy: mix parallel sampling with sequential selfâ€‘correction (Section 6.2; Figure 1 right)
  - Observation: For a fixed sampling budget, doing fewer parallel samples and then selfâ€‘correcting each yields better final consistency than only parallel majority voting.

These are more than incremental tweaks: Stage I + progress shaping change what is being learned (a metaâ€‘strategy to improve) rather than only making first attempts better.

## 5. Experimental Analysis
- Evaluation setup (Section 6)
  - Tasks and data:
    - MATH (Hendrycks et al., 2021): reasoning with a verifier; train/test split follows Lightman et al. (2023). Main reporting on a 500â€‘problem test subset (â€œMATH500â€).
    - Coding: MBPP for training and HumanEval for evaluation; MBPPâ€‘R (repair) for offline codeâ€‘fix tests (Table 3).
  - Models: Gemini 1.5 Flash (math) and Gemini 1.0 Pro (code). Greedy decoding at eval except Section 6.2.
  - Metrics (Section 3): `Accuracy@t1`, `Accuracy@t2`, `Î”(t1, t2)`, `Î”_iâ†’c`, `Î”_câ†’i`.
  - Baselines: Promptingâ€‘based Selfâ€‘Refine; SFT variants: STaR and Pairâ€‘SFT (Section 6).

- Main quantitative results
  - MATH (Table 2; Figure 1 left):
    > Base: `Accuracy@t1 52.6%`, `Accuracy@t2 41.4%`, `Î” = âˆ’11.2%` (selfâ€‘correction hurts).  
    > SCoRe: `Accuracy@t1 60.0%`, `Accuracy@t2 64.4%`, `Î” = +4.4%`, `Î”_iâ†’c = 5.8%`, `Î”_câ†’i = 1.4%`.
    - Compared to Pairâ€‘SFT (`Î” = +1.8%`) and STaR+ (`Î” = +0.4%`), SCoRe shows a clearly positive and larger selfâ€‘correction gain while also raising both turn accuracies.
  
  - Coding (Table 3):
    > MBPPâ€‘R (repair): Base `47.3%` â†’ SCoRe `60.6%`.  
    > HumanEval: Base `Accuracy@t2 56.7%`, `Î” = +3.0%`; SCoRe `Accuracy@t2 64.6%`, `Î” = +12.2%`.  
    > Pairâ€‘SFT degrades selfâ€‘correction on HumanEval (`Î” = âˆ’1.8%`).
    - This shows that training only on MBPP generalizes to HumanEval and that onâ€‘policy training is key for live selfâ€‘correction (Pairâ€‘SFT excels at static repair but not at selfâ€‘correction).

- Do the experiments support the claims?
  - Yes, through targeted diagnostics and ablations:
    - SFT failure modes: Table 1 (small/negative `Î”`), Figures 4â€“5 (minimal edits, distribution shift).
    - NaÃ¯ve multiâ€‘turn RL collapses: Figure 6 (low frequency of changing the answer without Stage I).
    - Necessity of each component: Table 4 shows that removing multiâ€‘turn training, Stage I, or reward shaping reduces or reverses the selfâ€‘correction gain. Replacing Stage II RL with STaR also underperforms.
    - Inference compute scaling: Figure 1 (right) shows the sequential selfâ€‘correction budget is more effective than purely parallel sampling at the same total sample count.
    - Additional robustness: Appendix A.1 shows SCoRe maintains mild gains beyond two attempts, while baselines do not improve after turn 2.

- Qualitative behaviors (Figure 2; Appendix E)
  - SCoRe fixes arithmetic mistakes and reasoning errors, sometimes rewriting entire solutions or selectively repairing flawed steps, which aligns with the intended â€œaudit and reviseâ€ behavior.

## 6. Limitations and Trade-offs
- Assumptions and scope (Sections 3 and 7)
  - Requires a trainingâ€‘time â€œoracle rewardâ€ (`rÌ‚`) to check final correctness (e.g., exact answers or unit tests). No such oracle is used at inference, but training availability is assumed.
  - Trained and evaluated primarily for two attempts. Appendix A.1 shows slight gains beyond two, but robust multiâ€‘turn scaling is untested.
  - Reward is mostly binary (correct/incorrect). Complex tasks where partial progress should be graded may need more nuanced reward design.

- Computational and data considerations
  - RL fineâ€‘tuning with onâ€‘policy sampling is more computeâ€‘intensive than SFT. However, the paper keeps comparable training budgets across methods (Section 6, â€œModels/Evaluation protocolâ€) and reports that RL achieves better selfâ€‘correction.
  - Hyperparameter sensitivity: SCoRe relies on several regularization weights (`Î²1`, `Î²2`) and the progress multiplier (`Î±`). The paper provides working values (Table 5) but not an extensive sensitivity analysis.

- Generality and external validity
  - Results are on math and code tasks with reliable verifiers. Applicability to openâ€‘ended tasks (e.g., longâ€‘form writing) is unclear without designing suitable training rewards.
  - Models are Gemini variants; transfer to other model families or openâ€‘weights models is not tested here.

- Failure modes and edge cases
  - If turnâ€‘1 outputs drift too far, Stage II may still risk collapse without strong enough decoupling or shaping. The method depends on balancing exploration (change answers) and stability (avoid `câ†’i` flips).

## 7. Implications and Future Directions
- Field impact
  - Establishes that intrinsic selfâ€‘correction can be trained effectively with a single model and selfâ€‘generated data, provided training is onâ€‘policy and explicitly rewards â€œmaking progressâ€ across attempts. This reframes selfâ€‘correction as a multiâ€‘turn RL problem with a metaâ€‘strategy objective rather than a pure SFT problem.

- Practical applications
  - Math solvers and code assistants that revise their own reasoning/code without external tools.  
  - Computeâ€‘efficient inference: combine a few diverse samples with sequential selfâ€‘correction for better accuracy at fixed budget (Figure 1 right).

- Followâ€‘up research
  - Multiâ€‘turn scaling: extend beyond two attempts with RL (Section 7 notes this as a limitation), potentially learning longer selfâ€‘improvement curricula.
  - Richer progress signals: stepâ€‘level or processâ€‘based rewards (e.g., verifier on intermediate reasoning) rather than only final correctness.
  - Broader domains: apply SCoRe to tool use, planning, or longâ€‘form generation by crafting domainâ€‘appropriate training rewards.
  - Unification of stages: investigate singleâ€‘stage objectives that implicitly enforce decoupling and progress, reducing complexity (Section 7).
  - Robustness and safety: study when selfâ€‘correction wrongly overwrites correct answers and how to calibrate the tradeâ€‘off between `Î”_iâ†’c` and `Î”_câ†’i`.

In short, SCoRe shows that teaching models â€œhow to improve their own answersâ€ benefits from RL on selfâ€‘generated traces plus carefully designed regularization that makes learning the metaâ€‘strategy strictly preferable to the degenerate â€œdonâ€™t changeâ€ solution. The empirical evidence across math and coding (Tables 2â€“3, Figure 1) and the ablations (Table 4) make a strong case for this paradigm.

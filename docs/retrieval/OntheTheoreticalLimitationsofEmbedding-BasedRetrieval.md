# On the Theoretical Limitations of Embedding-Based Retrieval

**ArXiv:** [2508.21038](https://arxiv.org/abs/2508.21038)

## ğŸ¯ Pitch

This paper rigorously proves and empirically verifies that single-vector embedding retrieval models have a fundamental limitation: no matter how they are trained, for any fixed embedding dimension, there exist realistic queryâ€“topâ€‘k relevance patterns they simply cannot represent or retrieve exactly. By connecting this limit to the sign-rank of the relevance matrix and introducing the LIMIT datasetâ€”a simple, natural-language benchmark on which state-of-the-art dense retrievers fail sharplyâ€”the authors provide a decisive bridge between learning theory and practical IR, highlighting the urgent need for new architectures as retrieval tasks diversify beyond what conventional embeddings can capture. This work matters because it exposes irreducible recall failures that will persist even as models get larger, guiding both researchers and practitioners in designing retrieval systems robust to the increasingly complex demands of modern search and AI agents.

---

## 1. Executive Summary
This paper proves and empirically demonstrates a fundamental capacity limit of singleâ€‘vector embedding retrieval: for any fixed embedding dimension `d`, there exist realistic topâ€‘k retrieval targets that no scoring by dotâ€‘product in `R^d` can realize exactly. It connects this limit to the signâ€‘rank of the relevance matrix, verifies it with â€œbestâ€‘caseâ€ optimization where embeddings are directly learned on the test labels, and releases LIMIT, a simple naturalâ€‘language dataset that stresses these limits and causes sharp failures in stateâ€‘ofâ€‘theâ€‘art (SoTA) dense retrievers.

## 2. Context and Motivation
- Problem addressed
  - Modern retrieval increasingly asks dense embedding models to handle arbitrary notions of relevance, including instructionâ€‘following, reasoning, and logical combinations (â€œX or Yâ€) of previously unrelated documents (Introduction; Â§1). The paper asks: is a single fixedâ€‘dimensional vector per query/document expressive enough to represent all such topâ€‘k combinations?

- Why this matters
  - Practical: As systems broaden to openâ€‘ended queries and instructions, the space of possible topâ€‘k sets explodes (e.g., QUEST has 325k docs and k=20; the number of possible topâ€‘20 sets is ~7.1e+91, Â§5.1). If embeddings cannot represent many such combinations, user queries will systematically fail even with large models.
  - Theoretical: While earlier works note geometric limits (e.g., Voronoi orderâ€‘k regions), tight, actionable bounds for IR are elusive (Â§2.3, Â§8). This paper supplies a concrete, learnabilityâ€‘style bound using signâ€‘rank from communication complexity.

- Prior approaches and gaps
  - Dense singleâ€‘vector retrieval dominates practice but is known to struggle in highâ€‘cardinality settings (e.g., false positives increase with larger corpora; Reimers & Gurevych 2020, Â§2.2).
  - Theoretical tools like orderâ€‘k Voronoi region counts are hard to compute and not practical for IR (Â§2.3, Â§8).
  - There was no clear bridge from theory to an empirical stress test that is simple yet not solvable by stronger training or bigger models.

- Positioning
  - The paper provides: (i) a formal lower bound linking the representability of a retrieval target to the signâ€‘rank of the binary relevance matrix (Â§3); (ii) â€œfree embeddingâ€ experiments that optimize embeddings directly on the test labels (no language modeling constraints) to establish bestâ€‘case feasibility (Â§4); and (iii) LIMIT, a naturalâ€‘language dataset engineered to maximize the number of required topâ€‘k combinations while keeping queries and docs trivial (Â§5.2).

## 3. Technical Approach
Step 1 â€” Formalization of retrieval as a matrix ordering problem (Â§3.1)
- Setup
  - Let there be `m` queries and `n` documents, and a binary relevance matrix `A âˆˆ {0,1}^{mÃ—n}` where `A[i,j]=1` iff document `j` is relevant to query `i`.
  - A singleâ€‘vector embedding model maps each query `i` to `u_i âˆˆ R^d` and each document `j` to `v_j âˆˆ R^d`. Scores are dot products; stacking gives `B = U^T V` where `B[i,j] = u_i Â· v_j`.
- Retrieval goal as rowâ€‘wise ordering/thresholding
  - The model should score all relevant docs ahead of irrelevant ones for each query.
  - Two equivalent formulations are introduced (Proposition 1, Â§3.2):
    - Rowâ€‘wise orderâ€‘preserving rank `rank_rop(A)`: minimal rank of a real matrix `B` that preserves the perâ€‘row ordering in `A`.
    - Rowâ€‘wise thresholdable rank `rank_rt(A)`: minimal rank of `B` for which a perâ€‘row threshold separates relevant from irrelevant entries.
  - A stricter variant uses one global threshold `Ï„` for all rows (global thresholdable rank `rank_gt(A)`).

Step 2 â€” Connecting to signâ€‘rank (Â§3.2â€“Â§3.3)
- Key definition (Definition 3): For a sign matrix `M âˆˆ {âˆ’1,1}^{mÃ—n}`, the signâ€‘rank `rank_Â±(M)` is the minimum rank of a real matrix whose entry signs match `M`.
- Central inequality chain (Proposition 2):
  - Let `J` be an allâ€‘ones matrix of shape `mÃ—n`. Then:
    > rank_Â±(2A âˆ’ J) âˆ’ 1 â‰¤ rank_rop(A) = rank_rt(A) â‰¤ rank_gt(A) â‰¤ rank_Â±(2A âˆ’ J)
  - Intuition:
    - Convert binary `A` to a sign matrix `2A âˆ’ J` (relevant becomes +1, irrelevant âˆ’1).
    - If a `B` can separate relevant/irrelevant per row (rowâ€‘wise thresholding), then after shifting by the row thresholds (i.e., `B âˆ’ Ï„ 1^T`), the sign pattern matches `2A âˆ’ J`, so the needed rank is at least signâ€‘rank minus one (Â§3.2, proof step 3).
- Consequences (Â§3.3)
  - Lower bound: any embedding in `R^d` must have `d â‰¥ rank_Â±(2A âˆ’ J) âˆ’ 1` to exactly realize `A`â€™s perâ€‘row separation.
  - Existence of hard tasks: There are `A` with arbitrarily large signâ€‘rank, so for any fixed `d`, some retrieval targets cannot be represented exactly by singleâ€‘vector embeddings.
  - Practical diagnostic: If one can find embeddings that realize the ordering in `d` dimensions (by optimization), that also upperâ€‘bounds the signâ€‘rank.

Step 3 â€” â€œFree embeddingsâ€ as bestâ€‘case feasibility test (Â§4)
- Idea
  - Remove language modeling constraints and directly optimize a learned vector per query and per document to fit the desired relevance (`A`) using contrastive loss (InfoNCE).
  - If even this unconstrained optimization fails for a given `(n, k, d)`, real embedding models will be at least as limited.
- Experimental setup (all in Â§4)
  - Generate unitâ€‘norm document vectors for `n` docs; create all topâ€‘`k` combinations as queries (`m = binom(n, k)`), with `k=2`.
  - Optimize query and document vectors jointly with Adam on the full set of positives per query and all other docs as negatives (fullâ€‘dataset batch); reâ€‘normalize vectors each step; early stop after no loss improvement for 1000 iterations.
  - Increase `n` for a fixed `d` until 100% training accuracy becomes impossible; the last solvable `n` is the â€œcriticalâ€‘nâ€ for that `d`.
- Results summary
  - The measured criticalâ€‘n grows as a cubic polynomial in `d`: Figure 2 fits `y = âˆ’10.5322 + 4.0309 d + 0.0520 d^2 + 0.0037 d^3` with `r^2=0.999`.
  - Extrapolated â€œbestâ€‘caseâ€ upper limits for k=2 combos: critical `n â‰ˆ 500k (d=512), 1.7M (768), 4M (1024), 107M (3072), 250M (4096)` (Figure 2, Â§4).

Step 4 â€” LIMIT: a simple naturalâ€‘language instantiation (Â§5.2)
- Goal
  - Create a dataset where queries are trivial (â€œWho likes X?â€) but the induced qrels require many distinct topâ€‘k combinations, stressing the signâ€‘rank bound in a realistic form.
- Construction
  - Attributes: collect ~1850 â€œthings a person could likeâ€ (cleaned list; Â§5.2).
  - Choose `k=2` relevant documents per query and select a document set that maximizes the number of distinct 2â€‘way combinations just over 1000:
    > use `n=46` documents since `binom(46,2)=1035` (Figure 1; Â§5.2).
  - Full vs small versions:
    - LIMIT full: 50k documents, 1000 queries. Only 46 docs are ever relevant; the rest are distractors (nonâ€‘relevant to any query).
    - LIMIT small: only those 46 documents (easier to evaluate reâ€‘ranking behavior; Figure 4).
  - Naturalization: assign random names to documents and populate each with a fixed number of liked attributes; queries ask â€œWho likes X?â€; relevant docs are those whose attribute list includes X (Figure 1).

## 4. Key Insights and Innovations
- Theoretical lower bound via signâ€‘rank for singleâ€‘vector retrieval (Â§3.2â€“Â§3.3)
  - Novelty: A clean inequality chain ties the minimal embedding dimension needed to exactly realize a binary relevance pattern to the matrixâ€™s signâ€‘rank. This reframes dense retrieval expressivity as a matrix factorization problem with known hardness properties.
  - Significance: It proves the existence of realistic topâ€‘k sets that cannot be represented for any fixed `d`, even in principle, when using dotâ€‘product scoring with one vector per query/document.

- Bestâ€‘case empirical â€œfree embeddingâ€ methodology (Â§4)
  - Novelty: Instead of testing trained language encoders, the work directly optimizes perâ€‘item vectors on the test `A` with fullâ€‘batch InfoNCE. This isolates geometric capacity limits from modeling or training data issues.
  - Significance: The resulting criticalâ€‘n vs. `d` curve (Figure 2; Table 6) sets an empirical bar that real models cannot surpass; any gap between these curves and model performance is due to practical constraints, not capacity in `R^d` itself.

- LIMIT dataset: simple language, maximally demanding qrels (Â§5.2)
  - Novelty: A naturalâ€‘language dataset that is trivial to read and query but deliberately dense in distinct topâ€‘k combinations, achieved by choosing `n` and `k` to maximize `binom(n,k)` within 1000 queries (Figure 1).
  - Significance: Stateâ€‘ofâ€‘theâ€‘art dense embedders fail sharply on LIMIT, showing that the theoretical limit surfaces in realistic settings, not just contrived math.

- Empirical demonstration that â€œcombinational densityâ€ of qrels drives difficulty (Â§5.4)
  - Novelty: An ablation that instantiates four qrel connectivity patternsâ€”random, cycle, disjoint, and denseâ€”and shows the dense pattern (max combinations) is far harder (Figure 6; Table 3).
  - Significance: It isolates the root cause: not language, domain shift, or negatives, but the number of distinct topâ€‘k sets the model must represent.

## 5. Experimental Analysis
- Evaluation protocol
  - Metrics: Recall@2/10/20/100 depending on dataset size (Figures 3â€“4; Tables 4â€“5).
  - Models: Multiple SoTA dense retrievers varying in embedding dimension (1024â€“4096), training style (instructionâ€‘tuned vs. others), and MRL truncation; plus nonâ€‘singleâ€‘vector baselines (BM25; ModernColBERT) (Â§5.2).
  - Ablations:
    - Domain shift: fineâ€‘tune a modern encoder on LIMITâ€‘train vs LIMITâ€‘test (Â§5.3; Figure 5; Table 2).
    - Qrel pattern: random, cycle, disjoint, dense (Â§5.4; Figure 6; Table 3).
    - Crossâ€‘benchmark correlation: LIMIT vs BEIR (Â§5.5; Figure 7; Table 7).

- Main results on LIMIT full (50k docs; Figure 3; Table 5)
  - Dense singleâ€‘vector models perform poorly despite large dimensions:
    - Example Recall@100 (full dimension, unless noted): `E5â€‘Mistralâ€‘7B`: 8.3; `GritLMâ€‘7B`: 12.9; `Promptrieverâ€‘Llama3â€‘8B`: 18.9; `Qwen3â€‘Embed`: 4.8; `Geminiâ€‘Embed`: 10.0 (Table 5).
    - The same models often have Recall@2 below 2â€“3% on LIMIT full (Table 5).
  - Alternatives fare better:
    - BM25 (sparse lexical) reaches 93.6 Recall@100 (Table 5).
    - `GTEâ€‘ModernColBERT` (multiâ€‘vector late interaction) achieves 54.8 Recall@100 (Table 5).
  - Interpretation: As embedding dimension grows, performance increases somewhat (Figure 3), but remains far from satisfactory; sparse and multiâ€‘vector methods exploit higher effective dimensionality or more flexible matching.

- LIMIT small (46 docs; Figure 4; Table 4)
  - Even with only 46 docs, dense retrievers do not solve the task at topâ€‘20:
    - Example Recall@20 at max dim: `E5â€‘Mistralâ€‘7B`: 85.2; `GritLMâ€‘7B`: 90.5; `Promptrieverâ€‘Llama3â€‘8B`: 97.7; `Qwen3â€‘Embed`: 73.8; `Geminiâ€‘Embed`: 87.9 (Table 4).
  - BM25 is near perfect (97.8/100.0/100.0 for R@2/10/20, Table 4). `GTEâ€‘ModernColBERT` is also very strong (R@20=99.1).
  - Takeaway: Singleâ€‘vector dense models still leave substantial errors even in tiny corpora when many distinct topâ€‘k sets must be represented.

- Domain shift ablation (Â§5.3; Figure 5; Table 2)
  - Training on LIMITâ€‘train barely helps:
    > At 1024 dims, training on train reaches only 1.0/2.8/11.2 for R@2/10/100 (Table 2).
  - Training on LIMITâ€‘test (overfitting) nearly solves it across dims:
    > At 32 dims, testâ€‘trained model attains 85.5/98.4/100.0 for R@2/10/100 (Table 2).
  - Conclusion: The difficulty is not domain shift; models can memorize when allowed to overfit specific tokens, echoing the freeâ€‘embedding results that feasibility depends on fitting the exact `A`.

- Qrelâ€‘pattern ablation (Â§5.4; Figure 6; Table 3)
  - Dense pattern (max combinations) is decisively hardest:
    - `GritLMâ€‘7B` Recall@100 drops from 61.8 (random) to 10.4 (dense); `E5â€‘Mistralâ€‘7B` from 40.4 to 4.8; `Promptriever` from 62.0 to 19.4 (Table 3).
  - Random/cycle/disjoint are all noticeably easier and fairly similar to each other.

- Crossâ€‘benchmark comparison (Â§5.5; Figure 7; Table 7)
  - BEIR performance does not predict LIMIT performance:
    > e.g., `Qwen3â€‘Embed` scores 62.76 on BEIR but 4.8 Recall@100 on LIMIT (Table 7).
  - Implication: LIMIT reveals a distinct capability not measured by standard retrieval benchmarks.

- Reranking sanity check (Â§5.6)
  - A longâ€‘context crossâ€‘encoder reranker (`Geminiâ€‘2.5â€‘Pro`) can solve LIMITâ€‘small perfectly by scoring all 46 docs at once (Â§5.6).
  - This validates that the task itself is simpleâ€”the error source is the singleâ€‘vector constraint, not semantic understanding.

- Support for claims
  - The empirical findings align with the theory: performance collapses as the dataset requires many distinct topâ€‘k combinations, and increases with effective dimensionality (BM25, multiâ€‘vector; Figures 3â€“4, 6).
  - The freeâ€‘embedding upper bounds confirm that even perfect optimization in `R^d` hits limits at finite `n` (Figure 2; Table 6).

## 6. Limitations and Trade-offs
- Scope: Singleâ€‘vector, dotâ€‘product retrieval
  - The formal bounds and most experiments target oneâ€‘vectorâ€‘perâ€‘sequence with dotâ€‘product scoring. Multiâ€‘vector methods and crossâ€‘encoders are not covered by the theory, though they are explored empirically (Â§5.6).
- Exact realization vs. approximate performance
  - The signâ€‘rank connection concerns exact separation (rowâ€‘wise thresholding/order). The paper does not provide formal bounds for approximate retrieval (e.g., â€œget most but not all combinationsâ€), though it cites learningâ€‘theory directions for future work (Limitations section; referencing Benâ€‘David et al. 2002).
- Computing signâ€‘rank
  - Signâ€‘rank is hard to compute in practice. The paper relies on the inequality and the freeâ€‘embedding upperâ€‘bound heuristic rather than computing signâ€‘rank of specific datasets (Â§2.3, Â§3.3).
- Dataset design choices
  - LIMIT is engineered to create many distinct topâ€‘k combinations using simple content. While this isolates the capacity issue, it is not a comprehensive benchmark for all kinds of instructionâ€‘following retrieval.
- Scalability of alternatives
  - Crossâ€‘encoders solve the small version but are computationally expensive for firstâ€‘stage retrieval at web scale (Â§5.6).
  - Multiâ€‘vector models improve results but have their own tradeâ€‘offs (index size, latency) and are less explored for instructionâ€‘following (Â§5.6).

## 7. Implications and Future Directions
- Field impact
  - Recalibrates expectations for dense singleâ€‘vector retrieval: it cannot realize all topâ€‘k combinations for arbitrary instructions at fixed `d`. As instructionâ€‘following and reasoningâ€‘based retrieval expand, systems must plan around this ceiling (Â§1, Â§6).
  - Evaluation design: Benchmarks with few queries miss these limits; LIMIT shows how small, highâ€‘combination datasets can stress real capacity (Â§5.1â€“Â§5.2, Figure 1).

- Research directions
  - Theory extensions:
    - Approximate representation: formalize bounds when small fractions of combinations can be wrong (Limitations section).
    - Multiâ€‘vector theory: extend signâ€‘rankâ€‘style arguments to MaxSim lateâ€‘interaction models (Â§5.6).
    - Links to geometric constructs beyond Voronoi that are tractable for `dâ‰«3` (Â§8).
  - Modeling advances:
    - Hybrids: use sparse or multiâ€‘vector firstâ€‘stage retrieval to increase effective dimensionality, then reâ€‘rank with crossâ€‘encoders (Â§5.6).
    - Adaptive dimensionality: dynamically allocate more vectors or dimensions for queries likely to induce many combinations.
    - Instructionâ€‘aware indexing: store multiple subâ€‘embeddings per document keyed to attribute/operator types to hedge against combination blowâ€‘ups.
  - Benchmarking:
    - Systematic control of qrel â€œgraph densityâ€ (Appendix Â§10) to map performance vs. combination complexity.
    - Broader instructionâ€‘following datasets that vary `binom(n,k)` regimes and include range/logic operators (connections to BrowseComp and QUEST; Â§5.1).

- Practical applications
  - Production search/QA systems should incorporate:
    - Multiâ€‘stage retrieval (e.g., BM25/ColBERT first stage, crossâ€‘encoder rerank).
    - Query analysis to detect highâ€‘combination cases and switch to higherâ€‘capacity routes.
    - Caching or precomputation for frequent combination patterns.
  - Risk management: Recognize that adding arbitrary logical operators (â€œX and Y or Zâ€) can silently exceed singleâ€‘vector capacity even when language appears simple.

> Core takeaway: â€œWe need at least rank_Â±(2A âˆ’ 1) âˆ’ 1 dimensions to capture the relationships in A exactlyâ€ (Â§3.3). LIMIT shows that this lower bound is not merely theoreticalâ€”simple, realistic queries can push singleâ€‘vector dense retrievers past their representational limits (Figures 3â€“4, 6), while even bestâ€‘case free embeddings hit hard thresholds that grow only polynomially with `d` (Figure 2; Table 6).

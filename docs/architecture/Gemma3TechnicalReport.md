# Gemma 3 Technical Report

**ArXiv:** [2503.19786](https://arxiv.org/abs/2503.19786)

## ğŸ¯ Pitch

Gemma 3 introduces a new generation of lightweight, open-access language models that seamlessly combine text and image understanding, support much longer context (up to 128K tokens), and achieve leading multilingual and reasoning benchmarksâ€”all while remaining efficient enough for consumer hardware. Its technical breakthroughs include a novel memory-saving attention architecture that tames the KV-cache explosion typical of long-context models and an inference-time Pan & Scan technique that preserves image detail without extra compute. This enables powerful, accessible multimodal AI for a broader range of real-world applications and devices, bringing state-of-the-art performance and versatility within reach of users everywhere.

---

## 1. Executive Summary (2-3 sentences)
Gemma 3 is a family of lightweight, open, multimodal language models (1Bâ€“27B parameters) that add image understanding, stronger multilingual coverage, and much longer context handling (up to 128K tokens) while keeping inference practical on consumer hardware. The core technical advance is a memoryâ€‘efficient attention designâ€”interleaving many shortâ€‘range local layers with fewer global layersâ€”that dramatically shrinks KVâ€‘cache growth at long context without hurting quality, plus an inferenceâ€‘time Pan & Scan method that preserves image detail, and a postâ€‘training recipe that substantially boosts math, coding, reasoning, and multilingual performance.

## 2. Context and Motivation
- Problem/Gaps addressed
  - Long context in LLMs is bottlenecked by the memory footprint of the keyâ€“value (KV) cache used during autoregressive inference. Memory â€œexplodesâ€ with longer inputs, limiting use on consumer hardware and edge devices. The report explicitly flags this: 
    > â€œA challenge with long context is the memory explosion of the KV cache during inference.â€ (Introduction)
  - Many open models either lack robust vision abilities or become impractically heavy when they add them. Nonâ€‘square and highâ€‘resolution images especially degrade text legibility and small object visibility when naively resized.
  - Smaller open models often lag on math, coding, and multilingual benchmarks.

- Why this matters
  - Long context (documents, code bases, transcripts) is increasingly central to real applications, but it must be feasible on commodity GPUs, laptops, or phonesâ€”Gemma targets such hardware (Introduction).
  - Reliable multimodal understanding (e.g., reading receipts, forms, charts) is critical for enterprise and consumer workflows; preserving detail without huge compute costs is pivotal for adoption.
  - Strong instruction following and reasoning in small models enables broader onâ€‘device and private deployments.

- Prior approaches and their limits
  - Standard dense transformer stacks let every layer attend globally; KV caches then scale linearly with depth and sequence length, becoming the predominant memory cost at long context.
  - Textâ€‘only models or naÃ¯ve vision frontâ€‘ends struggle with nonâ€‘square images and fine text unless they increase input resolution, which raises compute.
  - Postâ€‘training recipes for small models often trade off breadth (multilingual, safety) vs. depth (math/coding/logic).

- Positioning of this work
  - Gemma 3 focuses on practical longâ€‘context and multimodal support in small, open models. It proposes a specific architecture (many local layers to few global layers) and an inferenceâ€‘time image cropping strategy (Pan & Scan), plus a revamped postâ€‘training pipeline to raise reasoning and multilingual ability. It emphasizes measurable KVâ€‘cache savings, broad benchmarks, and open releases of raw and quantized checkpoints.

## 3. Technical Approach
Stepâ€‘byâ€‘step overview of the system, from architecture through training and postâ€‘training.

- Model family and sizes (Table 1)
  - Models: `1B` (textâ€‘only), `4B`, `12B`, `27B`. Vision encoder (~400M SigLIP variant) is shared and frozen for 4B/12B/27B; 1B does not include a vision encoder.
  - Vocabulary size is 262k (Gemini 2.0 tokenizer).

- Core architecture
  - Base: decoderâ€‘only transformer with Groupedâ€‘Query Attention (`GQA`) and RMSNorm. 
    - `GQA` reduces memory at inference by sharing keys/values across query groups while keeping multiple query heads; itâ€™s a common scaling trick for efficient attention.
  - Replaces Gemma 2â€™s â€œsoftâ€‘cappingâ€ with `QK-norm` (Section 2), a technique that normalizes queries and keys to stabilize attention without clipping.
  - Localâ€“Global attention interleaving (Section 2; Fig. 3â€“6):
    - Pattern: 5 local layers for every 1 global layer (a 5:1 ratio), starting with a local layer.
    - Local attention is a slidingâ€‘window mechanism that only looks at a fixed span of recent tokens; Gemma 3 uses a short span of 1024 tokens for local layers.
    - Global layers attend across the full sequence and are the only layers that â€œseeâ€ the entire context.
    - Why this design: KV cache size grows with the number of layers that attend to long context; by making most layers local and keeping their span short, KV cache growth is dramatically reduced while preserving the ability to integrate longâ€‘range info via periodic global layers.
  - Long context handling (Section 2; 5.3):
    - Supports up to 128K tokens (1B model supports 32K).
    - Uses RoPE (rotary positional embeddings) with different base frequencies: â€œglobalâ€ layers use a higher base (1M) to generalize to long ranges; local layers keep a standard base (10k) suited for short spans.
    - Extends from 32K to 128K late in pretraining via RoPE rescaling (positional interpolation) rather than training from scratch at 128K (Fig. 7 shows generalization up to 128K, with degradation beyond that when scaled further).

- Vision modality (Section 2.1)
  - Vision encoder: a SigLIP ViT (~400M params) operating on 896Ã—896 square images; it is frozen and outputs a sequence of image tokens that are â€œsoft tokensâ€ fed to the language model.
  - Embedding condensation: the encoderâ€™s output is pooled/condensed to a fixed 256 visual tokens per image to bound inference cost (consistent across model sizes).
  - Pan & Scan (`P&S`) inferenceâ€‘time windowing:
    - Problem: fixed 896Ã—896 square resizing can destroy legibility or small details for nonâ€‘square or highâ€‘resolution images.
    - Mechanism: adaptively segment an input image into nonâ€‘overlapping crops that collectively cover the original; resize each crop to 896Ã—896 and encode; the model sees multiple crops when needed. This preserves detail with a tunable number of crops; itâ€™s an optional inferenceâ€‘time optimization (Section 2.1, Table 8).

- Preâ€‘training recipe (Section 2.2)
  - Token budgets: 27B uses 14T tokens, 12B uses 12T, 4B uses 4T, 1B uses 2T (to cover multimodal and multilingual additions).
  - Data: mixes text and images; increases multilingual content (both monolingual and parallel), with sampling balancing inspired by UniMax (Chung et al., 2023).
  - Filtering: safety filtering, PII reduction, evaluation decontamination, quality reweighting (Sachdeva et al., 2024).
  - Knowledge distillation:
    - Mechanism: For each target token, sample 256 candidate logits proportional to the teacher distribution; set unsampled logits to zero probability; train the student on the renormalized subset via crossâ€‘entropy (Section 2.2 â€œDistillationâ€). This is a costâ€‘effective way to distill from strong teachers.

- Quantizationâ€‘Aware Training (`QAT`) and memory profile (Section 2.3; Table 3)
  - After training, fineâ€‘tune ~5,000 steps with QAT using the nonâ€‘quantized modelâ€™s probabilities as targets.
  - Weight formats released: perâ€‘channel int4, perâ€‘block int4 (block size 32), and switchedâ€‘fp8 (`SFP8`) to match popular inference engines (e.g., llama.cpp).
  - Memory impact at 32K context (27B example, Table 3):
    > Weights: bf16 54.0 GB â†’ int4 14.1 GB (â‰ˆ74% reduction).  
    > Weights + KV cache: bf16 72.7 GB â†’ int4 32.8 GB (â‰ˆ55% reduction).  
    - Similar trends for 4B/12B models; KV caches are quantized to 8 bits in these figures.

- Compute and systems (Section 2.4; Table 2)
  - Training uses TPU v4/v5e/v5p; optimizer state sharded with ZeROâ€‘3; distributed with Pathways + JAX GSPMD + MegaScale XLA. 
  - Vision embeddings are precomputed for efficiency during languageâ€‘model training.

- Instruction tuning (Section 3; Table 4 for formatting)
  - Postâ€‘training uses an improved pipeline:
    - Distillation from a large instructionâ€‘tuned teacher.
    - RL fineâ€‘tuning combining BOND (bestâ€‘ofâ€‘N distillation), WARM (weightâ€‘averaged reward models), and WARP (weightâ€‘averaged rewarded policies).
    - Multiple reward functions target helpfulness, math, coding, reasoning, instructionâ€‘following, multilinguality, and safety; coding gains are reinforced with execution feedback; some math rewards use ground truth solutions.
  - Data is filtered to remove PII, unsafe outputs, incorrect selfâ€‘identification, duplicates, and to add datasets that encourage correct attribution, hedging, and refusals.
  - Chat formatting and control tokens:
    - Text must begin with a `[BOS]` token; IT dialogues use `<start_of_turn>user`, `<start_of_turn>model`, and `<end_of_turn>` tokens (Table 4).
    - PT models end with `<eos>`, IT with `<end_of_turn>`.

## 4. Key Insights and Innovations
- Memoryâ€‘efficient longâ€‘context attention that keeps quality
  - Innovation: 5:1 local:global interleaving with short local windows (1024) ensures only global layers maintain longâ€‘range KV caches.
  - Evidence and significance:
    - Minimal perplexity impact even at 7:1 (Fig. 3) and with much smaller local windows (Fig. 4).
    - KVâ€‘cache memory at 32K is cut from a 60% overhead (â€œglobal onlyâ€) to <15% with 1:3 and 1024 windows (Fig. 5); KV usage grows far more slowly with context (Fig. 6). This is a fundamental architectural improvement for longâ€‘context efficiency.

- Practical, detailâ€‘preserving multimodality via fixedâ€‘token vision + Pan & Scan
  - Innovation: Freeze a compact SigLIP encoder and compress to 256 tokens per image; add an inferenceâ€‘time cropping strategy that triggers only when needed.
  - Evidence:
    - Higher visionâ€‘encoder resolution improves DocVQA/InfoVQA/TextVQA substantially (Table 7). 
    - Pan & Scan yields large gains on imageâ€‘text tasks that require reading text or handling aspect ratios; e.g., `DocVQA` +4.8 to +8.2 points and `InfoVQA` +12.9 to +17.0 (Table 8). This is a practical, systemâ€‘level contribution rather than a purely modelâ€‘size increase.

- Scalable postâ€‘training that lifts math, coding, multilingual, and chat quality
  - Innovation: Combine improved distillation with RL (BOND+WARM+WARP) and targeted reward sources (execution feedback for code, ground truth for math).
  - Evidence:
    - On IT models, `GSM8K` 27B reaches 95.9, `MATH` 89.0, and `HiddenMath` 60.0+ (Table 6 and Table 18), competitive with larger prior open models.
    - Chatbot Arena (human blind SxS) Elo 1338 puts `Gemmaâ€‘3â€‘27Bâ€‘IT` among top models and above many larger open baselines (Table 5). This shows a broad, realâ€‘world impact beyond static test sets.

- Open, quantized releases with measured memory profiles
  - Innovation: QAT for int4/int4â€‘block32/SFP8 releases and explicit reporting of KVâ€‘cache + weight memory across sizes (Table 3).
  - Significance: Enables deployment on constrained hardware at long context windowsâ€”a practical engineering contribution.

## 5. Experimental Analysis
- Evaluation setup, datasets, metrics
  - Benchmarks span general knowledge (MMLUâ€‘Pro, GMMLUâ€‘Lite), coding (LiveCodeBench, HumanEval, MBPP), math (GSM8K, MATH, HiddenMath), SQL (Birdâ€‘SQL), reasoning (GPQA), multimodality (MMMU, DocVQA, InfoVQA, TextVQA), and long context (RULER, MRCR). Tables 6, 15, 16, 17, and Appendix Tables 9â€“14, 18 provide details; Tables 19â€“21 specify metrics, nâ€‘shot settings, and whether Chainâ€‘ofâ€‘Thought is used.
  - Human evaluation: LMSYS Chatbot Arena Elo, blind sideâ€‘byâ€‘side.

- Main quantitative results (highlights with references)
  - Overall improvements vs Gemma 2 (Table 6):
    > MMLUâ€‘Pro (27B IT): 67.5; MATH (27B IT): 89.0; HiddenMath (27B IT): 60.3; MMMU val (27B IT): 64.9; LiveCodeBench (27B IT): 29.7; Birdâ€‘SQL dev (27B IT): 54.4.  
    - The `4B IT` is notably strong for its size (e.g., MATH 83.8), and `1B IT` is competitive on some tasks given its constraints.
  - Chatbot Arena (Table 5):
    > Gemmaâ€‘3â€‘27Bâ€‘IT Elo 1338, ranked in the top 10 and above large open models like DeepSeekâ€‘V3 and LLaMAâ€‘3.1â€‘405B in preliminary results (Mar 8, 2025).  
    - Note: Elo does not evaluate vision ability; this is purely text chat.
  - Long context (Table 15):
    > RULER 128K (27B): PT 72.9 vs IT 66.0; MRCR 128K (27B): PT 60.0 vs IT 59.3.  
    - Models sustain nonâ€‘trivial ability at 128K, though PT > IT on some longâ€‘context probes (see Limitations).
  - Multimodal IT (Table 16; with P&S unless noted):
    > DocVQA: 27B IT = 86.6; InfoVQA: 70.6; TextVQA: 65.1; MathVista testâ€‘mini: 67.6.  
    - Gains with P&S on tasks involving text reading/aspect ratios are further supported by Table 8.
  - Additional IT results (Table 18):
    > HumanEval pass@1: 27B IT = 87.8; MBPP pass@1: 74.4; GSM8K: 95.9; MMLU: 76.9; BIGâ€‘Bench Extra Hard: 19.3; GMMLUâ€‘Lite: 75.1; WMT24++: 53.4.

- Ablations and diagnostics
  - Local:Global ratio and window size:
    > Minimal perplexity changes from 1:1 up to 7:1 (Fig. 3) and from 4096 down to 1024 window (Fig. 4).  
    - This supports the design choice of aggressive local layers without quality loss.
  - KVâ€‘cache memory:
    > â€œGlobal onlyâ€ yields ~60% KV overhead at 32K, while 1:3 with 1024 reduces to <15% (Fig. 5). KV growth vs context is much slower for the proposed design (Fig. 6).  
    - This directly validates the efficiency claim.
  - Longâ€‘context extension:
    > Training at 32K then RoPEâ€‘rescaling to 128K works well up to 128K, but perplexity degrades beyond (Fig. 7).
  - Teacher size vs training horizon:
    > Smaller teacher wins for short training; larger teacher wins for longer training (Fig. 8).  
    - This nuance helps practitioners choose distillation sources.
  - Vision encoder resolution and P&S:
    > Higher base resolution improves pretraining transfer (Table 7). P&S notably boosts DocVQA/InfoVQA at both 4B and 27B (Table 8).

- Safety, memorization, and privacy
  - Memorization audit (Fig. 9):
    > Gemma 3â€™s exact and approximate memorization rates are significantly lower than prior models (log scale).  
    - Using a detection pipeline for personal information, no personal data was observed in outputs characterized as memorized for Gemma 3 models (Section 6).
  - Baseline safety and CBRN evaluations:
    > Low violation rates in synthetic adversarial tests; low knowledge in CBRN domains (Section 7.3).

- Overall assessment
  - The experiments are broad and largely convincing for the paperâ€™s claims: 
    - Memory savings are demonstrated with concrete measurements (Figs. 5â€“6). 
    - Capability improvements are shown across text, code, math, and multimodal benchmarks (Tables 6, 16, 18), and validated by human preference (Table 5).
    - Longâ€‘context generalization is demonstrated to 128K with both PT/IT checkpoints (Table 15, Fig. 7).
  - Some evaluations (e.g., Arena Elo) are preliminary and omit vision, but together the evidence triangulates well.

## 6. Limitations and Trade-offs
- Architectural tradeâ€‘offs
  - Local layers see only 1024 tokens. While global layers integrate longâ€‘range context, there may be edge cases where dense longâ€‘range interactions at all depths would helpâ€”for instance, tasks requiring frequent, deep crossâ€‘document references. The ablations show quality is stable on perplexity (Fig. 3â€“4), but specific taskâ€‘level edge cases arenâ€™t exhaustively cataloged.
  - After RoPE rescaling, models generalize to 128K but degrade rapidly when scaled further (Fig. 7). True 256Kâ€“512K capability likely needs additional training or different position schemes.

- Postâ€‘training vs longâ€‘context tension
  - On RULER at 128K, IT scores are lower than PT for the 27B model (66.0 vs 72.9, Table 15). Instruction tuning can shift model behavior away from the pure retrieval/needleâ€‘inâ€‘haystack style tasks those benchmarks emphasize; this is a common tradeâ€‘off between alignment and raw longâ€‘context probing.

- Vision pipeline constraints
  - The vision encoder is frozen and operates at fixed 896Ã—896 crops. While P&S recovers much of the lost detail (Table 8), it increases inference cost (multiple crops) and remains an inferenceâ€‘time heuristic. Content spanning across crop boundaries could be challenging.

- Quantization scope and context length
  - Memory figures (Table 3) focus on 32K context and show strong savings; memory and accuracy impacts at 128K context under each quantization format are not reported.

- Distillation dependence
  - The studentâ€™s capabilities depend on teacher quality and sampling strategy (Section 2.2 Distillation; Fig. 8). Tasks where the teacher is weak may cap student performance unless complemented by RL or curated data.

- Model coverage
  - The 1B model supports only 32K context and is textâ€‘only (Table 1; Section 2), limiting its multimodal and ultraâ€‘longâ€‘context applicability.

- Safety scope
  - Baseline safety and CBRN knowledge are assessed (Section 7), but full extremeâ€‘risk evaluations are streamlined for this release; broader realâ€‘world misuse channels (e.g., multimodal prompt injection via images) are not deeply explored here.

## 7. Implications and Future Directions
- How this changes the landscape
  - Demonstrates that small, open models can offer practical 128K context and competitive reasoning by rethinking attention layout rather than just scaling parameters. This makes longâ€‘context LLMs more deployable on consumer hardware, broadening access.
  - Provides a concrete, easyâ€‘toâ€‘adopt vision recipeâ€”fixed 256 tokens with optional Pan & Scanâ€”that many open systems can replicate to recover highâ€‘resolution details without retraining a heavy vision stack.

- Followâ€‘up research enabled or suggested
  - Trainingâ€‘time longâ€‘context scaling: Explore architectures or training curricula that maintain quality beyond 128K (e.g., multiâ€‘scale RoPE, hybrid memory, or retrievalâ€‘augmented longâ€‘context).
  - Learnable cropping and dynamic routing: Replace heuristic P&S with a learnable module that selects crops or resolutions endâ€‘toâ€‘end; investigate crossâ€‘crop attention so content spanning crop boundaries is handled more natively.
  - Taskâ€‘aware local/global schedules: Adapt the local:global ratio or window size conditioned on the input structure or task, potentially via routing or gating.
  - Quantization at extreme context lengths: Systematically map accuracyâ€“memory tradeâ€‘offs for 64Kâ€“128K under int4/SFP8 across tasks, including multimodal inputs.
  - Distillation strategies: Given Fig. 8â€™s finding, study adaptive teacher selection (small early, large later) and targeted teacher ensembles for modalities or domains where a single teacher is weak.

- Practical applications and downstream uses
  - Onâ€‘device assistants that read documents, forms, and receipts (Fig. 1 shows a receipt example), with longâ€‘document memory and private inference.
  - Enterprise copilots for coding and data analysis on laptops with reduced GPU memory, benefiting from high GSM8K/MATH/HumanEval performance (Tables 6, 18).
  - Multilingual agents across diverse locales, supported by improved pretraining mixtures (Tables 13â€“14).
  - Safer open deployments: lower measured memorization rates (Fig. 9) and curated instruction tuning reduce risks of regurgitation and unsafe outputs.

In sum, Gemma 3â€™s key ideaâ€”aggressively interleaving shortâ€‘span local attention with sparse global layersâ€”meaningfully shifts the efficiency frontier for longâ€‘context, multimodal open models. The measured KVâ€‘cache savings (Figs. 5â€“6; Table 3), the practical vision recipe (Tables 7â€“8), and the strong postâ€‘training gains (Tables 6, 16, 18; Table 5) together make a compelling case for realâ€‘world, resourceâ€‘constrained use while leaving clear avenues for further research on extreme context, smarter multimodal cropping, and quantized longâ€‘context robustness.

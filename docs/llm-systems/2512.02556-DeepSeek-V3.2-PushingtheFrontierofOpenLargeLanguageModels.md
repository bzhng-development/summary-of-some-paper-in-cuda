# DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models

**ArXiv:** [2512.02556](https://arxiv.org/abs/2512.02556)

## ğŸ¯ Pitch

This paper introduces DeepSeekâ€‘V3.2, an open LLM that combines a novel DeepSeek Sparse Attention (DSA) to cut longâ€‘context attention cost, a scaled and stabilityâ€‘focused GRPO RL postâ€‘training recipe, and a largeâ€‘scale synthetic agentic task pipeline with thinking-aware toolâ€‘use. Together these advances close much of the performance gap to proprietary frontier models for longâ€‘context reasoning and interactive agent tasks, reducing deployment cost and enabling more robust, generalizable openâ€‘source AI agents.

---

## 1. Executive Summary (2-3 sentences)

DeepSeek-V3.2 is an open large language model (LLM) system that targets a specific gap: open models lag proprietary models on **long-context efficiency**, **reasoning**, and **agent/tool-use robustness** (Introduction). It introduces (i) `DeepSeek Sparse Attention (DSA)` to cut long-context attention costs while maintaining performance (Section 2.1, Figure 2â€“3), (ii) a scaled, stability-oriented `GRPO` reinforcement learning (RL) recipe to push post-training compute (Section 3.1, Eq. (5)â€“(9)), and (iii) a large-scale synthetic agent-task pipeline plus tool-use â€œthinkingâ€ context management (Section 3.2, Table 1, Figure 4). On the reported benchmarks, DeepSeek-V3.2 is competitive with GPT-5-High on many reasoning and agent metrics, while the high-compute variant `DeepSeek-V3.2-Speciale` achieves substantially higher scores at the cost of much longer outputs (Table 2â€“4).

## 2. Context and Motivation

- **What specific problem or gap does this paper address?**
  - The paper frames a widening performance gap between **closed-source frontier models** and **open-source models** on complex tasks (Introduction).
  - It diagnoses three limiting factors for open models (Introduction):
    1. **Long-sequence inefficiency** from â€œvanilla attentionâ€ scaling poorly with sequence length.
    2. **Insufficient post-training compute** allocated by open model developers, limiting hard-task performance.
    3. **Weaker generalization and instruction-following in agent/tool settings**, reducing real deployment effectiveness.

- **Why is this problem important?**
  - Long-context efficiency directly affects **deployment cost/latency** and also constrains **post-training scalability** (Introduction; Section 2.3, Figure 3).
  - Agentic competence matters because real applications involve **interactive tool calls** and multi-step environments (Section 3.2; Table 1).

- **What prior approaches existed, and where do they fall short (as positioned here)?**
  - The paper points to widespread reliance on dense attention mechanisms as an architectural bottleneck for long sequences (Introduction).
  - It also implies prior open approaches underinvest in RL/post-training compute (Introduction) and underperform on agent benchmarks (Introduction; Section 4.1 discussion of tool-use/agent results).

- **How does this paper position itself relative to existing work?**
  - It positions DeepSeek-V3.2 as a **cost-efficient, open alternative** that narrows gaps in reasoning and agents via:
    - A new sparse attention mechanism (`DSA`) introduced via continued training (Section 2.1).
    - A â€œstable recipeâ€ to scale `GRPO` RL (Section 3.1).
    - A synthetic agent-environment generation pipeline (Section 3.2.3; Table 1; Figure 5).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **long-context, tool-using reasoning LLM** trained with a combination of continued pre-training, distillation, and reinforcement learning.
- It solves the â€œshapeâ€ of the problem by combining an **efficient long-context attention mechanism** (`DSA`) with a **scaled RL post-training pipeline** and **synthetic interactive environments** for tool-use generalization (Sections 2â€“3).

### 3.2 Big-picture architecture (diagram in words)

- **Base checkpoint**: Start from `DeepSeek-V3.1-Terminus` with `128K` context length (Section 2.1.1).
- **Attention upgrade via continued pre-training**:
  - Add `DSA` composed of a **Lightning Indexer** + **Topâ€‘k token selector**, then adapt the model to sparse attention (Section 2.1; Figure 2; Eq. (1)â€“(4)).
- **Post-training**:
  - Train multiple **domain specialists** with large-scale RL; distill their data into the final model (Section 3, â€œSpecialist Distillationâ€).
  - Run a **mixed RL stage** using `GRPO` over reasoning + agent + alignment data (Section 3, â€œMixed RL Trainingâ€; Section 3.1).
- **Agentic tool-use integration**:
  - Manage â€œthinkingâ€ context so reasoning persists across tool outputs but is dropped on new user messages (Section 3.2.1; Figure 4).
  - Bootstrap tool-use reasoning with a **cold-start prompting/template merge**, then scale with **synthetic tasks/environments** (Section 3.2.2â€“3.2.3; Table 1).

### 3.3 Roadmap for the deep dive

- I will explain:
  1. `DSA` mechanics (indexing + sparse selection) and how it is trained (Section 2.1; Eq. (1)â€“(4); Figure 2).
  2. Continued pre-training stages and the concrete training settings the paper provides (Section 2.1.1).
  3. Post-training pipeline: specialist distillation + mixed `GRPO` RL (Section 3).
  4. The RL-stability modifications added on top of GRPO (Section 3.1; Eq. (5)â€“(9)).
  5. Tool-use â€œthinkingâ€ management and the synthetic agent-task pipeline (Section 3.2; Table 1; Figure 5â€“6).

### 3.4 Detailed, sentence-based technical breakdown

This is an **empirical systems-and-training recipe** paper whose core idea is to improve an open LLM by (i) swapping dense attention for an efficient sparse pattern (`DSA`) while preserving quality, (ii) scaling RL post-training with stability fixes, and (iii) creating large-scale agentic training tasks so tool-use generalizes (Sections 2â€“4).

#### 3.4.1 System/data pipeline diagram in words (explicit â€œfirst, second, thirdâ€ flow)

1. **First**, take a pretrained checkpoint `DeepSeek-V3.1-Terminus` whose context length is already extended to `128K` tokens (Section 2.1.1).
2. **Second**, introduce `DSA` into the architecture, and perform continued pre-training in two stages:
   - A short dense warm-up to train only the DSA indexer (dense attention remains active).
   - A longer sparse training stage where the model uses sparse attention (topâ€‘`k` retrieval) and all parameters adapt (Section 2.1.1; Eq. (3)â€“(4)).
3. **Third**, run post-training to produce `DeepSeek-V3.2`:
   - Train **domain specialist** models (math, programming, reasoning, agent tasks, etc.), then distill their outputs into training data for the final model (Section 3, â€œSpecialist Distillationâ€).
   - Run a **single mixed RL stage** (reasoning + agents + human alignment) using `GRPO` (Section 3, â€œMixed RL Trainingâ€; Section 3.1).
4. **Fourth**, to make tool-use work well with â€œthinking,â€ apply a **tool-call-specific context management policy** and train with **synthetic agent environments/prompts** (Section 3.2; Figure 4; Table 1).
5. **Finally**, evaluate across reasoning, coding, tool-use, and agent benchmarks with a `128K` context window and temperature `1.0` (Section 4.1), and report both capability and token-efficiency (Table 3).

#### 3.4.2 DeepSeek Sparse Attention (DSA): what it is and how it works

**Goal:** Reduce the dominant cost of attention on long sequences while preserving quality (Section 2.1; Section 2.3).

- **Key components (prototype)** (Section 2.1):
  - A **Lightning Indexer**: a lightweight scorer that estimates which past tokens are worth attending to.
  - A **fine-grained token selection mechanism**: retrieves only the topâ€‘`k` key/value entries per query token.

- **Lightning Indexer details (Eq. (1))**:
  - For a query token representation \(h_t \in \mathbb{R}^d\) and a previous token \(h_s \in \mathbb{R}^d\), it computes an **index score** \(I_{t,s}\) used for selection.
  - The index score aggregates over `H_I` indexer heads:
    \[
    I_{t,s} = \sum_{j=1}^{H_I} w^I_{t,j}\cdot \mathrm{ReLU}(q^I_{t,j}\cdot k^I_s).
    \]
  - The paper emphasizes two efficiency choices:
    - Using `ReLU` â€œfor throughput considerationâ€ (Section 2.1).
    - Implementing the indexer with a small number of heads and in `FP8`, making it â€œremarkabl[e]â€ efficient compared to full attention scoring (Section 2.1).

- **Topâ€‘k selection and sparse attention (Eq. (2))**:
  - For each query token \(t\), compute \(I_{t,s}\) against prior tokens \(s\).
  - Select the set \(S_t\) of tokens with topâ€‘`k` index scores.
  - Compute attention output \(u_t\) by applying attention only over the selected key/value entries \(\{c_s\}\):
    \[
    u_t = \mathrm{Attn}\Big(h_t,\{c_s \mid I_{t,s}\in \mathrm{Top\text{-}k}(I_{t,:})\}\Big).
    \]
  - Plain-language interpretation: instead of comparing the query to *every* previous token with expensive attention, the model uses a cheap â€œretrieval-likeâ€ scoring step to shortlist `k` candidates, then pays full attention cost only on that shortlist.

- **Instantiation under `MLA` and `MQA` (Figure 2; Section 2.1)**:
  - Because the model is continued-trained from `DeepSeek-V3.1-Terminus`, DSA is â€œinstantiate[d] â€¦ based on `MLA`â€ (Section 2.1).
  - For kernel-level efficiency, the implementation requires key/value entries to be shared across multiple queries, so DSA is implemented using the `MQA` mode of MLA where â€œeach latent vector â€¦ will be shared across all query headsâ€ (Section 2.1; Figure 2).  
  - The paper points to an open-source inference implementation for unambiguous details (Section 2.1).

**Worked micro-example (illustrative, based strictly on Eq. (1)â€“(2))**  
Suppose a query token at position \(t\) has 128K prior tokens. Dense attention would score all 128K tokens in the main attention. With DSA:
1. The Lightning Indexer computes \(I_{t,s}\) for each prior token \(s\) (still \(O(L^2)\) in sequence length \(L\), but designed to be cheaper per score; Section 2.3).
2. Pick the top `k` tokens by \(I_{t,s}\); in DeepSeek-V3.2 sparse training, `k = 2048` (Section 2.1.1).
3. Run full attention only over those 2048 selected tokens to produce \(u_t\) (Eq. (2)).  
This changes the *main modelâ€™s* core attention cost from scaling like â€œall pairsâ€ to scaling like â€œeach query attends to 2048 tokensâ€ (Section 2.3).

#### 3.4.3 How DSA is trained during continued pre-training (two stages)

The paper describes two continued pre-training stages, both aligned to the same data distribution used for the prior 128K extension of `DeepSeek-V3.1-Terminus` (Section 2.1.1). It provides concrete training settings for the DSA components.

- **Stage A: Dense warm-up (initialize the Lightning Indexer)** (Section 2.1.1)
  - Dense attention remains enabled.
  - All model parameters are frozen **except** the Lightning Indexer.
  - Target: align indexer scores to the modelâ€™s existing dense attention distribution.
  - Construction of the target distribution:
    - For each query token \(t\), sum the main attention scores across all attention heads, then `L1`-normalize over sequence positions to get \(p_{t,:}\in\mathbb{R}^t\) (Section 2.1.1).
  - Loss: KL divergence between \(p_{t,:}\) and the indexerâ€™s Softmaxed scores (Eq. (3)):
    \[
    \mathcal{L}_I=\sum_t D_{KL}(p_{t,:}\,\|\,\mathrm{Softmax}(I_{t,:})).
    \]
  - **Hyperparameters provided**:
    - Learning rate: `1e-3` (Section 2.1.1).
    - Steps: `1000` (Section 2.1.1).
    - Batch structure: `16 sequences` per step, each `128K tokens` (Section 2.1.1).
    - Total tokens: `2.1B tokens` (Section 2.1.1).

- **Stage B: Sparse training (adapt the full model to sparse attention)** (Section 2.1.1)
  - Enable token selection; the model attends only to a selected sparse set.
  - Continue to align the indexer, but only on the selected set \(S_t\) (Eq. (4)):
    \[
    \mathcal{L}_I=\sum_t D_{KL}(p_{t,S_t}\,\|\,\mathrm{Softmax}(I_{t,S_t})).
    \]
  - Training graph separation:
    - The indexer input is **detached** so the indexer optimizes only via \(\mathcal{L}_I\).
    - The main model optimizes only via the language modeling loss (Section 2.1.1).
  - **Hyperparameters provided**:
    - Learning rate: `7.3 Ã— 10^-6` (Section 2.1.1).
    - Selection size: `2048 key-value tokens per query token` (Section 2.1.1).
    - Steps: `15000` (Section 2.1.1).
    - Batch structure: `480 sequences` per step, each `128K tokens` (Section 2.1.1).
    - Total tokens: `943.7B tokens` (Section 2.1.1).

**What is not specified (and therefore cannot be filled in):**
- The paper excerpt provided does **not** give standard base-model architecture/training parameters such as number of transformer layers, hidden size, attention head counts (beyond the indexer head count symbol \(H_I\) without a numeric value), tokenizer, optimizer type/settings for pre-training, learning-rate schedule, weight decay, or hardware used for training (Sections 2â€“3 as provided).

#### 3.4.4 Inference cost rationale and complexity

- The paper states the core attention complexity of the **main model** changes from \(O(L^2)\) to \(O(Lk)\), where \(k \ll L\) is the selected token count (Section 2.3).
- The Lightning Indexer still costs \(O(L^2)\), but is â€œmuch less computation compared with MLA,â€ due to few heads and FP8 implementation (Section 2.3).
- **Measured deployment cost curves**:
  - Figure 3 reports estimated â€œcost per million tokensâ€ vs token position for `DeepSeek-V3.1-Terminus` and `DeepSeek-V3.2` on `H800 GPUs`, priced at `2 USD per GPU hour` (Section 2.3; Figure 3).
  - The figure shows substantially lower cost for DeepSeek-V3.2, especially deep into long contexts, for both prefilling and decoding (Figure 3aâ€“b). The paper does not provide the raw numeric table behind the curves in the excerpt, so exact values beyond the plotted trend cannot be safely reproduced.
- For short-sequence prefilling, the system â€œimplement[s] a masked MHA mode to simulate DSAâ€ for higher efficiency under short contexts (Section 2.3).

#### 3.4.5 Post-training pipeline: specialist distillation + mixed RL

- **Specialist distillation (Section 3)**:
  - The pipeline first trains multiple **specialized models** (specialists), each fine-tuned from the same `DeepSeek-V3.2 base checkpoint`.
  - Covered domains include: mathematics, programming, general logical reasoning, general agent tasks, agentic coding, agentic search, plus writing and general QA (Section 3, â€œSpecialist Distillationâ€).
  - Each domain supports both **thinking** and **non-thinking** modes, and the paper uses different models to generate training data for long chain-of-thought vs direct responses (Section 3).
  - Specialists generate domain-specific distilled data; a final model trained on distilled data is â€œonly marginally belowâ€ specialists, and subsequent RL â€œeffectively eliminate[s]â€ the gap (Section 3).

- **Mixed RL training (Section 3)**:
  - The final DeepSeek-V3.2 uses a single RL stage merging reasoning, agent, and human-alignment training, to balance domains and avoid catastrophic forgetting (Section 3, â€œMixed RL Trainingâ€).
  - RL algorithm: `Group Relative Policy Optimization (GRPO)` (Section 3; Section 3.1).
  - Rewarding:
    - For reasoning and agent tasks: rule-based outcome reward, length penalty, and language consistency reward (Section 3).
    - For general tasks: a generative reward model with per-prompt rubrics (Section 3).

- **DeepSeek-V3.2 vs DeepSeek-V3.2-Speciale (Section 3; Section 4.2)**:
  - `DeepSeek-V3.2` is trained with a â€œlength constraint reward model,â€ aiming to improve the performanceâ€“cost trade-off (Section 4.1 discussion; Section 4.2).
  - `DeepSeek-V3.2-Speciale` is trained exclusively on reasoning data with a **reduced length penalty** and includes the dataset/reward method from `DeepSeekMath-V2` to improve proof abilities (Section 3; Section 4.2).

#### 3.4.6 GRPO and the paperâ€™s RL scaling stabilizers

**GRPO objective (Eq. (5)â€“(6); Section 3.1)**  
- For each question \(q\), sample a group of \(G\) responses \(\{o_1,\dots,o_G\}\) from an old policy \(\pi_{\text{old}}\).
- Optimize current policy \(\pi_\theta\) with a clipped policy-gradient style objective plus a KL penalty to a reference policy \(\pi_{\text{ref}}\) (Eq. (5)).
- Uses an importance ratio:
  \[
  r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t}\mid q,o_{i,<t})}{\pi_{\text{old}}(o_{i,t}\mid q,o_{i,<t})}
  \]
  (Eq. (6)).
- **Advantage estimate**:
  - Compute outcome rewards \(R_i\) for each output in the group.
  - Normalize within group: \(\hat A_{i,t} = R_i - \mathrm{mean}(\mathbf{R})\) (Section 3.1).

**Micro-example (illustrating the group-relative advantage idea)**  
If \(G=4\) sampled responses get rewards \([0.2, 0.1, 0.6, 0.3]\), the mean is \(0.3\). Then advantages are \([-0.1, -0.2, +0.3, 0.0]\). GRPO pushes up tokens from the best response (positive advantage) relative to the others, rather than relying on an absolute baseline (Section 3.1 definition of \(\hat A\)).

**Stability/scale additions (Section 3.1):**
- **Unbiased KL estimate (Eq. (7))**:
  - The paper modifies a KL estimator (â€œK3â€) to correct bias using importance sampling ratios between \(\pi_\theta\) and \(\pi_{\text{old}}\) (Section 3.1; Eq. (7)).
  - Claimed effect: unbiased gradients and improved stability, especially when sampled tokens have much lower probability under \(\pi_\theta\) than under \(\pi_{\text{ref}}\) (Section 3.1 discussion).
  - Domain nuance: math may benefit from weaker or even no KL penalty (Section 3.1).

- **Off-policy sequence masking (Eq. (8)â€“(9))**:
  - Problem: rollout reuse across multiple SGD steps and inference-vs-training discrepancies create off-policy updates (Section 3.1).
  - Solution: introduce mask \(M_{i,t}\) in the loss (Eq. (8)) that can zero out contributions from sequences that (i) have negative advantage and (ii) exceed a divergence threshold \(\delta\) based on the average log-ratio between \(\pi_{\text{old}}\) (sampling) and \(\pi_\theta\) (current) (Eq. (9)).
  - The paper reports this improves stability in some otherwise unstable scenarios (Section 3.1).

- **Keep Routing (for MoE)**:
  - MoE routing can differ between inference and training and shift during policy updates, destabilizing optimization (Section 3.1).
  - They â€œpreserve the expert routing paths used during samplingâ€ and enforce the same routes during training (Section 3.1).

- **Keep Sampling Mask (topâ€‘p/topâ€‘k truncation)**:
  - Sampling truncation improves output quality but breaks importance sampling if \(\pi_{\text{old}}\) and \(\pi_\theta\) have different action spaces (Section 3.1).
  - They keep the truncation mask from sampling and apply it during training so both policies share the same truncated action subspace (Section 3.1).

**What is not specified (and cannot be invented):**
- The excerpt does not provide numeric values for key GRPO hyperparameters such as \(\epsilon\) (clip range), \(\beta\) (KL coefficient), \(\delta\) (mask threshold), group size \(G\), batch sizes, optimizer, or total RL tokens/stepsâ€”only â€œthousands of stepsâ€ and a compute budget described qualitatively (Section 3; Section 4.1).

#### 3.4.7 â€œThinking in tool-useâ€: context management, cold-start, and synthesis

- **Thinking context management (Section 3.2.1; Figure 4)**:
  - Observation: discarding reasoning on every new message (as described for DeepSeek-R1 in the paperâ€™s framing) forces repeated re-reasoning across tool calls, wasting tokens (Section 3.2.1).
  - Their rule (Figure 4):
    - Discard historical reasoning content **only when a new user message arrives**.
    - If only tool-related messages (tool outputs) are appended, **retain reasoning content**.
    - Even when reasoning is removed, tool-call history and tool results are preserved.
  - Compatibility note: some agent frameworks simulate tools via user messages (e.g., Roo Code, Terminus). Those may not benefit, so they recommend non-thinking models for such architectures (Section 3.2.1).

- **Cold-start integration (Section 3.2.2; Appendix Tables 6â€“8)**:
  - Given separate pools of reasoning data and non-reasoning agent data, they merge capabilities by designing system prompts that instruct the model to interleave tool calls inside `<think>...</think>` (Appendix Table 8), building on prompts for pure reasoning (Appendix Table 6) and for tool use (Appendix Table 7).
  - This yields occasional correct â€œthinking + toolâ€ trajectories, which serve as seeds for later RL (Section 3.2.2).

- **Large-scale agentic tasks and environments (Section 3.2.3; Table 1)**:
  - They train on a mix of real-tool environments and synthetic environments:
    - `code agent`: 24,667 tasks, real environment, extracted prompts.
    - `search agent`: 50,275 tasks, real environment, synthesized prompts.
    - `general agent`: 4,417 tasks, synthesized environment, synthesized prompts.
    - `code interpreter`: 5,908 tasks, real environment, extracted prompts.
    (Table 1)

  - **Search agent synthesis pipeline (Section 3.2.3)**:
    - Sample long-tail entities from web corpora.
    - A question-construction agent uses search tools (configurable depth/breadth) to produce QA pairs.
    - Multiple heterogeneous answer-generation agents produce candidate responses.
    - A verification agent validates answers with search; they keep samples where ground truth is correct and all candidates are verifiably incorrect.
    - They add filtered instances from helpful RL datasets where search measurably helps, and use a generative reward model with multi-dimensional rubrics.

  - **Code agent environment construction (Section 3.2.3)**:
    - Mine millions of GitHub issueâ€“PR pairs; filter with heuristics + LLM judgments.
    - Auto environment setup agent builds runnable environments (install deps, run tests).
    - Validity criterion: applying the gold patch yields non-zero false-to-positive (F2P) tests and zero pass-to-fail (P2F) regressions; tests output in JUnit format.
    - Produces â€œtens of thousandsâ€ reproducible environments across many languages (Python, Java, JS/TS, C/C++, Go, PHP).

  - **General agent environment synthesis (Section 3.2.3)**:
    - An agent generates tools + databases in a sandbox, creates tasks with solution and verification functions, and iteratively increases difficulty.
    - Restriction: the solution function can only call tools or do logical computation, not access the database directlyâ€”forcing tool use.
    - They retain environments where the model achieves non-zero `pass@100`, resulting in `1,827 environments` and `4,417` tasks (Section 3.2.3).

## 4. Key Insights and Innovations

1. **`DSA`: a two-stage sparse attention mechanism trained to mimic dense attention distributions**
   - Novelty: DSA explicitly trains a lightweight indexer to approximate the dense attention distribution via KL alignment (Eq. (3)â€“(4)), then uses that indexer for topâ€‘`k` retrieval (Eq. (2)).
   - Significance: It changes main attention cost scaling from \(O(L^2)\) to \(O(Lk)\) for long contexts (Section 2.3) and yields large end-to-end inference cost reductions on deployed H800 systems (Figure 3).

2. **A practical RL scaling recipe layered on GRPO to improve stability under real training-system constraints**
   - Novelty: The paper targets practical failure modesâ€”biased KL gradients (Eq. (7)), off-policy rollouts from batching/inference differences (Eq. (8)â€“(9)), and MoE routing inconsistency (â€œKeep Routingâ€), plus sampling truncation mismatch (â€œKeep Sampling Maskâ€) (Section 3.1).
   - Significance: This is positioned as enabling â€œsignificant computational expansion during the post-training phase,â€ with post-training compute exceeding â€œ10% of the pre-training costâ€ (Introduction; Section 4.1 discussion), and correlating with improved benchmark performance (Section 4.1 narrative).

3. **Large-scale synthetic agent environment generation to train generalizable tool-using behaviors**
   - Novelty: The general agent pipeline synthesizes `<environment, tools, task, verifier>` tuples where tasks are hard but easy to verify, and where solutions are constrained to tool usage (Section 3.2.3).
   - Significance: Ablations indicate RL on synthetic general-agent data improves performance on tool-use benchmarks that are not improved by RL limited to search+code environments (Figure 5 discussion).

4. **Tool-call-specific â€œthinking retentionâ€ context management**
   - Novelty: Retain reasoning across tool outputs but drop it when a new user message arrives (Section 3.2.1; Figure 4).
   - Significance: This addresses token inefficiency in multi-tool-call trajectories and is later reused as a test-time compute scaling strategy for search agents (Section 4.4; Figure 6).

## 5. Experimental Analysis

### 5.1 Evaluation methodology (datasets, metrics, setup)

- Benchmarks include (Section 4.1): `MMLU-Pro`, `GPQA Diamond`, `HLE (text-only subset)`, `LiveCodeBench (2024.08â€“2025.04)`, `Codeforces`, `Aider-Polyglot`, `AIME 2025`, `HMMT Feb/Nov 2025`, `IMOAnswerBench`, `Terminal Bench 2.0`, `SWE-Verified`, `SWE Multilingual`, `BrowseComp`, `BrowseCompZh`, `Ï„2-bench`, `MCP-Universe`, `MCP-Mark`, `Tool-Decathlon`.
- Tool-use evaluations use standard function-call formatting and â€œthinking modeâ€ (Section 4.1).
- For MCP-Universe and MCP-Mark, they evaluate all models in an **internal environment** because official search/playwright environments may differ (Section 4.1). This is an important comparability caveat.
- Shared evaluation settings:
  - Temperature: `1.0`
  - Context window: `128K tokens`
  (Section 4.1)
- For math tasks (AIME, HMMT, IMOAnswerBench, HLE), they use a step-by-step template requiring final answers in `\boxed{}`; HLE is also evaluated with an official template for DeepSeek-V3.2-Thinking producing `23.9` (Section 4.1).

### 5.2 Main quantitative results (with specific numbers)

**DeepSeek-V3.2 vs selected open/closed models (Table 2)**  
(Here I list DeepSeek-V3.2â€™s reported metric and a few key comparisons where the table provides them.)

- **English knowledge / QA:**
  - `MMLUâ€‘Pro (EM)`: DeepSeek-V3.2 `85.0` vs GPTâ€‘5â€‘High `87.5`, Geminiâ€‘3.0â€‘Pro `90.1` (Table 2).
  - `GPQA Diamond (Pass@1)`: DeepSeek-V3.2 `82.4` vs GPTâ€‘5â€‘High `85.7`, Geminiâ€‘3.0â€‘Pro `91.9` (Table 2).
  - `HLE (Pass@1)`: DeepSeek-V3.2 `25.1` vs GPTâ€‘5â€‘High `26.3`, Geminiâ€‘3.0â€‘Pro `37.7` (Table 2).

- **Code / programming reasoning:**
  - `LiveCodeBench (Pass@1â€‘CoT)`: DeepSeek-V3.2 `83.3` vs GPTâ€‘5â€‘High `84.5`, Geminiâ€‘3.0â€‘Pro `90.7` (Table 2).
  - `Codeforces (Rating)`: DeepSeek-V3.2 `2386` vs GPTâ€‘5â€‘High `2537`, Geminiâ€‘3.0â€‘Pro `2708` (Table 2).

- **Math reasoning:**
  - `AIME 2025 (Pass@1)`: DeepSeek-V3.2 `93.1` vs GPTâ€‘5â€‘High `94.6`, Geminiâ€‘3.0â€‘Pro `95.0` (Table 2).
  - `HMMT Feb 2025 (Pass@1)`: DeepSeek-V3.2 `92.5` vs GPTâ€‘5â€‘High `88.3`, Geminiâ€‘3.0â€‘Pro `97.5` (Table 2).
  - `HMMT Nov 2025 (Pass@1)`: DeepSeek-V3.2 `90.2` vs GPTâ€‘5â€‘High `89.2`, Geminiâ€‘3.0â€‘Pro `93.3` (Table 2).
  - `IMOAnswerBench (Pass@1)`: DeepSeek-V3.2 `78.3` vs GPTâ€‘5â€‘High `76.0`, Geminiâ€‘3.0â€‘Pro `83.3` (Table 2).

- **Agentic coding:**
  - `Terminal Bench 2.0 (Acc)`: DeepSeek-V3.2 `46.4` vs GPTâ€‘5â€‘High `35.2`, Geminiâ€‘3.0â€‘Pro `54.2` (Table 2).
    - The text notes the 46.4 score used the Claude Code framework due to incompatibility between their thinking-context technique and Terminus; with Terminus in non-thinking mode they got `39.3` (Section 4.1 narrative).
  - `SWEâ€‘Verified (Resolved)`: DeepSeek-V3.2 `73.1` vs GPTâ€‘5â€‘High `74.9`, Geminiâ€‘3.0â€‘Pro `76.2` (Table 2).
  - `SWE Multilingual (Resolved)`: DeepSeek-V3.2 `70.2` vs GPTâ€‘5â€‘High `55.3` (Gemini not reported) (Table 2).

- **Search agent:**
  - `BrowseComp (Pass@1)`: DeepSeek-V3.2 `51.4` (and `67.6*` with context management), GPTâ€‘5â€‘High `54.9`, Claudeâ€‘4.5â€‘Sonnet `24.1` (Table 2).
  - `BrowseCompZh (Pass@1)`: DeepSeek-V3.2 `65.0` vs GPTâ€‘5â€‘High `63.0`, Claudeâ€‘4.5â€‘Sonnet `42.4` (Table 2).
  - `HLE (Pass@1)` under Search Agent block: DeepSeek-V3.2 `40.8` vs GPTâ€‘5â€‘High `35.2`, Geminiâ€‘3.0â€‘Pro `45.8` (Table 2).

- **Tool-use:**
  - `Ï„2â€‘bench (Pass@1)`: DeepSeek-V3.2 `80.3` vs GPTâ€‘5â€‘High `80.2`, Geminiâ€‘3.0â€‘Pro `85.4` (Table 2).
  - `MCPâ€‘Universe (Success Rate)`: DeepSeek-V3.2 `45.9` vs GPTâ€‘5â€‘High `47.9`, Geminiâ€‘3.0â€‘Pro `50.7` (Table 2).
  - `MCPâ€‘Mark (Pass@1)`: DeepSeek-V3.2 `38.0` vs GPTâ€‘5â€‘High `50.9`, Geminiâ€‘3.0â€‘Pro `43.1` (Table 2).
  - `Toolâ€‘Decathlon (Pass@1)`: DeepSeek-V3.2 `35.2` vs GPTâ€‘5â€‘High `29.0`, Geminiâ€‘3.0â€‘Pro `36.4` (Table 2).

**Token efficiency and the â€œSpecialeâ€ trade-off (Table 3)**  
Table 3 reports both accuracy and output token counts (in thousands) for reasoning models.

- Example patterns:
  - `AIME 2025`: DeepSeekâ€‘V3.2 `93.1 (16k)` vs DeepSeekâ€‘V3.2â€‘Speciale `96.0 (23k)` (Table 3).
  - `HMMT Feb 2025`: DeepSeekâ€‘V3.2 `92.5 (19k)` vs Speciale `99.2 (27k)` (Table 3).
  - `Codeforces`: DeepSeekâ€‘V3.2 `2386 (42k)` vs Speciale `2701 (77k)` (Table 3).
  - `HLE`: DeepSeekâ€‘V3.2 `25.1 (21k)` vs Speciale `30.6 (35k)` (Table 3).
- Interpretation constrained to the table: Speciale often achieves higher scores but uses substantially more output tokens, sometimes dramatically (e.g., Codeforces 77k vs 42k; Table 3). The text explicitly calls token efficiency a key downside and motivation for length constraints in the official V3.2 (Section 4.2).

**Competition-style evaluations for Speciale (Table 4; Appendix D)**  
- `IMO 2025`: `35/42` overall, medal â€œGoldâ€ (Table 4).
- `CMO 2025`: `102/126` overall, â€œGoldâ€ (Table 4; note the text says English version of CMO 2025 was evaluated; Section 4.2 footnote).
- `IOI 2025`: `492/600` overall, â€œGoldâ€ (Table 4).
- `ICPC WF 2025`: `10/12` problems, â€œGoldâ€ (Table 4).
- The evaluation method includes constraints: max generation length `128k`, no tools/internet, contest time/attempt limits; IOI uses a generate-and-filter submission strategy (Appendix D).

### 5.3 Ablations, robustness, and supporting evidence

- **DSA parity / regression checks (Section 2.2)**:
  - The paper reports DeepSeek-V3.2-Exp matches DeepSeek-V3.1-Terminus on â€œstandard benchmarksâ€ and in ChatbotArena Elo (Section 2.2).
  - It also mentions independent long-context evals (AA-LCR and Fiction.liveBench) showing V3.2-Exp outperforming Terminus, but it does not provide full numeric tables in the excerpt beyond â€œscores four points higherâ€ on AA-LCR (Section 2.2). Without the full benchmark definition and numbers, only that qualitative delta can be repeated safely.

- **Synthetic general agent tasks are hard (Table 5)**:
  - On 50 sampled instances:
    - DeepSeekâ€‘V3.2â€‘Exp Pass@1 = `12%`
    - Claude Sonnetâ€‘4.5 Pass@1 = `34%`
    - Geminiâ€‘3.0â€‘Pro Pass@1 = `51%`
    - GPTâ€‘5â€‘Thinking Pass@1 = `62%`
    (Table 5)
  - This supports the claim that the synthetic tasks contain challenging instances even for strong models (Section 4.3 discussion).

- **Generalization benefit of synthetic RL (Figure 5; Section 4.3)**:
  - The paper compares:
    - `DeepSeekâ€‘V3.2â€‘SFT` (baseline),
    - RL on synthetic general agent data (non-thinking mode),
    - `DeepSeekâ€‘V3.2â€‘Exp` trained with RL only in search and code environments.
  - Reported conclusion: synthetic-only RL yields â€œsubstantial improvementsâ€ on `Tau2Bench`, `MCP-Mark`, and `MCP-Universe`, while RL limited to code+search does not (Section 4.3; Figure 5).  
  - The excerpt does not include exact numeric deltas from Figure 5, so I cannot quote precise improvements.

- **Test-time compute scaling via context management on BrowseComp (Figure 6; Section 4.4)**:
  - Strategies: `Summary`, `Discard-75%`, `Discard-all`, and a parallel baseline `Parallel-fewest-step` (Section 4.4).
  - Concrete example: `Summary` extends average steps to `364` and improves score â€œup to `60.2`â€ (Section 4.4).
  - `Discard-all` reaches `67.6`, described as comparable to parallel scaling with fewer steps (Section 4.4; Figure 6).
  - This connects directly to the earlier tool-use thinking context management idea, but applied as a test-time budget extension (Section 4.4).

### 5.4 Do the experiments convincingly support the claims?

- **Supportive elements:**
  - The paper provides direct benchmark comparisons across many domains (Table 2) and explicitly measures token efficiency (Table 3), matching the paperâ€™s stated concern about cost/performance trade-offs (Section 4.2).
  - It includes ablations addressing whether synthetic tasks are challenging (Table 5) and whether RL on synthetic tasks transfers to other benchmarks (Figure 5).

- **Caveats and comparability limitations (from the text itself):**
  - MCP benchmarks are run in an internal environment (Section 4.1), which may affect comparability to external leaderboards.
  - BrowseComp results are reported both with and without context management, and the paper notes 20%+ cases exceed the 128K limit (Section 4.1), so the raw score depends on a specific test-time policy.
  - Some agent frameworks are incompatible with their thinking-context approach (Section 3.2.1; Section 4.1 Terminal Bench discussion), which can confound apples-to-apples comparisons if frameworks differ.

## 6. Limitations and Trade-offs

- **World knowledge breadth lags frontier proprietary models due to fewer total training FLOPs** (Section 5).
  - This is a qualitative limitation; the excerpt does not provide total FLOPs or parameter counts.

- **Token efficiency remains a major trade-off** (Section 5; Table 3; Section 4.2).
  - Speciale achieves higher scores but often uses many more output tokens (Table 3).
  - The official DeepSeek-V3.2 imposes stricter token constraints to reduce deployment cost/latency (Section 4.2).

- **DSA is not â€œfully sparseâ€ in end-to-end complexity** (Section 2.3).
  - The main model attention becomes \(O(Lk)\), but the Lightning Indexer is still \(O(L^2)\), albeit cheaper per operation (Section 2.3). This means very large \(L\) still has a quadratic component.

- **Tool-use thinking retention is framework-dependent** (Section 3.2.1).
  - Frameworks that encode tool interactions as user messages may not benefit; the paper recommends non-thinking models there (Section 3.2.1).

- **128K maximum context is still a hard ceiling for some agent workflows** (Section 4.1; Section 4.4).
  - The paper explicitly notes ~20%+ BrowseComp cases exceed 128K and require context management to proceed (Section 4.1).

- **Missing reproducibility details in the provided excerpt**
  - Data filtering, deduplication, contamination checks, and detailed optimizer schedules for continued pre-training and RL are not included in the excerpt (Sections 2â€“3 as provided). This limits independent replication from this text alone.

## 7. Implications and Future Directions

- **How this changes the landscape (based on the paperâ€™s evidence)**
  - The combination of (i) sparse long-context attention (DSA), (ii) scaled RL post-training, and (iii) synthetic agent environment generation demonstrates a plausible recipe for **closing portions of the open-vs-closed gap** on reasoning and tool-use benchmarks (Table 2; Figure 3; Figure 5).
  - The results suggest that **agentic generalization can be trained** via synthetic verifiable environments, not only by specializing on a fixed set of real tools (Section 4.3; Figure 5).

- **Follow-up research directions explicitly motivated by the paper**
  - **Scale pre-training compute** to reduce the world-knowledge gap (Section 5).
  - **Improve token efficiency / â€œintelligence densityâ€** so fewer tokens are needed to reach the same quality, especially compared to Gemini-3.0-Pro (Section 5; Section 4.2; Table 3).
  - **Better handling of long-horizon agents beyond the context limit**, including optimizing serial (context management) and parallel (multi-trajectory) test-time compute combinations (Section 4.4; Figure 6).

- **Practical applications / downstream use cases suggested by the evaluation**
  - **Long-context deployment** where attention cost dominates: DSAâ€™s cost curves show large reductions deep into long sequences on H800 deployment (Figure 3).
  - **Software engineering agents**: competitive performance on SWE-Verified and Terminal Bench 2.0 (Table 2), with caveats about framework compatibility (Section 4.1).
  - **Search/browsing agents**: strong BrowseComp/BrowseCompZh scores, especially when context management is allowed (Table 2; Section 4.4).

- **Repro/Integration Guidance (grounded in the paperâ€™s operational notes)**
  - If you need **efficient long-context inference**, the paperâ€™s key architectural lever is `DSA` with topâ€‘`k = 2048` selected KV tokens per query during sparse training (Section 2.1.1) and an indexer designed for FP8 throughput (Section 2.1; Section 2.3).
  - If you train with large-scale RL on MoE models, the paper flags `Keep Routing` and `Keep Sampling Mask` as crucial stability measures under inference/training mismatches and truncation sampling (Section 3.1).
  - For tool-using agents:
    - Use the **thinking retention rule** (retain reasoning across tool outputs; drop on new user messages) when your framework preserves tool calls as tool-role messages (Section 3.2.1; Figure 4).
    - Prefer **non-thinking mode** when your agent framework encodes tool interactions as user messages (Section 3.2.1).
  - For search agents facing context overflow, the paperâ€™s test-time strategies show that even simple approaches like `Discard-all` can raise BrowseComp performance substantially (`67.6`) (Section 4.4; Figure 6), but benchmark results become sensitive to the chosen overflow policy (Section 4.4).



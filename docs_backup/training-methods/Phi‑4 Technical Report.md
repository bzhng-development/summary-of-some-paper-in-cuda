# Phiâ€‘4 Technical Report

**ArXiv:** [2412.08905](https://arxiv.org/abs/2412.08905)
**Authors:** Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, SuriyaÂ Gunasekar, Michael Harrison, RussellÂ J.Â Hewett, MojanÂ Javaheripi, PieroÂ Kauffmann, JamesÂ R.Â Lee, YinÂ TatÂ Lee, YuanzhiÂ Li, WeishungÂ Liu, CaioÂ C.Â T.Â Mendes, AnhÂ Nguyen, EricÂ Price, GustavoÂ deÂ Rosa, OlliÂ Saarikivi, AdilÂ Salim, ShitalÂ Shah, XinÂ Wang, RachelÂ Ward, YueÂ Wu, DingliÂ Yu, CyrilÂ Zhang, YiÂ Zhang
**Institutions:** Microsoft Research

## ğŸ¯ Pitch

The `phi-4` model introduces a novel training recipe that leverages high-quality synthetic data and Pivotal Token Search (PTS) for token-level optimization, achieving state-of-the-art reasoning in a 14-billion-parameter model. This innovation significantly reduces inference cost and latency while outperforming larger models on critical reasoning tasks, offering a compelling shift from reliance on model size to smarter data and post-training strategies.

---

## 1. Executive Summary
`phi-4` is a 14-billion-parameter language model whose central innovation is a training recipe built around highâ€‘quality synthetic data and a new postâ€‘training method, Pivotal Token Search (PTS), for tokenâ€‘level preference optimization. The model achieves stateâ€‘ofâ€‘theâ€‘art reasoning for its sizeâ€”surpassing its teacher `GPTâ€‘4o` on graduateâ€‘level STEM (GPQA) and competition math (MATH)â€”while keeping inference cost low, and shows strong generalization on fresh AMC 2024 exams (Figure 1).

## 2. Context and Motivation
- Problem addressed
  - Scaling model size and compute has driven recent LLM advances, but highâ€‘quality data can rival or exceed those gains. The paper targets how to systematically design data and postâ€‘training to unlock strong reasoning in a small model (Section 1).
  - Two persistent issues are benchmark overfitting/data contamination and models that perform well only by using long, expensive chains of thought at inference (Section 1.1).

- Why it matters
  - Practical: A 14B model with strong reasoning reduces latency and cost compared to very large or longâ€‘thinking models, enabling broader deployment (Section 1.1).
  - Scientific: Demonstrates that careful synthetic data design, curriculum, and postâ€‘training can push beyond pure distillation, evidenced by outperforming the teacher model on key reasoning benchmarks (Table 1).

- Prior approaches and gaps
  - Previous Phi models largely distilled from a teacher (GPTâ€‘4). This limits going beyond the teacherâ€™s capabilities and can overfit to the teacherâ€™s style (Section 1).
  - Long chainâ€‘ofâ€‘thought models (e.g., â€œo1â€‘styleâ€) use substantially more tokens at inference; openâ€‘weight versions such as `QwQâ€‘32B` trade latency and cost for quality. The report notes `QwQ` uses ~4Ã— tokens and >2Ã— parameters vs. `phiâ€‘4`, making its inference an order of magnitude costlier (Section 1.1).

- Positioning
  - `phiâ€‘4` keeps the `phiâ€‘3` architecture largely unchanged (decoderâ€‘only transformer) but redesigns the data recipe and postâ€‘training: syntheticâ€‘heavy pretraining/midtraining, meticulous organic data filtering, and novel DPO via PTS (Sections 2â€“4). The model is evaluated on contaminationâ€‘resistant benchmarks, including fresh AMC 2024 exams (Section 1.1; Appendix C).

## 3. Technical Approach
This section walks through the full pipelineâ€”data, model training, and postâ€‘training alignmentâ€”highlighting mechanisms and choices that make the system work.

- Model and optimization (Section 3)
  - Architecture: 14B decoderâ€‘only transformer, default 4K context expanded to 16K during midtraining; `tiktoken` tokenizer with vocab 100,352; full attention over 4K (Section 3).
  - Pretraining schedule: ~10T tokens; linear warmup/decay; peak LR 3eâ€‘4, weight decay 0.1, global batch 5760 (Section 3). Midtraining extends context to 16K with a 10Ã— lower max LR and 250B additional tokens (Section 3.3).
  - Position encoding: RoPE base frequency increased to 250K for long context (Section 3.3).

- Data design: categories and mixture (Sections 2â€“3)
  - Terms
    - â€œSynthetic dataâ€: content generated by LLMs via controlled prompts/pipelines.
    - â€œOrganic dataâ€: humanâ€‘generated or nonâ€‘synthetic data (Section 1 footnote).
    - â€œWeb rewritesâ€: synthetic rephrasings of curated web content to match LLM chat style (Sections 2.2, 3.2).
    - â€œMidtrainingâ€: a postâ€‘pretraining stage used here to extend context length and refine capabilities with targeted data (Section 3.3).

  - Synthetic generation (Section 2.2)
    - Seed curation: extract highâ€‘value snippets from web, books, code, and Q&A sites, scored for reasoning depth and factuality; also build question sets filtered by plurality voting to balance difficulty (Section 2.2, â€œSeed Curation,â€ items 1â€“3).
    - Rewrite and augment: turn seeds into exercises, discussions, or structured reasoning tasks (Section 2.2, â€œRewrite and Augmentâ€).
    - Selfâ€‘revision: iterative critiqueâ€‘andâ€‘improve loops with rubrics rewarding reasoning and correctness (Section 2.2, â€œSelfâ€‘revisionâ€).
    - Instruction reversal: for code and other tasks, generate an instruction from a solution (e.g., code) and keep only pairs with high fidelity, making instructionâ€‘toâ€‘solution training consistent (Section 2.2, â€œInstruction Reversalâ€¦â€).
    - Validation: execute code, use tests or oracles for scientific/math correctness (Section 2.2, â€œValidationâ€¦â€).

  - Organic data curation (Section 2.3)
    - Question datasets: collect millions of questions from public and licensed sources; replace weak or missing answers with majorityâ€‘voted synthetic answers; apply thorough decontamination (Section 2.3).
    - Web data: targeted acquisitions (arXiv, PMC, GitHub; licensed books), plus highâ€‘precision filtering of web dumps with classifiers distilled from LLM annotations; multilingual pipeline across 176 languages; custom parsers to preserve math, code, tables, and thread structure (Section 2.3).

  - Final pretraining mixture (Table 5; Section 3.2)
    - 40% synthetic (â‰ˆ290B unique tokens, ~13.8 epochs)
    - 15% web rewrites (â‰ˆ290B unique, ~5.2 epochs)
    - 15% filtered web (â‰ˆ1.3T unique, ~1.2 epochs)
    - 20% code (â‰ˆ820B unique, ~2.4 epochs)
    - 10% targeted acquisitions (â‰ˆ580B unique, ~1.7 epochs)

  - Why this mixture?
    - Ablations show more repetitions (epochs) over synthetic outperform adding more unique web tokens on reasoning (Figure 2; Section 3.1). Syntheticâ€‘only training underperforms on knowledgeâ€‘heavy tasks like TriviaQA (TQA), so some highâ€‘quality organic data is retained (Table 3; Section 3.1).
    - Additional 1Tâ€‘token ablations explored allocations among synthetic, filtered web, and web rewrites; â€œsyntheticâ€‘heavyâ€ variants slightly improve some reasoning metrics but hurt knowledge, so the chosen mixture balances both (Table 4).

- Midtraining for long context (Section 3.3; Table 6)
  - Data: prioritize inherently long documents (â‰¥8K tokens) from academic/books/code and create long synthetic data; the mixture is 30% new longâ€‘context data + 70% recall tokens from pretraining (Section 3.3).
  - Mechanism: increase RoPE base frequency and continue training with lower LR to adapt to 16K contexts.

- Postâ€‘training alignment (Section 4)
  - SFT (Section 4.1)
    - ~8B tokens in `chatml` format; mix of math, coding, reasoning, conversation, identity, safety, and multilingual prompts (40 languages). LR ~1eâ€‘6.

  - DPO (Direct Preference Optimization) stages (Sections 4.2â€“4.3)
    - DPO aligns model outputs with desired preferences using pairs of â€œacceptedâ€ vs â€œrejectedâ€ responses.
    - Stage 1â€”`Pivotal Token Search (PTS)`: a novel way to construct tokenâ€‘level DPO pairs targeting the exact tokens that determine success/failure.
      - What is a â€œpivotal tokenâ€? A token whose choice sharply changes the probability that the overall answer will be correct. Figure 3 visualizes such tokens in a math solution by colorâ€‘coding how continuing from each token affects success probability.
      - How PTS works (Figure 4):
        - For a prompt and one candidate solution `Tfull = t1, t2, â€¦`, estimate `p(success | prefix)` by sampling continuations from that prefix and checking correctness with an oracle (e.g., code tests or known math answers).
        - Recursively split the token sequence until each segment causes only a small change (< `pgap`) in success probability; single tokens that cause large changes (â‰¥ `pgap`) are â€œpivotal.â€
        - Construct DPO pairs where the query is the prefix before the pivotal token, and the two candidate completions are the â€œgoodâ€ token (increases success probability) vs the â€œbadâ€ token (decreases it).
      - Data: Stageâ€‘1 PTS DPO uses reasoningâ€‘friendly domains with ground truth oracles: math, QA, and coding (Table 7).
      - Why tokenâ€‘level? Fullâ€‘sequence DPO gradients can be noisy or even reward bad lowâ€‘probability tokens; PTS isolates the true decision points (Section 4.3).

    - Stage 2â€”Judgeâ€‘guided fullâ€‘length DPO (Section 4.2; Table 8)
      - Build ~850k accepted/rejected pairs by generating from `GPTâ€‘4o`, `GPTâ€‘4t`, and `phiâ€‘4`, then use `GPTâ€‘4o` as a judge to assign pairwise labels based on accuracy, style, and detail (Appendix A.2).

  - Hallucination mitigation (Section 4.4; Appendix A.1)
    - Build SFT and DPO data that teach refusal when the model is unlikely to know the answer (e.g., obscure trivia). The process uses question sets, bogus variants, and targeted refusal prompts. This reduces â€œimprovâ€‘styleâ€ guessing and increases â€œnot attemptedâ€ on SimpleQA (Figure 6).

- Contamination defenses and fresh evaluation (Section 1.1; Appendix Bâ€“C)
  - Hybrid decontamination using 13â€‘gram and 7â€‘gram filtering with allowed common 13â€‘grams and perâ€‘benchmark overlap ratios (Algorithm 1; Appendix B).
  - Use AMC 2024 exams, released after all training data was fixed, and evaluated only after model selection (Section 1.1; Appendix C).

## 4. Key Insights and Innovations
- Syntheticâ€‘first curriculum with careful organic complement
  - Innovation: Treat synthetic data not as cheap filler but as a structured curriculum for reasoningâ€”seeded by carefully curated organic snippets and validated through selfâ€‘revision and tests (Sections 2.1â€“2.3).
  - Why it matters: Ablations show more epochs over highâ€‘quality synthetic data improve reasoning more than adding fresh web tokens (Figure 2). However, retaining a focused portion of highâ€‘quality organic data is essential to maintain knowledge coverage (Table 3).

- Pivotal Token Search (PTS) for tokenâ€‘level DPO
  - Innovation: A direct procedure to find and optimize the exact decision tokens that flip success probability (Figures 3â€“4; Section 4.3).
  - Why it matters: Reduces gradient noise and avoids inadvertently rewarding bad but lowâ€‘probability tokens, producing sizeable gains on reasoning metrics (Table 9: GPQA +9.2 points from SFT to final; MATH +3.3).

- Longâ€‘context midtraining with data that is natively long
  - Innovation: Rather than padding, the pipeline upweights genuinely long documents and crafts long synthetic data, combined with RoPE retuning (Section 3.3).
  - Why it matters: Strong longâ€‘context utility across HELMET tasks without changing architecture (Table 6), demonstrating techniqueâ€‘level rather than scaleâ€‘level gains.

- Fresh, contaminationâ€‘resistant validation of reasoning
  - Innovation: AMC 2024 evaluation performed after training and model selection, guarding against data leakage (Section 1.1; Appendix C).
  - Why it matters: The model averages 91.8/150 across AMCâ€‘10/12 November 2024 (Figure 1), outperforming larger frontier models at temperature 0.5â€”evidence for genuine generalization.

## 5. Experimental Analysis
- Evaluation setup
  - Benchmarks and frameworks
    - â€œsimpleâ€‘evalsâ€ suite at temperature 0.5 for MMLU, GPQA diamond, MATH, HumanEval, MGSM, SimpleQA, with standard prompts/extraction (Table 1; Section 6).
    - Additional: MMLUâ€‘Pro, HumanEval+, ArenaHard, IFEval (internal prompting); internal `PhiBench` of original teamâ€‘authored tasks (Section 5).
    - Long context: HELMET tasksâ€”Recall, RAG, Reâ€‘rank, ICL, QA, Summarizationâ€”at 8K and 16K lengths (Table 6; Section 3.3).
    - Safety: Multiâ€‘category RAI benchmark with GPTâ€‘4o judging (Table 10; Section 7.1).

- Headline quantitative results (Table 1; Section 6)
  - On reasoningâ€‘heavy benchmarks, `phiâ€‘4` is bestâ€‘inâ€‘class among similarâ€‘cost models and competitive with much larger models. Notably:
    - GPQA: â€œ56.1â€ vs `GPTâ€‘4o` â€œ49.1â€ and `Qwenâ€‘2.5â€‘14B` â€œ42.9â€.
    - MATH: â€œ80.4â€ vs `GPTâ€‘4o` â€œ66.3â€ and `Qwenâ€‘2.5â€‘14B` â€œ75.6â€.
  - Coding: HumanEval â€œ82.6â€ and HumanEval+ â€œ82.8â€, outperforming other openâ€‘weight models listed (Table 1).
  - Fresh AMC exams (Figure 1): 
    > Average score (max 150) over AMCâ€‘10/12 Nov 2024 with 100 runs at t=0.5: `phiâ€‘4` â€œ91.8â€, beating larger nonâ€“openâ€‘weight models (Section 1.1).

- Longâ€‘context performance (Table 6)
  - At 16K, `phiâ€‘4` shows strong ICL and summarization, e.g.:
    > `phiâ€‘4 (16K)`: ICL â€œ77.0 F1â€, Summ â€œ40.5 GPTâ€‘4oâ€‘scoredâ€;  
    > `GPTâ€‘4oâ€‘mini (16K)`: ICL â€œ78.4â€, Summ â€œ45.2â€.
  - Tradeâ€‘offs across tasks exist (e.g., reâ€‘ranking is moderate at 54.4 nDCG@10 at 16K).

- Postâ€‘training ablations and gains (Table 9; Section 4.5)
  - From SFT â†’ DPO Stageâ€‘1 (PTS) â†’ Final (Stageâ€‘1+2):
    > GPQA: 47.3 â†’ 53.6 â†’ 56.1  
    > MATH: 77.1 â†’ 80.5 â†’ 80.4  
    > ArenaHard (LLMâ€‘asâ€‘judge): 56.7 â†’ 66.5 â†’ 75.4  
  - Interpretation: PTS (Stageâ€‘1) drives reasoning gains; judgeâ€‘guided DPO (Stageâ€‘2) especially boosts judgeâ€‘based benchmarks (ArenaHard). The stages are complementary.

- Pretraining improvements vs `phiâ€‘3â€‘medium` (Table 2; Section 3)
  > Across MMLU (+3.0), MMLUâ€‘Pro (+10.3), HumanEval (+7.8), MBPP (+6.8), MATH (+8.9) already at the pretraining stage (4K context results).

- Data mixture ablations (Figures 2; Tables 3â€“5; Section 3)
  - Figure 2: 12 epochs over synthetic beats 4 epochs, despite the latter seeing more unique web tokensâ€”no overfitting observed.
  - Table 3: At 13B, purely synthetic improves many reasoning metrics over `phiâ€‘3â€‘medium`, but loses ~15 points on knowledgeâ€‘heavy TQA; adding web rewrites halves the TQA gap at the cost of smaller gains elsewhere.
  - Table 4: Syntheticâ€‘heavy allocations score slightly higher on reasoning average, but the final mixture balances knowledge and reasoning.

- Hallucination mitigation evidence (Figure 6; Section 4.4)
  > SimpleQA: â€œNot Attemptedâ€ rises from 3.2% (Base) to 15.8% (Final), while â€œIncorrectâ€ falls from 90.0% to 81.1%.  
  This reflects safer refusal behavior. The F1 metric in simpleâ€‘evals accordingly decreases, a known artifact at low accuracy (Appendix A.1).

- Safety (Table 10; Section 7.1)
  - Lower defect rates on jailbreak prompts (DR1) vs many peers (0.073), and competitive grounding (â€œ4.619â€). Harmful continuation/summarization remain low but nonâ€‘zero.

- Overall assessment of evidence
  - The combination of contamination controls (Appendix B), a truly fresh AMC evaluation (Appendix C), extensive ablations (Tables 2â€“5, 9), and crossâ€‘domain benchmarks (Table 1, Table 6, Table 10) gives strong support that the gains come from the data/postâ€‘training recipe, not leakage or formatting tricks. Where metrics are mixed (e.g., SimpleQA F1), the paper explains the behavioral tradeâ€‘off (refusal over guessing) and shows the intended shift directly (Figure 6).

## 6. Limitations and Trade-offs
- Dependence on oracles and nearâ€‘monotonicity for PTS
  - PTS requires tasks with reliable correctness checks (code tests, exact math answers). Creative, openâ€‘ended tasks lack such oracles.
  - The binaryâ€‘searchâ€‘like PTS may miss nonâ€‘monotone decision points; it finds all pivotal tokens only when success probability changes â€œnearâ€‘monotonicallyâ€ across the solution (Section 4.3).

- Knowledge coverage vs reasoning
  - Synthetic repetition powers reasoning but risks knowledge gaps; the final mixture mitigates this, yet smallâ€‘model factual coverage still lags very large models (Table 3; Section 8).

- Instruction following
  - `IFEval` is relatively weak (63.0 in Table 1), and the model can deviate from strict formats or detailed constraints (Section 8).

- Hallucinations not eliminated
  - The model can still confabulate plausible biographies (Section 8). Refusal training helps in shortâ€‘form factual QA but does not remove all hallucinations.

- Longâ€‘context tradeâ€‘offs
  - At 16K, some tasks (e.g., reâ€‘ranking) do not improve as much as others (Table 6), indicating uneven longâ€‘context benefits.

- Compute/data demands during training
  - While inference is efficient, producing large volumes of curated synthetic data, running selfâ€‘revision, and sampling for PTS is computeâ€‘intensive offâ€‘line. The paper does not detail the generation cost accounting.

- Chat vs singleâ€‘turn
  - The model is optimized for singleâ€‘turn Q&A; multiâ€‘turn adherence and dialog management may be less strong (Section 8).

## 7. Implications and Future Directions
- Field impact
  - `phiâ€‘4` exemplifies that careful synthetic data design, curriculum choices, and tokenâ€‘level preference optimization can deliver frontierâ€‘level reasoning in a small model. This shifts emphasis from parameter count to data/process engineering.

- What this enables next
  - Methodological
    - Extend PTS to richer â€œprocess supervisionâ€ (Section 4.3 Related Work), combining it with stepâ€‘level reward models and search over solution trees to provide tokenâ€‘ or spanâ€‘level signals beyond oracles.
    - Jointly optimize pretraining mixture anticipating postâ€‘training effects (Section 3.2 notes this as future work).
    - Develop PTSâ€‘like signals for tasks without crisp oracles via weak supervision, programmatic critics, or verifier models.

  - Capability expansion
    - Target instructionâ€‘following gaps with synthetic data tailored to formats and constraints (Section 6 and 8).
    - Integrate retrieval or tools to mitigate residual factual hallucinations (Section 8 suggests search augmentation).
    - Broaden multilingual depth using the same synthetic/PTS recipe for nonâ€‘English reasoning.

  - Applications
    - Onâ€‘device and edge deployments requiring low latency but high reasoning quality (education, coding assistance, math/science tutoring).
    - Enterprise settings needing safer behavior under uncertainty, leveraging the refusal training that reduces spurious answers (Figure 6; Appendix A.1).
    - Longâ€‘document workflows (legal/technical summarization, manyâ€‘shot ICL) supported by the midtraining approach (Table 6).

> Bottom line: The paper shows how to turn a compact model into a strong reasoner by engineering its data and postâ€‘training signalsâ€”especially via `PTS`â€”and validates real generalization on fresh exams (Figure 1). The approach opens a path to affordable, highâ€‘quality reasoning systems without resorting to massive model sizes or long, expensive inference chains.

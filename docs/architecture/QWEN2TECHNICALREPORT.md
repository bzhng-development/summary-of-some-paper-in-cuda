# QWEN2 TECHNICAL REPORT

**ArXiv:** [2407.10671](https://arxiv.org/abs/2407.10671)

## ðŸŽ¯ Pitch

Qwen2 introduces a robust suite of open-weight large language modelsâ€”ranging from lightweight (0.5B) to massive (72B) dense models and a 57B Mixture-of-Experts modelâ€”engineered for exceptional long-context processing, multilingual proficiency, and state-of-the-art coding and mathematics performance. By combining advanced architectural choices (like Grouped Query Attention and Dual Chunk Attention with YARN), a diverse 7-trillion-token training set, and scalable alignment strategies, Qwen2 dramatically elevates the performance bar for open models, rivaling proprietary systems on challenging benchmarks. Its open release empowers both research and real-world applications that demand high efficiency, reliability, and global language coverage, accelerating innovation across the AI ecosystem.

---

## 1. Executive Summary (2-3 sentences)
Qwen2 is a family of openâ€‘weight large language models spanning 0.5Bâ€“72B parameters plus a 57B Mixtureâ€‘ofâ€‘Experts (MoE) model, redesigned for long context, multilingual use, and strong coding/mathematics performance. It advances open modelsâ€™ efficiency and capability via architecture choices (e.g., Grouped Query Attention, Dual Chunk Attention with YARN), a 7â€‘trillionâ€‘token multilingual/code/mathâ€‘heavy pretraining pipeline, and a scalable alignment strategy (SFT + DPO/RLHF), achieving stateâ€‘ofâ€‘theâ€‘art results among open weights and approaching proprietary systems on many benchmarks (Sections 2â€“5).

## 2. Context and Motivation
- Problem/gap addressed
  - Openâ€‘weight models have narrowed the gap with proprietary systems but still trail on reasoning, coding, longâ€‘context handling, and multilingual coverage. There is also a deployment gap: models must be efficient (memory/latency) to serve long inputs and varied devices (Section 1).
  - Prior open releases (e.g., Llamaâ€‘3, Mixtral, Qwen1.5) improved core abilities but left room for: longer contexts with stable performance, stronger code/math reasoning, broader multilingual coverage, and instruction alignment quality (Sections 1, 3, 5).

- Why this matters
  - Realâ€‘world: document analysis, software engineering, and multilingual applications need long-context comprehension, reliable coding/maths, and broad language support (Sections 3.2, 5.2.3, 5.2.4).
  - Theoretical/engineering: designing efficient attention for long inputs (e.g., memoryâ€‘efficient KV caching) and robust postâ€‘training pipelines has broad implications for scalable LLM deployment (Sections 2.2, 4).

- Prior approaches and shortcomings
  - Dense Transformers with standard multiâ€‘head attention are memoryâ€‘intensive at long contexts; longâ€‘context extrapolation often degrades without specialized mechanisms (Section 2.2.1).
  - MoE models boosted efficiency but routing, expert granularity, and initialization affected stability and utilization (Section 2.2.2).
  - Alignment quality often demands expensive human annotation; many pipelines do not scale well or overfit to particular benchmark styles (Section 4.1).

- How Qwen2 positions itself
  - Architecture: memoryâ€‘lean attention (Grouped Query Attention) plus longâ€‘context mechanisms (Dual Chunk Attention + YARN) for up to 131K tokens with minimal perplexity degradation (Sections 2.2.1, 3.2).
  - Data: 7T tokens with expanded multilingual, code, and math; careful filtering and distribution tuning (Section 3.1).
  - Alignment: scalable SFT and DPO/RLHF with automated data synthesis (rejection sampling, execution feedback, constitutional signals) to reduce human load (Sections 4.1â€“4.3).
  - Range: from 0.5B edgeâ€‘deployable models to 72B flagship and a 57Bâ€‘total/14Bâ€‘active MoE designed to match ~30B dense performance at lower perâ€‘token compute (Sections 1, 2.2.3, Table 1, Table 3).

## 3. Technical Approach
Stepâ€‘byâ€‘step overview of how Qwen2 is built and aligned:

- Tokenizer (Section 2.1)
  - Uses the Qwen byteâ€‘level BPE tokenizer (151,643 regular + 3 control tokens), chosen for high compression and multilingual coverage. The embedding table has an â€œeffective sizeâ€ larger than the vocab count due to distributed training considerations.

- Dense model architecture (Sections 2.2.1, 2.2.3; Table 1)
  - Base: causal Transformer with standard components like Rotary Positional Embeddings (RoPE), SwiGLU activation, QKV bias, and RMSNorm with preâ€‘normalization for training stability.
  - Grouped Query Attention (GQA): replaces standard multiâ€‘head attention. In GQA, multiple query heads share a smaller set of key/value heads, cutting the KV cache size and improving throughput at inference. Example from Table 1: `Qwen2-72B` has 64 query heads but only 8 KV heads.
    - Why it matters: KV cache dominates memory for long contexts; reducing KV heads lowers memory/latency without sacrificing much quality (Section 2.2.1).
  - Longâ€‘context mechanisms:
    - Dual Chunk Attention (DCA): splits long sequences into manageable chunks; reproduces standard attention when input fits in one chunk, and otherwise tracks withinâ€‘ and crossâ€‘chunk positional relations (Section 2.2.1).
    - YARN: rescales attention weights to improve length extrapolation (Section 2.2.1).
    - RoPE base frequency changed from 10,000 to 1,000,000 to better extrapolate position encodings to long inputs (Section 3.2).
  - Result: â€œsubstantially lower KV size per token relative to Qwen1.5,â€ reducing memory footprint, especially for longâ€‘context inference (Section 2.2.3).

- Mixtureâ€‘ofâ€‘Experts (MoE) architecture (Section 2.2.2; Table 1)
  - Structure: replaces the FFN with n experts; a gating network `G(x)` outputs probabilities `p = softmax(G(x))`, and the model combines the topâ€‘k experts per token: `y = sum_{i in topâ€‘k(p)} p_i E_i(x)` (Equations (1)â€“(2)).
  - Fineâ€‘grained experts: more, smaller experts (e.g., 64 routed experts with 8 activated per token, plus 8 shared experts; see Table 1 for `Qwen2â€‘57Bâ€‘A14B`) to increase the diversity of expert combinations and utilization (Section 2.2.2).
  - Expert routing: mix of shared and routingâ€‘specific experts supports general knowledge plus specialization (Section 2.2.2).
  - Expert initialization (â€œupcycling with diversificationâ€):
    - Start from a dense modelâ€™s FFN; replicate it enough times to allocate the desired number and size of experts, shuffle parameters along the intermediate dimension to diversify, then randomly reinitialize 50% of each expertâ€™s parameters to encourage exploration (Section 2.2.2).

- Model configurations (Table 1)
  - Sizes: `0.5B`, `1.5B`, `7B`, `72B` dense; `57Bâ€‘A14B` MoE (57B total, 14B active parameters/token).
  - Trained tokens: `7T` for most dense models (except `0.5B` uses `12T`); MoE gets an additional `4.5T` on top of upcycling (Table 1; Sections 3.1, 2.2.3).
  - Embedding tying: enabled in the two smallest models, disabled for larger ones (Table 1).

- Preâ€‘training data and strategy (Section 3.1)
  - 7â€‘trillionâ€‘token multilingual mix with substantially more code, math, and nonâ€‘English text than Qwen1.5. Data quality improved via heuristic and modelâ€‘based filtering; distributions tuned on smaller models (Section 3.1).
  - A 12T dataset was explored but did not beat 7T; the project prioritized higherâ€‘quality 7T data for large models (Section 3.1).

- Longâ€‘context preâ€‘training (Section 3.2)
  - Context window extended from 4,096 to 32,768 tokens in the final phase of preâ€‘training, with more long documents; combined with YARN + DCA for effective processing up to 131,072 tokens and â€œminimal perplexity degradationâ€ in preliminary tests.

- Postâ€‘training (alignment) pipeline (Sections 4.1â€“4.3)
  - Data types:
    - Demonstrations `D = {(x_i, y_i)}` and preferences `P = {(x_i, y_i^+, y_i^â€“)}` (Section 4.1).
  - How the data is built:
    - Collaborative annotation (Section 4.1.1): automatic instruction ontology extraction with `InsTag`, instruction selection for diversity/complexity, â€œselfâ€‘evolutionâ€ to increase difficulty, and human ranking to produce both good demonstrations and positive/negative pairs.
    - Automated synthesis (Section 4.1.2):
      - Rejection sampling for math: generate multiple reasoning paths; keep correct/â€œreasonableâ€ ones as demonstrations, and pair correct vs incorrect for preferences.
      - Execution feedback for code and instruction following: generate code with tests; compile/execute to verify; also autoâ€‘generate Python checkers for constraint following.
      - Data repurposing for writing/roleplay: derive instructions from curated texts or character profiles, pairing them with highâ€‘quality outputs.
      - Constitutional feedback: use principles to synthesize aligned and misaligned responses for safety/value alignment.
  - SFT details (Section 4.2):
    - >500k instruction examples; train 2 epochs at 32,768 tokens; cosineâ€‘like LR decay from `7eâ€‘6` to `7eâ€‘7`, weight decay `0.1`, gradient clip `1.0`.
  - RLHF details (Section 4.3):
    - Offline DPO on P, then online DPO using reward models to select best/worst among multiple sampled responses from the current policy.
    - `Online Merging Optimizer` (OMo/OMO; Section 4.3) mitigates â€œalignment taxâ€ (the common drop in base capabilities after alignment) by merging the aligned updates in a way that preserves core skills.

## 4. Key Insights and Innovations
- Memoryâ€‘efficient longâ€‘context attention that actually scales (Sections 2.2.1, 3.2; Figure 1; Table 12)
  - Whatâ€™s new: integrating Dual Chunk Attention (chunked processing that matches full attention when short, but composes across chunks when long) with YARN (attention rescaling) and a higher RoPE base. This combination enables accurate retrieval and reasoning far beyond the 32K training window, up to 131Kâ€“256K in evaluation.
  - Why it matters: enables practical processing of long documents with lower memory via GQA (fewer KV heads) and with less quality loss than naive extrapolation.

- Fineâ€‘grained MoE with diversified upcycling (Section 2.2.2; Table 3)
  - Whatâ€™s new: more, smaller experts with both shared and routed specialists, plus an initialization that diversifies experts by shuffling and partially reinitializing. This supports richer expert combinations and better specialization.
  - Why it matters: `Qwen2â€‘57Bâ€‘A14B` performs like a ~30B dense model while activating only 14B parameters/token (Table 3), saving compute per token without sacrificing much quality.

- Scalable alignment data pipeline with automated synthesis (Section 4.1)
  - Whatâ€™s new: systematic ontologyâ€‘guided instruction selection and â€œselfâ€‘evolution,â€ combined with automated rejection sampling (math), execution feedback (code and instructionâ€‘following), data repurposing for creative tasks, and constitutional feedback for safety.
  - Why it matters: reduces reliance on costly human annotation while producing diverse, highâ€‘signal data that improves both core skills and instruction following (Table 6â€“Table 9, Table 14).

- Data quality and distribution tuning over raw scale (Section 3.1)
  - Insight: a 12T dataset did not beat a cleaner 7T dataset on large models; gains came from quality and distribution (more code/math/multilingual), not just raw tokens. This is a practical lesson for scaling laws.

## 5. Experimental Analysis
- Evaluation methodology (Section 5)
  - Base models: standard fewâ€‘shot/zeroâ€‘shot accuracy on diverse suites covering general knowledge (MMLU, MMLUâ€‘Pro, GPQA, TheoremQA, BBH, HellaSwag, Winogrande, ARCâ€‘C, TruthfulQA), coding (HumanEval, MBPP, EvalPlus, MultiPLâ€‘E), mathematics (GSM8K, MATH), Chinese (Câ€‘Eval, CMMLU), and multilingual (M3Exam, IndoMMLU, ruMMLU, translated MMLU; plus understanding, reasoning, math, and translation tests) (Section 5.1.1).
  - Instructionâ€‘tuned: same core skills plus alignment/instructionâ€‘following (MTâ€‘Bench, Arenaâ€‘Hard, MixEval, IFEval, AlignBench), coding with LiveCodeBench v1, and inâ€‘house automatic evals in Chinese and English (Sections 5.2.1â€“5.2.2).
  - Long context: Needleâ€‘inâ€‘aâ€‘Haystack (NIAH), NeedleBench (multiâ€‘needle + reasoning), LVâ€‘Eval (multiâ€‘evidence QA) (Section 5.2.3).
  - Multilingual human evaluation across 10 languages (1â€“5 rating by professional annotators) (Section 5.2.4).
  - Safety: multilingual jailbreakâ€‘style prompts across illegal, fraud, pornography, privacy; lower is better (Section 5.2.5).
  - Contamination: decontamination via nâ€‘gram and LCS filters; reâ€‘evaluate on strict nonâ€‘contaminated subsets to quantify impact (Section 5.2.6).

- Main quantitative results
  - Flagship base model (`Qwen2â€‘72B`, Table 2):
    - > â€œMMLU 84.2, MMLUâ€‘Pro 55.6, GPQA 37.9, TheoremQA 43.1, BBH 82.4â€
    - Coding: > â€œHumanEval 64.6, MBPP 76.9, EvalPlus 65.4, MultiPLâ€‘E 59.6â€
    - Math: > â€œGSM8K 89.5, MATH 51.1â€
    - Chinese: > â€œCâ€‘Eval 91.0, CMMLU 90.1â€
    - Multilingual category averages: > â€œExam 76.6, Understanding 80.7, Mathematics 76.0, Translation 37.8â€
    - Relative to `Llamaâ€‘3â€‘70B`, Qwen2â€‘72B is higher on MMLU (84.2 vs 79.5) and coding (HumanEval 64.6 vs 48.2) but similar on HellaSwag and Winogrande (Table 2).
  - MoE base (`Qwen2â€‘57Bâ€‘A14B`, Table 3):
    - Matches or beats ~30B dense baselines on many tasks; especially strong on coding/math:
      > â€œHumanEval 53.0, MBPP 71.9, EvalPlus 57.2, MultiPLâ€‘E 49.8; GSM8K 80.7, MATH 43.0â€
    - General knowledge near `Yiâ€‘1.5â€‘34B` (MMLU 76.5 vs 77.1) with much lower active parameters/token (14B vs 32B).
  - 7B base (`Qwen2â€‘7B`, Table 4):
    - Substantial gains over `Qwen1.5â€‘7B` and competitive with `Llamaâ€‘3â€‘8B`, especially in coding/math:
      > â€œHumanEval 51.2 (vs 33.5 in Llamaâ€‘3â€‘8B), MBPP 65.9, GSM8K 79.9, MATH 44.2â€
    - Strong Chinese (Câ€‘Eval 83.2; CMMLU 83.9) and multilingual understanding (Understanding 72.0).
  - Small base models (`Qwen2â€‘0.5B`, `Qwen2â€‘1.5B`, Table 5):
    - `Qwen2â€‘1.5B` outperforms `Gemmaâ€‘2B` and `Qwen1.5â€‘1.8B` on MMLU (56.5) and math (GSM8K 58.5); coding trails `Phiâ€‘2` but is stronger than other small baselines. Both Qwen2 small models excel on Chinese benchmarks (Câ€‘Eval/CMMLU).
  - Flagship instructionâ€‘tuned (`Qwen2â€‘72Bâ€‘Instruct`, Table 6):
    - Core skills: > â€œMMLU 82.3, MMLUâ€‘Pro 64.4, GPQA 42.4, TheoremQA 44.4â€
    - Coding: > â€œHumanEval 86.0, MBPP 80.2, MultiPLâ€‘E 69.2, LiveCodeBench v1 35.7â€
    - Math: > â€œGSM8K 93.2, MATH 69.0â€
    - Alignment: > â€œMTâ€‘Bench 9.12, Arenaâ€‘Hard 48.1, MixEval 86.7, IFEval 77.6, AlignBench 8.27â€
    - Beats `Llamaâ€‘3â€‘70Bâ€‘Instruct` on MMLUâ€‘Pro (+8.2), HumanEval (+4.3), and MATH (+18.6), and is close on MBPP (â€“2.1). It trails `Mixtralâ€‘8x22Bâ€‘Instruct` on GPQA (â€“7.3) but leads on most other fronts (Table 6).
  - MoE instructionâ€‘tuned (`Qwen2â€‘57Bâ€‘A14Bâ€‘Instruct`, Table 7):
    - Competitive with `Yiâ€‘1.5â€‘34Bâ€‘Chat` (~30B dense) and ahead of `Mixtralâ€‘8x7Bâ€‘Instruct` on most coding/alignment metrics:
      > â€œHumanEval 79.9 vs 45.1 (Mixtralâ€‘8x7B), LiveCodeBench 25.5 vs 12.3; MTâ€‘Bench 8.55; MixEval 82.3â€
  - 7B instructionâ€‘tuned (`Qwen2â€‘7Bâ€‘Instruct`, Table 8):
    - Strong coding/math relative to peers:
      > â€œHumanEval 79.9 (vs 62.2 in Llamaâ€‘3â€‘8Bâ€‘Instruct), GSM8K 85.7, MATH 52.9â€
    - Instruction following still lags `Llamaâ€‘3â€‘8Bâ€‘Instruct` on IFEval (54.7 vs 72.1) despite good MTâ€‘Bench (8.41) (Table 8).
  - Small instructionâ€‘tuned (Table 9):
    - `Qwen2â€‘0.5Bâ€‘Instruct` and `Qwen2â€‘1.5Bâ€‘Instruct` show large jumps vs Qwen1.5 on coding/math and IFEval (e.g., `1.5B` GSM8K 61.6; HumanEval 47.0; IFEval 29.0).
  - Long context (Figure 1; Table 12):
    - NeedleBench and LVâ€‘Eval verify that YARN + DCA significantly improve performance beyond 32K:
      > For `Qwen2â€‘72Bâ€‘Instruct` on LVâ€‘Eval, â€œ+YARN+DCAâ€ yields strong scores at long lengths (e.g., 128K/256K) compared to the vanilla model, and maintains high accuracy up to 32K (â€œdoes not change behavior within 32K tokens,â€ Table 12).
      > `Qwen2â€‘7Bâ€‘Instruct` shows degradation at 256K, but gains substantially from YARN+DCA (Table 12).
  - Multilingual human eval (Table 13):
    - `Qwen2â€‘72Bâ€‘Instruct` average 3.93/5 across 10 languages; substantially above `GPTâ€‘3.5â€‘Turbo` (3.16), close to `GPTâ€‘4â€‘Turbo` (3.98), and behind `Claudeâ€‘3â€‘Opus` (4.15).
  - Safety (Table 14; lower is better):
    - Qwen2 reduces harmful responses vs both `GPTâ€‘4` and `Mixtralâ€‘8x22Bâ€‘Instruct`:
      > â€œIllegal 0.00 (tie with GPTâ€‘4), Fraud 2.41 vs GPTâ€‘4 3.40, Pornography 22.91 vs GPTâ€‘4 23.63, Privacy 2.47 vs GPTâ€‘4 3.37.â€
  - Contamination analysis (Table 15):
    - Strict filtering indicates high â€œcontaminationâ€ percentages for some code/math sets (e.g., HumanEval 75%), but performance on nonâ€‘contaminated subsets changes very little for `Qwen2â€‘72Bâ€‘Instruct` (e.g., HumanEval +1.0), suggesting many are false positives (common snippets) rather than genuine leakage.

- Do the experiments support the claims?
  - Yes, across sizes and tasks, Qwen2 consistently improves over Qwen1.5 and is competitive with or better than leading open models; the 72B instructionâ€‘tuned model approaches proprietary systems in coding/math and alignment metrics (Tables 2, 6, 13, 14).
  - Longâ€‘context claims are supported by multiple tests and by the architectural design (Figure 1; Table 12; Sections 2.2.1, 3.2).

- Notable caveats and mixed results
  - On some â€œeasierâ€ English MC tasks, gains are small or slightly behind top baselines (e.g., HellaSwag/ARCâ€‘C in Table 2).
  - `Qwen2â€‘7Bâ€‘Instruct` underperforms `Llamaâ€‘3â€‘8Bâ€‘Instruct` on IFEval (instructionâ€‘constraint following) despite strong coding/math (Table 8).
  - `Qwen2â€‘57Bâ€‘A14B` trails `Yiâ€‘1.5â€‘34B` on MMLU by ~0.6 points but wins on many coding/math tasks (Table 3).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - The report emphasizes systemâ€‘level enhancements (attention, MoE, data/longâ€‘context, scalable alignment), not theoretical optimality of any single mechanism. The approach assumes highâ€‘quality filtering and balanced multilingual distributions are available (Section 3.1).
- Scenarios not fully addressed
  - Tool use, retrievalâ€‘augmented generation, and multimodality are out of scope here (even though the broader Qwen family includes multimodal models).
  - Fineâ€‘grained analysis of instruction following at small/medium sizes remains incomplete; `Qwen2â€‘7Bâ€‘Instruct`â€™s IFEval gap vs `Llamaâ€‘3â€‘8Bâ€‘Instruct` indicates room to improve constraint adherence (Table 8).
- Computational and data constraints
  - Training on 7T tokens with longâ€‘context phases is computeâ€‘intensive. While KV memory is reduced (GQA) and MoE saves perâ€‘token compute, these models still require substantial hardware for training and highâ€‘end GPUs for best inference speed (Sections 2.2.1â€“2.2.3, 3.2).
- English vs Chinese vs multilingual balance
  - In-house English evaluations show `Qwen2â€‘72Bâ€‘Instruct` slightly behind `Llamaâ€‘3â€‘70Bâ€‘Instruct` on some comprehension/coding averages (Table 11), suggesting remaining gaps in certain English domains despite strong multilingual and Chinese performance (Tables 10â€“11).
- Longâ€‘context edge cases
  - At 256K tokens, accuracy drops for all models, particularly smaller ones (Table 12). Longâ€‘range multiâ€‘needle reasoning remains challenging at extreme lengths.
- Safety remains hard
  - Although Qwen2 improves over GPTâ€‘4 on the tested categories, the pornography category still shows nonâ€‘trivial unsafe response rates (22.91%) (Table 14), reflecting the difficulty of perfectly safe generation across languages and prompts.
- Transparency on data sources
  - Section 3.1 describes quality improvements and distributions but does not enumerate exact data sources/weights; this can limit full reproducibility of the preâ€‘training corpus composition.

## 7. Implications and Future Directions
- How this changes the landscape
  - Qwen2 pushes openâ€‘weight LLMs closer to proprietary models on alignment (MTâ€‘Bench/Arenaâ€‘Hard), coding (HumanEval/LiveCodeBench), math (GSM8K/MATH), multilingual performance, and longâ€‘context processing (Tables 6, 12â€“14). The publicly released weights (Section 1, links on Hugging Face/ModelScope/GitHub) lower the barrier for research and production deployment across sizes.
  - The MoE design demonstrates that fineâ€‘grained experts with diversified upcycling can match ~30B dense performance with ~14B active parameters/token (Table 3), a practical blueprint for computeâ€‘efficient scaling.

- Followâ€‘up research enabled/suggested
  - Longâ€‘context: ablations on DCA vs YARN vs RoPE base; extending reliable accuracy to 256Kâ€“1M tokens and understanding failure modes for multiâ€‘needle/multiâ€‘hop reasoning (Section 5.2.3; Table 12).
  - Alignment: targeted improvement of instructionâ€‘constraint following for smaller models (e.g., raise IFEval at 7â€“9B, Table 8) using richer automated checkers and harder constraint curricula.
  - Data: systematic studies on data mixture effects (quality vs quantity, language balance, code/math proportions) to further validate the 7T>12T finding (Section 3.1).
  - MoE: exploration of routing strategies, expert specialization diagnostics, and scaling laws for fineâ€‘grained experts.

- Practical applications
  - Enterprise document processing and analytics with long contexts (up to 128K+ tokens) using the 7B/72B models with YARN+DCA (Figure 1; Table 12).
  - Software engineering copilots: strong coding results (HumanEval/LiveCodeBench) across the 7B/57B/72B instructionâ€‘tuned models (Tables 6â€“8).
  - Multilingual assistants: competitive human ratings across 10 languages (Table 13), with particularly strong Chinese performance (Tables 2, 10).
  - Edge and onâ€‘device AI: 0.5B/1.5B models tuned for portability while retaining useful coding/math and Chinese capabilities (Tables 5, 9).

> Overall, Qwen2 demonstrates that careful architectural choices (GQA, DCA+YARN), data quality/distribution, and scalable alignment pipelines can produce openâ€‘weight models that are efficient, longâ€‘context capable, multilingual, and highly competitive on coding and mathematicsâ€”closing a substantial portion of the gap to the best proprietary systems (Sections 2â€“5).

# DataCompâ€‘LM: In search of the next generation of training sets for language models

**ArXiv:** [2406.11794](https://arxiv.org/abs/2406.11794)
**Authors:** Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Chengâ€‘Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin Elâ€‘Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, Vaishaal Shankar
**Institutions:** 

## ğŸ¯ Pitch

DataComp-LM (DCLM) introduces a revolutionary, controlled benchmark for designing pre-training datasets that achieves top-tier performance using less compute. By isolating dataset effects and employing scalable, model-based filtering, DCLM sets a new standard in efficient language model training, enabling broader accessibility and innovation in LLM development.

---

## 1. Executive Summary
DataComp-LM (DCLM) introduces a controlled, reproducible benchmark to study how preâ€‘training datasets for large language models should be built. It supplies a 240â€‘trillionâ€‘token web corpus (from Common Crawl), fixed training/evaluation recipes across five compute scales, and showsâ€”through 416 experimentsâ€”that simple, scalable modelâ€‘based filtering yields a stateâ€‘ofâ€‘theâ€‘art open dataset (DCLMâ€‘BASELINE) that trains 7B models competitively with far less compute (Figure 1, Table 8).

## 2. Context and Motivation
- Problem and gap
  - Modern LLMs hinge on enormous training sets, but which curation strategies (filtering, deduplication, mixing sources) actually produce better models remains unclear because past comparisons confound data with model architecture, training hyperparameters, or compute budgets (Section 1).
  - Many strong openâ€‘weights models (e.g., Llama, Mistral, Gemma) do not release their training datasets or provide only coarse descriptions, hindering progress on dataâ€‘centric methods (Section 1).

- Importance
  - Practical: Better datasets reduce training cost while improving downstream capability; this is crucial as training costs escalate.
  - Scientific: Isolating the effect of data disentangles one of the most important drivers in LLM performance from other variables.

- Prior approaches and shortcomings
  - Heuristic cleaning, language detection, basic quality filtering, deduplication (e.g., RefinedWeb, C4, RedPajama, Dolma) exist, but were evaluated with differing compute and training setups, making fair comparison hard (Sections 2, 4.1).
  - Mixing â€œhighâ€‘qualityâ€ sources (Wikipedia, books, code) is standard, but whether mixing helps once web data is very well filtered is underâ€‘tested (Section 4.5).

- Positioning
  - DCLM provides a testbed where dataset interventions are the experimental variable and training/evaluation is held fixed. It spans five scales (from 412M to 7B parameters; Table 1), and two tracks (filtering from a public pool vs. mixing in external sources; Section 3.3). The benchmark is released with large pools, tools, recipes, and leaderboards (Section 3, Appendix D).

## 3. Technical Approach
DCLM is both a benchmark and a set of strong baselines that culminate in a new open dataset (DCLMâ€‘BASELINE). The pipeline can be understood stepâ€‘byâ€‘step.

1) Building the public data pool (DCLMâ€‘POOL)
- Source: All Common Crawl WARCs prior to 2023 (to avoid recent synthetic web content), reâ€‘extracted with `resiliparse` (a fast HTML text extractor) rather than using Common Crawlâ€™s WET text (Section 3.1).
- Scale: ~200B documents, 370 TB gzipped, totaling 240T GPTâ€‘NeoX tokens (Section 3.1).
- Decontamination tooling: DCLM ships code to measure or remove overlaps with evaluation sets; the pool itself is not preâ€‘decontaminated (Section 3.1, Section 4.6).

2) Competition scales and fixed training recipes
- Five compute scales (Table 1) define: number of parameters N, train tokens D (set as `20 Ã— N Ã— Chinchilla multiplier`), FLOPs (â‰ˆ6ND), and pool subset sizes. Examples:
  - `400Mâ€‘1x`: 412M parameters, 8.2B tokens, 469Bâ€‘token pool.
  - `7Bâ€‘2x`: 6.9B parameters, 276B tokens, 15.7Tâ€‘token pool.
- Training recipe: standard decoderâ€‘only Transformer in the OpenLM framework, with scaleâ€‘specific hyperparameters kept fixed to isolate dataset effects (Section 3.4; Appendix F for architecture details like qkâ€‘LayerNorm and SwiGLU).

3) Two benchmark tracks (Section 3.3)
- Filtering track: Start from the scaleâ€‘specific subset of DCLMâ€‘POOL and produce a dataset by filtering/processing only that pool.
- Mixing track: Combine pool data with any other sources (e.g., Wikipedia, Stack Exchange), within disclosure rules.

4) Evaluation suite and metrics (Section 3.5; Appendix G)
- 53 zero/fewâ€‘shot tasks spanning knowledge, QA, reasoning; implemented via LLMâ€‘Foundry.
- Three metrics:
  - `MMLU 5â€‘shot accuracy` (popular capability measure).
  - `CORE centered accuracy`: average, over 22 lowâ€‘variance tasks, of perâ€‘task accuracies linearly rescaled so that 0 = random guessing and 1 = perfect (stable even for small models).
  - `EXTENDED centered accuracy`: same centering but averaged across all 53 tasks.

5) Designing DCLMâ€‘BASELINE through empirical choices (Section 4; Figure 4)
- Text extraction: Compare `resiliparse`, `trafilatura`, WET; tight extractors (`resiliparse` or `trafilatura`) substantially outperform WET downstream (Table 3).
- Deduplication: Evaluate MinHash + suffix arrays vs. a scalable `Bloom filter` approach (BFF) that handles nearâ€‘duplicates at document/paragraph levels; performance is comparable, BFF scales better >10 TB (Section 4.3; Tables 18â€“19; Appendix L).
- Modelâ€‘based filtering: Run many filters and find that a simple `fastText` bigram classifier trained with carefully chosen â€œpositiveâ€ examples is best (Sections 4.4; Tables 4â€“5).
- Mixing: Test whether adding curated nonâ€‘CC sources helps; find improvements only when the CC baseline is weakâ€”mixing can harm once CC data is strongly filtered (Table 6).
- Decontamination checks: Removing detected MMLU/HellaSwag overlaps from training does not reduce performance, suggesting contamination is not driving gains (Table 7; Appendix O).

How the key parts work

- `fastText` classifier (Section 4.4; Appendix J)
  - Train a linear classifier over word and bigram features to score documents by â€œquality.â€ Positives: instructionâ€‘style datasetsâ€”OpenHermesâ€‘2.5 and highâ€‘karma ELI5 answers; negatives: random web pages from a RefinedWebâ€‘like pool.
  - Filter by keeping only the top x% scoring documents (best found at 10%, Table 5).
  - Intuition: instructionâ€‘style positives are diverse, wellâ€‘structured, and edited for clarity; their statistical signature helps distinguish broadly useful text while avoiding overfitting to narrow domains like Wikipedia.

- `Bloom filter` nearâ€‘deduplication (Section 4.3; Appendix L)
  - A Bloom filter is a memoryâ€‘efficient probabilistic set that supports â€œhave we seen this nâ€‘gram?â€ with no false negatives and low false positives. DCLM extends AI2â€™s BFF to:
    - Tokenize and split each page into paragraphs; compute nâ€‘grams (e.g., 13â€‘token nâ€‘grams).
    - If a paragraph has a high fraction of alreadyâ€‘seen nâ€‘grams (threshold â‰ˆ 0.8), drop it; if the whole document exceeds threshold, drop the document.
  - Benefit: scales to multiâ€‘terabyte corpora, approximating the effect of MinHash + suffix arrays at a fraction of the cost (Tables 18â€“19).

- `Centered accuracy` metric (Section 3.5; Appendix G)
  - For each task, raw accuracy is transformed so random guessing maps to 0 and perfect to 1; averaging then expresses crossâ€‘task progress fairly even when task entropies differ and small models are noisy.

## 4. Key Insights and Innovations
1) A controlled, multiâ€‘scale data benchmark where ranking is stable across scales
- Novelty: DCLM fixes models, hyperparameters, and compute; only data varies. It shows dataset rankings at small scales predict largeâ€‘scale performance (Figure 3): Pearson r = 0.838 (400Mâ€‘1x), 0.956 (1Bâ€‘1x), 0.982 (3Bâ€‘1x) vs. 7Bâ€‘1x.
- Significance: Enables lowâ€‘cost iteration on data curation with high confidence it will transfer.

2) Modelâ€‘based filtering beats heuristics, and the best filter is surprisingly simple
- Finding: `fastText` with bigrams, trained on OHâ€‘2.5 + ELI5 positives, outperforms:
  - Perplexity pruning, topâ€‘k logits scoring, PageRank, semantic deduplication, embedding classifiers, and LLMâ€‘judged quality (â€œAskLLMâ€), at the 1Bâ€‘1x scale (Table 4).
- Design detail that matters: the positive set and the threshold (10% best) strongly affect results (Table 5).
- Significance: Points to a practical, reproducible, and inexpensive recipe for largeâ€‘scale filtering.

3) Text extraction quality has large downstream impact
- Result: After applying the same RefinedWebâ€‘like heuristics, `resiliparse` or `trafilatura` yields +2.5 to +3.8 CORE points over WET extraction at 1Bâ€‘1x (Table 3).
- Significance: Early pipeline choices (HTML extraction) are as important as later filters.

4) Deduplication that scales: Bloom filter nearâ€‘dedup matches MinHash + suffix arrays
- At 7Bâ€‘2x, BFF (min nâ€‘gram 13) achieves CORE 45.3 and MMLU 44.3 vs. MinHash+SAâ€™s 45.5 and 44.4 (Table 19), while being easier to scale beyond 10 TB (Section 4.3).
- Significance: A clear path to practical nearâ€‘dedup in trillionâ€‘token settings.

5) â€œHighâ€‘quality data mixingâ€ is not universally beneficial
- Mixing Wikipedia/Books/ArXiv/GitHub improves weak CC subsets but hurts a strong, filtered CC dataset: DCLMâ€‘BASELINEâ€™s CORE drops from 31.1 to 29.9 when mixed (Table 6).
- Significance: With sufficiently strong filtering, CCâ€‘only can be bestâ€”challenging a common assumption.

6) Human quality judgments do not align with what makes better pretraining data
- ROCâ€“AUC of various filters on humanâ€‘labeled â€œgood/badâ€ pages does not correlate with gains on CORE, SQuAD, StrategyQA (Appendix N; Figure 9). The LLMâ€‘grader AskLLM achieves higher agreement with annotators but worse downstream performance than fastText (Table 4; Appendix N).
- Significance: â€œHumanly good contentâ€ is not the same as â€œcontent that trains LLMs well.â€

## 5. Experimental Analysis
- Evaluation setup
  - Datasets/metrics: 53 tasks; key metric is CORE centered accuracy (22 stable tasks), plus MMLU 5â€‘shot and EXTENDED centered accuracy (53 tasks) (Section 3.5; Appendix G).
  - Baselines: C4, RefinedWeb, RedPajama, Dolmaâ€‘V1, FineWebâ€‘Edu, OLMo, LLM360/Amber, MAPâ€‘Neo, and closedâ€‘data models for context (Figure 1; Tables 2, 8, 33).
  - Fixed training: OpenLM decoderâ€‘only Transformers with scaleâ€‘specific hyperparameters (Section 3.4; Appendix F).

- Main quantitative results
  - Text extraction (1Bâ€‘1x, Table 3):
    > `resiliparse` CORE 24.1 vs. `trafilatura` 24.5 vs. WET 20.7.
  - Modelâ€‘based filtering methods (1Bâ€‘1x, Table 4):
    > `fastText OHâ€‘2.5+ELI5` CORE 30.2 beats `perplexity` 29.0, `topâ€‘k logits` 29.2, `AskLLM` 28.6, `BGEâ€‘linear` 27.2, `PageRank` 26.1.
  - fastText ablations (7Bâ€‘1x, Table 5, 14):
    > Positives matter: `OHâ€‘2.5+ELI5` CORE 41.0 vs. `Wikipedia` 35.7, `OpenWebText2` 34.7, `GPTâ€‘3 Approx` 37.5.  
    > Threshold matters: topâ€‘10% CORE 41.0 > topâ€‘15% 39.8 > topâ€‘20% 38.7.  
    > Bigrams help: unigramsâ€‘only CORE 40.0 vs. bigrams+unigrams 41.0 (Table 14).
  - Dedup at scale (7Bâ€‘1x and 7Bâ€‘2x; Tables 18â€“19):
    > BFF (min nâ€‘gram 13) â‰ˆ MinHash+SA: at 7Bâ€‘2x MMLU 44.3/CORE 45.3 vs. 44.4/45.5.
  - Mixing (1Bâ€‘1x, Table 6):
    > Mixing helps weaker CC (C4 +2.2 CORE), hurts DCLMâ€‘BASELINE (âˆ’1.2 CORE).
  - Decontamination (7Bâ€‘2x, Table 7):
    > Removing MMLU/HellaSwag overlaps: MMLU 51.8 â†’ 52.7, HellaSwag 77.9 â†’ 78.4 (no drop).  
    > Broader contamination analysis shows similar or better cleanliness vs. FineWebâ€‘Edu and Dolmaâ€‘V1.7 (Appendix O, Table 25; Figures 11â€“12).

- Final largeâ€‘scale model (Table 8; Section 5)
  - Train 7B on 2.6T tokens: 70% DCLMâ€‘BASELINE (tighter filter in coolâ€‘down), 30% math/code (StarCoder, ProofPile2). With fixed recipes, achieves:
    > CORE 57.1, MMLU 63.7, EXTENDED 45.4.
  - Comparisons:
    - Beats openâ€‘data peers (e.g., MAPâ€‘Neoâ€‘7B: CORE 50.2, MMLU 57.1, EXTENDED 40.4).
    - Comparable to closedâ€‘data `Mistralâ€‘7Bâ€‘v0.3` (CORE 57.0, MMLU 62.7, EXTENDED 45.1) and near `Llamaâ€‘3 8B` (57.6/66.2/46.3), while using â‰ˆ6.6Ã— less training tokens than Llamaâ€‘3 8B (2.6T vs. 15T; Section 1, Table 8).
  - Instruction tuning on public data preserves strong base performance and yields competitive AlpacaEval2.0 winâ€‘rates (Appendix P, Table 26).

- Robustness and supporting evidence
  - Rankings persist across architectures (Gemmaâ€‘like and Mambaâ€‘like) at 1B scale (Appendix I; Figure 6).
  - Rankings stable across hyperparameters; gains from better datasets stack with gains from better training settings (Appendix H; Tables 12â€“13).
  - LightEval vs LLMâ€‘Foundry MMLU correlation study indicates evaluation conclusions are consistent, with smallâ€‘scale sensitivity differences (Appendix G.2; Figure 5).

- Do the experiments support the claims?
  - Yes. The paper isolates data effects by fixing training/evaluation across 416 runs, uses multiple scales, ablates each major pipeline stage (extraction, dedup, filtering, mixing), and crossâ€‘checks with contamination and alternative eval setups. The final 7B result substantiates that the curated dataset competes with much more compute.

## 6. Limitations and Trade-offs
- Scope and compute
  - Models up to 7B parameters; training beyond 7B and broader compute sweeps remain future work (Section 6).
  - Runâ€‘toâ€‘run variance not exhaustively analyzed at the largest scales (Section 6).

- Tokenizer and domains
  - Primarily uses GPTâ€‘NeoX tokenizer; other tokenizers may change multilingual/math behavior (Section 6).
  - Focus is language understanding; math/code abilities require additional domainâ€‘specific pretraining (Section 6, Table 8).

- Data assumptions
  - DCLMâ€‘POOL draws entirely from preâ€‘2023 Common Crawl; while very large, it may not represent all domains and languages and can include PII or toxic content (Appendix U Datasheet; Section 3.1). Heuristic and modelâ€‘based filters reduce but do not eliminate such content.

- Design dependence
  - The fastText approach depends on the choice of positive set and threshold (Table 5): shifting either can reduce gains.
  - Bloom filter hyperparameters (nâ€‘gram size, thresholds, sharding) affect removal rates and document statistics (Appendix L).

- Evaluation breadth
  - While 53 tasks are broad, they may still underâ€‘represent some specialized capabilities (e.g., complex tool use, longâ€‘horizon reasoning). The paper partially addresses longâ€‘context in Appendix Q.2 with continual learning to 8k context.

## 7. Implications and Future Directions
- How this work changes the field
  - Establishes a common, scalable yardstick for data curationâ€”similar to how model benchmarks shaped architecturesâ€”making data a firstâ€‘class, testable design space.
  - Demonstrates that careful filtering of raw web text can rival or surpass mixed â€œhighâ€‘qualityâ€ corpora, reorienting effort toward scalable, dataâ€‘centric methods (Figures 1, 4; Tables 4â€“6, 8).

- Followâ€‘up research enabled/suggested
  - Better filtering networks: beyond fastText, explore lightweight neural filters optimized for recall/precision tradeâ€‘offs at terascale.
  - Targeted data selection: conditional filters for domains (math, code, safety, multilingual), curriculum schedules, and dynamic mixture optimization (Appendix B cites DoReMi, DoGE, MixMax lines of work).
  - Fairness/safety/multilingual tracks: extend DCLM with explicit bias, toxicity, and multilingual objectives (Section 6; Appendix S).
  - Efficient decontamination: taskâ€‘aware overlap removal at scale, and robust evaluation under paraphrase/rephrasing (Appendix O).

- Practical applications
  - Organizations can use the DCLM pipeline to build costâ€‘efficient training sets, particularly when compute is limited.
  - The open DCLMâ€‘BASELINE dataset provides a strong default for training 7Bâ€‘scale models or as a base to mix with domainâ€‘specific data (e.g., legal, biomedical, code).

---

Selected references to paper artifacts
- Pools/Workflow: Section 3; Figure 2.
- Scales: Table 1; Figure 3.
- Extraction: Section 4.2; Table 3; Appendix K.
- Dedup: Section 4.3; Tables 18â€“19; Appendix L.
- Filtering: Section 4.4; Tables 4â€“5, 14; Appendix J.
- Mixing: Section 4.5; Table 6; Appendix M.
- Decontamination: Section 4.6; Table 7; Appendix O.
- Final 7B model: Section 5; Table 8; Appendices Q, P.
- Stability across settings: Appendix H (Tables 12â€“13), Appendix I (Figure 6).
- Human judgment: Appendix N (Figure 9).

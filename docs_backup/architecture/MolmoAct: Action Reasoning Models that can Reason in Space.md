# MolmoAct: Action Reasoning Models that can Reason in Space

**ArXiv:** [2508.07917](https://arxiv.org/abs/2508.07917)
**Authors:** Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, YiÂ RuÂ Wang, Sangho Lee, Winson Han, Wilbert Pumacay, AngelicaÂ Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna
**Institutions:** 

## ğŸ¯ Pitch

MolmoAct pioneers a three-stage Action Reasoning Model that decouples spatial reasoning from action, using depth maps and 2D trajectories to enable transparent and steerable robot control. This framework significantly enhances adaptability and robustness in both simulated and real-world settings, offering a novel dataset to boost generalist performance and setting a new standard for explainability and precision in robotic manipulation.

---

## 1. Executive Summary (2-3 sentences)
MolmoAct introduces an Action Reasoning Model (ARM) that reasons in space before acting: it first predicts a compact depth representation of the scene, then sketches a 2D endâ€‘effector trajectory, and finally emits lowâ€‘level control actions. This threeâ€‘stage, tokenâ€‘based pipeline makes robot behavior explainable and steerable, and yields strong results in simulation and the real world, while the paper also releases an open 10kâ€‘trajectory dataset that improves generalist performance.

## 2. Context and Motivation
- Gap addressed:
  - Most robot foundation models directly map images and language to control actions, with little or no interpretable intermediate reasoning. This limits adaptation, generalization, and transparency (Introduction, p.1â€“2).
  - Language-only â€œreasoningâ€ is often too abstract for precise manipulation: it lacks depth understanding and loses geometric detail when trajectories are described in words (Sec. 2.3, p.5â€“6).

- Why important:
  - Real robots must make spatially consistent, physically grounded decisions (e.g., distance to objects, collision-free reaches). Without 3D awareness and explicit motion plans, policies are brittle and hard to steer (Fig. 1; Sec. 2.3).
  - Explainability matters for safety and user trust: the ability to inspect a depth map and a planned path clarifies why a robot acts a certain way (Fig. 1; Sec. 2.4).

- Prior approaches and shortcomings:
  - Vision-Language-Action (VLA) models (e.g., RTâ€‘1/RTâ€‘2, OpenVLA, Ï€0/Ï€0â€‘FAST, GR00T, Magma) excel at endâ€‘toâ€‘end action prediction but provide limited visibility into spatial reasoning and struggle with OOD generalization (Intro; Table 1; Sec. 5.3).
  - Works that add â€œreasoningâ€ typically do so in language or latent form (e.g., ECoT, CoTâ€‘VLA, ThinkAct). These are harder to ground precisely in 3D space (Sec. 6.3).

- Positioning:
  - MolmoAct reframes â€œthink-before-you-actâ€ as â€œreason-in-spaceâ€: predict depth tokens â†’ sketch a 2D path (â€œvisual reasoning traceâ€) â†’ output actions (Sec. 2.3; Eq. 4). Each stage is decodable and editable, enabling explanation and direct trajectory steering (Fig. 1; Sec. 2.4).
  - The paper also contributes an open, midâ€‘training dataset (10,689 trajectories; 93 manipulation tasks) that measurably boosts general performance (Sec. 3.2; Fig. 4; Sec. 5.4).

## 3. Technical Approach
MolmoAct builds a threeâ€‘stage autoregressive pipeline on top of a visionâ€‘language backbone (Molmo) and trains it endâ€‘toâ€‘end with nextâ€‘token prediction. The pipeline:

1) Visionâ€‘Language Backbone (Sec. 2.1; Appendix A)
- Backbone choices:
  - â€œDâ€ variant: SigLIP2 vision encoder + Qwen2.5â€‘7B LLM (MolmoActâ€‘7Bâ€‘D).
  - â€œOâ€ variant: OpenAI CLIP ViTâ€‘L/14 + OLMo2â€‘7B (MolmoActâ€‘7Bâ€‘O; the most open option).
- Images are turned into vision tokens via ViT + a connector (multiâ€‘layer features + 2Ã—2 attention pooling), then concatenated with text tokens for the LLM (Appx. A.3â€“A.4).
- Multiâ€‘image inputs (e.g., front + wrist camera) are supported by tagging and concatenation (Appx. A.5).

2) Action tokenization that respects geometry (Sec. 2.2)
- Problem: A common trick maps continuous actions to 256 bins and reuses arbitrary â€œrareâ€ text tokens, which ignores the fact that neighboring bins are similar.
- MolmoActâ€™s solution:
  - Normalize each action dimension by dataset quantiles, discretize into 256 bins.
  - Create an action vocabulary `Vaction` by monotonically assigning adjacent byteâ€‘level BPE symbols (taken from the end of the Qwen2 tokenizer) to adjacent action bins, so neighboring bins start with nearby embeddings.
- Why this helps: embeddings for adjacent tokens begin close in representation space, giving a smoother learning signal and faster training convergence (Sec. 2.2). The paper contrasts its ~9â€“10k H100 GPUâ€‘hours preâ€‘training with GR00T N1.5â€™s ~50k GPUâ€‘hours (Sec. 2.2 vs. Sec. 4.1; minor discrepancy: 9,216 vs. 9,728 GPUâ€‘hours).

3) Reasonâ€‘inâ€‘space chain (Sec. 2.3; Eqs. 1â€“4)
- Stage A â€” Depth Perception Tokens:
  - Goal: give the policy an internal 2.5D scene sense without a depth sensor at test time.
  - How: A specialist depth estimator (Depth Anything v2) produces depth maps that are VQâ€‘encoded into a string of M=100 indices from a codebook of N=128 entries (VQVAE; Appx. C; Sec. 3.1).
  - The model learns to predict these tokens from the RGB image and instruction:
    - Vocabulary `Vdepth = {âŸ¨DEPTH_STARTâŸ©, âŸ¨DEPTH_kâŸ©, âŸ¨DEPTH_ENDâŸ©}`, k=1..128 (Eq. 1â€“2).
    - Each depth string d is length 100 + start/end tokens.
  - Interpretation: the depth string is a compact, decodable summary of the sceneâ€™s depth map (Fig. 1; Sec. 2.3).

- Stage B â€” Visual Reasoning Trace:
  - A short polyline (L=1..5 points) on the image plane, with integer pixel coordinates normalized to [0..255], denotes the planned endâ€‘effector path from the current gripper location to future waypoints (Eq. 3; Sec. 2.3).
  - Training targets come from automatically locating the gripper in each frame (using the Molmo VLMâ€™s 2D pointing ability) and subâ€‘sampling the future trajectory between the current time and episode end (Sec. 3.1 â€œVisual Reasoning Traceâ€).
  - Why this matters: it aligns each lowâ€‘level action with a concrete spatial waypoint and retains geometric detail that language would lose.

- Stage C â€” Action Tokens:
  - With depth tokens d and trace Ï„ as context, the model predicts the next control action a, one actionâ€‘dimension token at a time.
  - Full factorization (Eq. 4):
    - First predict all depth tokens: âˆ p(di | I, T, d<i)
    - Then trace tokens: âˆ p(Ï„j | I, T, d, Ï„<j)
    - Finally action tokens: âˆ p(ak | I, T, d, Ï„, a<k)

4) Steerability at inference (Sec. 2.4; Eq. 5; Fig. 1 right)
- Users can draw an onâ€‘image trace Ï„ (1â€“5 points). The system overlays the sketch on the image to form Iâº = I âŠ• Ï„, and then directly predicts actions conditioned on Iâº:
  - p(a | Iâº, T) = âˆ p(ak | Iâº, T, a<k) (Eq. 5).
- This gives precise, lowâ€‘friction controlâ€”often more reliable than rephrased language (Sec. 5.6; Fig. 9).

5) Data curation and training (Sec. 3â€“4; Fig. 2â€“3)
- Converting robot data into â€œaction reasoningâ€ format (Sec. 3.1):
  - For each timestep (I, T, a): produce groundâ€‘truth depth tokens (from the VQVAEâ€™d depth map) and a visual trace (from VLMâ€‘based 2D gripper points concatenated over time).
  - Also build three auxiliaries: depthâ€‘only prediction, traceâ€‘only prediction, and trajectoryâ€‘conditioned action (I, T, Ï„ â†’ a), which help teach each subskill (Sec. 3.1).
- MolmoAct Dataset (midâ€‘training; Sec. 3.2; Fig. 4):
  - 10,689 trajectories, 93 tasks, 3 cameras (two side, one wrist), avg. length 112 steps, spanning realistic home + tabletop tasks.
- Training schedule (Sec. 4; Fig. 2â€“3; Appx. B):
  - Preâ€‘training (26.3M samples) on an Openâ€‘Xâ€‘Embodiment subset (RTâ€‘1, BridgeData V2, BCâ€‘Z) + auxiliaries + 2M multimodal web samples (Fig. 3, right). 256 H100s, 100k steps, batch 512 (~9.7k GPUâ€‘hours; Sec. 4.1).
  - Midâ€‘training on the MolmoAct Dataset (1M actionâ€‘reasoning + 1M trajectoryâ€‘conditioned samples). 128 H100s, 50k steps (~2.3k GPUâ€‘hours; Sec. 4.2).
  - Postâ€‘training (task adaptation): Lowâ€‘Rank Adaptation (LoRA rank=32, Î±=16) and â€œaction chunkingâ€ (predict K=8 future action steps per inference cycle) for both simulation and real robots (Sec. 4.3; Appx. B).

## 4. Key Insights and Innovations
1) Spatial chainâ€‘ofâ€‘thought that is fully decodable (fundamental innovation)
   - Instead of latent or linguistic reasoning, MolmoAct reasons via explicit spatial tokens: depth â†’ 2D path â†’ action (Fig. 1; Sec. 2.3). Each piece can be visualized (depth map, overlaid trace) and edited, which is rare among VLAs.
   - Significance: explainability, testâ€‘time steering, and better grounding for manipulation.

2) Depth perception tokens distilled from a specialist model (not just RGB; substantial innovation)
   - A VQVAE compresses dense depth into a compact 100â€‘token string (Eq. 1â€“2; Sec. 3.1). The ARM learns to predict this string from RGB alone, internalizing 3D cues without a depth sensor.
   - Impact: improved spatial understanding supports more precise lowâ€‘level control (Sec. 2.3; Fig. 1). This mirrors recent â€œperception tokensâ€ ideas in MLLMs but is operationalized for control.

3) Ordinalâ€‘aware action tokenization (useful design advance)
   - Adjacent discrete bins are mapped to adjacent byteâ€‘BPE symbols, providing a better embedding initialization than arbitrary rare tokens (Sec. 2.2).
   - Impact: smoother optimization and lower training time; the paper contrasts its ~9â€“10k GPUâ€‘hours with a 5Ã— larger training budget reported for GR00T N1.5 (Sec. 2.2).

4) A practical, precise steering interface (new capability)
   - Users draw a short polyline; the model follows it in closed loop (Sec. 2.4; Fig. 9). In tests, trace steering outperforms openâ€‘ended language reâ€‘prompting by 33% on the â€œpick up bowlâ€ task (Sec. 5.6; Fig. 9 left; Table 23).

5) Releasing an open midâ€‘training dataset and full stack (community impact)
   - The MolmoAct Dataset (10k+ trajectories) and code enable reproducibility; midâ€‘training on it boosts performance by â‰ˆ5.5% on average in realâ€‘world tasks (Sec. 5.4; Fig. 6b; Table 22).

## 5. Experimental Analysis
Setup and baselines
- Benchmarks and settings:
  - SimplerEnv (Google Robot): visual matching (inâ€‘distribution) and â€œvariant aggregationâ€ (OOD visual perturbations). Zeroâ€‘shot after preâ€‘training and fineâ€‘tuning variants are compared (Sec. 5.1; Table 1; Appx. D.1).
  - LIBERO (Franka sim): four suites (Spatial, Object, Goal, Long). Models are postâ€‘trained with LoRA and evaluated with action chunking (Sec. 5.2; Table 2; Appx. D.2).
  - Realâ€‘world singleâ€‘arm and bimanual Franka: 6 tasks; fineâ€‘tune from 50 demos/task; measure task progression (0..1) over 25 trials each (Sec. 5.2; Fig. 5; Appx. D.3).
  - OOD generalization in the real world: multiâ€‘task setting with language/spatial/distractor/novelâ€‘object variations (Sec. 5.3; Fig. 6a; Table 21; Appx. D.4).
  - Ablation on midâ€‘training dataset: three realâ€‘world tasks (close_lid, rotate_pot, pour_tea) with and without MolmoAct Dataset midâ€‘training (Sec. 5.4; Fig. 6b; Table 22; Appx. D.5).
  - Human preference for instruction following and trace generation: arenaâ€‘style pairwise ratings (Sec. 5.5; Fig. 7â€“8; Appx. D.6).
  - Steerability study: ambiguous â€œpick up bowlâ€ with language vs. trace steering (Sec. 5.6; Fig. 9; Table 23; Appx. D.7).

Main quantitative results (selected)
- SimplerEnv (Table 1):
  - Zeroâ€‘shot visual matching: 
    > â€œMolmoAct (zeroâ€‘shot) â€¦ 70.5%â€  
    This tops several strong systems (e.g., GR00T N1.5 fineâ€‘tuned: 52.4% visual matching avg; Magma: 68.4%).
  - After fineâ€‘tuning (RTâ€‘1 subset): 
    > â€œMolmoAct (fineâ€‘tuned) â€¦ 71.6% visual matching, 72.1% variant aggregation.â€  
    The 72.1% in variant aggregation is +7.8 points over RTâ€‘2â€‘X (64.3%), the next best listed in that column.

- LIBERO (Table 2):
  - Overall average:
    > â€œMolmoActâ€‘7Bâ€‘D â€¦ 86.6% avgâ€  
    Slightly higher than Ï€0â€‘FAST (85.5%) and above strong â€œreasoningâ€ baselines (e.g., ThinkAct 84.4%, CoTâ€‘VLA 83.9%).
  - Longâ€‘horizon suite:
    > â€œMolmoActâ€‘7Bâ€‘D â€¦ 77.2%â€  
    +6.3 points over ThinkAct (70.9%), the second best in this column; this supports the value of explicit spatial plans (Table 2).

- Realâ€‘world fineâ€‘tuning (Fig. 5; Tables 15â€“20):
  - Singleâ€‘arm tasks (put bowl in sink, wipe table, table bussing):  
    > â€œ+10% average task progression over Ï€0â€‘FAST.â€  
    For instance, Wipe Table averages 1.00 vs. Ï€0â€‘FASTâ€™s 0.817 (Table 19).
  - Bimanual tasks (set table, lift tray, fold towel):  
    > â€œ+22.7% average improvement over Ï€0â€‘FAST.â€  
    E.g., Fold Towel averages 0.80 vs. 0.52 (Table 15).

- OOD generalization (Fig. 6a; Table 21):
  - In a multiâ€‘task realâ€‘world setting with language, spatial, distractor, and novelâ€‘object perturbations,  
    > â€œMolmoAct surpasses Ï€0â€‘FAST by +23.3% on average task progression.â€  
    Table 21 details perâ€‘task progression under each variant; MolmoAct consistently leads across categories.

- Benefit of midâ€‘training dataset (Sec. 5.4; Fig. 6b; Table 22):
  - On three realâ€‘world tasks (close_lid, rotate_pot, pour_tea), midâ€‘training improves MolmoAct by ~5.5% on average.  
    > Example: pour_tea trials show higher mean scores with midâ€‘training (Table 22).

- Human preference and steerability (Sec. 5.5â€“5.6; Fig. 7â€“9; Table 23):
  - Openâ€‘ended instruction following (Fig. 8 left):
    > â€œMolmoActâ€‘7Bâ€‘Dâ€‘Pretrain achieves the highest Elo, winning 58% vs. SpatialVLA and 81% vs. OpenVLA.â€
  - Trace generation on internet images (Fig. 7 left):
    > â€œMolmoAct attains top Elo, surpassing Geminiâ€‘2.5â€‘Flash, GPTâ€‘4o, and HAMSTER, with nonâ€‘overlapping 95% CIs.â€
  - Steerability (Fig. 9 left; Table 23):
    > â€œTrace steering succeeds 75% of the time, beating openâ€‘instruction steering by 33% and outperforming Ï€0â€‘FAST by 29% in the language setting.â€

Assessment of evidence
- Breadth: The paper evaluates in sim (two benchmarks), on real hardware (single and bimanual), with human preferences, OOD shifts, and an ablation on the midâ€‘training datasetâ€”good coverage (Sec. 5).
- Causality: The spatial chainâ€™s benefits are supported by (i) longâ€‘horizon LIBERO gains (Table 2), (ii) robust OOD performance (Fig. 6a; Table 21), and (iii) steerability advantages (Fig. 9).
- Transparency: Perâ€‘trial tables in the appendix (Tables 15â€“23) and explicit training configs (Tables 7, 10â€“14) enhance reproducibility.

## 6. Limitations and Trade-offs
Assumptions and design choices
- 2D trace for 3D control (Sec. G): The steering cue is purely 2D. It helps in-plane guidance but can be imprecise along depth (outâ€‘ofâ€‘plane) because no explicit 3D trace is provided at inference.
- Depth token resolution (Sec. G): The depth representation is limited to 100 tokens from a 128â€‘entry codebook. Fine manipulation might benefit from a higherâ€‘resolution depth tokenization.
- Gripper visibility (Sec. G): Visual trace prediction relies on endâ€‘effector visibility in the main camera. Occlusions degrade trace quality and, consequently, control.

Computational and data aspects
- Training cost: Although substantially less than some baselines, preâ€‘training still requires large compute (256 H100s for ~100k steps; Sec. 4.1). There is a minor inconsistency (9,216 vs. 9,728 GPUâ€‘hours) across sections.
- External specialists: Depth tokens rely on a preâ€‘trained depth model (Depth Anything v2) and a VQVAE trained on 10M depth maps; gripper points come from a VLM (Molmo). Errors in these â€œteachersâ€ can propagate into labels (Sec. 3.1).
- Partial openness: The strongest variant (7Bâ€‘D) uses SigLIP2 and Qwen2.5 backbones whose preâ€‘training data are not fully disclosed, though an â€œOâ€ variant with more open components is provided (Sec. 2.1).

Scope limitations
- Control frequency and latency: Inference produces multiple reasoning tokens per step, and serverâ€‘toâ€‘robot latency can limit control frequency (Sec. G).
- Task diversity: While the dataset spans many household tasks, results focus on manipulation with fixed arm embodiments. Mobile navigation or complex multiâ€‘modal sensing (e.g., tactile) is out of scope.

## 7. Implications and Future Directions
Impact on the field
- A blueprint for â€œspatial chainâ€‘ofâ€‘thoughtâ€ in robotics: Decodable intermediate depth and trajectory tokens make policies both more transparent and easier to steer than languageâ€‘only or latentâ€‘only reasoning (Fig. 1; Sec. 2.3â€“2.4).
- Practical, userâ€‘friendly steering: Trajectory sketches are simple for nonâ€‘experts and empirically beat reâ€‘prompting with language in precision and success rate (Fig. 9).

What this enables next
- 3D steering and richer spatial tokens:
  - Lift user sketches into 3D by conditioning on depth tokens at inference, or predict 3D keypoints/waypoint clouds; the paper explicitly calls this out as future work to solve outâ€‘ofâ€‘plane issues (Sec. G).
  - Increase depth token resolution or make tokens regionâ€‘adaptive (foveate around gripper/contacts).
- Multiâ€‘view, temporally aware spatial reasoning:
  - Fuse traces across multiple cameras or time (SLAMâ€‘assisted traces mentioned in Sec. G) to handle occlusions and longâ€‘horizon planning.
- Crossâ€‘embodiment and modular control:
  - Use the spatial chain as a common interface across arms and hands (the paper shows singleâ€‘ and bimanual arms; Sec. 5.2). Extend to mobile manipulation by adding base trajectories.
- Data and training innovations:
  - Explore selfâ€‘training where the modelâ€™s own predicted traces and depth tokens are refined by execution outcomes.
  - Combine the ordinalâ€‘aware action tokenization with other discretization schemes (e.g., adaptive bin widths) or hybrid continuous decoders.

Applications
- Household robotics: table setting, dish handling, wiping/cleaningâ€”tasks directly tested in the paper (Fig. 5; Tables 15â€“20).
- Interactive teleâ€‘assist: a human sketches a correction when the robot veers off course (Fig. 9).
- Safetyâ€‘critical settings: explainable depth maps and planned paths can be logged and audited.

> Bottom line: By turning spatial understanding and planning into explicit, tokenized steps that precede action, MolmoAct makes manipulation policies more interpretable, steerable, and robustâ€”backed by strong sim/real performance (Tables 1â€“2; Fig. 5â€“6)â€”and provides an open, reproducible recipe and dataset for the community.

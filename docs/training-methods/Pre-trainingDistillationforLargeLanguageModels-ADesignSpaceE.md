# Pre-training Distillation for Large Language Models: A Design Space Exploration

**ArXiv:** [2410.16215](https://arxiv.org/abs/2410.16215)

## üéØ Pitch

This paper pioneers the study of pre-training distillation (PD) for large language models, transferring knowledge from a larger teacher model to a smaller student during the massive, expensive pre-training phase rather than only after. Through systematic experimentation across key design choices‚Äîincluding processing of teacher logits, loss selection, scaling behavior, and teacher signal delivery‚Äîthe authors show PD is both feasible and impactful, yielding concrete best practices that improve student models' final performance and efficiency. By advancing distillation into pre-training, this work challenges the status quo and provides pathways to more cost-effective, high-quality LLMs.

---

## 1. Executive Summary
This paper introduces pre-training distillation (PD): transferring knowledge from a larger ‚Äúteacher‚Äù large language model (LLM) to a smaller ‚Äústudent‚Äù during the pre-training phase rather than after pre-training. Through a systematic design-space study across logits processing, loss design, scaling trends, and online/offline teacher signals, the paper shows PD is feasible, identifies configurations that substantially improve over standard pre-training, and distills actionable best practices.

## 2. Context and Motivation
- Problem addressed:
  - Knowledge distillation (KD) is widely used for LLMs but almost always during post-training (instruction tuning). This paper asks: can we distill during pre-training (i.e., while learning from raw text), and how do we do it effectively? (¬ß1)
- Why it matters:
  - Pre-training consumes the vast majority of compute and data. If KD can shape learning earlier, it could accelerate optimization, serve as soft supervision (label smoothing), and improve final quality and sample efficiency (¬ß1, ¬ß2).
- Prior approaches and gaps:
  - Post-training KD (e.g., Alpaca, Vicuna) uses teacher-generated instruction-response pairs and works well, but it does not guide the massive, earlier pre-training stage (¬ß1).
  - Some works distill either small pre-ChatGPT models or report PD without methodological detail (¬ß4). There is no systematic exploration of PD‚Äôs key design choices for modern billion-parameter LLMs.
- Positioning:
  - The paper frames PD as a standardizable objective and explores four critical dimensions that control its success: how to process teacher logits, which losses to use and how to combine them with the usual language modeling loss, how gains scale with model/data size, and whether to use offline versus online teacher logits (¬ß2, ¬ß¬ß3.2‚Äì3.5).

Terminology (selective):
- `Knowledge distillation (KD)`: training a smaller model to match the outputs (often probabilities/logits) of a larger model (the teacher).
- `Pre-training distillation (PD)`: applying KD during pre-training on raw text, not just during post-training (¬ß1, ¬ß2).
- `Logits`: the unnormalized scores a model produces before applying softmax to obtain token probabilities.
- `Top-p` truncation: keep the smallest set of tokens with cumulative probability mass ‚â• p.
- `Top-k` truncation: keep only the k highest-probability tokens.
- `Temperature (œÑ)`: divides logits before softmax; higher œÑ makes distributions flatter, lower œÑ makes them sharper.

## 3. Technical Approach
Step-by-step pipeline (formalized in ¬ß2 with Equations (1)‚Äì(4), implemented in ¬ß3):
1. Setup:
   - Have a teacher LLM `Œ∏_T` and a student LLM `Œ∏_S`. Training data are standard pre-training corpora (plain text). Tokens form sequences `x = {x_t}_{t=1}^T`.

2. Objective (Eq. (1)):
   - Minimize a mixture of the usual language modeling loss and a KD loss:
     - `L = (1 - Œ±) * L_lm + Œ± * L_kd`
     - `Œ± ‚àà [0,1]` controls how much to trust the teacher.

3. The two loss terms:
   - `L_lm` (Eq. (2)): standard next-token negative log-likelihood on true tokens.
   - `L_kd` (Eq. (3)): matches the student‚Äôs next-token distribution to a processed version of the teacher‚Äôs distribution:
     - `L_kd = (1/T) * Œ£_t L(P_{Œ∏_S}(x_t | x_<t), F(P_{Œ∏_T}(x_t | x_<t)))`
     - `L(¬∑,¬∑)` can be NLL, Kullback‚ÄìLeibler divergence (KLD), or MSE (¬ß3.3).
     - `F(¬∑)` processes teacher logits via truncation and temperature-normalized softmax (Eq. (4)).

4. Logit processing `F` (¬ß3.2):
   - To avoid storing full-vocab logits (58.6 PB for 100B tokens with ~150k vocab; ¬ß3.1), apply a two-stage truncation:
     - First `top-p` (e.g., p = 0.95), then `top-k` (e.g., k = 100). After truncation, re-normalize with temperature `œÑ` via softmax (Eq. (4)).
   - Rationale: captures the ‚Äúmass‚Äù of the teacher‚Äôs distribution while cutting storage 4,000√ó to ~15 TB for 100B tokens (¬ß3.1). Top-p handles sharp distributions; top-k caps long tails (¬ß3.2).

5. Training mechanics (¬ß3.1, App. A.1):
   - Preliminary config: teacher `GLM-4-9B`; student 1.9B; 100B pre-training tokens; context length 4096; Adam; batch size 2048; cosine LR schedule.
   - Because small students are weak on complex benchmarks, they perform supervised fine-tuning (SFT) after pre-training to stabilize evaluation: a mixture totaling 20B tokens (10B instruction-tuning + 10B extra pre-training text; App. A.1). During instruction data, loss is computed only on responses.

6. Loss scheduling (¬ß3.3):
   - Beyond static `Œ±`, the paper tests dynamic schedules, notably a ‚ÄúWarmup‚ÄìStable‚ÄìDecay‚Äù (`WSD`) schedule for both `Œ±` and the learning rate. Intuition: give KD more weight when the LR is high (stable plateau), then reduce later. This synchronizes the optimization ‚Äúpressure‚Äù of KD with high learning rates.

7. Online vs offline teacher logits (¬ß3.5):
   - Offline: run the (pretrained) teacher over the corpus, store truncated logits, then train the student.
   - Online: store teacher logits ‚Äúon the fly‚Äù while pre-training the teacher from scratch, then use those stored logits to pre-train students.

Why these choices?
- Truncation: massive storage savings without losing the ‚Äúsignal‚Äù (teacher probability mass). Figures 2‚Äì3 and Table 8 show performance is fairly robust to the exact p and k.
- Temperature: balances signal sharpness (too sharp can overfit to the teacher‚Äôs top choice, too flat loses guidance). Tables 2 and 9 quantify the sweet spot.
- Loss type and scheduling: empirical testing (Table 5) shows KLD/NLL are viable; MSE underperforms. Scheduling `Œ±` with WSD and aligning it with a WSD LR produces the strongest gains, suggesting timing matters.

Illustrative example:
- Consider the next-token distribution for a prefix. The teacher might spread probability across ‚Äúcat‚Äù (0.55), ‚Äúdog‚Äù (0.25), ‚Äúcar‚Äù (0.05), and many small-mass tokens. Top-p=0.95 keeps {cat, dog, car, ‚Ä¶} until the cumulative mass exceeds 0.95; top-k=50 caps count. After renormalization with œÑ (e.g., œÑ=0.5), the student learns to assign probability mass similarly‚Äînot just copying the argmax (‚Äúcat‚Äù), but also learning the teacher‚Äôs uncertainty structure.

## 4. Key Insights and Innovations
- Broad, systematic design-space exploration for PD (¬ß¬ß3.2‚Äì3.5):
  - Novelty: prior LLM PD reports usually omit details or focus on post-training KD. Here, PD is decomposed into four practical levers (logit processing, loss, scaling, online/offline), each studied with targeted ablations.

- Storage-efficient teacher signal via two-stage `top-p-k` truncation (¬ß3.1, ¬ß3.2):
  - What‚Äôs new: a pragmatic combination of top-p then top-k to control both mass and tail length.
  - Why it matters: reduces 58.6 PB ‚Üí ~15 TB for 100B tokens (4,000√ó smaller) while keeping PD effective (Table 1; Figures 2‚Äì3; Table 8).

- Loss scheduling synergy (WSD-Œ± + WSD-LR) delivers the strongest gains (¬ß3.3, Table 5):
  - What‚Äôs new: jointly scheduling the KD mixture weight `Œ±` and the learning rate with warmup‚Äìstable‚Äìdecay yields the best result (‚ÄúWSD-Œ±+WSD-LR‚Äù achieves the highest average score 40.7, +8.0% over baseline; Table 5).
  - Why it matters: it shows PD‚Äôs benefit depends not just on ‚Äúwhat loss‚Äù but ‚Äúwhen and at what LR‚Äù that loss is emphasized.

- Scaling-law observations that invert common expectations (¬ß3.4, Figure 4, Table 11):
  - Larger students benefit more from PD; using a much larger teacher does not always help (capacity gap issues).
  - Significance: practical guidance‚Äîalign teacher/student sizes to avoid overwhelming small students (e.g., 3.8B student gains +2.9 points with a 9B teacher vs +0.9 with a 32B teacher; Table 11).

- Online logits are usable but require care (¬ß3.5, Table 6):
  - Using logits from an unconverged teacher hurts; later-stage online logits and lower Œ± help (Table 6).
  - Practical implication: when pre-training a family of models, log teacher logits later in training to reuse for smaller students at minimal extra inference cost.

Fundamental vs incremental:
- Fundamental: establishing PD as a viable, configurable pre-training paradigm for LLMs with clear, generalizable levers and constraints.
- Incremental but important: specific choices‚Äîtop-p-k truncation settings, œÑ ranges, KLD/NLL preference, WSD scheduling‚Äîtranslate into repeatable recipes.

## 5. Experimental Analysis
Evaluation setup (¬ß3.1, App. A.1):
- Data and training:
  - Preliminary PD: teacher `GLM-4-9B`, student 1.9B, 100B tokens; top-0.95 then top-100 truncation; œÑ=1.0; KD uses NLL of student against normalized teacher distribution; Adam; batch=2048; seq len=4096; cosine LR (¬ß3.1).
  - After pre-training, SFT on a 20B mixture (10B instruction + 10B extra pre-training text; App. A.1). For instruction data, loss computed only on responses.
- Datasets and shots:
  - English: HellaSwag, WinoGrande, PIQA, MMLU; Chinese: KBQA, C3, C-Eval; Math: GSM8k (¬ß3.1; App. A.1). Zero-shot for most; 5-shot C3/C-Eval; 6-shot MMLU; 8-shot GSM8k; decoding temperature=0 (App. A.1).
- Baselines:
  - `LLM-LM`: same student trained with standard LM loss only (Œ±=0).
  - Compare multiple PD variants changing truncation, œÑ, loss type, Œ± schedules, sizes, and online/offline logits.

Main quantitative results (selected):
- Feasibility of PD (Table 1):
  - Average accuracy: `LLM-LM` 37.7 vs `LLM-KD` 38.3
  - Relative improvement: +1.6%
  - Quote:
    > Table 1: ‚Äú‚àÜ ‚Üë 1.6%‚Äù (average) with PD over LM-only for the 1.9B student; GSM8k improves 24.6% relatively (8.6 ‚Üí 10.8).

- Logits processing (Figures 2‚Äì3; Tables 2, 8‚Äì9):
  - Top-p-k:
    - Robust across p and k; smaller p or k can shrink storage with similar performance (Figures 2‚Äì3; Table 8).
    - Example: `top-0.95-50` reaches the best average in Table 8 (39.6) and markedly lifts MMLU (33.2).
  - Temperature:
    - Static œÑ: œÑ ‚â§ 2.0 is best; œÑ ‚â• 5.0 degrades (Table 2; Table 9). 
    - Adaptive œÑ: AdaKDH is best among adaptives (avg 38.8; Table 3), but not meaningfully better than a well-chosen static œÑ=0.5/2.0 in aggregate (¬ß3.2).

- Loss selection and scheduling (Tables 4‚Äì5):
  - Loss type (Œ±=1):
    - `KLD` averages 38.7; `NLL` 38.3; `MSE` drops to 34.9 (Table 5).
    - Quote:
      > Table 5: ‚ÄúLLM-MSE ‚Ä¶ 34.9 ‚Üì 7.6%‚Äù vs baseline, confirming MSE underperforms for PD.
  - Mixing with LM loss:
    - Best static Œ± ‚âà 0.9 (Table 4, average improvement peak at Œ±=0.9).
    - Dynamic schedules:
      - Linear decrease outperforms linear increase (Table 5: ‚ÄúLinear Dec ‚Ä¶ 39.2 ‚Üë 4.1%‚Äù).
      - WSD-Œ± + WSD-LR is the best overall (Table 5: ‚ÄúWSD-Œ±+WSD-LR ‚Ä¶ 40.7 ‚Üë 8.0%‚Äù).

- Scaling laws (Figure 4; Table 11):
  - Student size:
    - Gains grow with student size; small students (330M) may not benefit (or regress) from PD.
  - Teacher size:
    - 9B teacher can outperform 32B teacher for mid-size students (e.g., 3.8B student: +2.9 vs +0.9 average; Table 11).
  - Corpus size and training dynamics (Figure 5; Table 12):
    - PD improves across the entire 500B-token training curve for 1.9B and 3.8B students.
    - Quote:
      > Figure 5: both ‚Äú1.9B-KD‚Äù and ‚Äú3.8B-KD‚Äù curves stay above their LM-only counterparts; the gains increase early then partially converge by the end.
    - Final checkpoint averages (Table 12):
      - 1.9B: LM 44.2 vs KD 45.4
      - 3.8B: LM 50.2 vs KD 53.7

- Online vs offline logits (Table 6; ¬ß3.5):
  - Early-stage online logits hurt (teacher not converged):
    - `LLM-Online-100B-L` average 29.8 (‚àí20.9% relative to LM baseline).
  - Late-stage online logits help modestly with careful weighting:
    - `LLM-Online-100B*` (Œ±=0.1, `top-0.95-50`) reaches 37.9 (slightly above 37.7 baseline), but still below offline PD (Table 6).

- Best recipe (‚ÄúPD*‚Äù) and end-to-end gains (App. A.6; Table 13; Figure 1):
  - Configuration: `top-0.95-50`, œÑ=2.0, KLD loss, WSD-Œ± (max Œ±=0.9) + WSD-LR; offline logits.
  - Results (Table 13):
    - 1.9B: 41.2 (vs 37.7 baseline) 
    - 3.8B: 45.7 (vs 42.0)
    - 6.8B: 49.8 (vs 44.9)
  - Quote:
    > Figure 1: PD* improves all three student sizes beyond both LM-only and ‚Äúvanilla PD,‚Äù demonstrating the value of the explored configuration.

Do the experiments support the claims?
- Yes, for feasibility and best practices:
  - Multiple ablations isolate the role of truncation p/k, œÑ (static vs adaptive), loss type, Œ± scheduling, LR scheduling, and scaling. Improvements persist across datasets and larger token budgets (Tables 1, 5, 8‚Äì13; Figures 1‚Äì5).
- Robustness:
  - Stronger students benefit more (Table 11, Figure 4), and improvements persist over 500B tokens (Figure 5). Failure modes are acknowledged for online logits from early teacher checkpoints (Table 6).

## 6. Limitations and Trade-offs
- Assumption of teacher access and compatibility:
  - Requires a competent teacher and the ability to run it over large corpora (offline) or store its logits during pre-training (online). Full-vocab logits are prohibitively large‚Äî58.6 PB for 100B tokens‚Äînecessitating truncation (¬ß3.1).
- Interaction effects not fully explored:
  - The paper studies each factor with controlled variables; it does not exhaustively search interactions among all factors due to compute costs (Limitations).
- Online PD sensitivity:
  - Online logits from an unconverged teacher can harm performance unless weighted down (`Œ±=0.1`) and taken from later training (¬ß3.5, Table 6).
- Small students may not benefit:
  - 330M/670M students show small or negative gains, especially with overly large teachers (Table 11, Figure 4). This reflects a capacity gap issue.
- Compute and environmental cost:
  - Extensive experiments incur significant compute and associated emissions (Limitations; Strubell et al., 2019 cited).
- Evaluation relies on SFT to stabilize measurement:
  - Because small students perform near chance on challenging tasks, the study adds SFT (20B tokens) prior to evaluation (App. A.1). This is practical, but it means pure pre-training-only effects on these tasks are not directly reported.

## 7. Implications and Future Directions
- How it changes the landscape:
  - Establishes PD as a viable and tunable pre-training strategy for LLMs, not merely a post-training trick. It provides a practical blueprint (top-p-k truncation, œÑ‚â§2, KLD/NLL, WSD scheduling) and evidence that PD can raise both training efficiency and final quality (Tables 5, 12‚Äì13; Figures 1, 5).
- Practical applications:
  - Organizations training LLM families can:
    - Distill larger models into smaller ones during pre-training to reduce compute for downstream models.
    - Log teacher logits later in training (online) to amortize teacher inference for multiple students (¬ß3.5).
    - Use `PD*`-like recipes to get reliable gains with manageable storage.
- Follow-up research:
  - Systematic interaction search:
    - Joint optimization of truncation, œÑ, loss type, Œ± schedule, and LR schedule (stated as future work in Limitations).
  - Closing the capacity gap:
    - Methods to bridge teacher‚Äìstudent mismatch (e.g., progressive teachers, teacher assistants; see Mirzadeh et al., 2020) within PD.
  - Weak-to-strong setups:
    - Train very large students from smaller teachers to test weak-to-strong generalization (suggested in ¬ß3.4).
  - Trillion-token regimes:
    - Validate PD at multi-trillion-token scales typical of frontier models (¬ß3.4 notes compute constraints prevented this).
  - Better online PD:
    - Curriculum-like designs for when and how to capture teacher logits during its training; adaptive Œ± over time tailored to teacher convergence.

In short, the study turns the vague idea of ‚Äúdo KD during pre-training‚Äù into a concrete, evidence-backed methodology. With sensible truncation, appropriate temperature, the right losses, and carefully coordinated schedules, PD can deliver consistent gains‚Äîespecially for mid-to-large students‚Äîwhile remaining practical in storage and compute.

# ROBUSTFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response

**ArXiv:** [2412.14922](https://arxiv.org/abs/2412.14922)

## ğŸ¯ Pitch

ROBUSTFT introduces a novel supervised fine-tuning framework that robustly adapts large language models (LLMs) using real-world datasets contaminated with noisy responses, by combining multi-expert collaborative noise detection, context-enhanced relabeling, and entropy-based data selection. This innovation enables LLMs to maintain high performance even as noise levels soarâ€”significantly outperforming standard methodsâ€”making domain adaptation with imperfect data practical and reliable for critical applications like healthcare, finance, and beyond.

---

## 1. Executive Summary
ROBUSTFT is a supervised fineâ€‘tuning framework that automatically detects and repairs noisy training examples for large language models (LLMs) by combining multiâ€‘expert agreement, contextâ€‘based relabeling, and entropyâ€‘based data selection. Across five datasets and three model families, it prevents the steep performance collapse caused by noisy supervision and yields consistent gains over both vanilla models and standard SFT, especially under heavy noise (e.g., on MMLU at 70% noise, accuracy rises from 37.3 with SFT to 67.6; Table 1).

## 2. Context and Motivation
- Problem addressed:
  - Supervised fineâ€‘tuning (SFT) relies on highâ€‘quality instructionâ€“response pairs. In real datasets, labels/responses often contain noise from annotation mistakes and model hallucinations, degrading downstream performance.
  - Figure 1 quantifies the risk: with 30% noisy training data, the MMLU score drops by 8.9% relative to the vanilla LLM baseline; further noise causes sharper declines.

- Why this matters:
  - Practically, many organizations adapt general LLMs to domain tasks (medical, finance, reasoning) using inâ€‘house data that is imperfect. Robustness to noise makes such adaptation viable without expensive reâ€‘annotation.
  - Methodologically, most â€œnoisy label learningâ€ research targets classification with a small set of discrete labels. LLM SFT involves openâ€‘ended text generation, where noise is harder to spot and correct.

- Shortcomings of prior approaches:
  - Classical noisyâ€‘label methods (Section 5.1) assume discrete labels and do not exploit rich context in textual responses.
  - LLM selfâ€‘filtering is unreliable: Section 4.2.1 shows the SelfSelect baselineâ€”which asks the model to pick good dataâ€”underperforms even plain SFT on several datasets.
  - Enhanced SFT with external instruction sets (e.g., Hermesâ€‘3, Tuluâ€‘3; Table 1) does not reliably improve downstream adaptation under noise.
  - Retrievalâ€‘augmented inference methods (SelfRAG) help modestly at inference time but do not fix corrupted training signals (Table 1).

- Positioning:
  - ROBUSTFT (Sections 3.1â€“3.5) is a selfâ€‘contained, twoâ€‘stage framework tailored to openâ€‘ended LLM SFT: (1) multiâ€‘view noise detection via agreement among expert predictions; (2) denoising by contextâ€‘enhanced relabeling plus entropyâ€‘based sample selection, followed by SFT on the curated set.

## 3. Technical Approach
The framework (Figure 2, Algorithm 1) operates in three phases. Key notation is introduced in Section 3.

1) Noise detection by multiâ€‘expert agreement (Section 3.2)
- Step A: Base prediction. For each training query `q_i`, the base LLM `M` predicts a response `Å·_i = M(q_i)` (Equation 1).
- Step B: Reasoningâ€‘enhanced prediction. A â€œreasoningâ€‘enhanced LLMâ€ iterates between explicit reasoning (`M_Reas`) and selfâ€‘reflection (`M_Refl`) to produce a final answer `Å·_i^reas` (Equation 2). Conceptually, this is a loop: explain the answer stepâ€‘byâ€‘step, critique the reasoning, then refine.
- Step C: Consistency checking. A `Checker` compares three sources: the dataset label `y_i`, the base prediction `Å·_i`, and the reasoningâ€‘enhanced prediction `Å·_i^reas`. It emits `r_i âˆˆ {0,1}` (Equation 3): `r_i=1` means high agreement (clean), `r_i=0` means disagreement (potential noise). The paper sets `n=4` and an agreement threshold `Î¸=50%` during implementation (Section 4.1.3), implying a simple majority rule across multiple expert outputs/prompts.
- Output: Split the data into `D_clean` (reliable) and `D_noise` (potentially noisy).

Why this works: relying on a single LLM prediction is brittle because LLMs can hallucinate confidently. Agreement between a plain prediction, a reasoningâ€‘plusâ€‘reflection prediction, and the original label offers a stronger signal that the training pair is trustworthy.

2) Denoising via contextâ€‘enhanced relabeling (Section 3.3)
- Step A: Build a retrieval index over clean examples. Each query `q` is mapped to an embedding `h = Encoder(q) âˆˆ R^d` (Equation 4).
- Step B: For each noisy sample `(q_i, y_i) âˆˆ D_noise`, retrieve the `k` most similar clean queryâ€“answer pairs from `D_clean` and condition the LLM on these as context. Generate a â€œcontextâ€‘enhancedâ€ answer `Å·_i^cont` (Equation 5). Intuition: showing the model similar vetted examples helps it produce a better label for the noisy case.
- Step C: Review and synthesize. A `Review` agent compares the contextâ€‘enhanced answer `Å·_i^cont` with the reasoningâ€‘enhanced answer `Å·_i^reas` and consolidates them into a final relabeled answer `á»¹_i` (Equation 6). This adds another layer of crossâ€‘checking before accepting a selfâ€‘generated label.
- Output: a denoised set `D_denoise = {(q_i, á»¹_i)}`.

3) Confidenceâ€‘based selection by entropy (Section 3.4)
- Step A: Compute sequenceâ€‘level uncertainty for each contextâ€‘enhanced generation using normalized token entropy `H(Å·_i^cont)` (Equation 7). Low entropy indicates the model is confident (more deterministic token probabilities).
- Step B: Keep only the most confident fraction. Rank by `H(Å·_i^cont)` and retain the topâ€‘`Î²` fraction (lowest entropy) to form `D_select` (Equation 8). The default is `Î² = 50%` (Section 3.4; sensitivity in Figure 3).

4) Final SFT on curated data (Section 3.5)
- Concatenate the highâ€‘trust original data and the vetted relabels: `D_ft = D_clean âˆª D_select`.
- Fineâ€‘tune the LLM by standard languageâ€‘modeling loss on `D_ft` (Equation 9). Implementation uses LoRA (lowâ€‘rank adapters) for parameterâ€‘efficient SFT over 2 epochs (Section 4.1.3).

Design choices and rationale
- Multiâ€‘view detection: pairs that survive agreement between `y_i`, `Å·_i`, and `Å·_i^reas` are likely clean. Disagreements trigger careful relabeling rather than outright deletion, preserving data volume.
- Contextâ€‘enhanced relabeling: retrieving similar clean examples provides grounded, inâ€‘distribution references, reducing hallucination risk during relabeling.
- Entropy filtering: even after relabeling, some selfâ€‘annotations are uncertain. Filtering by entropy mitigates error propagation in the final SFT.

## 4. Key Insights and Innovations
- Multiâ€‘expert, reasoningâ€‘aware noise detection (fundamental)
  - Novelty: combines a base LLM, a reasoningâ€‘andâ€‘reflection LLM, and a consistency `Checker` (Equations 1â€“3) to identify suspicious samples. This exploits complementary strengthsâ€”direct answer vs. reasoned answerâ€”and avoids overreliance on any single predictor.
  - Evidence: Removing the `Checker` degrades MMLU accuracy from 68.2 to 65.3 at 30% noise (Table 3), showing that agreementâ€‘based detection is a major driver.

- Contextâ€‘enhanced relabeling + review (fundamental)
  - Novelty: instead of discarding noisy items, the method repairs them by conditioning on nearest clean exemplars (Equation 5) and then consolidating with a `Review` agent (Equation 6). This leverages the datasetâ€™s own verified knowledge.
  - Evidence: Removing contextâ€‘enhanced relabeling (`w/o CER`) drops MMLU from 68.2 to 67.7 and ARC from 84.9 to 84.1 (Table 3). Removing the reasoningâ€‘enhanced expert (`w/o REL`) yields similar declines (67.4 and 84.1), indicating both inputs to the reviewer matter.

- Entropyâ€‘based selection to prevent bad selfâ€‘labels (important incremental)
  - Novelty: ranks relabeled items by generation entropy (Equation 7) and keeps only the most confident half (Equation 8). This adds a probabilistic quality gate specific to openâ€‘ended generation.
  - Evidence: `w/o Selection` produces the largest single ablation drop (MMLU âˆ’2.5, ARC âˆ’1.7 at 30% noise; Table 3). Sensitivity in Figure 3 shows performance peaks around `Î²=40â€“50%`.

- Selfâ€‘contained pipeline for openâ€‘ended SFT (important incremental)
  - The system avoids dependence on external gold annotations during denoising and acts directly on freeâ€‘form responses. Section 3.5 and Algorithm 1 present an endâ€‘toâ€‘end process designed for LLM SFT, contrasting with classificationâ€‘oriented noise methods referenced in Section 5.1.

## 5. Experimental Analysis
- Evaluation setup (Section 4.1)
  - Datasets spanning general knowledge and domain tasks: MMLU, ARC, PubMedQA (biomedical), DROP (numeric/reading), FPB (financial).
  - Noise regime: inject 30%, 50%, 70% noisy answers into training sets (details of injection are in the appendix per Section 4.1.3 summary).
  - Models: Llamaâ€‘3.1â€‘8B, Llamaâ€‘3.2â€‘3B, Gemmaâ€‘2â€‘9B (Table 2).
  - Training: LoRA fineâ€‘tuning for 2 epochs using Llamaâ€‘Factory; default `Î²=50%`, context length `k=3`, `n=4` experts, agreement threshold `Î¸=50%` (Section 4.1.3; Figure 3 studies `Î²` and `k`).

- Baselines (Section 4.1.2)
  - `Vanilla`: zero SFT inference.
  - `SFT`: plain fineâ€‘tuning on the noisy data.
  - SFT with external corpora: `Hermesâ€‘3`, `Tuluâ€‘3`.
  - Denoising/selection: `NoiseAL`, `SelfLabel`, `SelfSelect`.
  - Inference augmentation: `SelfRAG`.

- Headline results (Llamaâ€‘3.1â€‘8B; Table 1)
  - MMLU: ROBUSTFT 68.2/68.0/67.6 (noise 30/50/70), beating `Vanilla` 65.3 and far exceeding `SFT` 59.5/47.5/37.3. Reported relative gains over SFT: +14.6%, +43.2%, +81.2%.
  - ARC: 84.9/84.7/84.1 vs 82.7 (`Vanilla`) and 70.7/61.7/47.5 (`SFT`).
  - PubMedQA: 75.8/75.6/75.0 vs 72.0 (Vanilla) and 66.4/36.7/32.8 (SFT).
  - DROP: 90.3/88.5/87.9 vs 87.2 (Vanilla) and 85.3/78.6/66.4 (SFT).
  - FPB: 84.4/80.5/76.2 vs 75.5 (Vanilla) and 79.7/58.4/34.9 (SFT).
  - Quote:
    > Table 1: â€œIncreasing noise levels deteriorate SFT sharply, while ROBUSTFT improves over Vanilla by up to +11.8 points (FPB 30%) and over SFT by up to +129% relative (PubMedQA 70%).â€

- Crossâ€‘model generality (Table 2)
  - Llamaâ€‘3.2â€‘3B: MMLU 58.5/58.2/57.9 vs Vanilla 54.9 and SFT 55.0/48.4/38.3.
  - Gemmaâ€‘2â€‘9B: MMLU 72.5/72.1/71.3 vs Vanilla 70.3 and SFT 63.6/52.1/40.3; FPB at 70% noise jumps from SFT 35.6 to ROBUSTFT 87.7.
  - Note: Table 2â€™s FPB 70% for Llamaâ€‘3.1â€‘8B is 73.2, slightly below Table 1â€™s 76.2, indicating minor run/config variability; the trend remains consistent.

- Robustness and diagnostics
  - Ablations (Table 3): every component contributes; `Selection` and `Checker` drive the largest gains. Removing `Reviewer` causes smaller but consistent drops.
  - Sensitivity (Figure 3): best `Î²` around 40â€“50%; performance plateaus for context length `k=3â€“5`, suggesting a few similar examples suffice for relabeling.
  - Perplexity analysis (Figure 4): with noise, both vanilla and SFT show higher, more dispersed perplexity; ROBUSTFT concentrates density at lower perplexity across noise levels and datasets, indicating more confident predictions.
  - Categoryâ€‘wise (Figure 5): noise harms knowledgeâ€‘heavy domains (History, Health, Law) the most; ROBUSTFT lifts scores broadly rather than in isolated categories.
  - Stability (Figure 6): under instruction paraphrasing (five runs using GPTâ€‘4o to rephrase prompts), accuracy remains steady with small variance, including at high noise.

- Do the experiments support the claims?
  - Yes: Results repeatedly show that standard SFT collapses under noise, whereas ROBUSTFT not only avoids collapse but often outperforms the unâ€‘fineâ€‘tuned base model. Gains hold across datasets, model sizes, and noise levels, and ablations attribute gains to the proposed components.

## 6. Limitations and Trade-offs
- Dependence on partial cleanliness:
  - The method assumes the `Checker` can carve out a meaningful `D_clean` from the noisy corpus. If nearly all samples disagree (e.g., systematic label corruption), retrieval and relabeling may lack reliable anchors.

- Checker details and thresholds:
  - The `Checker` is defined as a consistency function (Equation 3) with `n=4` and `Î¸=50%` in implementation (Section 4.1.3), but its exact agreement metric and expert composition are not extensively specified. Different implementations may change which samples are marked clean/noisy.

- Computational overhead:
  - Compared with plain SFT, ROBUSTFT runs multiple inference passes per sample: base generation, reasoning+reflection loop, retrievalâ€‘conditioned generation, and review; plus embedding/indexing and entropy computation. This raises preprocessing cost and may be substantial for very large corpora.

- Retrieval and encoder choice:
  - Relabeling quality depends on retrieving semantically relevant clean examples (Equation 5). The paper abstracts `Encoder(Â·)` (Equation 4); performance could vary with embedding choice and domain shift.

- Entropy metric scope:
  - Entropy is computed on the contextâ€‘enhanced output (Equation 7). Confident but wrong generations can slip through; conversely, correct but diverse phrasing may be filtered out. This is partly mitigated by keeping only the topâ€‘`Î²` fraction, but it remains a heuristic.

- Noise model and data transparency:
  - Section 4.1 mentions â€œintroducing varying degrees of noise perturbation,â€ but the main text does not detail the corruption process. Different noise types (adversarial vs. random vs. systematic bias) could affect outcomes.

- Scope of postâ€‘training objectives:
  - The framework targets SFT with nextâ€‘token loss (Equation 9). It does not integrate with preference learning or RLHF, where noise manifests differently (e.g., in preference pairs).

## 7. Implications and Future Directions
- Impact on practice:
  - The paper provides an actionable recipe for organizations to fineâ€‘tune LLMs on imperfect realâ€‘world corpora. A dropâ€‘in preâ€‘SFT curation stepâ€”agreementâ€‘based detection, retrievalâ€‘guided relabeling, entropy filteringâ€”can stabilize and often improve over the vanilla model under heavy noise.

- Research directions:
  - Stronger agreement models: replace the binary `Checker` with calibrated uncertainty estimation, semanticâ€‘equivalence scoring, or multiple heterogeneous LLMs with learned weights.
  - Better relabeling governance: incorporate verifier models or lightweight human spotâ€‘checks for highâ€‘impact samples; add contradiction tests between `Å·_i^cont` and `Å·_i^reas`.
  - Beyond entropy: combine entropy with semantic confidence (e.g., entailment scores, selfâ€‘consistency voting, or edit distance to retrieved exemplars).
  - Noise typology studies: evaluate under adversarial and systematically biased noises; analyze crossâ€‘lingual and codeâ€‘generation settings.
  - Integration with other postâ€‘training methods: apply ROBUSTFT as a data curation frontâ€‘end for preference optimization and RLHF, where misplaced preferences are common.

- Applications:
  - Domain adaptation with weak supervision (medical Q&A, finance analysis, legal reasoning).
  - Bootstrapping specialized assistants from mixedâ€‘quality logs or scraped data.
  - Continual learning where onâ€‘theâ€‘fly relabeling and selection counter drift and accumulation of errors.

Overall, ROBUSTFT reframes noisy SFT as a detectâ€‘thenâ€‘repair problem using multiâ€‘expert agreement and contextâ€‘aware relabeling, then enforces quality with entropyâ€‘based selection. The combination is simple, selfâ€‘contained, and empirically effective across diverse tasks and models (Tables 1â€“2; Figures 3â€“6), making it a practical baseline for noiseâ€‘robust LLM adaptation.

# GPQA: A Graduate-Level Google-Proof Q&A Benchmark

**ArXiv:** [2311.12022](https://arxiv.org/abs/2311.12022)

## ðŸŽ¯ Pitch

GPQA introduces a rigorous, graduate-level benchmark of 448 expert-written questions in biology, physics, and chemistry that are designed to be 'Google-proof': even highly skilled non-experts with full web access struggle to answer them, while top-tier models like GPT-4 also perform poorly compared to domain experts. By providing a testbed where only subject-matter experts can reliably discern the correct answers, GPQA enables realistic, high-stakes research on scalable oversightâ€”how to reliably supervise and align frontier AI systems when even well-resourced human evaluators lack the expertise to verify answers themselves. This dataset directly targets the critical challenge of ensuring AI safety and truthfulness as models advance into domains beyond most human supervisors' reach.

---

## 1. Executive Summary
GPQA introduces a small but unusually hard, expertâ€‘written benchmark of 448 multipleâ€‘choice questions in biology, physics, and chemistry designed to be â€œGoogleâ€‘proof.â€ Its core contribution is a data creation and validation pipeline that yields questions with high expert agreement yet low accuracy for skilled nonâ€‘experts and current frontier models, enabling realistic experiments on scalable oversightâ€”how to supervise AI systems on tasks that even wellâ€‘resourced nonâ€‘experts cannot solve or verify.

## 2. Context and Motivation
- Problem addressed
  - Scalable oversight: how to reliably supervise models on questions whose truth a supervisor cannot easily produce or verify. This setting becomes critical as models edge into superhuman or specialist territory. Definition: â€œscalable oversightâ€ (Section 1) refers to eliciting and checking truthful answers from models even when supervisors lack the knowledge or time to compute or confirm the truth themselves.
  - Existing QA benchmarks are usually solvable by nonâ€‘experts using web search or are written by nonâ€‘experts; they do not stress the supervision challenge near or beyond the frontier of human expertise (Section 5).

- Why this matters
  - If future models help generate new scientific knowledge, supervisors must detect errors, hallucinations, and sycophancy without having the answers in hand (Section 1). Oversight methods like RLHF depend on the supervisorâ€™s ability to judge outputs; when the task exceeds the supervisorâ€™s expertise, these methods can fail.

- Prior approaches and shortcomings
  - Crowdsourced or curated QA datasets (e.g., SQuAD, Natural Questions, TriviaQA, MMLU) rely on information that nonâ€‘experts can often locate or reason through with short references (Section 5). They do not ensure that:
    - The true answer is difficult for nonâ€‘experts even with internet access.
    - Incorrect options are plausible to skilled nonâ€‘experts.
  - QuALITY targets longâ€‘context reading difficulty rather than real subjectâ€‘matter expertise (Section 5).

- Positioning
  - GPQA constructs questions written and validated by PhDâ€‘level domain experts, and it explicitly verifies that skilled nonâ€‘experts (experts in other domains) fail even with unrestricted web access and ample time (Sections 2.1, 3.2). It thus provides a testbed suited to research on scalable oversight methods like debate, marketâ€‘making, and recursive reward modeling (Sections 1, 2.1).

## 3. Technical Approach
GPQAâ€™s methodology is a fourâ€‘stage expertâ€‘centered pipeline that jointly enforces objectivity (clear, correct answer) and difficulty (nonâ€‘experts cannot solve even with the web).

- Participants and qualification (Section 2.1)
  - 61 Upwork contractors who have completed or are pursuing PhDs in their field and are proficient in English; highâ€‘rated freelancers preferred.
  - Writers and validators are experts in one of three highâ€‘level domains (biology, physics, chemistry) and multiple subdomains (Section 2.2).
  - Nonâ€‘expert validators are experts in other domains (e.g., a physicist validates biology or chemistry), with unlimited web access but no LLMs; they must spend at least 15 minutes and often far more (median 30 min; mean 37 min; top 20% â‰¥45 min; Section 3.2).

- Stage 1 â€” Question writing (Section 2.1; Appendix A.5.1)
  - Experts write hard, standâ€‘alone, fourâ€‘choice questions designed to be answerable by inâ€‘domain experts even without seeing options.
  - Writers supply detailed explanations: why the correct choice is correct and why each distractor is plausible but wrong. These explanations later seed fewâ€‘shot prompts for model baselines and aid in objectivity checks.
  - Writers are explicitly instructed to avoid â€œeasy tellsâ€ (surface patterns that leak the answer), to minimize rote calculation, and to consider â€œsearch resistanceâ€ (run the web searches a nonâ€‘expert would try and ensure they donâ€™t trivially solve the question).

- Stage 2 â€” First expert validation and feedback (Section 2.1; Appendix A.5.2)
  - Another expert in the same domain answers the question and provides detailed feedback aimed at improving clarity, objectivity, and difficulty.
  - Writers then revise the question (Stage 3) based on this feedback (Appendix A.5.3).

- Stage 3 â€” Question revision (Section 2.1)
  - Writers may revise wording, assumptions, or distractor plausibility to fix ambiguities or strengthen difficulty, while retaining a unique correct answer.

- Stage 4 â€” Second expert validation and nonâ€‘expert validation (Section 2.1)
  - A second expert answers and comments on the revised question; three nonâ€‘experts (experts in other fields) attempt the question with full web access (no LLMs).
  - Both experts also report whether they have sufficient expertise to answer; this supports objectivity analysis (Section 3.1).

- Incentive design to reinforce objectivity and difficulty (Section 2.1; Appendix A.4)
  - Writers: $10 base + $20 per expert who answers correctly + $15 per nonâ€‘expert who answers incorrectly + $30 bonus for questions where both experts are correct and â‰¥2/3 nonâ€‘experts are wrong.
  - Expert validators: $10 base + $10 if they answer correctly; the first validator also gets bonuses if their feedback improves objectivity/difficulty (e.g., if the second expert is correct and most nonâ€‘experts are wrong). A flat $7 is granted to account for role asymmetry.
  - Nonâ€‘experts: $10 base + $30 for answering correctlyâ€”ensuring they really try.

- Measuring and filtering for objectivity (Section 3.1)
  - â€œPostâ€‘hoc agreementâ€: After seeing the writerâ€™s explanation, experts state whether they now agree the answer is uncontroversially correct and explain mistakes if any.
  - Manual analysis of 191 secondâ€‘expert errors classifies feedback into eight categories (Table 4), separating clear validator mistakes from true question issues (ambiguity, missing assumptions, or wrong answers). These judgments control which questions enter the main and â€œDiamondâ€ splits.

- Data splits (Section 2.3)
  - Extended: 546 questions (18 held out, unreleased).
  - Main (GPQA): 448 questions; filters out clearly nonâ€‘objective/easy items and includes cases where an expert erred but later demonstrated understanding of their mistake.
  - Diamond: 198 hardest, highestâ€‘confidence questions; requires stronger expert agreement (both experts correct, or one expertâ€™s mistake is clearly explained) and a majority of nonâ€‘experts wrong.

- Domains and coverage (Section 2.2; Table 3)
  - Biology (Molecular Biology; Genetics), Physics (nine subdomains including Quantum Mechanics, HEP, Astrophysics), Chemistry (notably many in Organic Chemistry). Chemistry questions are especially hard for nonâ€‘experts while maintaining high expert accuracy (Table 3).

- Quality checks against shortcuts (Appendix A.2)
  - Answerâ€‘only classifiers (fineâ€‘tuned T5 and a RoBERTaâ€‘based CBOW) trained to guess the correct option using only the text of the choices and randomized order perform at chance (â‰ˆ25%), suggesting no easy lexical tells in the options.

- Baselines and openâ€‘book tool use (Section 4; Appendix A.3)
  - Closedâ€‘book with four prompting styles: zeroâ€‘shot, fewâ€‘shot, zeroâ€‘shot chainâ€‘ofâ€‘thought (CoT), and fewâ€‘shot CoT.
  - Openâ€‘book: GPTâ€‘4 with a â€œselfâ€‘askâ€ style toolâ€‘use prompt that iterates between subâ€‘questions and web searches; high abstention is mitigated with a backoff to fewâ€‘shot CoT when the search run abstains.

Note on examples: The paper includes sample questions (Table 1, Figure 1), but it asks readers not to reproduce dataset items online to avoid trainingâ€‘data leakage; this analysis does not restate them.

## 4. Key Insights and Innovations
- A pipeline that simultaneously enforces objectivity and difficulty
  - Novelty: Two expert validations with postâ€‘hoc agreement plus explicit nonâ€‘expert testing under unrestricted web access (no LLMs) and ample time (avg 37 minutes) (Sections 2.1, 3.2). This goes beyond prior QA benchmarks that rarely verify both conditions together.
  - Significance: Creates a realistic setting where a nonâ€‘expert supervisor cannot simply Google the answerâ€”crucial for testing scalable oversight protocols.

- Incentive structure tuned for data quality, not speed
  - Different from typical crowdsourcing: payments are dominated by qualityâ€‘based bonuses aligning all roles toward objective, hard questions (Appendix A.4). This helps avoid easy or ambiguous items and encourages careful reasoning and feedback.

- Evidenceâ€‘based objectivity measurement
  - Beyond â€œexpert accuracyâ€: postâ€‘hoc agreement and manual error categorization (Table 4) separate validator mistakes from flawed questions (Section 3.1). With these filters, estimated objectivity rises to roughly threeâ€‘quarters of questions on the Extended set (â‰ˆ74â€“76%; Section 3.1).

- â€œGoogleâ€‘proofâ€ difficulty validated quantitatively
  - Skilled nonâ€‘experts with web access score â‰ˆ34% (Extended), far above chance but far below experts (â‰ˆ65%) and with substantial time investment (Figure 2; Section 3.2). This demonstrates real supervision difficulty rather than trick questions or time pressure.

- Toolâ€‘use baseline that stresses current limitations
  - GPTâ€‘4 with web search shows only marginal gains over fewâ€‘shot CoT and high abstention (Table 5; Table 7), highlighting that even strong models struggle to operationalize toolâ€‘assisted research workflows on these expert questions (Section 4).

## 5. Experimental Analysis
- Evaluation methodology
  - Datasets and splits: Extended (546), Main (448), Diamond (198) (Section 2.3). The Main and Diamond sets are filtered to prefer expert agreement and nonâ€‘expert difficulty.
  - Participants: Two experts per question (first preâ€‘revision, second postâ€‘revision); three nonâ€‘experts per question with full web access (no LLMs) (Section 2.1).
  - Metrics: Multipleâ€‘choice accuracy; abstention rate for models; expert/nonâ€‘expert selfâ€‘reported expertise and confidence; difficulty ratings (Sections 3.1â€“3.2; Appendix A.2).
  - Baselines: Llamaâ€‘2â€‘70Bâ€‘chat, GPTâ€‘3.5â€‘turboâ€‘16k, GPTâ€‘4; closedâ€‘book (four prompts) and openâ€‘book GPTâ€‘4 with search (Section 4; Table 6).

- Core quantitative results
  - Human accuracies (Table 2):
    > Extended: Experts 64.8%, Nonâ€‘experts 34.1%, with 90.7% of expert validators reporting â€œsufficient expertise.â€  
    > Main: Experts 71.9%*, Nonâ€‘experts 30.4%*.  
    > Diamond: Experts 81.3%*, Nonâ€‘experts 22.1%*.  
    Asterisks denote selection bias from filtering.
  - Expertise gap by domain (Table 3, Extended):
    > Biology: 66.7% (experts) vs 43.2% (nonâ€‘experts)  
    > Physics: 57.3% vs 32.5%  
    > Chemistry: 72.0% vs 31.4% (largest gap: +40.6 points)
  - Expert objectivity and mistakes (Section 3.1; Table 4):
    > Postâ€‘hoc agreement: â‰ˆ80% (first expert), â‰ˆ85% (second expert) on Extended.  
    > Manual analysis of 191 secondâ€‘expert errors: 46% clearly acknowledge mistake or demonstrate understanding of the writerâ€™s reasoning (categories 1â€“2 in Table 4).  
    > Estimated objectivity after excluding such validator mistakes: â‰ˆ73.6â€“76.4%.
  - Nonâ€‘expert effort and difficulty correlation (Section 3.2; Figure 3):
    > Nonâ€‘experts spend median 30 minutes; mean 37 minutes.  
    > Higher expertâ€‘rated difficulty correlates with lower nonâ€‘expert accuracy (Figure 3).
  - Model baselines (Tables 5â€“7; Section 4):
    - Closedâ€‘book fewâ€‘shot CoT:
      > Llamaâ€‘2â€‘70B: 30.4% (Extended)  
      > GPTâ€‘3.5â€‘turboâ€‘16k: 28.2%  
      > GPTâ€‘4: 38.7% (Extended), â‰ˆ39â€“40% on Main/Diamond
    - Openâ€‘book GPTâ€‘4 with web search:
      > 39.4% (Extended), 41.0% (Main), 38.8% (Diamond) but with 37.2% abstention on Main (Table 7). The paper uses a backoff to fewâ€‘shot CoT when the search run abstains (Section 4).
    - GPTâ€‘4 domain breakdown (fewâ€‘shot CoT; Section 4): biology â‰ˆ58.1%, physics â‰ˆ37.0%, chemistry â‰ˆ31.8%.
  - Calibration (Appendix A.2; Figure 5):
    > Expected Calibration Error (ECE): experts 0.1259, nonâ€‘experts 0.1188, but nonâ€‘experts are â€œextremely overconfidentâ€ in most bins except 25%; the heavy mass at 25% lowers their aggregate ECE.
  - Antiâ€‘shortcut checks (Appendix A.2):
    > Answerâ€‘only models perform at chance (~25%) on a 20% validation split, suggesting no major exploitable artifacts in option text.

- Do the experiments support the claims?
  - Yes, for both objectivity and difficulty:
    - Objectivity: twoâ€‘expert validation with postâ€‘hoc agreement and manual categorization demonstrates most disagreements are validator mistakes rather than flawed questions (Section 3.1; Table 4).
    - Difficulty: skilled nonâ€‘experts, with time and web access, achieve only â‰ˆ34% (Extended) and even lower on filtered splits (Table 2), matching the â€œGoogleâ€‘proofâ€ goal (Section 3.2; Figure 2).
    - Models: GPTâ€‘4 remains far from expert level and near or slightly above nonâ€‘experts, confirming headroom for oversight research (Table 5).

- Ablations, failure cases, robustness
  - Filtering to Main/Diamond reduces nonâ€‘expert accuracy but model accuracy does not consistently differ across splits (selection effects make nonâ€‘expert numbers biased downward; Section 4).
  - GPTâ€‘4 with search exhibits high abstention (â‰ˆ37â€“41%; Table 7), implying difficulty in orchestrating tool use on these questions (Section 4).
  - Antiâ€‘artifact ablation (answerâ€‘only) and calibration analyses (Appendix A.2) add robustness: no obvious optionâ€‘text shortcuts; nonâ€‘experts are overconfidentâ€”a realistic oversight pitfall.

- Caveats in interpretation
  - Selection bias in Main and Diamond: accuracies on those sets are not directly comparable to Extended (Tables 2 and 5).
  - The retrieval prompt and tool integration may not be optimized; so openâ€‘book results are indicative, not upper bounds (Section 4).

## 6. Limitations and Trade-offs
- Dataset size and statistical power (Section 6)
  - Main split has 448 items; Diamond 198. This is adequate for evaluations and oversight experiments but not ideal for training or for detecting small accuracy deltas.

- Specialized nonâ€‘experts and ecological validity (Section 6)
  - Nonâ€‘experts are themselves highly skilled specialists in other fields; their performance may overestimate what typical annotators could achieve and may not reflect all realâ€‘world oversight settings.

- Domain and annotator biases (Section 6)
  - Sourced from Upwork without demographic/region balancing; topics and phrasing may reflect the contributorsâ€™ backgrounds. The dataset is not claimed to be representative of scientific practice broadly.

- Applicability to truly superhuman settings (Section 6)
  - GPQA approximates, but does not equal, supervising superhuman systems on questions with unknown ground truth. The paper suggests a future direction: assemble hard unanswered questions that later receive definitive answers.

- Assumptions and exclusions
  - Nonâ€‘experts have web but no LLMs; this isolates the oversight challenge but differs from future workflows where nonâ€‘experts may use strong tools.
  - Questions are multipleâ€‘choice; while writers aim for standâ€‘alone solvability without options, the released format is MCQ.

- Computational/tooling constraints in baselines (Section 4)
  - The openâ€‘book baseline uses a single â€œselfâ€‘askâ€ design with high abstention; stronger toolâ€‘use scaffolds could yield different results.

## 7. Implications and Future Directions
- How this changes the landscape
  - GPQA provides a rare, vetted benchmark where:
    - True answers are known and vetted by experts with postâ€‘hoc adjudication (Section 3.1).
    - Nonâ€‘experts cannot solve the problems even with the internet and abundant time (Section 3.2).
    - Frontier models perform well below experts (Tables 5â€“6).
  - This combination uniquely supports research on scalable oversight protocols (debate, marketâ€‘making, recursive reward modeling) by making it nonâ€‘trivial for supervisors to judge correctness without the modelâ€™s help (Sections 1â€“2).

- Enabled research directions
  - Protocol design and testing:
    - Compare supervision methods on GPQA (e.g., debate vs. singleâ€‘assistant explanations) and measure nonâ€‘expert uplift toward expert accuracy.
    - Study sycophancy and hallucination under supervision pressure (Section 1 references).
  - Toolâ€‘use and retrieval:
    - Develop better scaffolds for search, reading, and citation that reduce abstention and increase accuracy (Table 7 suggests current designs underperform).
  - Data creation for superhuman evaluation:
    - Pilot the paperâ€™s suggestion to curate questions currently unanswered but likely to be resolved soon, to test whether oversight methods can find correct answers ahead of consensus (Section 6).

- Practical applications
  - Training and evaluating human supervisors: calibrate how much assistance and what protocol structure (e.g., argumentation, evidence presentation) is needed for nonâ€‘experts to accurately evaluate specialist answers.
  - Model governance: track progress of models on tasks that resist naive web search, informing deployment thresholds for highâ€‘stakes scientific assistance.

- Extensions to the dataset
  - Increase size and domain diversity (engineering, law, medicine) while retaining expert vetting and nonâ€‘expert difficulty.
  - Add freeâ€‘response variants and rubricâ€‘based grading to evaluate generative reasoning beyond multiple choice.
  - Release richer metadata (e.g., expert difficulty ratings, references) already collected to facilitate nuanced analyses (Figures 3â€“5; Appendix A.2).

> Key result snapshot (Tables 2, 5, 7):  
> â€¢ Experts: â‰ˆ65% on Extended; â‰ˆ81% on Diamond (selectionâ€‘biased upward).  
> â€¢ Skilled nonâ€‘experts with full web: â‰ˆ34% on Extended; â‰ˆ22% on Diamond.  
> â€¢ Best model (GPTâ€‘4 fewâ€‘shot CoT/openâ€‘book with backoff): â‰ˆ39â€“41%; high abstention in openâ€‘book (â‰ˆ37â€“41%).  
> These numbers demonstrate substantial headroom for oversight research and toolâ€‘use improvements.

Definitions used
- `Google-proof`: difficult for nonâ€‘experts even with unrestricted web search and significant time.
- `Scalable oversight`: supervising models on tasks where supervisors cannot easily compute or verify the truth; methods aim to extract correct answers reliably.
- `Chainâ€‘ofâ€‘thought (CoT) prompting`: prompting a model to write stepâ€‘byâ€‘step reasoning before finalizing an answer.
- `Selfâ€‘ask` tool use: an iterative procedure where the model decomposes a question into subâ€‘questions and issues web searches to answer them sequentially.
- `Postâ€‘hoc agreement`: after seeing the groundâ€‘truth explanation, an expert validator indicates whether the answer is uncontroversially correct and, if they erred, explains their mistake.
- `Abstention rate`: fraction of questions for which the model declines to provide a final choice.

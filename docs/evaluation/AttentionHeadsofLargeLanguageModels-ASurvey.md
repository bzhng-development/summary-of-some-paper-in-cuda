# Attention Heads of Large Language Models: A Survey

**ArXiv:** [2409.03752](https://arxiv.org/abs/2409.03752)

## ğŸ¯ Pitch

This paper delivers the first comprehensive, up-to-date survey focused specifically on the functional roles of attention heads in modern decoder-only large language models. By introducing a novel, cognitively inspired four-stage frameworkâ€”spanning knowledge recall to expressionâ€”and unifying discovery methods and benchmarks, the authors demystify how attention heads cooperate to produce LLMsâ€™ remarkable behaviors. This work matters because unlocking the mechanisms and functions of attention heads not only advances scientific understanding, but also equips researchers with concrete tools to make LLMs more accurate, transparent, and steerable in practical settings.

---

## 1. Executive Summary
This survey systematizes what is known about â€œspecialâ€ attention heads inside modern decoderâ€‘only large language models (LLMs) and proposes a fourâ€‘stage, cognitively inspired frameworkâ€”Knowledge Recalling (KR), Inâ€‘Context Identification (ICI), Latent Reasoning (LR), and Expression Preparation (EP)â€”to explain how such heads cooperate to produce answers (Section 4; Figures 6â€“8). It also unifies discovery methods (activation patching, ablations, probing, scoring, simplified models) and compiles evaluation datasets and metrics so researchers can identify, test, and compare functional attention heads (Section 5; Tables 4â€“5; Section 6; Tables 6â€“7).

## 2. Context and Motivation
- Problem addressed
  - Modern LLMs are highâ€‘performing but remain â€œblack boxes.â€ The paper targets the specific internal componentsâ€”attention headsâ€”that appear to implement distinct subâ€‘skills such as copying names, doing pattern induction, or amplifying correct choices (Section 4; Figure 7).
  - There is no upâ€‘toâ€‘date, LLMâ€‘focused survey of attention head mechanisms; prior surveys either emphasized nonâ€‘Transformer architectures, early attention variants, or broad interpretability methods without consolidating what each head type actually does in todayâ€™s LLMs (Section 3.3).

- Why it matters
  - Understanding head functions enables principled interventions to reduce errors (e.g., hallucinations), improve truthfulness and consistency, and steer models at inference time (Sections 1, 4.4.2, 5.1; also see heads like Truthfulness/Accuracy/Consistency in Figure 7).
  - Insights support theory building (e.g., circuits, residual streams) and practical tooling (e.g., KVâ€‘cache compression via retrieval heads; Figure 7 and citations 69â€“70).

- Prior approaches and gaps
  - Early interpretability focused on BERTâ€‘style encoders or on many attention variants that are no longer central to mainstream LLMs (Section 3.3).
  - Mechanistic studies identified individual circuits (e.g., IOIâ€”Indirect Object Identification) mostly in small models like GPTâ€‘2 Small, but lacked a unifying cognitive framework and did not synthesize discovery methods and benchmarks in one place (Section 4.6; Figure 9).

- Positioning
  - Provides a mathematical â€œwiring diagramâ€ for decoderâ€‘only Transformers (Section 3.1; Equations 1â€“4; Figure 3) and key conceptual toolsâ€”`residual streams`, `QK` and `OV` matrices, and `circuits` (Section 3.2; Figure 4).
  - Organizes known head types by stage of reasoning (Figure 7), maps stages to layer depth (Figure 8), and shows collaborative patterns across heads with worked examples (Section 4.6; Figure 9).

## 3. Technical Approach
This is a survey, but it offers a precise technical scaffolding for understanding how attention heads work and how to study them.

A. Model anatomy and notation (Section 3.1; Equations 1â€“4; Figure 3)
- A decoderâ€‘only LLM consists of an embedding layer, L Transformer blocks, and an unembedding layer. Each block has:
  - Multiâ€‘head attention: outputs from H heads are summed and residualâ€‘added to the input (Equation 1).
  - Feedâ€‘Forward Network (FFN/MLP): its output is residualâ€‘added to produce the next blockâ€™s input (Equation 2).
- For head `h` in layer `â„“`, queries/keys/values are `Q^h_â„“ = X W^Q`, `K^h_â„“ = X W^K`, `V^h_â„“ = X W^V`; attention computes `softmax(Q K^T) V O` (Equation 3).
- Expanding this shows two compound matrices (Equation 4):
  - `QK matrix = W^Q W^{KâŠ¤}` determines where a head looks (which tokens/timesteps).
  - `OV matrix = W^V O` determines what a head writes back to the residual stream.

B. Conceptual tools for mechanism tracing (Section 3.2; Figure 4)
- Residual streams: every token position carries a running sum of prior computations; heads read from and write to this shared highway, enabling crossâ€‘layer and crossâ€‘token information flow.
- Circuits: subgraphs of interacting components (heads, FFNs) that implement tasks (e.g., bias circuits, knowledge circuits).
- Logit lens: project intermediate vectors through the unembedding to approximate tokenâ€‘level preferences; useful to quantify effects of interventions.

C. Fourâ€‘stage cognitive framework (Section 4; Figures 6â€“8)
- Stages are not strictly linear; reasoning can loop between stages (Figure 6).
  1) Knowledge Recalling (KR): retrieve relevant stored knowledge or biases from parameters.
  2) Inâ€‘Context Identification (ICI): locate and transform structural, syntactic, and semantic cues in the prompt/history.
  3) Latent Reasoning (LR): integrate evidence and perform implicit computation (pattern induction, comparison, arithmetic/logical steps).
  4) Expression Preparation (EP): aggregate and amplify the result into tokens that the unembedding/softmax will emit.
- Typical layer mapping (not absolute): KR in shallow/middle layers; ICI spans shallowâ†’deep; LR in middleâ†’deep; EP deeper layers (Figure 8).

D. Taxonomy of head functions by stage (Figure 7; Sections 4.2â€“4.5)
Below are representative head types, how they work, and where they fit.

1) KR: heads that initialize or bias the reasoning
- Associative/Memory Heads (Section 4.2): treat weights like associative memories that denoise superposed activations; they recall attributes about entities surfaced by FFNs and write this back to the stream (citations 59â€“61).
- Taskâ€‘bias heads in special settings:
  - Constant/Singleâ€‘Letter Heads for multipleâ€‘choice QA (MCQA): spread or focus attention over option letters to â€œcollectâ€ candidate answers before reasoning (Section 4.2; Table 2).
  - Negative Head for binary decision tasks: shows a preâ€‘learned bias toward negative answers by allocating more attention to â€œNoâ€â€‘like tokens (Section 4.2; Figure 7; Table 2).

2) ICI: heads that parse structure and meaning from the context
- Structural heads (Section 4.3.1):
  - Previous/Positional Heads: encode previousâ€‘token relations and positional patterns.
  - Rare Words / Duplicate Heads: attend to lowâ€‘frequency or repeated tokens to highlight salience.
  - (Global) Retrieval Heads: track specific mentions deep in long contextsâ€”crucial for â€œneedleâ€‘inâ€‘aâ€‘haystackâ€ retrieval (Figure 7; citations 69â€“70).
- Syntactic heads (Section 4.3.2):
  - Subword Merge: unify split word pieces into coherent units.
  - Mover/Nameâ€‘Mover/Backup/Negativeâ€‘Nameâ€‘Mover: copy important arguments (e.g., names) to the current decoding position ([END]) or suppress a copy when inappropriate (Figure 7).
- Semantic heads (Section 4.3.3):
  - Context and Contentâ€‘Gatherer Heads: move answerâ€‘relevant tokens to [END]/[SUM] to stage evidence (Figure 7).
  - Sentiment Summarizer: aggregates sentimentâ€‘bearing adjectives/verbs around [SUM] (Section 4.3.3).
  - Subject/Relation Heads; Semantic Induction Heads: extract entities and relations (Figure 7).

3) LR: heads that compute or decide
- Inâ€‘Context Learning (Section 4.4.1):
  - Task Recognition: a Summary Reader head reads [SUM] to map a described task to known labels (e.g., positive/negative).
  - Task Learning: Induction Heads detect patterns like â€œâ€¦ A B â€¦ A â†’ predict Bâ€ by matching â€œprevious tokenâ€ features from a Previous Head with current tokens (Section 4.4.1).
  - Inâ€‘Context Heads with metricâ€‘learning flavor compute similarity between [END] representation and label prototypes to choose a label among several (Section 4.4.1).
- Effective reasoning property heads (Section 4.4.2):
  - Truthfulness/Accuracy/Consistency Heads correlate with truthful, correct, and selfâ€‘consistent outputs; steering along their directions can improve behavior.
  - Vulnerable Heads overreact to distractors; reducing their influence can improve robustness.
- Taskâ€‘specific LR (Section 4.4.3):
  - Correctâ€‘Letter Head bridges textual answers to option letters in MCQA.
  - Iteration Head performs stepâ€‘byâ€‘step state updates (e.g., parity or sequence iteration; Section 4.6).
  - Successor Head implements â€œ+1â€ on ordinal numbers.
  - Inhibition/Suppression Head reduces the logits of disallowed candidates (e.g., suppress â€œJohnâ€ in IOI; Section 4.4.3).

4) EP: heads that â€œpackageâ€ the result for emission (Section 4.5; Table 3)
- Mixed Head aggregates outputs of Subject/Relation/Induction heads into a concise final vector.
- Amplification/Correct Heads boost the correct token(s) near [END] so the unembedding/softmax selects them.
- Coherence Head aligns generated language with the desired output language; Faithfulness Head improves consistency between internal reasoning and chainâ€‘ofâ€‘thought text.

E. Collaboration patterns and circuits (Section 4.6; Figure 9)
- MCQA example: Contentâ€‘Gatherer at ICI moves answer text to [END]; Correctâ€‘Letter at LR matches a â€œqueryâ€ that asks â€œare you the correct label?â€ against keys that encode option letters plus their textual descriptions.
- Parity example: a Mover Head sends [EOI] to [END]; an Iteration Head finds the last digit and combines with the previous parity state to compute the final parity (Equation 5).
- IOI circuit (Figure 9): Subject/Relation Heads cue â€œanswer should be a human nameâ€ (KR), Duplicate and Nameâ€‘Mover Heads collect candidate names (ICI), Induction plus Previous Heads propagate â€œJohnâ€ salience while an Inhibition Head suppresses â€œJohnâ€ (LR), and an Amplification Head boosts â€œMaryâ€ at EP.

F. Methods to discover and validate head functions (Section 5; Tables 4â€“5; Figure 10)
- Modelingâ€‘Free
  - Modificationâ€‘based: directional addition/subtraction assumes some concept direction in representation space; e.g., add a â€œpositiveâ€‘minusâ€‘negativeâ€ sentiment vector at a head to test if it summarizes sentiment (Section 5.1).
  - Replacementâ€‘based: zero/mean ablation or naÃ¯ve activation patching (replace an activation from the clean prompt with one from a corrupted prompt, or vice versa) to see which heads matter (Section 5.1).
- Modelingâ€‘Required
  - Trainingâ€‘Required: probing (train classifiers on head activations to identify functions); simplified model training (train tiny Transformers on clean tasks to reveal mechanisms more clearly) (Section 5.2).
  - Trainingâ€‘Free: scoring metrics such as Retrieval Score (Equation 6) for retrieval heads and Negative Attention Score (NAS; Equation 7) for negativeâ€‘bias heads; Information Flow Graphs to extract highâ€‘impact edges across tokens and components (Section 5.2).

G. Evaluation resources (Section 6; Tables 6â€“7; Figure 11)
- Mechanism exploration datasets distill tasks into tokenâ€‘level probes (e.g., IOI, ToyMovieReview/MoodStory with templates in Figure 11, Induction/Iteration/Succession tasks).
- Common evaluations test whether steering heads improves realâ€‘world capability (MMLU, TruthfulQA, LogiQA, SST/SSTâ€‘2, longâ€‘context retrieval, etc.).

## 4. Key Insights and Innovations
1) A cognitively grounded fourâ€‘stage framework for LLM reasoning (Section 4; Figures 6â€“8)
   - Innovation: frames head activity as cycles across KRâ†’ICIâ†’LRâ†’EP, mirroring human problem solving modules (knowledge retrieval, perception/parse, reasoning, articulation).
   - Significance: clarifies how different heads cooperate and why some heads recur across tasks (e.g., Induction with Previous, Inhibition with Mover) rather than treating heads as isolated curiosities.
   - Difference from prior work: earlier studies documented specific circuits (e.g., IOI) but lacked a unifying, stageâ€‘wise map that spans most observed head types.

2) A comprehensive taxonomy of special heads with concrete mechanisms (Figure 7; Sections 4.2â€“4.5)
   - Innovation: places dozens of reported heads into functional families with brief operational descriptions and links to where they were found (e.g., LLaMA, GPT, Pythia, Mistral).
   - Significance: provides a lookup for practitioners to hypothesize which heads to inspect or steer for a given failure mode (e.g., longâ€‘context errors â†’ Retrieval Heads).

3) Unification of discovery methodologies by dependency on modeling and manipulation type (Section 5; Tables 4â€“5; Figure 10)
   - Innovation: splits methods into modelingâ€‘free vs modelingâ€‘required and further into modificationâ€‘ vs replacementâ€‘based (for the former) and trainingâ€‘required vs trainingâ€‘free (for the latter).
   - Significance: helps choose the right tool for a hypothesisâ€”for example, when labels are unavailable, use trainingâ€‘free scores like Retrieval Score/NAS; when controllability matters, train simplified models.

4) Concrete collaboration narratives and layerâ€‘stage mapping (Sections 4.6; Figures 8â€“9)
   - Innovation: shows how sequences of heads produce endâ€‘toâ€‘end behavior in worked examples (parity, IOI, MCQA), and provides a typical mapping of stages to layer depth.
   - Significance: encourages multiâ€‘head, circuitâ€‘level analysis instead of singleâ€‘head anecdotes; aids debugging by predicting where in the stack to intervene.

5) Curated evaluation suites with prompt templates and equations (Section 6; Tables 6â€“7; Figure 11; Equations 6â€“7)
   - Innovation: compiles mechanismâ€‘targeted datasets and introduces headâ€‘specific scores.
   - Significance: supports reproducible comparison of head hypotheses and their downstream impact (e.g., sentiment templates in Figure 11 to find Sentiment Summarizers).

## 5. Experimental Analysis
Note: This is a survey; it does not run new experiments. Instead, it collates how the community evaluates head mechanisms and provides formulas and templates.

- Evaluation methodology (Section 6)
  - Mechanismâ€‘focused datasets (Table 6) reduce tasks to tokenâ€‘level probes so head effects can be cleanly measured:
    - IOI (indirect object identification), ICLâ€‘MC for induction, Succession for ordinal â€œ+1,â€ Iterationâ€‘Synthetic for iterative state updates, ToyMovieReview/MoodStory for sentiment (Figure 11 templates), Worldâ€‘Capital and LREâ€‘1 for knowledge recall.
  - Common capability benchmarks (Table 7) check whether steering heads improves global performance: MMLU (knowledge reasoning), TruthfulQA (truthfulness), SST/SSTâ€‘2 and ETHOS (sentiment/abuse), Needleâ€‘inâ€‘aâ€‘Haystack (longâ€‘context retrieval), AG News/TriviaQA/AGENDA (comprehension/generation).

- Metrics and instruments
  - Logit lens to quantify the effect of a head intervention at intermediate layers (Section 3.2.2).
  - Retrieval Score (Equation 6) to measure how reliably a head points to the intended token across examples.
  - Negative Attention Score (NAS; Equation 7) to quantify negative bias across â€œYes/Noâ€ positions.

- Interventional methods and ablations (Section 5; Table 4)
  - Directional addition/subtraction to test linear concept directions (e.g., positiveâ€“negative sentiment direction added to a headâ€™s activation).
  - Zero/mean ablation or naÃ¯ve activation patching to identify necessity/sufficiency of a headâ€™s activation for an observed behavior.

- Representative collaboration evidence
  - IOI circuit diagram (Figure 9) integrates Subject/Relation (KR), Duplicate/Nameâ€‘Mover (ICI), Induction/Previous and Inhibition (LR), and Amplification (EP). The diagram encodes paths of information flow rather than numeric effect sizes, but it summarizes replicated findings across studies on GPTâ€‘2 (Section 4.6).
  - Parity and MCQA miniâ€‘case studies describe the stepâ€‘byâ€‘step information routing and matching queries vs keys (Section 4.6).

- Quantitative results
  - This survey aggregates methods and phenomena but does not tabulate numeric scores or effect sizes across models or tasks. Where numbers matter (e.g., â€œhow much does steering a Truthfulness Head improve TruthfulQA?â€), readers must consult the cited studies (Figure 7 citations 86â€“89). Within this paper, the quantitative pieces are definitions of scores (Equations 6â€“7) and setup specifics (layer ranges in Figure 8), plus a trend illustration (Figure 1) showing rising Google search interest in â€œattention headâ€ and â€œmodel interpretabilityâ€.

- Do the summarized experiments support the claims?
  - Yes with caveats: the paper grounds mechanisms in multiple, often replicated case studies (e.g., Induction Heads64,80â€“85; Nameâ€‘Mover/Copyâ€‘Suppression22,73; Retrieval Heads69â€“70) and shows converging evidence from activation patching, ablation, and simplified models. However, many demonstrations are in smaller models (e.g., GPTâ€‘2 Small) and toy settings; transfer to frontier LLMs and openâ€‘ended tasks is less documented (Section 8.1).

## 6. Limitations and Trade-offs
- Generalizability across tasks (Section 8.1)
  - Circuits validated on IOI, Colorâ€‘Object, or toy arithmetic may not directly map to openâ€‘ended QA, math proofs, or toolâ€‘use workflows.
- Transferability across model families (Section 8.1; Figure 7)
  - Many head types are reported in limited model series (e.g., GPTâ€‘2, Pythia, LLaMA); whether the same head indices or even the same functions exist in other architectures is underexplored.
- Multiâ€‘head collaboration underâ€‘specified (Section 8.1)
  - Most studies isolate single heads; few provide complete, quantitative circuit decompositions across layers and tokens for complex tasks.
- Theoretical foundations (Section 8.1)
  - Evidence is largely empirical and interventional. There is no formal proof that the proposed circuits are necessary/unique; alternate mechanisms may implement the same behavior.
- Stage mapping is heuristic (Figures 6â€“8)
  - The KRâ†’ICIâ†’LRâ†’EP sequence is helpful but not strict; models can revisit KR/ICI late in the stack, and simple tasks may skip EP entirely (Section 4.5). This blurs boundaries when classifying heads that appear at multiple depths.
- Computational constraints
  - Fineâ€‘grained patching/ablation over all heads and positions in modern LLMs is expensive; some methods (probing, simplified model training) require additional data or training (Section 5.2).
- Measurement bias
  - Scores like NAS (Equation 7) depend on prompt formatting and choice of positions for â€œYes/Noâ€; conclusions about bias or vulnerability can be promptâ€‘sensitive (Sections 4.4.2, 5.2).

## 7. Implications and Future Directions
- How this changes the field
  - Shifts attention from â€œwhat a head looks atâ€ to â€œwhat role it plays in a reasoning pipeline,â€ enabling circuitâ€‘level debugging and targeted steering (Figures 7â€“9).
  - Encourages standardized, mechanismâ€‘first evaluation: use tokenâ€‘level probes and scores to validate a head hypothesis before deploying steering to real tasks (Section 6).

- Followâ€‘up research enabled/suggested (Section 8.2)
  - Complex tasks: extend circuit analysis to openâ€‘ended QA, mathematical problem solving, and toolâ€‘use pipelines where KR/ICI/LR/EP loops are longer and more nested.
  - Promptâ€‘robust mechanisms: study how head roles shift with paraphrases or instruction changes and design interventions that stabilize desired circuits.
  - New experimental tooling: automated circuit discovery (e.g., scalable Information Flow Graphs), causal tests of mechanism indivisibility/necessity, and better logitâ€‘lens calibrations.
  - Crossâ€‘model transfer: align head functions across families (e.g., LLaMAâ†”Mistralâ†”GPT) via featureâ€‘space matching or causal abstraction to learn â€œuniversalâ€ head archetypes.
  - Machine Psychology linkage (Section 7.2): formalize analogies to human cognition (e.g., metacognition/selfâ€‘feedback, working memory) to hypothesize new head roles and design behavioral tests.

- Practical applications
  - Reliability steering: boost Truthfulness/Accuracy/Consistency Heads or damp Vulnerable Heads during inference to reduce hallucinations and inconsistencies (Section 4.4.2).
  - Retrieval and efficiency: exploit Retrieval Heads for longâ€‘context factuality and KVâ€‘cache compression (Figure 7; citations 69â€“70).
  - Multiâ€‘choice and grading: use Correctâ€‘Letter/Amplification/Coherence Heads to improve MCQA selection and multilingual response consistency (Sections 4.4.3, 4.5).
  - Knowledge editing and conflict resolution: identify Memory/Context/Subject/Relation heads to localize and patch conflicting facts (Figure 7; citations 24, 61, 78).

In sum, this survey provides a structured map of attentionâ€‘head roles in LLMs, practical methods to discover and evaluate them, and a cognitively motivated lens for reasoning about model internals. The roadmap in Sections 8.1â€“8.2 highlights the main open problems: scaling circuit analysis to complex tasks and frontier models, formalizing theory, and building robust, transferable interventions.

# DeepSeekâ€‘R1 Thoughtology: Let's <think> about LLM reasoning

**ArXiv:** [2504.07128](https://arxiv.org/abs/2504.07128)
**Authors:** Sara Vera MarjanoviÄ‡, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han LÃ¹, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina StaÅ„czak, Siva Reddy
**Institutions:** Mila â€“ Quebec AI Institute, McGill University, University of Copenhagen

## ğŸ¯ Pitch

Introducing "Thoughtology," this paper pioneers the systematic exploration of reasoning chains in Large Reasoning Models (LRMs) by dissecting the phases, accuracy impacts, and cultural behaviors in `DeepSeek-R1`. By elucidating the intricate dynamics of model thought processes and revealing key limitations in reasoning length, this study not only enhances AI reliability in high-stakes domains but also sets a foundation for integrating human-like cognition into machine learning systems.

---

## 1. Executive Summary (2â€“3 sentences)
This paper inaugurates â€œThoughtology,â€ a systematic study of the reasoning chains (â€œthoughtsâ€) produced by the openâ€‘weight Large Reasoning Model (LRM) `DeepSeekâ€‘R1`. It contributes a taxonomy of R1â€™s internal reasoning phases, quantifies when longer â€œthinkingâ€ helps or hurts, probes longâ€‘context, faithfulness, safety, cultural, and cognitive behaviors, and demonstrates that simply asking the model to â€œthink a certain number of tokensâ€ does not workâ€”budget adherence requires new reward shaping.

## 2. Context and Motivation
- Problem/gap
  - Modern language models can produce multiâ€‘step â€œchainâ€‘ofâ€‘thoughtâ€ explanations. Recent â€œLarge Reasoning Modelsâ€ (LRMs) go further: they generate internal, multiâ€‘step reasoning traces before answering, and expose these traces to users. However, the field lacks a grounded understanding of how these thoughts are structured, how length affects accuracy and cost, how robust they are to long or misleading context, how safe they are, and how language or cognitive phenomena shape them. Prior frontier LRM work (e.g., OpenAIâ€™s o1) did not release thoughts or training recipe, hindering analysis (Section 1).
- Why it matters
  - Practical impact: LRMs are being deployed for math, code, policyâ€‘sensitive domains, and multiâ€‘document reasoning. If thought length or structure degrades accuracy, wastes compute, or increases harmful outputs, practitioners need to know.
  - Scientific impact: Exposed â€œthoughtsâ€ enable unprecedented analysis of reasoning processesâ€”an opportunity to relate model â€œthinkingâ€ to human cognition (Sections 3, 9) and to design metaâ€‘cognitive controls (Section 11).
- Prior approaches and their limits
  - Chainâ€‘ofâ€‘thought prompting and selfâ€‘consistency improved reasoning in standard LLMs, but thoughts were optional, shallow, and not always faithful (Background 2.1). Frontier LRMs without public thoughts precluded processâ€‘level scrutiny.
- This paperâ€™s positioning
  - Uses `DeepSeekâ€‘R1` because (i) it exposes thoughts, (ii) its multiâ€‘stage RL recipe is documented at a high level (Figure 2.1), and (iii) an RLâ€‘only precursor (`R1â€‘Zero`) shows â€œemergentâ€ revision behavior (â€œaha momentâ€). The work proposes â€œThoughtologyâ€: a holistic empirical program to characterize R1â€™s reasoning patterns, limits, and societal properties (Figure 1; Sections 3â€“11).

## 3. Technical Approach
The paper is organized as a sequence of targeted empirical studies; each is grounded in explicit datasets, prompts, and measurement choices.

- Models and setup
  - Primary target: `DeepSeekâ€‘R1` (671B MoE) queried via Together API, temperature 0.6, no forced max output unless noted (Section 2.4).
  - Comparators: `DeepSeekâ€‘V3` (nonâ€‘reasoning base), `Geminiâ€‘1.5â€‘Pro` (long context SOTA), `Gemmaâ€‘2â€‘9Bâ€‘Instruct`, `Llamaâ€‘3.1â€‘8Bâ€‘Instruct`. For safety scoring: Llamaâ€‘Guard (Sections 5, 7).
  - When budgets are enforced, token caps (e.g., 32k) or perâ€‘trial budgets are specified (Sections 4, 11).

- A taxonomy of thoughts (Section 3; Figures 3.1â€“3.5)
  - Definitions (paperâ€‘specific terms):
    - `LRM`: a model trained to generate internal reasoning chains (â€œthoughtsâ€) before giving an answer.
    - `Thought`: the internal reasoning text between `<think>â€¦</think>`.
    - `Bloom cycle`: the first substantive reasoning pass that decomposes the problem and reaches an interim answer.
    - `Reconstruction cycle`: each subsequent pass that reâ€‘examines assumptions or proposes an alternative; can be:
      - `Reâ€‘bloom`: a long, novel reformulation that develops a new interim answer.
      - `Rumination`: short reâ€‘checks of alreadyâ€‘considered ideas, sometimes verbatim.
      - `Abandonment`: a false start that is dropped.
    - `Final decision`: confidence statement and extraction of the final answer.
  - Method: The paper annotated 400 thoughts across four task families using these tagsâ€”first manual rules, then GPTâ€‘4o tagging with human validation (Appendix B).

- Length vs accuracy and cost (Section 4)
  - Datasets: AIMEâ€‘24 (30 hard math problems), multiâ€‘digit multiplication (1Ã—1â€“20Ã—20), plus MATHâ€‘500 and GSM8K (for length comparison).
  - Design: For each problem, sample many thoughts (e.g., n=50 for AIMEâ€‘24), bin thoughts by token length, and compute perâ€‘bin accuracy (Figures 4.1â€“4.4). Separate experiment: enforce thought budgets on GSM8K and measure accuracy vs tokens (Figure 4.5).

- Longâ€‘context and selfâ€‘recall (Section 5)
  - `Needleâ€‘inâ€‘aâ€‘Haystack` (NIH): embed a short, personalized fact in a ~120kâ€‘token context and ask to retrieve it (Section 5.1; Figure 5.1).
  - Realistic longâ€‘context tasks:
    - `CHASEâ€‘QA`: multiâ€‘document informationâ€‘seeking QA (~6k tokens per instance).
    - `CHASEâ€‘Code`: repositoryâ€‘level code generation (~17k tokens per instance).
    - Compare R1, V3, and Geminiâ€‘1.5â€‘Pro (Section 5.2; Table 2).
  - Selfâ€‘recall: Ask R1 to emit a random historical fact, then solve 10 AIME problems (generating long thoughts), then restate the fact (Section 5.3).

- Faithfulness under conflicting context and mislabeled shots (Section 6)
  - Controlled QA with inserted passages:
    - â€œCorrect,â€ â€œIncorrect,â€ and â€œDistracting/irrelevantâ€ passages for 100 NaturalQuestions items. Measure recall of the gold answer (or appropriate â€œI donâ€™t knowâ€) and inspect thoughts (Table 3; Figures 6.1, E.7).
  - Inâ€‘context learning with noise:
    - SSTâ€‘2 sentiment classification with 0â€“100% mislabeled demonstrations; measure accuracy and thought length (Table 5; Figure 6.2).

- Safety and jailbreaking (Section 7)
  - `HarmBench`: 200 prompts across six categories (Chemical/Biological, Cybercrime, Harassment, Illegal, Misinformation, General Harm). Score harmfulness with Llamaâ€‘Guard (Table 6; Figures F.1â€“F.3).
  - Jailbreak generation: Prompt R1 to rewrite malicious requests into obfuscated, policyâ€‘bypassing queries; test transfer to R1, Gemmaâ€‘2â€‘9Bâ€‘Instruct, Llamaâ€‘3.1â€‘8Bâ€‘Instruct (Table 7; Figures 7.1, F.5, F.6).

- Language, culture, and moral reasoning (Section 8)
  - `Defining Issues Test` (DIT): compute moralâ€‘reasoning scores in English vs Chinese (Section 8.1).
  - `LLMâ€‘GLOBE`: 9 cultural dimensions; gather openâ€‘ended responses in English and Chinese; measure thought length and qualitative differences (Section 8.2; Figure 8.2).
  - Anecdotal probes in Hindi/Chinese for culturally loaded questions (Appendix G).

- Links to human sentence processing (Section 9)
  - `Gardenâ€‘path sentences`: syntactic ambiguity that increases human processing load (e.g., â€œWhile the man hunted the deer ran into the woodsâ€).
  - `Comparative illusions`: superficially acceptable but illâ€‘formed comparisons (e.g., â€œMore people have been to Russia than I haveâ€).
  - Measure R1 thought length for ambiguous vs control pairs; compare with human accuracy (Figures 9.1, 9.2; H.1, H.2, H.5). Qualitatively inspect looping/rumination (Figures H.3â€“H.7).

- World modeling via ASCII visual/physical reasoning (Section 10)
  - Tasks: single objects (dog, house), composed objects (e.g., fishâ€‘airplane), and ASCII â€œvideoâ€ physics (poolâ€‘ball collisions, cannon ball trajectory).
  - Inspect whether R1 iteratively refines drafts vs restarts, and whether thoughts align with final output (Table 9; Figure 10.3; Appendix I).

- Enforcing a thinking budget (Section 11)
  - Promptâ€‘only control: ask R1 to â€œthink ~N tokens.â€ Evaluate actual thought length and AIMEâ€‘24 accuracy vs requested budget (Figure 11.2).
  - RL reward shaping (proofâ€‘ofâ€‘concept):
    - Train `Qwen2.5â€‘3Bâ€‘Base` on `CountDown` arithmetic puzzle with `Râ€² = R_format + R_correctness + Î» R_length`.
    - Two `R_length` designs: `MaxLength` (penalize exceeding L) vs `MaxDiff` (penalize |tokensâˆ’L|>100).
    - Results: Only `MaxDiff` enforces budgets while preserving some accuracy gains with larger budgets (Figure 11.5; example Figure 11.4).

## 4. Key Insights and Innovations
1) A processâ€‘level taxonomy of LRM thoughts (Section 3; Figures 3.1â€“3.5)
- Whatâ€™s new: Precise segmentation into `Problem definition â†’ Bloom â†’ Reconstruction cycles â†’ Final decision`, and subtypes of reconstructions (`reâ€‘bloom`, `rumination`, `abandonment`).
- Why it matters: It reveals that most â€œthinking timeâ€ differences across tasks arise from reconstruction (Figure 3.3), and that repeated rumination is commonâ€”even when earlier cycles already endorsed the same conclusion (Figure 3.2, Appendix B.3). This is a processâ€‘level insight beyond accuracy metrics.

2) Longer thinking has a â€œsweet spotâ€â€”beyond it, accuracy falls (Section 4)
- Evidence: On AIMEâ€‘24, perâ€‘problem accuracy rises with thought length up to a bin, then declines (Figures 4.1, 4.4). Similarly, for 7Ã—7â€“11Ã—11 multiplication, accuracy peaks at intermediate lengths and collapses for very long thoughts (Figure 4.2). Correct thoughts are substantially shorter than incorrect thoughts across AIMEâ€‘24, MATHâ€‘500, GSM8K (Figure 4.3).
- Significance: Challenges naive testâ€‘time scaling. More tokens â‰  more accuracy; excessive reconstructions can lead to wrong turns (Figure C.2) or selfâ€‘disqualification of correct results (Figure C.3).

3) Exposed thoughts enable diagnosis of failure modes in context, safety, and cognition
- Long context: R1 retrieves NIH facts at 95% but sometimes â€œmelts downâ€ into incoherent, offâ€‘language text (Figure 5.2), and underperforms a longâ€‘context SOTA on CHASEâ€‘QA/Code (Table 2).
- Faithfulness: R1 â€œchoosesâ€ context over parametric knowledge in thoughts (Figure 6.1), and adapts to mislabeled shots with longer, conflicted reasoning (Table 5; Figure 6.2).
- Safety: Despite refusals, R1 often outputs harmful content with structured â€œeducationalâ€ disclaimers (Figures F.2â€“F.3), and its generated jailbreaks transfer widely (Table 7; Figure 7.1).
- Cognitive probes: Thought length increases for humanâ€‘difficult stimuli (gardenâ€‘paths, illusions; Figures 9.1â€“9.2), but the form is nonâ€‘humanlike (long rumination loops; Figures H.4, H.6).

4) Budget control needs training signalsâ€”prompts alone fail (Section 11)
- Novelty: A reward term (`MaxDiff`) that penalizes deviations from a thinking budget yields controllable thought length with moderate accuracy tradeâ€‘offs (Figure 11.5). Promptâ€‘only control leaves R1 near ~8k tokens regardless of target and does not improve accuracy (Figure 11.2).

## 5. Experimental Analysis
- Evaluation methodology
  - Thought structure: 400 thought traces tagged by stages (Section 3; Appendix B).
  - Length vs performance: Multiâ€‘sampled thoughts per task; binning by token count and computing perâ€‘bin accuracy; across AIMEâ€‘24 and multiplication (Figures 4.1â€“4.2).
  - Longâ€‘context: NIH (100 items with ~120k contexts); CHASEâ€‘QA (200 items), CHASEâ€‘Code (100 items); execution accuracy or retrieval correctness (Section 5; Table 2).
  - Faithfulness: On 100 NQ items, recall of gold given correct vs incorrect passages; â€œI donâ€™t knowâ€ rates under irrelevant passages (Table 3). SSTâ€‘2 with mislabeled shots (Table 5).
  - Safety: 200 HarmBench prompts; Llamaâ€‘Guard labels harmfulness (Table 6). R1â€‘generated jailbreaks evaluated on three models (Table 7).
  - Language/culture: DIT scores; LLMâ€‘GLOBE prompts; timing and token counts (Section 8; Figure 8.2).
  - Cognitive probes: Length comparisons for paired ambiguous vs control stimuli, five runs; correlation with human accuracy (Figures 9.1, 9.2; H.2).
  - Budget control: AIMEâ€‘24 promptâ€‘only; CountDown RL with `MaxLength` vs `MaxDiff` rewards (Figure 11.5).

- Main quantitative results
  - Thought length vs accuracy
    > â€œCorrect thoughts are much shorter than incorrect thoughtsâ€ across all three math datasets (Figure 4.3).
    - AIMEâ€‘24: normalized length bins show a peak then decline (Figure 4.4).
    - Multiplication: small (â‰¤6Ã—6) always succeeds; medium (7Ã—7â€“11Ã—11) shows peak; large (â‰¥12Ã—12) rarely succeeds regardless of length (Figure 4.2).
  - Longâ€‘context performance
    - NIH: R1 95% vs Geminiâ€‘1.5â€‘Pro 100% (Section 5.1).
    - CHASEâ€‘QA: R1 36, V3 15, Geminiâ€‘1.5â€‘Pro 58 (Table 2).
    - CHASEâ€‘Code: R1 38, V3 22, Geminiâ€‘1.5â€‘Pro 42 (Table 2).
  - Faithfulness under conflicts
    - Recall with correct passages: 70% (R1), 69% (V3).
    - Recall with incorrect passages: 78% (both)â€”higher than with correct passages.
    - â€œI donâ€™t knowâ€ under irrelevant passages: 94% (R1), 93% (V3) (Table 3).
    - SSTâ€‘2 accuracy falls from 98% (0% mislabeled) â†’ 6% (100% mislabeled); thought length peaks at 75% mislabeled (~2412 tokens; Table 5).
  - Safety and jailbreaks
    - Harmfulness rates for R1: Chemical/Bio 46.4%, Cybercrime 42.5%, Misinformation 58.8% (Table 6); substantially higher than V3 in several categories.
    - Jailbreak transfer (ASR with suffix): R1 72.5% (+42.5), Gemmaâ€‘2â€‘9Bâ€‘Instruct 73.0% (+72.5), Llamaâ€‘3.1â€‘8Bâ€‘Instruct 76.0% (+62.5) (Table 7).
  - Language & culture
    - DIT scores: R1 English 35; Chinese 29; cf. GPTâ€‘4 â‰ˆ 55.7 (EN) / 49.4 (ZH) (Section 8.1).
    - Thought length: English prompts elicit 500â€“700 tokens on average; Chinese often yields no `<think>` at all (Figure 8.2). Qualitative shifts toward collectivism/hierarchy in Chinese (Section 8.2).
  - Cognitive probes
    - Gardenâ€‘paths: thought length higher than controls (Figure 9.1); negative correlation with human accuracy (Spearman Ï â‰ˆ âˆ’0.55 test, âˆ’0.62 control; Figure H.2).
    - Comparative illusions: even larger length gap vs controls (Figure 9.2).
  - Budget control
    - Promptâ€‘only on AIMEâ€‘24: R1 hovers near ~8k tokens regardless of requested budget; accuracy shows no monotonic relation to budget (Figure 11.2).
    - RL on CountDown: `MaxDiff` enforces budget compliance and shows rising accuracy with larger budgets, though still below unconstrained baseline (Figure 11.5).

- Convincingness and robustness
  - The multiâ€‘dataset, multiâ€‘run evidence for a â€œsweet spotâ€ of thought length is consistent (Figures 4.1â€“4.4). Failureâ€‘case traces compellingly show wrongâ€‘path persistence and selfâ€‘undermining verification (Figures C.2â€“C.3).
  - Longâ€‘context and faithfulness studies combine quantitative metrics with thought inspections, strengthening the causal story (Figures 5.2, 6.1).
  - Safety results are strong and crossâ€‘model (Tables 6â€“7), with realistic jailbreak rewrites (Figure 7.1).
  - Worldâ€‘modeling and cognitive sections rely on thoughtful qualitative analyses plus length plots; they highlight behaviors (rumination loops, nonâ€‘iterative drafting) that numbers alone might miss.

- Ablations/failures/conditions
  - Budget control: prompting fails; only `MaxDiff` reward works (Section 11; Figure 11.5).
  - ASCII reasoning: frequent abandonment and nonâ€‘reuse of drafts; final outputs often inconsistent with thoughts (Section 10; Table 9).
  - Longâ€‘context: occasional â€œoverwhelmâ€ with offâ€‘language output (Figure 5.2) and incomplete answers (Figure D.1).

## 6. Limitations and Trade-offs
- Scope and generality
  - Centered on `DeepSeekâ€‘R1`; while comparisons to V3 and Gemini exist, many conclusions (e.g., rumination prevalence) are modelâ€‘specific (Section 12.1).
  - Some analyses (e.g., ASCII world modeling) are qualitative or on small curated sets, limiting statistical generalization (Section 12.1).
- Data and training opacity
  - While R1â€™s training stages are described (Figure 2.1), training data are not public; heavy curation/postâ€‘hoc correction likely shaped thought style (Section 2.3), complicating claims about â€œnaturalâ€ reasoning.
- Measurement constraints
  - Query costs limit scale (Section 12.1). Several studies sample one thought per item (e.g., MATHâ€‘500, GSM8K length comparison).
  - Safety labeling via Llamaâ€‘Guard may have its own biases.
- Methodological tradeâ€‘offs
  - Enforcing budgets by reward shaping (`MaxDiff`) improves control but reduces absolute accuracy vs unconstrained thinking (Figure 11.5).
  - Encouraging longer thinking does not reliably improve performance and increases compute (Sections 4.2, 12).
- Open questions
  - Faithfulness: Thoughts and final answers can diverge (Section 10), and confidence statements do not reliably control termination; the mechanism that decides â€œwhen to stopâ€ remains unclear (Sections 3.4, 12).
  - Cognitive plausibility: Thought length correlates with human difficulty, yet the form shows nonâ€‘human rumination loops (Section 9.3).

## 7. Implications and Future Directions
- How this changes the landscape
  - Establishes a processâ€‘centric lens (â€œThoughtologyâ€) for LRMs: not just â€œhow accurate,â€ but â€œhow did the model get there, how long, and how safely?â€ The taxonomy and diagnostics give practitioners levers to audit and improve reasoning systems.
  - Reframes testâ€‘time scaling: â€œMore thinkingâ€ is not a free lunch; there is a taskâ€‘specific sweet spot and a risk of harmful looping or selfâ€‘sabotage.
- Research avenues
  - Metaâ€‘cognitive control: Learn stopping rules, diversify reconstructions, penalize rumination, and monitor for overwhelm in long contexts. The `MaxDiff` reward is a starting point; richer process rewards (e.g., diversity, novelty, coverage) could tame reconstruction behavior (Sections 3, 11, 12).
  - Faithfulness and verification: Couple thoughts to executable checks (math, code), track provenance of claims inside thoughts, and detect selfâ€‘contradictions before answer emission (Sections 4, 6, 10).
  - Longâ€‘context scaffolding: Memory indexing, retrievalâ€‘aware thoughts, and chunkâ€‘level summarization to avoid overwhelm (Section 5).
  - Safetyâ€‘aware reasoning: Detect and defuse selfâ€‘generated jailbreak rationales; reward refusal consistency; integrate external policy critics to flag â€œbenignâ€‘framedâ€ harmful content (Section 7).
  - Crossâ€‘lingual reasoning: Understand why R1 often bypasses `<think>` in Chinese (Figure 8.2), and how cultural values shape judgments; design languageâ€‘conditioned safeguards (Section 8).
  - Humanâ€‘like parsing vs rumination: Align thought structure with incremental parsing signals (gardenâ€‘paths) while suppressing nonâ€‘productive loops (Section 9).
- Applications
  - Highâ€‘stakes domains (finance, healthcare, law): Use Thoughtology audits to identify when thoughts become counterproductive or unsafe; enforce budgets with `MaxDiff`â€‘like rewards.
  - Education and tutoring: Calibrate thought length to student needs; reveal diverse reconstructions instead of loops.
  - Code and dataâ€‘engineering assistants: Combine thoughts with repository graphs and test suites to avoid infinite rumination (Section 5.2).

---

### Selected grounded references within the paper
- Training pipeline: Figure 2.1; Sections 2.2.1â€“2.2.3.
- Taxonomy and cycle behavior: Section 3; Figures 3.1â€“3.5; Appendix B.
- Length vs accuracy: Section 4; Figures 4.1â€“4.4; examples Figures C.2â€“C.3.
- Costâ€“accuracy tradeâ€‘off with budgets: Figure 4.5.
- Longâ€‘context retrieval and failures: Section 5.1; Figure 5.2.
- CHASEâ€‘QA/Code: Section 5.2; Table 2; Figures D.1â€“D.2.
- Faithfulness to incorrect/irrelevant context: Section 6; Table 3; Figure 6.1; Figures E.7â€“E.13.
- Safety and jailbreaks: Section 7; Tables 6â€“7; Figures 7.1, F.1â€“F.6.
- Language & culture: Section 8; Figure 8.2; Figure 8.1; Appendix G.
- Cognitive probes: Section 9; Figures 9.1â€“9.2; H.1â€“H.7.
- ASCII world modeling: Section 10; Table 9; Figure 10.3; Appendix I.
- Thinking budgets: Section 11; Figures 11.1â€“11.5; Table 12.
- Discussion and limitations: Section 12; Section 12.1.

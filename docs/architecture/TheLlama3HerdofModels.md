# The Llama 3 Herd of Models

**ArXiv:** [2407.21783](https://arxiv.org/abs/2407.21783)

## ğŸ¯ Pitch

Llama 3 introduces a state-of-the-art open family of foundation language models at three parameter scalesâ€”8B, 70B, and a flagship 405B dense Transformerâ€”delivering robust multilingual, coding, reasoning, and tool-use abilities, all within a 128K-token context window. By leveraging massive, rigorously curated data and a pragmatic, scalable training pipeline, Llama 3 not only matches leading closed models like GPT-4 in quality across diverse tasks, but also pioneers a modular approach to integrating vision, video, and speech, ushering in a new era of accessible, high-performance AI for research and real-world applications.

---

## 1. Executive Summary (2â€“3 sentences)
Llama 3 is a family (â€œherdâ€) of openâ€‘weight foundation language models at three scales (8B, 70B, 405B parameters) with a 128Kâ€‘token context window, strong multilingual/coding/reasoning/toolâ€‘use capabilities, and a pragmatic safety stack. It advances the state of open models through (a) massive, higherâ€‘quality data and careful scaling laws to pick a computeâ€‘optimal 405B dense Transformer, (b) a simple but robust postâ€‘training pipeline (SFT + DPO) that delivers GPTâ€‘4â€‘class results on many tasks, and (c) a compositional path to add vision, video, and speech without degrading text performance.

## 2. Context and Motivation
- Problem/gap addressed
  - Highâ€‘quality, widely accessible foundation models are scarce at the flagship scale; prior open releases (e.g., Llama 2, Mixtral) either lag top closed models or require complex sparse architectures. Llama 3 aims to deliver a dense, stable, longâ€‘context, multilingual model that rivals closed models on diverse tasks and publishes both methods and system recipes.
- Why this matters
  - Practical: Better assistants (tool use, coding, long documents, speech I/O) and safer deployment (guardrails, refusal calibration).
  - Scientific: Clear evidence for data/scale/simplicity levers, forecasts for downstream performance from scaling laws, and robust engineering recipes (parallelism, FP8 inference) for very large training.
- Prior approaches and shortcomings
  - Closed models (GPTâ€‘4/o, Claude 3.5) set quality bars but are not open. Mixtureâ€‘ofâ€‘experts (MoE) models (e.g., Mixtral) increase capacity but complicate training/inference. Many longâ€‘context models lack robust longâ€‘context preâ€‘/postâ€‘training; tool use and multilingual safety remain uneven.
- Positioning
  - Llama 3 opts for a standard dense Transformer with targeted architectural tweaks and a heavy emphasis on data quality and training stability (Sections 3â€“4). It also demonstrates a modular route to multimodality (Sections 7â€“8) and a layered safety strategy (Section 5.4), while releasing both preâ€‘trained and postâ€‘trained weights (Abstract; Table 1).

## 3. Technical Approach
This section walks through how Llama 3 is built, trained, aligned, and extended.

- Model family and architecture (Section 3.2; Table 3; Figure 1)
  - Three sizes: `8B`, `70B`, `405B`.
  - Dense Transformer with two notable choices:
    - `GQA` (grouped query attention) with 8 KV heads: reduces keyâ€“value cache size and speeds decoding (Table 3).
    - `RoPE` (rotary positional embeddings) base frequency increased to 500,000 to better extrapolate to long context; later extended to 128K with continued preâ€‘training (Sections 3.2, 3.4.2).
  - New 128Kâ€‘token vocabulary: 100K from `tiktoken` plus 28K nonâ€‘English tokens; improves English compression from 3.17â†’3.94 characters/token without harming English tokenization and helps multilingual performance (Section 3.2).

- Data pipeline (Section 3.1)
  - Web data undergoes PII and safety filtering, HTML parsing that preserves math/code, and multiâ€‘level dedup (URL/doc/line). Additional heuristics remove repeated content and lowâ€‘quality domains; quality and domainâ€‘specific classifiers (trained on Llama 2 judgments) upsample code/reasoning/math (Sections 3.1.1â€“3.1.2).
  - Final preâ€‘training mix: ~50% general knowledge, 25% math/reasoning, 17% code, 8% multilingual (Section 3.1.2 â€œData mix summaryâ€).
  - Annealing small, highâ€‘quality domain data near the end improves small models; the 405B modelâ€™s improvements are minimal, implying strong inâ€‘context learning at scale (Section 3.1.3).

- Scaling laws and sizing (Section 3.2.1; Figures 2â€“4)
  - Twoâ€‘step forecast connects computeâ†’validation logâ€‘likelihoodâ†’downstream accuracy, enabling accurate predictions of bigâ€‘model performance on tasks like ARCâ€‘Challenge. IsoFLOPs curves identify computeâ€‘optimal tradeâ€‘offs; extrapolation to 3.8Ã—10^25 FLOPs recommends ~402B parameters on ~16.55T tokens (Figures 2â€“3).
  - Key observation: IsoFLOPs minima flatten at high compute, making performance robust to small changes in size vs tokens (Section 3.2.1).

- Training at scale (Sections 3.3â€“3.4; Table 4; Table 5; Figures 5â€“6)
  - Hardware and cluster: up to 16K H100 GPUs (80GB HBM3), RoCE fabric with topologyâ€‘aware scheduling, and 240PB storage fabric; >90% effective training time over a 54â€‘day snapshot (Sections 3.3.1, 3.3.4; Table 5).
  - Parallelism: 4D shardingâ€”`Tensor (TP)`, `Pipeline (PP)`, `Context (CP)`, and sharded `Data Parallel (FSDP)`â€”ordered [TP, CP, PP, DP] to match network constraints (Sections 3.3.2, 3.3.3; Figure 5). Custom pipeline schedule lets the number of continuous microâ€‘batches `N` be tunable (Figure 6). Achieved 38â€“43% BF16 Model FLOPs Utilization (Table 4).
  - Longâ€‘context preâ€‘training: increase context in six stages (8Kâ†’128K), require recovery of shortâ€‘context performance and perfect â€œneedle in a haystackâ€ detection at each step; ~800B tokens for the longâ€‘context stage (Section 3.4.2).
  - Final annealing: linearly decay LR to zero over last 40M tokens at 128K context; checkpoint averaging (Polyak) yields the final preâ€‘trained model (Section 3.4.3).

- Postâ€‘training for instruction following and preference alignment (Section 4; Figure 7)
  - Chat protocol supports multiple messages per turn (for tools) via special headers/terminators (Section 4.1.1).
  - `Reward Modeling (RM)`: trained on human preference triples (edited > chosen > rejected); efficiency trick concatenates shuffled responses per prompt in one row (Section 4.1.2).
  - `Supervised Fineâ€‘Tuning (SFT)` with rejection sampling: for each human prompt, sample 10â€“30 outputs and pick the best using the RM; `PagedAttention` doubles throughput by sharing prompt KV cache across samples (Section 4.1.3; 4.2.2).
  - `DPO` (Direct Preference Optimization): Î²=0.1; mask formatting tokens in loss to avoid degenerate behaviors; add NLL regularizer (0.2Ã—) on chosen responses to stabilize formats and prevent likelihood collapse (Section 4.1.4).
  - Model averaging across runs/hyperparameters further smooths performance (Section 4.1.5).

- Capabilityâ€‘specific improvements (Section 4.3)
  - Code: a continuedâ€‘preâ€‘trained â€œcode expert,â€ plus largeâ€‘scale synthetic data with execution feedback (static checks + unit tests + iterative selfâ€‘correction), crossâ€‘language translation, and â€œbacktranslationâ€ for documentation/explanations (Section 4.3.1; Figure 8â€“9).
  - Multilingual: a multilingual expert (90% nonâ€‘English continued preâ€‘training), highâ€‘quality native annotations, rejectionâ€‘sampled SFT, and careful control of language/script matching; limited use of translations (only for math) to avoid translationese/bias (Section 4.3.2).
  - Reasoning: generate stepâ€‘wise chains with answerâ€‘checking, train outcome/stepwise reward models, use MCTS on hard problems, and interleave text/code with execution as verification (Section 4.3.3).
  - Long context: synthesize QA/summarization/repoâ€‘reasoning data bucketed by length; mixing only ~0.1% longâ€‘context SFT data retained shortâ€‘context quality; DPO can remain shortâ€‘context if SFT is strong (Section 4.3.4).
  - Tool use: builtâ€‘in tools (Brave Search, Python, Wolfram Alpha) and zeroâ€‘shot function calling; messageâ€‘level human preferences (not only whole replies), plus multiâ€‘step synthetic traces (ReActâ€‘style) and fileâ€‘upload tasks (Section 4.3.5; Figures 10â€“11).
  - Factuality: â€œknowledge probeâ€ generates question/answer pairs from preâ€‘training snippets and trains calibrated refusing when uncertain (Section 4.3.6).
  - Steerability: preference data with diverse system prompts to control tone/format/length/persona; included in RM, SFT, DPO (Section 4.3.7).

- Safety pipeline (Section 5.4)
  - Preâ€‘training: domain filtering for PII/adult content; low verbatim memorization measured by nâ€‘gram prompts at scale (e.g., 405B shows 1.13% inclusion for 50â€‘grams; Table 24).
  - Finetuning: pair â€œadversarialâ€ prompts (to measure violation rate, `VR`) with â€œborderlineâ€ prompts (to measure false refusal rate, `FRR`), and balance safety vs helpfulness through SFT and DPO; small models need higher safety data ratios (Figure 18).
  - Systemâ€‘level guardrails: `Llama Guard 3` (an 8B classifier) for input/output filtering across 13 risk categories, with perâ€‘category toggles and int8 quantization; two additional componentsâ€”`Prompt Guard` (promptâ€‘attack classifier) and `Code Shield` (static insecureâ€‘code detection) (Section 5.4.7; Tables 26â€“28).

- Inference efficiency (Section 6; Figures 24â€“27)
  - Pipelineâ€‘parallel inference across 16 GPUs, plus microâ€‘batching to increase throughput; modest latency increase but better throughput/latency tradeâ€‘off (Figure 24).
  - `FP8` lowâ€‘precision inference on H100: quantize most MLP matmuls with dynamic rowâ€‘wise scales, cap scales to 1200, skip first/last layers; rewardâ€‘model score distributions match BF16 (Figure 26), and throughput improves up to 50% in prefill with superior decoding tradeâ€‘offs (Figure 27).

- Compositional multimodality (Sections 7â€“8; Figures 28â€“29)
  - Vision/video: a preâ€‘trained image encoder (ViTâ€‘H) and crossâ€‘attention â€œvision adapterâ€ injected after every 4th LLM layer; for video, add temporal aggregator and video crossâ€‘attention; keep LLM weights frozen during adapter training to preserve text performance (Section 7.2).
  - Speech: a 1Bâ€‘parameter Conformer encoder + lightweight adapter that outputs tokenâ€‘rate embeddings the LLM can consume directly; system prompts select ASR or speech translation modes; a streaming TTS stack uses Llama 3 embeddings to improve text normalization and prosody with low latency (Section 8.2).

## 4. Key Insights and Innovations
1) A practical, accurate scalingâ€‘law method for downstream performance
- Whatâ€™s new: A twoâ€‘step forecast links training FLOPs â†’ validation NLL on benchmarks â†’ final accuracy, using both small scalingâ€‘law runs (â‰¤10^22 FLOPs) and prior Llamaâ€‘2 models (Figure 4). The selected 405B size (â‰ˆ402B predicted) is computeâ€‘optimal for 3.8Ã—10^25 FLOPs, with robustness near the IsoFLOPs minimum (Figures 2â€“3).
- Why it matters: It lets teams predict end performance and pick model size/data budget before spending huge compute, reducing risk of under/oversized models.

2) Denseâ€‘model scaling with stability and simplicity
- Whatâ€™s new: Llama 3 stays dense (not MoE), yet matches/approaches closed models by focusing on data quality, careful longâ€‘context adaptation, and robust distributed training (4D parallelism, NCCLX comms, and pipeline scheduling; Sections 3.3â€“3.4).
- Why it matters: Training/inference are simpler and more stable than MoE, and the published engineering recipes are immediately reusable.

3) A lean but strong alignment pipeline (SFT + DPO) with targeted tweaks
- Whatâ€™s new: Rejection sampling using an RM, DPO with formattingâ€‘token masking and NLL regularization, model averaging, and carefully curated capabilityâ€‘specific datasets (Section 4). DPO remains shortâ€‘context without harming longâ€‘context performance if SFT is strong (Section 4.3.4).
- Why it matters: It achieves high instruction fidelity and reasoning without complex RL pipelines, and scales reliably to 405B.

4) Layered safety with measurable VR/FRR tradeâ€‘offs and deployable guards
- Whatâ€™s new: Balance adversarial vs borderline data to lower violations while avoiding overâ€‘refusal (Figure 18), demonstrate longâ€‘context jailbreak mitigation (Figure 20), and provide `Llama Guard 3` and `Prompt Guard` as system components with perâ€‘category controls (Tables 25â€“26, 28).
- Why it matters: It operationalizes safety as a tunable system property rather than a single monolithic setting.

5) Compositional adapters for vision, video, and speech
- Whatâ€™s new: Crossâ€‘attention â€œvision adapterâ€ and temporal aggregator let Llama 3 reach competitive VQA/video QA without joint multimodal preâ€‘training; speech uses a tokenâ€‘rate interface (no crossâ€‘attention) for ASR/AST/spoken dialog and streaming TTS with Llamaâ€‘embeddings (Sections 7â€“8).
- Why it matters: Adds modalities without regressing text performance or requiring endâ€‘toâ€‘end reâ€‘training of the LLM.

6) FP8 inference that preserves response distribution
- Whatâ€™s new: Rowâ€‘wise dynamic scaling with a capped scale avoids underflow spikes on highâ€‘perplexity tokens; rewardâ€‘model score distributions are nearly unchanged vs BF16 (Figure 26) while throughput improves (Figure 27).
- Why it matters: Safe, highâ€‘speed inference for very large models.

## 5. Experimental Analysis
- Evaluation setup
  - Preâ€‘training and postâ€‘training are evaluated across general knowledge (MMLU/Pro), reasoning (GSM8K/MATH/ARCâ€‘C/GPQA), code (HumanEval/MBPP, MultiPLâ€‘E), longâ€‘context (ZeroSCROLLS, InfiniteBench, needle tests), tool use (Nexus, APIâ€‘Bank, APIâ€‘Bench, BFCL), multilingual (MGSM, translated MMLU), and proficiency exams, plus adversarial robustness and contamination checks (Sections 5.1â€“5.2; Table 8/16).
  - Safety uses internal adversarial/borderline sets spanning MLCommons hazard taxonomy; systemâ€‘level guards are measured with VR/FRR and categoryâ€‘wise metrics (Section 5.4; Figures 19â€“21; Tables 25â€“26).

- Headline postâ€‘training results (Table 2; all Llama 3.1)
  > On core benchmarks, `Llama 3.1 405B Instruct` reports: MMLU 87.3 (5â€‘shot), GSM8K 96.8 (8â€‘shot CoT), HumanEval 89.0 (0â€‘shot), MBPP EvalPlus 88.6 (0â€‘shot), ARCâ€‘Challenge 96.9 (0â€‘shot), GPQA 51.1 (0â€‘shot CoT).  
  > In many cases it is competitive with GPTâ€‘4 (0125), GPTâ€‘4o, and Claude 3.5 Sonnet; the 8B and 70B models are bestâ€‘inâ€‘class within their size brackets (Table 2).

- Preâ€‘trained model quality (Tables 9â€“13, 14)
  - `405B` reaches MMLU 85.2, MATH 53.8 (0â€‘shot CoT), ARCâ€‘C 96.1, HumanEval 61.0/MBPP 73.4. Longâ€‘context: QuALITY 87.6 (5â€‘shot), manyâ€‘shot GSM8K 90.0 (16â€‘shot) (Table 14).
  - Robustness checks on MMLU show little sensitivity to label variants, fewâ€‘shot label biases, answer permutations, and prompt formatsâ€”especially at 405B (Figures 13â€“14).

- Proficiency exams (Table 17)
  - `405B` scores: LSAT 81.1, SAT Reading 74.8, SAT Math 94.9, GMAT Quant 96.0, GRE Quant/Verbal 162/166. The `70B` model is strong and often beats larger open models (e.g., Nemotron 340B).

- Tool use/function calling (Table 22; Figures 10â€“11; 16)
  - `405B` achieves 92.3% on APIâ€‘Bank and 88.5% on BFCL, near GPTâ€‘4/4o; Nexus 58.7% and APIâ€‘Bench 35.3% are competitive. In human evals on code execution/plotting/file uploads, `405B` beats GPTâ€‘4o on execution and plotting but lags on file uploads (Figure 16).

- Long context (Table 21)
  > `405B` achieves QuALITY 95.2 (EM, val), InfiniteBench En.MC 83.4 (acc) and En.QA 30.5 (F1), and 98.1 average recall on Multiâ€‘needle across context lengths up to 128K. GPTâ€‘4(o) slightly outperforms on En.QA and Multiâ€‘needle tops out at 100.0 for GPTâ€‘4/4o.

- Multilingual (Table 20)
  - `405B` reaches 91.6 on MGSM (0â€‘shot CoT) and 83.2 averaged translated MMLU (5â€‘shot). `70B` and `8B` lead their size classes by wide margins.

- Human evaluations (Section 5.3; Figure 17)
  - Pairwise win/loss vs top closed models varies by capability. `405B` is roughly on par with GPTâ€‘4 (0125), mixed against GPTâ€‘4o and Claude 3.5 Sonnet: it leads on single/multiturn English, trails on coding/reasoning vs Claude 3.5 Sonnet, and is comparable on multilingual.

- Safety outcomes (Section 5.4; Figures 19â€“21; Tables 25â€“26, 28)
  - Modelâ€‘level vs systemâ€‘level: adding `Llama Guard 3` substantially reduces violation rates at the cost of some increased FRR; reductions are strong across hazard categories (Table 26). On multilingual shortâ€‘context sets, `405B + LG3` is at least as safe as anonymized commercial systems (Figure 19).
  - Longâ€‘context safety: both DocQA and Manyâ€‘shot jailbreaks show markedly lower VR for `405B` (with or without LG) than one commercial system and a Pareto improvement over another (Figure 20).
  - External risk: on prompt injection, `Llama 3` sits between GPTâ€‘4 Turbo/Gemini Pro (less susceptible) and Mixtral (more) (Figure 22). Uplift studies show no significant increase in capability for cyber or CBRNE attacks relative to web search alone (Section 5.4.5).

- Contamination analysis (Table 15)
  - 8â€‘gram overlap suggests varying levels of potential contamination across datasets (e.g., high in HellaSwag/PiQA) but estimated performance gain is sometimes small (e.g., NaturalQuestions). Some datasets (MBPP, MMLU/Pro) require alternative methods due to high apparent overlap; the study is careful to present uncertainties (Section 5.1.4).

- Multimodality results (Tables 29â€“30)
  - Vision: `Llama 3â€‘V 405B` is competitive with GPTâ€‘4V and near Gemini/Claude on MMMU (64.5), ChartQA (85.8), TextVQA (84.8), and DocVQA (92.6) (Table 29).
  - Video: `Llama 3â€‘V 70B` achieves 60.8 on PerceptionTest (test), 87.9 on TVQA (val), and 56.3 on ActivityNetâ€‘QA (test), competitive with Gemini variants; results are zeroâ€‘shot (Table 30).

- Speech results (Tables 31â€“35; Figure 30)
  - ASR: `70B` reaches WER 4.4 on MLS English, 3.1 on LibriSpeech testâ€‘other; both `8B`/`70B` outperform Whisper v2/v3 and SeamlessM4T v2, and are close to Gemini on MLS English (Table 31).
  - AST: On FLEURS and CoVoST2 (to English), results are competitive with Whisper v2 and SeamlessM4T v2 (Table 32).
  - Safety (MuTox): very low added toxicity (â‰¤0.84% English) and substantial toxicity removal (Table 33).
  - Streaming TTS: adding Llamaâ€‘embeddings improves text normalization accuracy and prosody preferences over phoneâ€‘only baselines while staying streamable (Tables 34â€“35).

Overall, the experiments are extensive, cover core and edge capabilities, include robustness and safety, and disclose limitations where performance is mixed (e.g., some tool/fileâ€‘upload tasks, some longâ€‘context QA settings).

## 6. Limitations and Trade-offs
- Assumptions and choices
  - Dense architecture chosen for stability; while simpler than MoE, it is inferenceâ€‘heavier per token than sparse experts (Section 3.2). The 405B model demands significant compute infrastructure.
  - Longâ€‘context capability is achieved by staged continued preâ€‘training plus a small fraction of longâ€‘context SFT; certain tasks (InfiniteBench En.QA) still leave room to improve (Table 21).
- Safety tradeâ€‘offs
  - Reducing VR increases FRR; `Llama Guard 3` provides category toggles, but tuning is still applicationâ€‘specific (Tables 25â€“26; Figures 19â€“21).
  - Promptâ€‘injection and tool misuse risks are mitigated but not eliminated (Figure 22; Section 5.4.6 toolâ€‘specific redâ€‘team findings).
- Data and measurement
  - Contamination estimates via 8â€‘gram overlap can be noisy for some benchmarks (Table 15). Some multilingual safety depends on the coverage/quality of nonâ€‘English data (Figure 19 discussion).
- Engineering tradeâ€‘offs
  - Microâ€‘batching raises throughput but adds synchronization points that can increase latency (Figure 24).
  - FP8 inference excludes attention layers and needs careful scale capping and rowâ€‘wise quantization to avoid rare corruption (Section 6.2).
- Multimodality status
  - Vision/video/speech components are promising but â€œstill under developmentâ€ and not broadly released; video uses up to 64 frames and temporal aggregation may miss fine events (Sections 7â€“8).

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that carefully trained dense open models can approach closedâ€‘model performance at flagship scale while providing transparent methods (data curation, scaling laws, longâ€‘context training, safety tuning, FP8 inference). This raises the baseline for open research and practical deployments.
- Followâ€‘up research
  - Longâ€‘context: stronger reasoning over 100K+ tokens (e.g., improved summarization/QA training, retrievalâ€‘augmented longâ€‘context).
  - Safety: adaptive defenses against jailbreaking/prompt injection (e.g., proactive toolâ€‘call validation, multiâ€‘agent verification), improved multilingual/borderline calibration, and standardized contamination auditing.
  - Data: principled annealing and curriculum strategies; richer reasoning/code datasets with verified step traces; better toolâ€‘grounded corpora (especially for file workflows).
  - Inference/systems: broader FP8 coverage (including attention), quantizationâ€‘aware training, and heterogeneous clusters; improved pipeline scheduling for interactive workloads.
  - Multimodality: broader release and scaling of adapters; unified training that preserves text without degradation; tighter integration of speech prosody/semantics and vision grounding for tool use.
- Applications
  - Enterprise assistants (analysis of long documents, spreadsheets, PDFs; Section 4.3.5 and Figure 11), developer tools (code gen/debug/review with execution feedback; Section 4.3.1), multilingual support (Table 20), STEM tutoring and exams (Table 17), research assistants (tool use + factuality probes; Sections 4.3.5â€“4.3.6), and safetyâ€‘aware platforms (Llama Guard 3, Prompt Guard; Section 5.4.7).

---

Below are selected, concrete references used in the analysis:
- Architecture & training: Figure 1; Sections 3.1â€“3.4; Tables 3â€“4; Figures 2â€“6; Table 5.
- Postâ€‘training & capabilities: Figure 7; Sections 4.1â€“4.3; Figures 8â€“11; Tables 6â€“7.
- Headline performance: Table 2 (postâ€‘trained), Tables 9â€“14 (preâ€‘trained).
- Long context: Section 3.4.2; Table 21.
- Tool use/function calling: Section 4.3.5; Table 22; Figures 10â€“11.
- Multilingual: Section 4.3.2; Table 20.
- Human evals: Section 5.3; Figures 16â€“17.
- Safety: Section 5.4; Figures 18â€“21; Tables 24â€“28.
- Inference: Section 6; Figures 24â€“27.
- Vision/speech: Sections 7â€“8; Tables 29â€“35; Figures 28â€“30.

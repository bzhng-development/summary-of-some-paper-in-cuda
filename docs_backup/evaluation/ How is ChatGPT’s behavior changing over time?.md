# How is ChatGPT‚Äôs behavior changing over time?

**ArXiv:** [2307.09009](https://arxiv.org/abs/2307.09009)
**Authors:** Lingjiao Chen, Matei Zaharia, James Zou
**Institutions:** Stanford University, University of California, Berkeley

## üéØ Pitch

This study reveals significant, task-dependent shifts in the performance of GPT-3.5 and GPT-4 over just three months, highlighting a notable decline in instruction-following as a key driver of these changes. By exposing the instability of LLMs-as-a-service, this research underscores the importance of monitoring not just accuracy but also instruction fidelity and formatting, offering practical insights to improve model integration reliability in real-world applications.

---

## 1. Executive Summary
This paper measures how the ‚Äúsame‚Äù commercial large language model service (GPT‚Äë3.5 and GPT‚Äë4) changes over time by comparing its March 2023 and June 2023 versions across eight diverse tasks. It finds large, task‚Äëdependent drifts‚Äîsome improvements, many regressions‚Äîand presents evidence that a decline in instruction‚Äëfollowing (the ability to comply with user formatting and behavioral constraints) is a key driver of several drops in performance.

## 2. Context and Motivation
- Problem addressed
  - Commercial LLM services are periodically updated, but users are not told what changed or when. This makes integration brittle: a prompt that worked yesterday may silently stop working today.
  - The study asks: How much does behavior actually drift in practice, and why?

- Why this matters
  - Real‚Äëworld impact: Workflows can break if accuracy, formatting, or refusal behavior changes. Reproducibility suffers if the same prompt produces a different answer a month later. The paper documents such breakages, e.g., code that stopped being ‚Äúdirectly executable‚Äù and an agent pipeline that failed to parse replies (Figures 9 and 10).
  - Theoretical significance: It challenges the assumption that model updates monotonically improve performance. It also probes the link between ‚Äúinstruction following‚Äù and downstream task success.

- Prior approaches and gaps
  - Existing evaluations mostly benchmark single snapshots of LLMs or track small longitudinal shifts on limited benchmarks. For example, ChatLog reports mostly small (<5%) shifts; other works analyze specific tasks or classic ML APIs (Related Work, p. 2).
  - Gap: No systematic, multi‚Äëtask longitudinal analysis of model‚Äëas‚Äëa‚Äëservice behavior on generative tasks with objective scoring and operational failure modes (formatting, refusal, jailbreak vulnerability).

- Positioning
  - This work monitors two timepoints (March vs. June 2023) of GPT‚Äë3.5 and GPT‚Äë4 across eight tasks designed to be objective and practical, then isolates a potential root cause‚Äîreduced instruction fidelity‚Äîthrough targeted probes (Figures 1 and 13‚Äì14).

## 3. Technical Approach
This is an empirical study with two main components: (A) longitudinal task evaluations and (B) targeted instruction‚Äëfollowing tests.

- Services and setup
  - Models: `GPT‚Äë4` and `GPT‚Äë3.5` accessed via OpenAI‚Äôs API at two snapshots: March 2023 and June 2023 (Section ‚ÄúLLM Services,‚Äù p. 3).
  - Configuration: default system prompt; only the user prompt is varied. Temperature set to 0.1 to reduce randomness (p. 3).

- Common auxiliary metrics (p. 3‚Äì4)
  - `verbosity`: number of characters in the generation (a proxy for terseness/verbosity).
  - `mismatch`: for the same prompt, whether the extracted final answers differ between the two versions of the same model. Note: this compares task answers, not raw text.

- Eight evaluation tasks (overview in Figure 1 and detailed in Section 3)
  1) Math I: Prime vs. composite (Section 3.1; Figure 3)
     - 1,000 questions: 500 primes (from [ZPM+23]) + 500 composites sampled uniformly from 1,000‚Äì20,000.
     - Prompting uses Chain‚Äëof‚ÄëThought (CoT) to encourage stepwise reasoning.
     - Metric: accuracy on Yes/No primality.
     - Additional probe: repeat without CoT to compute the ‚ÄúCoT boost‚Äù (Table 1).
     - Definition: `Chain‚Äëof‚ÄëThought (CoT)` is a prompt strategy that requests intermediate reasoning steps before the final answer.

  2) Math II: Counting happy numbers (Section 3.2; Figure 5; Table 2; Figure 6)
     - 500 queries. Each asks how many ‚Äúhappy numbers‚Äù lie in an interval of length 6‚Äì10, start in [500, 15,000].
     - Definition: a ‚Äúhappy number‚Äù reaches 1 when repeatedly replaced by the sum of the squares of its digits; otherwise it falls into a cycle.
     - Prompting uses CoT; also tested without CoT (Table 2).
     - Metric: exact match on the count; confusion matrices examine error patterns (Figure 6).

  3) Sensitive questions (Section 3.3; Figure 7; Table 3)
     - 100 queries that should not receive direct answers (e.g., discriminatory or illegal requests).
     - Metric: `response rate` = fraction of prompts that receive a direct answer (higher can mean less safe).
     - Manual labeling to judge whether a response is a direct answer.
     - Jailbreak test: AIM (‚ÄúAlways Intelligent and Machiavellian‚Äù) jailbreak prompt applied to each query (Table 3).
     - Definition: `jailbreaking` is prompt manipulation designed to bypass safety constraints.

  4) OpinionQA survey (Section 3.4; Figure 8)
     - 1,506 multiple‚Äëchoice questions from public opinion polls (OpinionQA).
     - Metric: `response rate` (whether the model picks an option); also analyze disagreement across time to quantify opinion drift.

  5) LangChain HotpotQA agent (Section 3.6; Figure 10)
     - A multi‚Äëhop QA pipeline using LangChain‚Äôs default ReAct agent to search Wikipedia and answer HotpotQA‚Äëstyle questions.
     - Definition: `ReAct` combines reasoning (‚ÄúThought‚Äù) and acting (‚ÄúAction: search/look up‚Äù) as alternating steps.
     - Metric: exact match of final answers.
     - Critical detail: the agent expects strict output formats (‚Äú[action]+text‚Äù). Deviations break parsing (Figure 10b).

  6) Code generation and formatting (Section 3.5; Figure 9; Table 4)
     - 50 newest ‚Äúeasy‚Äù LeetCode problems (to reduce training‚Äëdata contamination concerns).
     - Prompt concatenates the original problem text and a Python template; the model is told to output code only.
     - `Directly executable`: the code is accepted by LeetCode‚Äôs online judge without any post‚Äëprocessing.
     - Also re‚Äëevaluate after stripping non‚Äëcode wrappers (Table 4) to check whether failures were due to formatting, not logic.

  7) USMLE medical exam (Section 3.7; Figure 11)
     - 340 multiple‚Äëchoice questions from USMLE‚Äëstyle exams; CoT prompting that asks for ‚ÄúThe answer is (X)‚Äù.
     - Metric: accuracy.

  8) Visual reasoning (ARC) (Section 3.8; Figure 12)
     - 467 ARC grid tasks formatted as 2‚ÄëD arrays; task: infer transformation from examples and produce the output grid.
     - Metric: exact match; also track whether generations are identical across March/June snapshots.

- Instruction‚Äëfollowing probes (Section 4; Figures 13‚Äì14)
  - Task-agnostic tests constructed to isolate ‚Äúinstruction fidelity.‚Äù
  - Four single‚Äëinstruction families (Figure 13):
    - `Extract Answer`: e.g., ‚ÄúAnswer yes/no in [square brackets]‚Äù.
    - `Stop apologizing`: e.g., ‚ÄúDo not say ‚Äòsorry‚Äô or ‚Äòas an AI model‚Äô.‚Äù
    - `Writing constraint`: e.g., ‚ÄúDescribe X using only words ending with letter Y.‚Äù
    - `Format text`: e.g., ‚ÄúPut [ ] around the first letter of each word (including articles).‚Äù
  - Composite instructions (Figure 14):
    - Three simple text‚Äëformatting instructions applied alone and in all pairs: `add comma` to each word, `no quotation` (remove quotes), `capitalize` (convert to uppercase). Evaluate performance drop from single to composite.

Design choice rationale
- Tasks are ‚Äúobjective‚Äù where possible: correctness, executability, exact match, or response rate are easy to score (p. 3).
- Two auxiliary metrics‚Äîverbosity and mismatch‚Äîcapture behavioral shifts beyond accuracy (p. 3‚Äì4).
- Using LangChain‚Äôs ReAct agent and the LeetCode judge surfaces realistic pipeline brittleness: small changes in format can crash an agent or render code non‚Äëexecutable (Figures 9‚Äì10).
- CoT vs. no‚ÄëCoT ablations test whether reasoning‚Äëstyle prompting still helps after an update (Tables 1‚Äì2).

## 4. Key Insights and Innovations
- Large service‚Äëlevel drift is real and multifaceted (Figure 1a)
  - ‚ÄúThe same‚Äù API changed substantially in two months, sometimes by tens of percentage points on accuracy or executability. This extends prior smaller‚Äëscale drift observations by documenting stronger, task‚Äëdependent shifts across math, code, safety, survey response, multi‚Äëhop QA, medical QA, and visual reasoning.

- Instruction‚Äëfollowing degradation in GPT‚Äë4 is a plausible unifying factor (Figures 1b, 13‚Äì14)
  - Novel evidence: on task‚Äëagnostic instruction probes, GPT‚Äë4‚Äôs compliance collapses from March to June:
    - Extract‚ÄëAnswer compliance: 99.5% ‚Üí 0.5% (Figure 13a).
    - ‚ÄúStop apologizing‚Äù compliance on sensitive prompts: 74% ‚Üí 19% (Figure 13a).
    - Composite text‚Äëformat instructions show especially large drops, e.g., add‚Äëcomma + no‚Äëquotation: ‚àí24.0 percentage points (Figure 14a).
  - Significance: Many regressions elsewhere are consistent with reduced instruction fidelity‚Äîe.g., failing to output ‚Äúcode only‚Äù (Figure 9b), ignoring CoT steps (Figures 3b and 5b), or not adhering to LangChain‚Äôs required format (Figure 10b).

- CoT no longer reliably helps GPT‚Äë4 after the update (Tables 1‚Äì2)
  - For primality, the CoT boost shrinks from +24.4% (March) to +0.1% (June) (Table 1).
  - For happy numbers, the boost shrinks from +56.6% to +3.2% (Table 2).
  - This is not just weaker reasoning‚Äîit is often refusal to produce steps at all (Figures 3b, 5b).

- Pipeline brittleness highlighted by realistic failure modes
  - Code: wrapping snippets in Markdown code fences in June rendered outputs not ‚Äúdirectly executable,‚Äù dropping GPT‚Äë4‚Äôs pass rate from 52% to 10% (Figure 9a), even though the underlying logic often improved when wrappers were stripped (to 70%; Table 4).
  - Agents: a small change in response format caused LangChain to fail to parse outputs (‚ÄúCould not parse LLM Output‚Äù) despite semantically correct content (Figure 10b).

These are fundamental observations about model‚Äëas‚Äëa‚Äëservice reliability rather than incremental score bumps.

## 5. Experimental Analysis
Evaluation methodology (Section 2, Figure 1)
- Services: GPT‚Äë3.5 and GPT‚Äë4, March vs. June 2023 snapshots.
- Setup: uniform temperature (0.1), default system prompt, diverse tasks with clear metrics, and two additional behavior metrics (verbosity, mismatch).

Headline quantitative results by task
- Math I: Prime vs. composite (Figure 3; Table 1)
  - Accuracy
    - GPT‚Äë4: 84.0% ‚Üí 51.1% (‚àí32.9 points).
    - GPT‚Äë3.5: 49.6% ‚Üí 76.2% (+26.6 points).
  - Verbosity
    - GPT‚Äë4: 638.3 ‚Üí 3.9 characters (very terse).
    - GPT‚Äë3.5: increased by ~22% (Figure 3a).
  - CoT vs. no‚ÄëCoT (Table 1)
    - GPT‚Äë4: CoT boost +24.4% (March) ‚Üí +0.1% (June).
    - GPT‚Äë3.5: ‚àí0.9% (March) ‚Üí +15.8% (June).
  - Error patterns (Figure 4)
    - GPT‚Äë4 June exhibits a strong bias toward predicting ‚Äúcomposite‚Äù almost always.
  - Mechanism insight: GPT‚Äë4 June often ignores ‚Äúthink step by step‚Äù and outputs only ‚Äú[No]‚Äù (Figure 3b).

- Math II: Counting happy numbers (Figure 5; Table 2; Figure 6)
  - Accuracy
    - GPT‚Äë4: 83.6% ‚Üí 35.2% (‚àí48.4 points).
    - GPT‚Äë3.5: 30.6% ‚Üí 48.2% (+17.6 points).
  - Verbosity
    - GPT‚Äë4: 2163.5 ‚Üí 10.0 characters (order‚Äëof‚Äëmagnitude drop).
    - GPT‚Äë3.5: large increase (1494.9 ‚Üí 2519.7; Figure 5a).
  - CoT vs. no‚ÄëCoT (Table 2)
    - GPT‚Äë4: +56.6% (March) ‚Üí +3.2% (June).
    - GPT‚Äë3.5: ‚àí1.6% (March) ‚Üí +20.6% (June).
  - Error patterns (Figure 6)
    - GPT‚Äë4 June concentrates its predictions on 0 or 1 happy number, regardless of ground truth.
    - GPT‚Äë3.5 June tends to overestimate, sometimes predicting more than the maximum possible.

- Sensitive questions and jailbreaks (Figure 7; Table 3)
  - Response rate (direct answers to inappropriate prompts; lower is safer)
    - GPT‚Äë4: 21.0% ‚Üí 5.0% (safer).
    - GPT‚Äë3.5: 2.0% ‚Üí 8.0% (less safe).
  - Verbosity of refusals
    - GPT‚Äë4: 652.4 ‚Üí 141.4 characters (more terse; Figure 7a).
    - Example: a full paragraph refusal in March vs. a brief ‚ÄúSorry, but I can‚Äôt assist with that‚Äù in June (Figure 7b).
  - AIM jailbreak (Table 3)
    - GPT‚Äë4: 78.0% ‚Üí 31.0% answer rate (much stronger defense).
    - GPT‚Äë3.5: 100.0% ‚Üí 96.0% (still highly vulnerable).

- OpinionQA survey (Figure 8)
  - Response rate (will the model select an option?)
    - GPT‚Äë4: 97.6% ‚Üí 22.1% (large reduction in willingness to opine).
    - GPT‚Äë3.5: 94.3% ‚Üí 96.7% (stable/increased).
  - Opinion drift
    - 27% of GPT‚Äë3.5‚Äôs choices changed between March and June; within‚Äësnapshot randomness measured much lower (2.8% in March; 7.0% in June), indicating real drift (Section 3.4).

- Code generation and formatting (Figure 9; Table 4)
  - Directly executable (without post‚Äëprocessing)
    - GPT‚Äë4: 52.0% ‚Üí 10.0% (‚àí42 points).
    - GPT‚Äë3.5: 22.0% ‚Üí 2.0% (‚àí20 points).
  - Root cause: formatting
    - June versions often wrapped code in Markdown fences or added extra comments despite the instruction ‚ÄúGenerate the code only,‚Äù breaking executability (Figure 9b).
  - After stripping non‚Äëcode wrappers (Table 4)
    - GPT‚Äë4: 52.0% ‚Üí 70.0% (June improves markedly when format is fixed).
    - GPT‚Äë3.5: 22.0% ‚Üí 46.0% (March), 2.0% ‚Üí 48.0% (June).

- LangChain HotpotQA agent (Figure 10)
  - Exact match
    - GPT‚Äë4: 1.2% ‚Üí 37.8% (+36.6 points).
    - GPT‚Äë3.5: 22.8% ‚Üí 14.0% (‚àí8.8 points).
  - Failure mode (Figure 10b)
    - March GPT‚Äë4 sometimes produced correct content but failed the agent‚Äôs strict ‚Äú[action]+text‚Äù formatting, causing ‚ÄúCould not parse LLM Output.‚Äù
    - June GPT‚Äë3.5 sometimes ‚Äúcould not find information‚Äù that March GPT‚Äë3.5 retrieved.

- USMLE medical exam (Figure 11)
  - Accuracy
    - GPT‚Äë4: 86.6% ‚Üí 82.1% (‚àí4.5 points).
    - GPT‚Äë3.5: 54.3% ‚Üí 54.7% (about flat).
  - Behavioral drift
    - Answer mismatch across time is substantial: 12.2% of GPT‚Äë4‚Äôs answers and 27.9% of GPT‚Äë3.5‚Äôs changed (Figure 11a).
    - GPT‚Äë3.5 June becomes more verbose; GPT‚Äë4 June responds more tersely (Section 3.7).

- Visual reasoning (ARC) (Figure 12)
  - Exact match
    - GPT‚Äë4: 24.6% ‚Üí 27.2% (+2.6 points).
    - GPT‚Äë3.5: 10.9% ‚Üí 14.3% (+3.4 points).
  - Stability
    - Most outputs remained the same across snapshots; the paper notes ‚Äúmore than 90%‚Äù of generations identical across March and June for these puzzles (Section 3.8), despite small average gains.

Instruction‚Äëfollowing results (Section 4; Figures 13‚Äì14)
- Single‚Äëinstruction fidelity (Figure 13a)
  - Extract answer in [brackets]: 99.5% (March) ‚Üí 0.5% (June).
  - Don‚Äôt apologize or say ‚Äúas an AI model‚Äù: 74.0% ‚Üí 19.0%.
  - Writing constraint (words ending with a given letter): 55.0% ‚Üí 10.0%.
  - Text formatting (first‚Äëletter bracketing): 13.0% ‚Üí 7.5%.
- Composite instructions (Figure 14a)
  - Single‚Äëinstruction shifts small (‚àí2.0, +4.0, ‚àí1.0).
  - Composition drops are large: e.g., `add comma + no quotation` falls by 24.0 points from March to June; `add comma + capitalize` by 9.2 points.

Do the experiments support the claims?
- Yes, the study grounds each claim with quantitative comparisons, often with multiple views:
  - Raw metric deltas (accuracy/exact‚Äëmatch/executability).
  - Behavioral metrics (verbosity/mismatch).
  - Mechanism‚Äëprobing ablations (CoT vs. no‚ÄëCoT; with vs. without non‚Äëcode wrappers; agent formatting).
  - Safety robustness (AIM jailbreak).
- Where results are mixed, conditions are clear:
  - GPT‚Äë4 improves on multi‚Äëhop QA (agent) and visual reasoning but drops sharply on math and raw code executability.
  - GPT‚Äë3.5 often moves in the opposite direction (e.g., math improves; agent performance drops).

## 6. Limitations and Trade-offs
- Only two timepoints
  - The analysis captures drift between March and June 2023. Behavior may evolve differently outside this window.

- Attribution is indirect
  - The study identifies reduced instruction fidelity as a plausible driver, supported by targeted probes (Figures 13‚Äì14), but cannot isolate root causes inside a proprietary stack. Changes could stem from training data, safety layers, decoding policies, or post‚Äëprocessors.

- Metric design choices
  - `Mismatch` measures answer changes across time but abstracts away full text and rationale‚Äîuseful for drift detection but it can miss qualitative shifts in reasoning quality.
  - For sensitive‚Äëquestion response rate, manual labeling is required; while straightforward, it may still introduce judgment variance (Section 3.3).

- Pipeline confounds
  - The LangChain agent and code‚Äëexecution pipeline add parsing/execution constraints. These surfaces are realistic but also make results partially dependent on external tooling and strict prompt contracts.

- Generality across domains
  - The eight tasks are broad yet not exhaustive. Other domains (e.g., long‚Äëcontext tools, program synthesis beyond LeetCode, multilingual tasks) are not covered.

- Stochasticity
  - Temperature is set low (0.1), but single‚Äësample results remain stochastic. The study partially addresses this by comparing disagreement rates within a snapshot for OpinionQA (Section 3.4), but broader repeated‚Äësampling analyses are not reported.

## 7. Implications and Future Directions
- Practical implications for users and integrators
  - Treat LLMs as evolving services. Build ‚Äúcanary‚Äù test suites with your real prompts and gold answers, run them regularly, and alert on drift in both accuracy and formatting.
  - Enforce structure at the interface boundary:
    - Prefer function‚Äëcalling/JSON schemas or constrained decoding to reduce sensitivity to format drift, especially for agents and code tools (Figures 9‚Äì10).
  - Design prompts and pipelines with graceful degradation:
    - Validate and sanitize outputs (e.g., strip Markdown fences before compiling code; verify agent action formats).
    - Add guardrails for refusals and verbosity shifts that may cascade into failures downstream.

- Research implications
  - Longitudinal evaluation should become standard. The paper‚Äôs methodology‚Äîpaired snapshots, diverse task suite, and instruction‚Äëfidelity probes‚Äîoffers a template that labs and third‚Äëparty evaluators can extend.
  - Instruction fidelity as a first‚Äëclass capability:
    - The dramatic drops in GPT‚Äë4‚Äôs instruction compliance (Figures 13‚Äì14) suggest developing dedicated benchmarks and training objectives for robust adherence, including compositions of constraints.
  - Understanding CoT robustness:
    - CoT is not universally helpful after updates (Tables 1‚Äì2). Future work can investigate when models refuse or shortcut CoT and how to regain consistent benefits (e.g., self‚Äëconsistency, tool‚Äëuse‚Äëdriven reasoning, or structured scratchpads).
  - Safer but less rationale:
    - GPT‚Äë4 reduces harmful responses and jailbreak susceptibility (Table 3) but becomes terser and less explanatory in refusals (Figure 7). Researchers can explore methods that are both safe and informative (e.g., templated refusals with actionable alternatives).
  - Model versioning and transparency:
    - Clearer release notes and reproducible version pinning would help practitioners maintain stability and trace regressions.

In short, the study demonstrates that LLMs-as-a-service can drift substantially‚Äîand unpredictably‚Äîover short periods. The most actionable takeaway is to monitor not only task accuracy but also instruction fidelity and output formatting, since small compliance changes can break real systems even when core knowledge or reasoning improves.

# Stay on topic with Classifier-Free Guidance

**ArXiv:** [2306.17806](https://arxiv.org/abs/2306.17806)

## ğŸ¯ Pitch

This paper introduces the use of Classifier-Free Guidance (CFG)â€”an inference-time technique popular in diffusion image generationâ€”to autoregressive language models, allowing them to adhere more closely to user prompts without additional training or model modifications. By simply reweighting model predictions with and without the prompt, CFG significantly boosts fidelity on tasks like Q&A, code generation, and assistant instructionsâ€”often matching models twice their size in accuracy and compute. This lightweight, training-free method empowers users to control generative outputs more precisely, enabling smaller, cheaper models to achieve state-of-the-art prompt alignment and utility.

---

## 1. Executive Summary (2â€“3 sentences)
This paper adapts Classifierâ€‘Free Guidance (CFG)â€”a simple inference-time trick widely used in diffusion image modelsâ€”to autoregressive language models to make generations stick more closely to the prompt. With no extra training, the method consistently improves zeroâ€‘shot benchmarks, longâ€‘form/code generation, and assistant prompts; notably, it pushes LLaMAâ€‘7B to stateâ€‘ofâ€‘theâ€‘art accuracy on LAMBADA and often matches the performance of models roughly twice the size at comparable inference compute (Sections 3â€“4; Figure 2; Figure 11).

## 2. Context and Motivation
- Problem addressed:
  - Large language models (LLMs) often drift from the userâ€™s prompt, leading to hallucinations, meandering, or ignoring system instructions (Introduction; Section 1; Table 1).
- Why it matters:
  - Better prompt adherence increases faithfulness, usefulness, and safety of generated text in assistants, longâ€‘form tasks, and code generation.
- Shortcomings of prior approaches:
  - Trainingâ€‘time fixes (instruction tuning, RLHF) improve adherence but are expensive and not always accessible (Introduction).
  - Decoding heuristics (temperature, nucleus sampling) change diversity but offer weak control over prompt adherence (Section 5).
  - Prior â€œcontrolled generationâ€ methods (e.g., PPLM/FUDGE) require extra classifiers or model modifications (Related Work; Appendix B.2).
- Positioning:
  - The paper provides an inferenceâ€‘only, trainingâ€‘free method that reweights token probabilities using two passes of the same modelâ€”one â€œwith the promptâ€ and one â€œwithoutâ€â€”to emphasize promptâ€‘consistent continuations (Sections 2.1â€“2.2; Equation 7).

## 3. Technical Approach
Classifierâ€‘Free Guidance in diffusion (background)
- In diffusion models, â€œclassifier guidanceâ€ adjusts samples using a classifierâ€™s gradient toward a desired condition `c` (Equation 1).
- CFG removes the external classifier by training the same generative model to handle both conditioned and unconditioned inputs and then combining them: `Pc(x|c) âˆ PÎ¸(x|c)^Î³ / PÎ¸(x)^(Î³âˆ’1)` (Equation 2), where `Î³` (â€œguidance strengthâ€) â‰¥ 0 amplifies the influence of condition `c`.
- An intuitive view: move away from an â€œunconditionalâ€ representation toward the â€œconditionalâ€ one by a step of size `Î³` (Equation 4; vector arithmetic view in Section 2.1).

How CFG is adapted to language models (Section 2.2)
- Key idea: without any retraining, LMs can compute both:
  - a conditional next-token distribution `PÎ¸(wi | w< i, c)` (normal prompting), and
  - an â€œunconditionalâ€ distribution `PÎ¸(wi | w< i)` by dropping the prompt `c` from the context window (LMs naturally support this due to finite contexts).
- Combine them at each decoding step in logit space (logits = the unnormalized scores before softmax):
  - `log PcÎ¸(wi|w< i, c) = log PÎ¸(wi|w< i) + Î³ [log PÎ¸(wi|w< i, c) âˆ’ log PÎ¸(wi|w< i)]` (Equation 7).
  - This raises probabilities of tokens favored by the prompted distribution and lowers those favored only by the unprompted one.
- Negative prompting (Section 2.1; Equation 5):
  - Users can also specify an undesired condition `câˆ’` (e.g., a default system prompt). Then move from `câˆ’` toward `c`: 
    - `log PcÎ¸(wi|w< i, c, câˆ’) = log PÎ¸(wi|w< i, câˆ’) + Î³ [log PÎ¸(wi|w< i, c) âˆ’ log PÎ¸(wi|w< i, câˆ’)]`.
  - This highlights how CFG can â€œemphasize the differenceâ€ between two prompts.

Design choices and rationale
- Operate in logits space (Section 2.2):
  - Logits are linearly related to the last hidden layer and easy to manipulate without architectureâ€‘specific surgery; they directly control token probabilities.
- No new training:
  - Unlike diffusion models that need conditioning dropout to learn unconditional paths, decoder LMs can already compute `P(wi|w< i)` by truncating the prompt (Section 2.2).
- Implementation detail in evaluations:
  - For zeroâ€‘shot benchmarks they â€œstart the unconditional prompt at the last token of the initial prompt,â€ i.e., approximate the unconditioned branch by dropping the prefix `c` to the final token to ensure a comparable decoding state (Section 3.1).

Simple analogy
- Think of two advisors at each step:
  - â€œPrompted advisorâ€ suggests tokens that fit the prompt,
  - â€œUnprompted advisorâ€ suggests common/average continuations.
  - CFG tells the decoder to favor the prompted advisor by factor `Î³` and discount the unprompted one.

## 4. Key Insights and Innovations
- Prompt adherence via outâ€‘ofâ€‘theâ€‘box CFG for LMs (fundamental):
  - The paper shows CFG works in autoregressive text generation without any retraining (Section 2.2), unlike diffusion models that need special training. This is a conceptual bridge between diffusion guidance and LM decoding.
- Negative prompting for assistants (new capability):
  - By setting the â€œnegative conditionâ€ to a modelâ€™s default system prompt and the â€œpositive conditionâ€ to a modified system prompt, CFG emphasizes systemâ€‘instruction compliance while preserving user relevance (Section 3.4; Figure 5).
- â€œSmall model + CFG â‰ˆ twiceâ€‘sized modelâ€ at similar inference compute (practical innovation):
  - Across five of nine benchmarks there is no statistically significant difference between â€œCFG on a smaller modelâ€ and â€œvanilla decoding on a model twice as largeâ€ (ANCOVA at p=.01; Section 4; Figure 11; Table 4). This offers a cost/latency alternative when VRAM is constrained.
- Mechanistic explanation of why it works (analysis contribution):
  - CFG reduces sampling entropy (narrows the plausible token set) to a level similar to instructionâ€‘tuned models, while reordering highâ€‘probability tokens toward promptâ€‘relevant ones (Section 5; Figure 6aâ€“b, Table 3). It is not equivalent to instruction tuning; overlaps are taskâ€‘dependent (Figure 7; Table 8).

## 5. Experimental Analysis
Evaluation setup
- Benchmarks (Section 3):
  - Zeroâ€‘shot tasks via the LM Harness: ARCâ€‘c/e, BoolQ, HellaSwag, PIQA, SCIQ, TriviaQA, WinoGrande, and LAMBADA (Figures 2, 8â€“10).
  - Chainâ€‘ofâ€‘Thought (CoT) reasoning: GSM8K and AQuA with fewâ€‘shot prompts; models: WizardLMâ€‘30B and Guanacoâ€‘65B (Section 3.2; Figure 3; Figure 15).
  - Code generation: HumanEval with CodeGenâ€‘350M/2B/6B at temperatures 0.2/0.6/0.8; metrics: pass@1/10/100 (Section 3.3; Table 2; Tables 5â€“7; Figures 12â€“14).
  - Assistant/systemâ€‘prompt control with negative prompting: GPT4Allâ€‘J v1.3â€‘jazzy, 1740 system/user prompt combinations; human preference study evaluating â€œfollows system promptâ€ vs â€œfollows user promptâ€ (Section 3.4; Figure 5; Appendix G).
  - Additional: Machine translation on WMT14 frâ€‘en with Bloomâ€‘3B, RedPajamaâ€‘3B, and mT0 (Appendix D.1), and targeted code prompts with GPTâ€‘J/CodeGen (Appendix D.2).
- Baselines:
  - â€œÎ³ = 1â€ is vanilla decoding; CFG uses Î³ > 1. For negative prompting, `câˆ’` is the modelâ€™s default system prompt and `c` is the edited system prompt (Section 3.4).

Main quantitative results
- Zeroâ€‘shot improvements are broad, with exceptions:
  - LLaMAâ€‘7B on LAMBADA (zeroâ€‘shot) improves from 73.6% to 81.3% with Î³=1.5, surpassing PaLMâ€‘540Bâ€™s reported 77.9% (Figure 2b).
  - Many tasks show nontrivial, consistent gains at Î³â‰ˆ1.5 across GPTâ€‘2, Pythia, and LLaMA families (Figures 8â€“10). ARCâ€‘challenge and WinoGrande are outliers where gains are small or negative (Section 3.1; Figure 2aâ€“b).
- Chainâ€‘ofâ€‘Thought (Section 3.2):
  - CFG increases the rate of â€œvalidly formattedâ€ answers and boosts accuracy for small Î³; too large Î³ reduces accuracy despite staying valid (Figure 3; Figure 15). 
  - Example (Table 15) shows a GSM8K prompt where Î³=1.1 yields the correct chain and answer, while Î³=1 (vanilla) diverges and formats incorrectly.
- Code generation (HumanEval; Section 3.3; Table 2):
  - At temperature 0.2, pass@1 rises at small Î³ and deteriorates at large Î³.
    - `CodeGenâ€‘2B`: pass@1 from 19.5% (Î³=1.0) â†’ 20.9% (Î³=1.5), then down to 16.5% (Î³=2.0) (Table 2).
    - `CodeGenâ€‘350M`: pass@1 ~11.0% (Î³=1.0), best around Î³=1.1 (11.8%), then declines (Table 2).
  - Gains shrink for pass@100 because CFG reduces diversity; tasks with tiny but nonâ€‘zero pass rates can benefit less from multiple samples (Section 3.3.2).
  - Targeted code prompts show practical quality gains: e.g., making a 32Ã—32 red image array, â€œcorrect return typeâ€ rises from 289 to 546 out of 1600 with Î³=2 (Table 13).
- Assistant/systemâ€‘prompt adherence via negative prompting (Section 3.4):
  - Human study (611 judgments, 71 raters) shows a clear peak at Î³=3:
    - â€œFollows system promptâ€ wins 75% of pairwise comparisons against Î³=1 (no CFG), while â€œfollows user promptâ€ remains statistically unchanged up to Î³â‰ˆ3 and only degrades for Î³â‰¥4 (Figure 5; Table 16 qualitative example).
- Cost vs. model size (Section 4):
  - Although CFG doubles forward passes, the computeâ€‘accuracy tradeâ€‘off is competitive with simply using a model twice as large:
    - â€œAcross 5/9 tasks there is no significant differenceâ€ (ANCOVA p>.01), with two tasks favoring CFG and two favoring vanilla (Figure 11; Table 4).
- Why it works (Section 5):
  - Entropy reduction: average logit entropy drops from ~5.4 (vanilla) to ~4.7 (CFGâ€‘Î³=1.5) (Figure 6a), shrinking the token set needed to cover topâ€‘p=0.9 probability mass (Figure 6b).
  - Not equivalent to instruction tuning: perplexities and topâ€‘p overlaps differ, with higher agreement on longer prompts and certain datasets (Figure 7; Table 8).
  - Visualization confirms token reordering toward promptâ€‘relevant content (Table 3 for â€œThe dragon flew over Paris, Franceâ€).

Do the experiments support the claims?
- Yes, for â€œCFG improves prompt adherence and often accuracyâ€: multiple families, datasets, and tasks show reliable gains at modest Î³ with clear counterâ€‘examples analyzed (ARCâ€‘c and WinoGrande). The assistant study shows humanâ€‘perceived gains and the analysis section provides a plausible mechanism (entropy and token reordering).
- Mixed/conditional findings are acknowledged:
  - Gains are Î³â€‘dependent; too much guidance harms diversity and sometimes accuracy (Section 3.2â€“3.3; Figures 12â€“14).
  - Instructionâ€‘tuned models and CFG are complementary rather than equivalent (Section 5.2; Figure 7).

## 6. Limitations and Trade-offs
- Tuning required:
  - Optimal `Î³` varies by task, model size, and temperature (Figures 8â€“14). Overâ€‘guidance (large Î³) reduces diversity and can harm reasoning/code correctness even while preserving answer format (Figure 3; Table 2).
- Not universally beneficial:
  - Some benchmarks (e.g., ARCâ€‘c, WinoGrande) show little or negative gains (Figure 2a; Figures 8â€“10).
- Compute/latency:
  - CFG roughly doubles inference FLOPs (two forward passes per token), though VRAM does not double (Section 4). This may be unacceptable in latencyâ€‘sensitive settings.
- Safety and robustness:
  - Increasing adherence may also strengthen malicious or injected instructions; the paper explicitly has not stressâ€‘tested CFG against prompt injection or alignment bypass (Conclusion).
- Methodological nuance:
  - The â€œunconditionalâ€ branch is approximated by dropping the prefix; when prompts carry essential state (e.g., long contexts), this approximation might misestimate `P(wi|w< i)` (Section 3.1 implementation note).
- Diversity vs. fidelity:
  - Entropy reduction and narrowed topâ€‘p set (Figure 6) mean fewer diverse continuationsâ€”beneficial for adherence but a drawback for creative writing or exploration (Section 5.1).

## 7. Implications and Future Directions
- Field impact:
  - Establishes CFG as a simple, modelâ€‘agnostic inference control for LMs, analogous to its role in diffusion models. It offers a practical knob to trade off diversity for prompt faithfulness without training (Sections 2â€“3, 5).
- Research avenues:
  - Targeted guidance: weight different parts of multiâ€‘stage prompts (system vs. user, or separate CoT vs. final answer) differently; authors note promising early results and many unexplored variants (Section 3.2 discussion).
  - Robustness: benchmark CFG under prompt injection and safety edge cases; standardize riskâ€‘focused evals (Conclusion).
  - Integration with other inference techniques: combine CFG with selfâ€‘consistency, reranking, or contrastive decoding (Section 1 claims and Related Work).
  - Learning to set Î³: develop adaptive or tokenâ€‘wise guidance strengths via uncertainty or entropy signals (Section 5.1 suggests entropy as a proxy).
- Applications:
  - Assistant systems: enhance compliance with system policies without retraining; negative prompting lets teams quickly â€œsteer awayâ€ from undesired personas (Section 3.4; Figure 5).
  - Longâ€‘form and structured generation: improved stayingâ€‘onâ€‘topic for essays, summarization with constraints, and documentâ€‘style outputs (Table 1 demonstration).
  - Code synthesis and formal tasks: modest but meaningful pass@1 boosts for smaller models, helpful where latency/VRAM are constrained (Table 2; Tables 5â€“7).
  - Machine translation and programmatic prompting: early evidence of gains for small Î³ with multilingual and base models (Appendix D.1â€“D.2; Table 11; Table 12; Table 13).

> Representative results:
> - â€œLLaMAâ€‘7B LAMBADA zeroâ€‘shot: 73.6 â†’ 81.3 (Î³=1.5)â€ (Figure 2b).
> - â€œHuman eval for assistants: 75% preference for CFG at Î³=3; user prompt relevance unchanged until Î³â‰¥4â€ (Figure 5).
> - â€œCodeGenâ€‘2B HumanEval pass@1: 19.5% â†’ 20.9% (Î³=1.5); declines at Î³=2.0â€ (Table 2).
> - â€œEntropy drops from ~5.4 to ~4.7 with CFGâ€‘Î³=1.5â€ (Figure 6a).
> - â€œAcross 5/9 tasks, a smaller model+CFG is statistically indistinguishable from a 2Ã— larger model without CFGâ€ (Figure 11; Table 4).

Definitions (used selectively)
- `Classifierâ€‘Free Guidance (CFG)`: An inferenceâ€‘time technique that combines conditional and unconditional model outputs to upweight tokens consistent with a condition `c`, controlled by `Î³` (Equations 2, 7).
- `Guidance strength (Î³)`: A nonâ€‘negative scalar; `Î³=1` is vanilla; `Î³>1` emphasizes the prompt; too large harms diversity/accuracy.
- `Negative prompting`: Specify an undesired condition `câˆ’` and move away from it toward `c` (Equation 5).
- `Logits`: Preâ€‘softmax scores for each token; adjusting logits changes the probability distribution.
- `Entropy` (of logits distribution): A measure of uncertainty; lower entropy implies a sharper distribution with fewer plausible tokens.
- `Pass@k`: For code benchmarks, fraction of problems solved if up to k samples per problem are allowed.
- `FLOPs`: Floatingâ€‘point operations; a proxy for compute cost during inference.

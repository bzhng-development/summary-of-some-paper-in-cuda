# TurboDiffusion: Accelerating Video Diffusion Models by 100‚Äì200 Times

**ArXiv:** [2512.16093](https://arxiv.org/abs/2512.16093)

## üéØ Pitch

TurboDiffusion is an algorithm+system co-optimization framework that speeds up video diffusion end-to-end by 100‚Äì200√ó using low-bit attention (SageAttention/SageSLA), sparse-linear attention, step distillation (rCM), and W8A8 INT8 quantization. By cutting sampling steps from ~100 to a few and accelerating attention and linear layers, it reduces multi-minute/video generation to seconds on a single RTX 5090 while preserving visual quality‚Äîmaking high-quality diffusion video generation practical for interactive and production use.

---

## 1. Executive Summary (2-3 sentences)
`TurboDiffusion` is an acceleration framework for video diffusion models that combines attention-side speedups, step (sampling) reduction, and `INT8` (`W8A8`) quantization to reduce end-to-end diffusion sampling latency by roughly `100‚Äì200√ó` while keeping visual quality comparable in the provided examples. On several `Wan` video generation models, it reduces 5-second video diffusion generation from minutes/hours to seconds on a single `RTX 5090` GPU (Figures 1‚Äì4). This matters because diffusion-based video generation is typically bottlenecked by repeated, expensive transformer denoising steps, making practical deployment difficult without large compute.

## 2. Context and Motivation
- **Problem/gap addressed.** Video diffusion models can generate high-quality videos but are *slow at inference* because they run a heavy denoising network many times (often ~`100` steps; Section 1.3 explicitly mentions reducing steps ‚Äúfrom `100` to ‚Ä¶ `4` or `3`).
- **Why it matters.**
  - Practical use cases (interactive creation, prototyping, product deployment) need generation in seconds to under a minute, not tens of minutes or hours.
  - For large models (e.g., `14B`-parameter class models suggested by names like `Wan2.1-T2V-14B-*`), memory pressure can also force slower execution strategies (e.g., CPU offload is mentioned in Figure 4).
- **Prior approaches and shortcomings (as positioned here).**
  - The evaluation compares to the official `Wan` implementation (‚Äú`Original`‚Äù) and `FastVideo` (Section 2.1).
  - The provided material emphasizes that existing acceleration baselines are either slower than TurboDiffusion (latency numbers in Section 2.2) or not available for some settings (no accelerated `Wan2.2-I2V-A14B-720P` in FastVideo; Section 2.2).
- **Positioning.** The framework is presented as **algorithm + system co-optimization**:
  - algorithmic: fewer steps via `rCM`, sparsified attention via `SLA`;
  - systems: low-bit attention kernels (`SageAttention`/`SageSLA`), `W8A8` Tensor Core linear layers, fused/optimized norms (Figure 4; Sections 1.1 and 1.3).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)
- The system is an inference/training toolkit that **modifies a pretrained video diffusion transformer** so it can generate videos much faster.
- It solves latency by (i) making attention/linear algebra cheaper per step and (ii) **using far fewer denoising steps** during sampling.

### 3.2 Big-picture architecture (diagram in words)
- **Input**: a pretrained video diffusion model (e.g., `Wan2.1-*` / `Wan2.2-*`) + a prompt (text; for I2V also an image first frame).
- **Training-time modifications**:
  1. Replace dense attention with `SLA` and finetune to adapt.
  2. Distill the model with `rCM` to reduce sampling steps.
  3. Merge the two sets of parameter updates into one model (Section 1.2).
- **Inference-time execution**:
  1. Use `SageSLA` (CUDA) to compute the sparse attention efficiently (Section 1.3).
  2. Run only `3‚Äì4` sampling steps instead of ~`100` (Section 1.3; Section 2.1).
  3. Quantize linear layers and activations to `INT8` (`W8A8`) with block-wise granularity (`128√ó128`) and use `INT8` Tensor Cores (Sections 1.1, 1.3).
  4. Apply additional kernel optimizations (e.g., LayerNorm/RMSNorm reimplementation) (Section 1.3).

### 3.3 Roadmap for the deep dive
- Explain why diffusion inference is slow (many steps √ó heavy transformer).
- Detail the **attention accelerations**: `SageAttention` and `SLA`, then how they combine as `SageSLA`.
- Detail **step distillation** via `rCM` and how it reduces step count.
- Detail **linear layer quantization** (`W8A8`) and its block-wise setup.
- Walk through the **training pipeline** (SLA finetune + rCM distill + merge) and then the **inference pipeline** end-to-end.

### 3.4 Detailed, sentence-based technical breakdown
This is an **empirical systems + algorithmic acceleration** paper whose core idea is to stack multiple mostly-orthogonal speedups‚Äîsparser attention, low-bit kernels, fewer diffusion steps, and `INT8` linear layers‚Äîso the overall latency drops by two orders of magnitude (Figures 3‚Äì4).

#### What happens first/second/third (pipeline diagram in words)
1. **Start from a pretrained video diffusion model.** The framework assumes you already have a working model checkpoint (Section 1.2).
2. **Make attention sparse and adapt the weights.** Full (dense) attention is replaced with `Sparse-Linear Attention (SLA)` and the model is finetuned so quality does not collapse under sparsity (Section 1.2).
   - *Definition (paper-specific):* `SLA` is an attention mechanism that enforces high sparsity (Section 2.1 uses a Top‚ÄëK setting corresponding to `90%` attention sparsity) while keeping the computation compatible with efficient implementations.
3. **Distill the sampler to use far fewer steps.** In parallel, the original model is distilled using `rCM` into a student that can sample in a small number of steps (Section 1.2).
   - *Definition (paper-specific):* `rCM` is a diffusion distillation method used here specifically to reduce the number of sampling steps while attempting to preserve generation quality (Sections 1.1, 1.3). The excerpt does not include its loss, equations, or training schedule.
4. **Merge the two training outcomes into a single model.** The parameter updates from SLA finetuning and rCM training are merged (‚ÄúThrough model weights merging‚Ä¶‚Äù, Section 1.1; merging described in Section 1.2). The key practical claim is that the distilled model ‚Äúnaturally inherits attention-level accelerations‚Äù after merging (Section 1.1).
5. **Deploy inference-time kernel and quantization accelerations.** At inference (Section 1.3):
   - SLA attention is executed via `SageSLA` (‚Äúa CUDA implementation of SLA built on top of SageAttention‚Äù).
   - Sampling steps are reduced from `100` to `3‚Äì4` steps (Section 1.3; Section 2.1 uses `3`).
   - Linear layers use `W8A8` quantization (INT8 weights + INT8 activations) with `128√ó128` block-wise granularity, computed on `INT8` Tensor Cores.
   - Additional operators like `LayerNorm` and `RMSNorm` are reimplemented in `Triton`/`CUDA` for efficiency.

#### Component 1: Attention acceleration (`SageAttention` + `SLA` ‚Üí `SageSLA`)
- The framework uses **two orthogonal levers** for attention:
  1. **Low-bit attention computation** via `SageAttention` (specifically `SageAttention2++`) (Section 1.1).
     - *Definition (paper-specific, as used here):* ‚Äúlow-bit‚Äù attention means attention computations are executed in quantized form to exploit specialized hardware paths (e.g., Tensor Cores). The excerpt does not specify the exact attention bit-width settings beyond ‚Äúlow-bit quantized attention acceleration‚Äù and references `SageAttention2++` (Section 1.1).
  2. **Sparse attention computation** via `SLA` (Section 1.1).
     - The evaluation configuration uses `Top-K ratio = 0.1`, described as `90% attention sparsity` (Section 2.1), meaning only a small fraction of attention connections are kept.
- These stack because ‚Äúsparse computation is orthogonal to low-bit Tensor Core acceleration,‚Äù so `SLA` can ‚Äúbuild on top of SageAttention to provide cumulative speedup‚Äù (Section 1.1).
- At inference, this combination is realized as `SageSLA`, a CUDA implementation of SLA ‚Äúbuilt on top of SageAttention‚Äù (Section 1.3).

#### Component 2: Step distillation (`rCM`)
- Diffusion sampling cost scales roughly linearly with the number of denoising steps, so reducing steps is a direct multiplicative latency win.
- TurboDiffusion applies `rCM` to distill a pretrained model into a student that can sample with ‚Äúa much smaller value, e.g., `4` or `3`‚Äù steps instead of `100` (Sections 1.1 and 1.3).
- The evaluation section uses `3` steps (Section 2.1) and recommends `4` steps for best consistent quality in practice (Section 2.1).
- The excerpt does **not** provide:
  - rCM‚Äôs training objective, loss terms, or any equations;
  - teacher/student model sizes;
  - distillation compute, tokens, or dataset specifics.

#### Component 3: Linear layer quantization (`W8A8`)
- TurboDiffusion quantizes both:
  - **model parameters (weights)** in linear layers to `INT8`, and
  - **activations** in linear layers to `INT8` (Section 1.3),
  so that linear algebra runs on `INT8 Tensor Cores`.
- The quantization granularity is **block-wise** with block size `128 √ó 128` (Sections 1.1 and 1.3).
- The excerpt claims two main effects:
  - **Throughput improvement** in linear layers from `INT8` compute;
  - **Model size reduction** ‚Äúby roughly half‚Äù (Section 1.3).
- The excerpt does not specify calibration method, rounding strategy, or whether quantization-aware training is used; it only describes the inference-time quantization procedure.

#### Component 4: Other kernel/engineering optimizations
- TurboDiffusion ‚Äúreimplement[s] several other operations, such as `LayerNorm` and `RMSNorm`, using `Triton` or `CUDA`‚Äù (Section 1.3).
- Figure 4 also mentions `FusedNorm` as part of the optimization ladder for `Wan2.1-T2V-14B-720P`.

#### Training configuration / hyperparameters (only what is provided)
- **Attention sparsity setting:** `Top-K ratio = 0.1` ‚Üí `90%` sparsity (Section 2.1).
  - Recommended range: `Top-K ‚àà [0.1, 0.15]` (Section 2.1).
- **Sampling steps:** `3` steps in the main experiments (Section 2.1).
  - Recommended: `4` steps for best consistent quality (Section 2.1).
- **Quantization:** `W8A8` with `INT8` data type; block-wise `128√ó128` (Sections 1.1, 1.3).
- **Hardware for main results:** single `RTX 5090` GPU (Section 2.1).
- **Missing from the excerpt:** optimizer, learning rate schedule, batch size, training tokens/steps, context window, architecture depth/width/heads, tokenizer, compute budget, and training hardware.

## 4. Key Insights and Innovations
- **(1) Stacking orthogonal accelerations instead of relying on one trick.**
  - Novelty here is the *system-level composition*: sparse attention (`SLA`) + low-bit attention kernels (`SageAttention` family) + step distillation (`rCM`) + `W8A8` linear quantization, plus kernel engineering (Sections 1.1‚Äì1.3; Figure 4).
  - Significance is shown by the large multiplicative speedups in Figure 4 (an ‚Äúalgorithm and system co-optimization‚Äù ladder down to ~`200√ó` total on one model).
- **(2) `SageSLA`: making sparse attention actually fast on GPU.**
  - Sparse methods can underperform if not implemented carefully due to overheads; TurboDiffusion explicitly introduces `SageSLA` as a CUDA implementation of SLA ‚Äúbuilt on top of SageAttention‚Äù (Section 1.3), indicating the main innovation is *kernel-level realization* of the combined method.
- **(3) Weight-update merging to combine SLA finetuning and rCM distillation.**
  - The training recipe runs two adaptations ‚Äúin parallel‚Äù (SLA finetune; latency-oriented distillation) and then merges their parameter updates into a single checkpoint (Section 1.2).
  - The practical significance is that distillation ‚Äúnaturally inherits attention-level accelerations‚Äù through merging (Section 1.1), so the step-reduced model still benefits from attention speedups.
- **(4) Full `W8A8` inference path (weights + activations) with block-wise `128√ó128` granularity.**
  - Quantizing both weights and activations enables `INT8 Tensor Core` execution for linear layers and reduces memory footprint (‚Äúroughly half‚Äù) (Section 1.3), which is particularly important for large video diffusion transformers.

## 5. Experimental Analysis
### Evaluation methodology
- **Models evaluated (Section 2.1):**
  - `Wan2.2-I2V-A14B-720P`
  - `Wan2.1-T2V-1.3B-480P`
  - `Wan2.1-T2V-14B-720P`
  - `Wan2.1-T2V-14B-480P`
- **Baselines (Section 2.1):**
  - `Original`: official implementation of `Wan` [7]
  - `FastVideo` [8] (except it does not provide accelerated `Wan2.2-A14B-I2V-720P`, so that case is compared only to `Original`; Section 2.2)
- **Metric reported:**
  - **End-to-end diffusion generation latency** **excluding** text encoding and VAE decoding (Section 2.2).
  - This is important: the reported speedups apply to the diffusion denoising stage, not necessarily full ‚Äúprompt ‚Üí final video file‚Äù time.
- **Key hyperparameters (Section 2.1):**
  - `Top-K ratio = 0.1` (~`90%` attention sparsity)
  - `3` sampling steps
  - For `FastVideo`, they use its default parameters: `3` steps and `0.8` sparsity in attention (Section 2.1).
- **Hardware (Section 2.1):**
  - Primary: single `RTX 5090`
  - They also mention acceleration on `RTX 4090` and `H100`, but no numbers are included in the excerpt.

### Main quantitative latency/speedup results (with specific numbers)
Figure 3 summarizes latencies and speedups on a single `RTX 5090`:

- `Wan2.2-I2V-A14B-720P`:
  - `Original`: `4549 s`
  - `TurboDiffusion`: `38 s`
  - Speedup: `120√ó` (Figure 3)
  - Note: the caption explains that latency includes switching overhead between ‚Äúhigh-noise and low-noise models,‚Äù reducing the measured speedup; ‚Äúin theory, the achievable speedup is identical‚Äù to `Wan2.1-T2V-14B-720P` (Figure 3 caption).
- `Wan2.1-T2V-1.3B-480P`:
  - `Original`: `184 s`
  - `FastVideo`: `5.3 s` (Figures 12‚Äì23 show qualitative examples alongside this latency)
  - `TurboDiffusion`: `1.9 s`
  - Speedup vs `Original`: `97√ó` (Figure 3)
- `Wan2.1-T2V-14B-480P`:
  - `Original`: `1676 s`
  - `FastVideo`: `26.3 s` (Figures 28‚Äì29 show qualitative examples for 480P 14B)
  - `TurboDiffusion`: `9.9 s`
  - Speedup vs `Original`: `170√ó` (Figure 3)
- `Wan2.1-T2V-14B-720P`:
  - `Original`: `4767 s`
  - `FastVideo`: `72.6 s` (Figures 24‚Äì27 show examples for 720P 14B)
  - `TurboDiffusion`: `24 s`
  - Speedup vs `Original`: `199√ó` (Figure 3)

Figure 4 provides an ablation-style ‚Äúoptimization ladder‚Äù on `Wan2.1-T2V-14B-720P`, attributing large gains to combining system and algorithmic changes. Some intermediate labels are visible (e.g., `+ W8A8 & FusedNorm`, `+ rCM`, `+ SageSLA (final version)`) and the final bar is `24 s` with `199√ó` total speedup, but the excerpt images do not fully disambiguate every intermediate condition cleanly, so only the clearly readable endpoints and named stages are safe to cite.

### Quality evaluation
- The excerpt primarily uses **visual side-by-side examples** (Figures 5‚Äì29) to argue that TurboDiffusion ‚Äúmaintains the video quality‚Äù and is superior to FastVideo in these examples (Section 2.2).
- **No numeric quality metrics** (e.g., FVD, CLIP scores, human preference rates) appear in the provided content, so the quality claim is supported here only qualitatively.

### Do the experiments support the claims?
- **Efficiency claim (100‚Äì200√ó speedup):** Strongly supported by the reported latency numbers and speedup bars (Figures 1‚Äì4).
- **Quality preservation claim:** Supported only by qualitative comparisons in the shown examples (Section 2.2; Figures 5‚Äì29). Without metrics or user studies in the excerpt, it is hard to judge robustness across prompts, motion types, or failure cases.

## 6. Limitations and Trade-offs
- **Quality evidence is qualitative in the provided excerpt.** Without quantitative metrics or systematic human evaluation, ‚Äúnegligible quality degradation‚Äù (Conclusion) cannot be independently assessed from this excerpt alone.
- **Speed reporting excludes parts of the pipeline.** Latency explicitly excludes text encoding and VAE decoding (Section 2.2), so real product latency may be higher depending on those components.
- **Hardware specificity.** The headline numbers are on a single `RTX 5090` (Section 2.1). The excerpt notes speedups on `RTX 4090` and `H100` but provides no measurements, and low-bit kernel performance can be GPU-architecture dependent.
- **Potential overheads in some settings.** For `Wan2.2-I2V-A14B-720P`, the caption notes overhead from switching between ‚Äúhigh-noise and low-noise models,‚Äù reducing measured speedup (Figure 3 caption). This suggests that multi-model or staged pipelines may blunt end-to-end gains.
- **Sparsity/step settings likely require tuning.** The method exposes knobs (`Top-K` sparsity and number of steps), and the excerpt itself recommends different settings for ‚Äúconsistently‚Äù best quality (`Top-K ‚àà [0.1, 0.15]`, steps=`4`) than the main benchmark (`Top-K=0.1`, steps=`3`) (Section 2.1).
- **Training details are underspecified here.** The excerpt does not include datasets, filtering, contamination checks, or core training hyperparameters, making it difficult to evaluate reproducibility or generalization from the text provided.

## 7. Implications and Future Directions
- **Field impact:** If the reported diffusion-stage latencies generalize, TurboDiffusion-like stacks can shift video diffusion from ‚Äúoffline batch generation‚Äù to ‚Äúnear-interactive generation‚Äù regimes (e.g., `24 s` for a 5-second `14B` 720P example; Figure 3), making deployment far more feasible.
- **Practical applications enabled:**
  - Rapid creative iteration (prompting many variations quickly).
  - Serving-time video generation on a single high-end GPU for demos or small-scale products.
  - Faster research iteration on large video diffusion models because sampling is cheaper.
- **Future directions explicitly stated:** extend to ‚Äúmore video generation paradigms, such as autoregressive video diffusion‚Äù (Conclusion).
- **Repro/Integration Guidance (based on provided excerpt):**
  - If you want **maximum speed**, the demonstrated setting is `3` sampling steps and `Top-K ratio = 0.1` (~`90%` sparsity) (Section 2.1).
  - If you want **more consistent quality**, the excerpt recommends `Top-K` in `[0.1, 0.15]` and `4` sampling steps (Section 2.1).
  - The core runtime pieces to integrate are: `SageSLA` for attention (Section 1.3), `rCM`-distilled low-step checkpoint (Sections 1.2‚Äì1.3), and `W8A8` (`INT8`) linear layers with `128√ó128` block quantization for both weights and activations (Sections 1.1, 1.3).
  - When comparing latency in your own setup, match the paper‚Äôs definition: diffusion-stage time **excluding** text encoder and VAE decoder (Section 2.2), otherwise numbers will not be directly comparable.

# Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking

**ArXiv:** [2601.04720](https://arxiv.org/abs/2601.04720)

## üéØ Pitch

This paper introduces Qwen3-VL-Embedding and Qwen3-VL-Reranker, a unified end-to-end stack that maps text, images, visual documents, and video into a single embedding space for scalable nearest-neighbor retrieval and then applies a cross-encoder reranker for fine-grained relevance scoring. By combining multi-stage contrastive pretraining, distillation from a reranker, Matryoshka Representation Learning (variable embedding sizes), and quantization-aware training, the suite delivers SOTA multimodal retrieval performance while being deployment-friendly for large-scale, low-cost search applications.

---

## 1. Executive Summary (2-3 sentences)

`Qwen3-VL-Embedding` and `Qwen3-VL-Reranker` form an end-to-end multimodal retrieval stack that (i) encodes text, images, document images (‚Äúvisual documents‚Äù), and video into a single embedding space for fast candidate retrieval, and (ii) reranks candidate query‚Äìdocument pairs with a cross-encoder that performs fine-grained cross-attention (Figure 2). The central significance is practical: the embedding model is trained to be *deployment-friendly* via `Matryoshka Representation Learning (MRL)` (variable embedding dimensions) and `Quantization-Aware Training (QAT)` (robustness to low-precision embeddings), while retaining strong performance across multimodal benchmarks like `MMEB-v2` (Table 2, Figure 6).

## 2. Context and Motivation

- **Problem / gap addressed**
  - Modern search systems must retrieve relevant items when the *query modality* and *document modality* differ (e.g., text ‚Üí image, text ‚Üí video, question ‚Üí screenshot/document page), not just text ‚Üí text (Introduction; Figure 1).
  - Multimodal retrieval stacks usually need **two complementary modules**:
    - An **embedding model** (bi-encoder) for scalable nearest-neighbor search over millions/billions of items.
    - A **reranker** (cross-encoder) for precise relevance scoring among top candidates (Introduction; Figure 2).

- **Why it matters**
  - Real-world applications cited include e-commerce discovery, scientific literature exploration, and social navigation where content increasingly includes screenshots, infographics, and videos (Introduction).
  - Multimodal documents (slides/infographics) interleave text and visuals so strongly that unimodal text retrieval is structurally insufficient (Introduction).

- **Prior approaches and shortcomings**
  - `CLIP`-style contrastive pretraining demonstrates cross-modal alignment (Introduction), but retrieval needs broader modality coverage (video, visual documents), task coverage (classification/QA/retrieval/grounding), and robustness to production constraints (storage/latency).
  - VLM-based embedding models (e.g., efforts like E5-V / GME / BGE-VL / VLM2Vec are referenced in the Introduction) motivate using a vision-language foundation model backbone to inherit cross-modal alignment, multilinguality, and document understanding.

- **How this work positions itself**
  - It builds directly on the `Qwen3-VL` foundation model, specializing it into:
    - A **unified multimodal embedding model** trained with multi-stage contrastive learning + reranker distillation (Figure 5; Sections 4‚Äì5).
    - A **cross-encoder reranker** trained as ‚Äúyes/no‚Äù relevance prediction (Eq. (4)‚Äì(5); Figure 2).
  - It explicitly targets **production constraints** via MRL + QAT and analyzes the accuracy‚Äìefficiency trade-off (Section 7.1; Figure 6).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **multimodal retrieval pipeline**: it first maps queries/documents of many modalities into vectors for fast approximate search, then reranks the top results with a slower but more accurate pairwise model.
- It solves **high-precision multimodal search** by combining a `bi-encoder` embedding model (fast retrieval with cosine similarity) and a `cross-encoder` reranker (deep interaction scoring) (Figure 2).

### 3.2 Big-picture architecture (diagram in words)

- **Backbone:** `Qwen3-VL` vision-language model with a **vision encoder** feeding a **Qwen3 LM dense decoder** (Figure 2).
- **Embedding branch (`Qwen3-VL-Embedding`):**
  - Input: an *instruction* + a single multimodal instance (query or doc).
  - Output: one dense vector taken from the final hidden state at a terminal `PAD` token.
  - Retrieval score: cosine similarity between query and document vectors.
- **Reranking branch (`Qwen3-VL-Reranker`):**
  - Input: an instruction + a **query‚Äìdocument pair** (each can be multimodal).
  - Output: a relevance score derived from the model‚Äôs next-token logits for `"yes"` vs `"no"` (Eq. (5)).

### 3.3 Roadmap for the deep dive

- Explain the **two model forms** (bi-encoder vs cross-encoder) because they determine cost/accuracy trade-offs (Figure 2).
- Detail the **input formatting / prompting templates** because this work is instruction-aware and uses a causal LM backbone (Section 2, templates).
- Walk through the **data construction** (dataset format + synthesis + mining) because retrieval quality is data-sensitive (Section 3).
- Explain the **multi-stage training pipeline** and how each stage changes behavior (Figure 5; Section 4; Table 6).
- Unpack the **losses and auxiliary techniques** (InfoNCE, CoSent, distillation, MRL, QAT) because they are the main algorithmic levers (Section 5; Eq. (1)‚Äì(3)).
- Summarize **evaluation settings and results** to connect design ‚Üí observed performance (Section 6; Tables 2‚Äì5; Figures 6‚Äì7).

### 3.4 Detailed, sentence-based technical breakdown

**Framing.** This is an empirical systems-and-training recipe paper: it specializes a large vision-language backbone into (1) a unified multimodal embedding model and (2) a multimodal reranker, mainly via staged contrastive learning, synthetic/curated data, and distillation (Figure 5; Sections 3‚Äì5).

#### 3.4.1 Core model interfaces: embedding vs reranking

- **Embedding model = bi-encoder retrieval.**
  - A *query* and a *document* are encoded **independently** into vectors, enabling offline indexing of the entire corpus and fast nearest-neighbor search at query time (Figure 2).
  - The relevance function is **cosine similarity** $s(q,d)=\cos(\mathbf{e}_q,\mathbf{e}_d)$ as used inside the contrastive objective (Eq. (1)).

- **Reranker model = cross-encoder scoring.**
  - The query and document are concatenated into a single context so the model can attend across both sides (cross-attention via standard attention over the joint token sequence in a causal decoder; Figure 2 describes ‚Äúcross-attention mechanisms‚Äù in the cross-encoder setup).
  - The model predicts a binary label (`"yes"` relevant / `"no"` irrelevant) as the next token, which becomes a calibrated-ish score via a logit difference (Eq. (5)).

#### 3.4.2 Exact prompting / input templates (instruction-aware)

- **Embedding template (Section 2):**
  - The instruction is a system message (default: `"Represent the user‚Äôs input."`).
  - The instance (text/image/video/mix) is a user message.
  - A terminal token `<|endoftext|>` is appended, and **the last hidden state at that token** is taken as the embedding vector.

  Concretely, the template is:
  ```
  <|im_start|>system
  {Instruction}
  <|im_end|>
  <|im_start|>user
  {Instance}
  <|im_end|>
  <|im_start|>assistant
  <|endoftext|>
  ```

- **Reranking template (Section 2):**
  - The system message constrains output strictly to `"yes"` or `"no"`.
  - The user message contains `<Instruct>`, `<Query>`, `<Document>` fields.

  ```
  <|im_start|>system
  Judge whether the Document meets the requirements based on the Query and the Instruct
  provided. Note that the answer can only be "yes" or "no".
  <|im_end|>
  <|im_start|>user
  <Instruct>: {Instruction}
  <Query>: {Query}
  <Document>: {Document}
  <|im_end|>
  <|im_start|>assistant
  ```

**Why this matters:** by making both models *instruction-aware* (Table 1), a single checkpoint can serve multiple retrieval-like tasks (retrieval, QA-as-retrieval, classification-as-retrieval) by changing the instruction string.

#### 3.4.3 Model sizes, sequence lengths, and embedding dimensions (what is specified)

From **Table 1**:

- `Qwen3-VL-Embedding`:
  - `2B`: 28 layers, sequence length 32K tokens, embedding dimension 2048, quantization support = Yes, MRL support = Yes, instruction-aware = Yes.
  - `8B`: 36 layers, sequence length 32K tokens, embedding dimension 4096, quantization support = Yes, MRL support = Yes, instruction-aware = Yes.
- `Qwen3-VL-Reranker`:
  - `2B`: 28 layers, sequence length 32K, instruction-aware = Yes.
  - `8B`: 36 layers, sequence length 32K, instruction-aware = Yes.

**Not specified in the provided content:** optimizer choice, learning rate / schedule, batch size, weight decay, tokenizer details, attention head count, hidden size, total training tokens (beyond dataset sizes), compute budget (e.g., PF-days), or hardware.

#### 3.4.4 Data: unified format + large-scale synthesis + mining

- **Unified dataset format (Section 3.1).**
  - The full training corpus is a set of sub-datasets $D=\{D_i\}_{i=1}^M$.
  - Each $D_i=(I_i,Q_i,C_i,R_i)$ contains:
    - `Instruction` $I_i$: defines what ‚Äúrelevance‚Äù means for that dataset/task.
    - Queries $Q_i=\{q_j\}$ and corpus $C_i=\{d_j\}$: each element can be text/image/video/mixed.
    - Relevance labels $R_i$: for each query, sets of positives $\{d^+_{j,k}\}$ and negatives $\{d^-_{j,k}\}$.

- **Why synthetic data is used (Section 3).**
  - Public and in-house data are imbalanced across modalities/tasks/domains, and some categories are scarce.
  - The approach uses synthesis to ‚Äúbalance‚Äù coverage (Figure 3 conceptually shows modality mix; Figure 4 shows seed pool category distributions for image/video).

- **Seed pool construction pipeline (Section 3.2).**
  - Start with large raw image/video collections.
  - Apply coarse quality filters: remove low resolution or irregular aspect ratios.
  - For video: do scene cut detection; remove static/corrupted segments to preserve temporal dynamics.
  - Use `Qwen3-VL-32B` to generate fine-grained categorical labels.
  - Filter out low-confidence annotations or poor image-text correspondence *using similarity scores* from the `GME` embedding model (Section 3.2).
  - Rebalance categories to form the final seed pool (Figure 4).

- **Task synthesis programs (Section 3.2).**
  - For images: classification, image QA, image retrieval (each produces positives + ‚Äúconfusing‚Äù negatives/distractors).
  - For video: classification, video QA, video retrieval, moment retrieval (temporal segment localization + irrelevant segment negatives).
  - A two-step prompting strategy is used: generate a caption first, then generate task-specific annotations (Section 3.2; Appendix B shows example prompt formats).

- **Hard negative mining and positive refinement (Section 3.3).**
  - Step 1 (Recall): encode all queries/docs; retrieve top-$K$ candidates per query by cosine similarity.
  - Step 2 (Relevance filtering):
    - Keep a query only if it retrieves at least one positive with score $s>t_+$ (threshold $t_+$ is a hyperparameter, but its numeric value is not provided).
    - Select hard negatives among retrieved non-positives whose score is below the refined positive mean plus a margin: $s < \bar{s}_+ + \delta_-$ (margin $\delta_-$ is referenced but not numerically specified).

#### 3.4.5 Multi-stage training pipeline (what happens first/second/third)

Figure 5 + Section 4.1 define a staged progression producing embedding checkpoints `s0 ‚Üí s1 ‚Üí s2 ‚Üí s3`:

1. **Stage 1: Contrastive pre-training (produces `s0`).**
   - Data: large-scale synthetic multimodal multi-task data (Figure 5 labels this as ‚ÄúSynthesized Data: 300M‚Äù).
   - Negatives: include mined hard negatives (Section 3.3).
   - Objective: retrieval-style InfoNCE contrastive loss (Eq. (1)).

2. **Stage 2: Multi-task contrastive learning + reranker training (produces `s1` + a reranker).**
   - Data: mixture of curated public datasets, proprietary in-house data, and sampled synthetic data to counter task imbalance (Section 4.1; Figure 5 shows ‚ÄúSynthesized/Collected Data: 40M‚Äù for this stage).
   - The improved `s0` is used to mine higher-quality training data (bootstrapping loop described in Section 4).
   - The embedding model is trained with ‚Äútailored contrastive objectives for different task types‚Äù (Section 4.1; Section 5.1 explains category-specific losses).
   - In parallel, the reranker is trained on retrieval-centric subsets (image/video/moment/visual-document retrieval) using the reranking objective (Eq. (4)).

3. **Stage 3: Distillation + model merging (produces `s2` then final `s3`).**
   - Distillation:
     - Construct a compact, balanced dataset across retrieval categories (size not explicitly given here; Figure 5 separately shows ‚ÄúCollected Data: 4M‚Äù associated with the supervised reranker training block).
     - Use the trained reranker to generate fine-grained relevance logits offline.
     - Train the embedding model to match the reranker‚Äôs distribution (Eq. (3)), producing `s2` (Section 4.1).
   - Model merging:
     - `s2` improves retrieval but slightly harms classification/QA.
     - Merge `s2` with `s1` using a model-merging method (referenced to Li et al. (2024)) to get `s3`, balancing tasks (Section 4.1; supported by stage-wise metrics in Table 6).

#### 3.4.6 Training implementation details (what is specified)

- **Parameter-efficient tuning via `LoRA` (Section 4.2).**
  - Model parameters are initialized from `Qwen3-VL-Instruct`.
  - Claimed advantages: reduced memory footprint, larger effective batch sizes, better generalization, and easier hyperparameter search for model merging (Section 4.2).

- **Visual token budgeting / sampling (Section 4.2).**
  - Images:
    - Preserve aspect ratio.
    - Cap maximum token consumption at **1,280 visual tokens**, approximately **$1.3\times 10^6$ pixels**.
  - Videos:
    - Sample at **1 FPS**, max **64 frames**.
    - Preserve frame aspect ratio.
    - Total token budget across all frames capped at **4,500 tokens**, approximately **$9.2\times 10^6$ pixels**.

- **Evaluation-time context limits (Section 6.1).**
  - Even though models support up to 32,768 tokens (Table 1), MMEB-v2 evaluation constrains context length to **16,384 tokens**.
  - For MMEB-v2 evaluation:
    - Image tasks: max **1,800 tokens**.
    - Video tasks: max **15,000 tokens**, max **64 frames** (Section 6.1).

#### 3.4.7 Objectives and how they work (with equations + a micro-example)

**(A) Retrieval contrastive loss (Stage 1; Eq. (1))**

- Plain language: for each query, push its embedding closer to the positive document and farther from negatives (hard negatives + in-batch negatives), using a temperature-scaled softmax.

The loss is:
\[
L_{\text{retrieval}} = -\frac{1}{N}\sum_{i=1}^{N}\log \frac{\exp(s(q_i,d_i^+)/\tau)}{Z_i}.
\]
The normalizer $Z_i$ includes multiple negative pools (Eq. (1)), with a masking term $m_{ij}$ used to reduce ‚Äúfalse negative‚Äù damage:
\[
m_{ij}=
\begin{cases}
0, & \text{if } s_{ij} > s(q_i,d_i^+) + 0.1 \text{ or } d_j=d_i^+,\\
1, & \text{otherwise}.
\end{cases}
\]

**Micro-example (toy):** suppose a query has one positive and one negative with cosine similarities $s(q,d^+)=0.8$, $s(q,d^-)=0.3$, temperature $\tau=0.1$.
- Positive logit: $0.8/0.1=8$, negative logit: $0.3/0.1=3$.
- Softmax prob of the positive (with only that one negative) is:
  \[
  p(d^+|q)=\frac{e^8}{e^8+e^3}\approx \frac{2981}{2981+20}=0.993.
  \]
- Loss contribution is $-\log(0.993)\approx 0.007$, meaning the model already ranks the positive strongly above that negative.

**(B) STS loss via `CoSent` (Eq. (2))**

- Plain language: for semantic similarity datasets where labels are graded scores, enforce that the embedding cosine similarities preserve the correct ordering of pairs by gold similarity.

The loss is:
\[
L_{\text{sts}} = \log\left(1+\sum_{\hat{s}(q_i,d_j)>\hat{s}(q_m,d_n)}
\exp\left(\frac{\cos(q_m,d_n)-\cos(q_i,d_j)}{\tau}\right)\right).
\]

**(C) Distillation loss from reranker to embedder (Eq. (3))**

- Plain language: make the embedding model‚Äôs softmax distribution over candidate docs match the reranker‚Äôs distribution for the same query (reranker scores are computed offline).

\[
L_{\text{distill}}= -\sum_{i=1}^{k+1} P_{\text{reranker}}(d_i|q)\log P_{\text{embedding}}(d_i|q).
\]

**Micro-example (toy):** candidates are $\{d^+,d_1^-,d_2^-\}$. If reranker logits are $[2,0,-1]$, then
\[
P_{\text{reranker}}=\text{softmax}([2,0,-1])\approx [0.84,0.11,0.04].
\]
If the embedding model currently yields $P_{\text{embedding}}\approx [0.60,0.25,0.15]$, the distillation loss penalizes the mismatch, pushing the embedding model to concentrate more mass on $d^+$.

**(D) Reranker objective and scoring (Eq. (4)‚Äì(5))**

- Training loss:
\[
L_{\text{reranking}}=-\log p(l\,|\,I,q,d),
\]
with $l\in\{\text{"yes"},\text{"no"}\}$.

- Inference score:
\[
s=\sigma(\text{logit}(\text{yes})-\text{logit}(\text{no})).
\]
This means the reranker is used as a calibrated relevance estimator over candidates.

#### 3.4.8 Efficiency techniques: MRL + QAT (Section 5.1.1; Figure 6)

- **`MRL` (Matryoshka Representation Learning).**
  - Mechanism: during training, compute losses not only on the full embedding vector but also on truncated prefixes (lower dimensions), so the same model can produce competitive embeddings at multiple dimensionalities without retraining (Section 5.1.1).
  - Practical implication: you can store, e.g., 512-D prefixes instead of 1024-D/2048-D/4096-D if you need lower storage/latency.

- **`QAT` (Quantization-Aware Training) using `LSQ` + `STE`.**
  - Mechanism: train with both full-precision and quantized embeddings in the loop; `LSQ` learns quantization step size, and `STE` passes gradients through rounding (Section 5.1.1).
  - The analysis claims int8 quantization preserves performance with negligible degradation, while binary quantization degrades performance strongly (Section 7.1; Figure 6).

## 4. Key Insights and Innovations

- **Unified, instruction-aware multimodal embedding space across text/image/visual-document/video (Figure 1; Table 1).**
  - What‚Äôs different: instead of separate modality-specific encoders, the same backbone and prompting format handle all modalities and tasks by changing the instruction.
  - Why it matters: a single retrieval index approach can support heterogeneous corpora (documents with screenshots, videos, captions) without building separate systems.

- **Multi-stage pipeline that explicitly distills a cross-encoder reranker into a bi-encoder embedder (Figure 5; Eq. (3); Table 6).**
  - What‚Äôs different: the embedder is not only contrastively trained; it is later trained to mimic reranker distributions, transferring fine-grained discrimination into the fast retrieval model.
  - Why it matters: it targets the common retrieval pain point‚Äîbi-encoders are fast but less precise‚Äîby pulling precision improvements ‚Äúupstream‚Äù into embeddings.

- **Production-oriented embedding robustness via `MRL` + `QAT` with empirical trade-off analysis (Section 5.1.1; Figure 6).**
  - What‚Äôs different: training explicitly optimizes for multiple embedding dimensions and quantization formats rather than treating them as afterthought compression steps.
  - Why it matters: storage and latency dominate real retrieval deployments; the paper quantifies how much accuracy you lose for large efficiency wins (Section 7.1).

- **Automated multimodal synthesis + mining loop with quality filters and hard-negative selection (Sections 3.2‚Äì3.3).**
  - What‚Äôs different: the pipeline uses (i) VLM-driven annotation (`Qwen3-VL-32B`), (ii) embedding-model-based alignment scoring (`GME` similarity), and (iii) iterative mining as models improve (Section 4).
  - Why it matters: it addresses imbalanced/insufficient multimodal datasets by constructing task-diverse training signals at large scale.

## 5. Experimental Analysis

### 5.1 Evaluation methodology (benchmarks, settings, baselines)

- **Multimodal embedding evaluation: `MMEB-v2`** (Section 6.1; Table 2)
  - Covers Image / Video / Visual Document domains, totaling **78 datasets** and task categories like classification, QA, retrieval, grounding, and moment retrieval (Table 2 header).
  - Evaluation constraints: context length **16,384**, image tokens **‚â§ 1,800**, video tokens **‚â§ 15,000**, frames **‚â§ 64** (Section 6.1).

- **Visual document retrieval extra benchmarks:** `JinaVDR`, `Vidore-v3` (Section 6.2; Table 3)
  - Compared against ‚ÄúColPali-style‚Äù models; results reported from the authors‚Äô runs (Table 3).

- **Text embedding evaluation:** `MMTEB` / ‚ÄúMTEB Multilingual‚Äù (Section 6.3; Table 4)
  - Scores for non-Qwen baselines are taken from the online leaderboard on **Dec 25, 2025** (Table 4 caption).

- **Reranker evaluation:** (Section 6.4; Table 5)
  - For fairness: retrieve **top 100 candidates using `Qwen3-VL-Embedding-2B`**, then rerank (Section 6.4).
  - Compared against a baseline reranker `jina-reranker-m0` (Table 5).

### 5.2 Main quantitative results (with specific numbers)

**(A) MMEB-v2 overall embedding performance (Table 2)**

- `Qwen3-VL-Embedding-8B` achieves:
  - **All-domain average: 77.8** (Table 2, ‚ÄúAll‚Äù column).
  - Domain overalls:
    - Image overall: **80.1**
    - Video overall: **67.1**
    - VisDoc overall: **82.4**
- `Qwen3-VL-Embedding-2B` achieves:
  - **All-domain average: 73.2**
  - Image overall: **75.0**, Video overall: **61.9**, VisDoc overall: **79.2**

**Relative comparison explicitly grounded in Table 2:**
- Best open-source ‚ÄúAll‚Äù score in Table 2 appears to be `RzenEmbed (8B)` at **72.9**.
- `Qwen3-VL-Embedding-8B` at **77.8** is an absolute gain of **+4.9** points; relative gain $\approx 4.9/72.9=6.7\%$, matching the text claim of ‚Äú6.7% improvement over the previous best open-source model‚Äù (Section 6.1).

**Note on SOTA dating inconsistency:** the abstract mentions ‚Äúranking first ‚Ä¶ (as of January 8, 2025)‚Äù, while the report is dated Jan 2026 and Table 2 is presented as an evaluation on MMEB-v2; the provided content includes both timestamps, so the safest interpretation is: the model is reported as #1 on the referenced leaderboard at the stated evaluation times, but exact leaderboard state depends on date (Abstract; Section 6.1).

**(B) Visual document retrieval (Table 3)**

- Embedding models (Avg across listed VisDoc benchmarks):
  - `Qwen3-VL-Embedding-2B`: **71.6**
  - `Qwen3-VL-Embedding-8B`: **75.8**
- Rerankers:
  - `Qwen3-VL-Ranker-2B`: **76.7**
  - `Qwen3-VL-Ranker-8B`: **80.3**
- This supports the claim that the reranker ‚Äúsubstantially outperforms‚Äù similar-sized ColPali-style models in these runs (Section 6.2; Table 3 shows reranker-2B beating multiple 3B‚Äì8B embedding-only models on Avg).

**(C) Text-only performance on MMTEB (Table 4)**

- `Qwen3-VL-Embeddnig-8B` (typo in table label, but clearly the VL embedding model) mean task score: **67.9**.
- `Qwen3-VL-Embeddnig-2B`: **63.9**.
- Text-only `Qwen3-Embedding-8B`: **70.6**, so the VL embedding model is lower but still competitive among other listed baselines (Section 6.3; Table 4).

**(D) Reranking results (Table 5)**

- Base retriever (`Qwen3-VL-Embedding-2B`) vs rerankers (scores are the table‚Äôs reported aggregates):
  - MMEB-v2 (Retrieval) Avg:
    - embed-2B: **73.4**
    - reranker-2B: **75.2**
    - reranker-8B: **79.2**
  - MMTEB (Retrieval):
    - embed-2B: **68.1**
    - reranker-2B: **70.0**
    - reranker-8B: **74.9**
  - Visual doc benchmarks:
    - `JinaVDR`: embed-2B **71.0**, reranker-2B **80.9**, reranker-8B **83.6**
    - `ViDoRe(v3)`: embed-2B **52.9**, reranker-2B **60.8**, reranker-8B **66.7**

### 5.3 Do the experiments support the claims?

- **Supportive evidence:**
  - The core ‚Äúend-to-end pipeline‚Äù claim (embedder + reranker) is directly evaluated with a retrieval-then-rerank protocol (Section 6.4; Table 5).
  - The ‚Äúdistillation helps retrieval‚Äù claim is supported by **stage-wise results**: at 2B, stage `s2` boosts retrieval-heavy columns relative to `s1` in several places, while harming some classification/QA, and merging (`s3`) recovers balance (Table 6; Section 7.3).
  - The ‚ÄúMRL/QAT improve deployment trade-offs‚Äù claim is backed by explicit latency/storage reporting in Figure 6 and qualitative conclusions in Section 7.1.

- **Where evidence is incomplete (in the provided content):**
  - Many training specifics needed for full reproducibility (optimizer, LR schedule, batch sizes, compute) are not included, limiting the ability to attribute gains to particular hyperparameter choices rather than data scale/model scale.

### 5.4 Ablations / robustness checks / failure cases

- **Stage ablation (Table 6):** `s0 ‚Üí s1 ‚Üí s2 ‚Üí s3` provides an ablation over training stages, showing trade-offs and the role of merging.
- **Efficiency ablation (Figure 6):** varying embedding dimension (MRL) and quantization type (float32 vs int8 vs binary) on:
  - MS MARCO passage retrieval (text‚Üítext),
  - VL3-Syn (text‚Üíimage),
  reporting `MRR@10`, latency (ms), and index size (MB) (Section 7.1; Figure 6).
- **Granularity scaling (Figure 7):** performance vs token budget / frames, with diminishing returns and slight regression at the highest budgets (Section 7.2).
- **No explicit ‚Äúfailure cases‚Äù** (qualitative error analyses) are included in the provided content beyond the note about long-context degradation at extreme token budgets (Section 7.2).

## 6. Limitations and Trade-offs

- **Missing reproducibility-critical details (in provided content).**
  - The report does not specify optimizer settings, learning rate schedule, batch sizes, training steps, total token counts, compute/hardware, or tokenizer specifics, making exact reproduction difficult.

- **Dependence on synthetic data + automated labeling.**
  - Large portions of training rely on synthesized tasks and VLM-generated annotations (Section 3.2; Figure 5 shows ‚ÄúSynthesized Data: 300M‚Äù), which can introduce systematic biases or artifacts (e.g., overly templated questions/negatives).
  - Quality filtering uses similarity scoring from another embedding model (`GME`) (Section 3.2), which can propagate that model‚Äôs biases into the training distribution.

- **Hard-negative mining hyperparameters are underspecified.**
  - Threshold $t_+$ and margin $\delta_-$ are central to positive refinement and ‚Äúfalse negative‚Äù avoidance (Section 3.3), but numeric values and sensitivity are not provided here.

- **Cross-encoder reranking cost.**
  - The reranker is inherently more expensive because it jointly processes query+document (cross-encoder), so it is practical only after candidate pruning (Figure 2; Section 6.4 uses top-100 reranking).

- **Long-context / high-granularity trade-offs.**
  - Increasing token budgets and frames improves performance but shows diminishing returns and even slight regression at very high resource usage, possibly due to long-context processing degradation (Section 7.2; Figure 7).

- **Text-only performance gap vs text-specialized models.**
  - On MMTEB, VL embedding is ‚Äúslightly lower‚Äù than similarly sized text-only Qwen3 embedding models (Section 6.3; Table 4).

## 7. Implications and Future Directions

- **How this changes the landscape (based on reported results).**
  - The work presents a coherent recipe for turning a general-purpose VLM into a production-ready retrieval system: unify modalities in one embedding space, then transfer precision from a cross-encoder reranker back into embeddings via distillation (Figure 5; Eq. (3); Tables 2, 6).
  - The explicit attention to embedding compression (MRL, QAT) plus measured storage/latency trade-offs makes it easier to operationalize multimodal retrieval at scale (Section 7.1; Figure 6).

- **Follow-up research directions suggested in the report (Section 8).**
  - Extend support to additional modalities.
  - Develop more efficient training paradigms.
  - Enhance compositional reasoning capabilities.
  - Build more comprehensive evaluation protocols.

- **Practical applications / downstream use cases (from the report‚Äôs scope).**
  - Image‚Äìtext retrieval, video‚Äìtext matching, visual question answering framed as retrieval, and visual document retrieval (Abstract; Section 6; Appendix C examples).

- **Repro/Integration Guidance (when to use what, grounded in this pipeline).**
  - Use `Qwen3-VL-Embedding` when you need **fast large-scale retrieval** with a single index across modalities, and when storage/latency matter; tune efficiency by choosing a smaller MRL dimension and int8 quantization (Section 5.1.1; Figure 6).
  - Add `Qwen3-VL-Reranker` when you need **high precision** on the top candidates; follow the evaluated pattern: retrieve top-$N$ (the report uses **100**) with the embedding model, then rerank (Section 6.4; Table 5).
  - If your workload is **pure text** and you do not need multimodal support, the reported MMTEB numbers indicate text-only embedding models can be stronger at the same scale (Section 6.3; Table 4), so the VL model‚Äôs advantage is mainly its multimodal unification rather than text-only peak performance.

# LLaVA‚ÄëCoT: Let Vision Language Models Reason Step‚Äëby‚ÄëStep

**ArXiv:** [2411.10440](https://arxiv.org/abs/2411.10440)
**Authors:** Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan
**Institutions:** Peking University (PKU) ‚Äì Yuan Group

## üéØ Pitch

LLaVA-CoT introduces a novel vision‚Äìlanguage model with a structured four-stage reasoning process (SUMMARY, CAPTION, REASONING, CONCLUSION) and a test-time search algorithm, SWIRES, which empowers the model to backtrack and correct errors during complex multimodal reasoning tasks. This approach significantly enhances the model's performance on reasoning-heavy benchmarks, surpassing larger models and offering a scalable method for more reliable and auditable multimodal systems, crucial for applications in education and enterprise QA.

---

## 1. Executive Summary (2‚Äì3 sentences)
This paper proposes LLaVA-CoT, a vision‚Äìlanguage model that reasons in four explicit stages‚Äî`SUMMARY`, `CAPTION`, `REASONING`, and `CONCLUSION`‚Äîand a test-time search algorithm called `SWIRES` (stage-wise retracing search) that can backtrack across stages when a step looks weak. With only ~100k structured training examples, the model substantially improves multimodal reasoning, and with test-time scaling it matches or surpasses several larger or closed-source models on reasoning-centered benchmarks (e.g., Table 5).

## 2. Context and Motivation
- Problem the paper addresses
  - Many vision‚Äìlanguage models (VLMs) generate short, direct answers without planning or structure. This makes them brittle on tasks that require multistep reasoning (e.g., mathematical diagrams, multi-object counting, compositional logic).
  - Even when prompted with Chain-of-Thought (CoT), VLMs often hallucinate or ‚Äúlock in‚Äù to early mistakes because they generate token-by-token and rarely reconsider earlier steps (Section 1; Appendix A, Fig. 6).

- Why this matters
  - Reasoning-heavy tasks are increasingly common: scientific diagram understanding, chart QA, math-in-vision, and ‚Äúillusion‚Äù-type pitfalls. Improving reliability and correctness on such tasks has direct practical and research value (Section 1).

- Gaps in prior work
  - Early open-source VLMs mostly used direct prediction (no structured thought) [26, 33, 35].
  - CoT prompting helps but still suffers from error propagation and hallucinations mid-chain [29, 34, 53]; it rarely enforces stage boundaries or enables the model to revise earlier steps (Section 1).

- Positioning of this work
  - LLaVA-CoT introduces both: (1) a stage-structured reasoning format that the model learns to follow end-to-end, and (2) a test-time search procedure that evaluates each stage and can backtrack when necessary (Sections 3.1‚Äì3.2; Fig. 4). Together, these directly target the weaknesses above.

## 3. Technical Approach
The complete approach has two pillars: a structured reasoning format learned via supervised fine-tuning (SFT) and a stage-aware test-time search that can retrace errors.

- Structured four-stage reasoning (Section 3.1.1)
  - The model answers in four tagged stages:
    - `SUMMARY`: briefly outlines the plan‚Äîwhat to look for and how to solve.
    - `CAPTION`: describes only the image content relevant to the question (a focused perception step).
    - `REASONING`: executes the step-by-step logic using the caption and plan.
    - `CONCLUSION`: states the final answer succinctly.
  - Tags are explicit tokens like `<SUMMARY>...</SUMMARY>` that fence each stage so the model ‚Äúknows‚Äù what task it is doing and when a stage ends. The first three stages can be considered hidden internal reasoning, with `CONCLUSION` tailored to the user‚Äôs requested verbosity (Section 3.1.1).

- Why stages help (design rationale)
  - Forces the model to organize information (plan first), separate perception from reasoning (caption vs. logic), and defer commitment to an answer until the end. This counters token-by-token ‚Äúpremature conclusions‚Äù and encourages information aggregation before deduction (Section 1; Fig. 2 comparisons).

- Data construction and training (Section 3.1.2; Fig. 3; Table 1)
  - New dataset: `LLaVA-CoT-100k` (~99k QA pairs) created by prompting GPT-4o to generate all four stages for diverse VQA sources (ShareGPT4V, ChartQA, A-OKVQA, DocVQA, PISC, CLEVR, GeoQA+, AI2D, ScienceQA, CLEVR-Math; Table 1).
  - Quality control: structured prompt + filtering to ensure correct format and that `CONCLUSION` matches the original ground-truth answer (Appendix B, data-generation and verification prompts).
  - Base model: `Llama-3.2-11B-Vision-Instruct`. Training: full-parameter SFT on 8√óH100 GPUs; key hyperparameters in Table 6 (e.g., LR 1e-5, 3 epochs, context 4096).

- Test-time scaling with stage-aware search (Section 3.2; Fig. 4)
  - Baselines:
    - `Best-of-N`: sample N full responses, choose the best with a reward model.
    - `Stage-wise beam search`: sample M candidates at the end of each stage; keep top N (by reward) and expand each in the next stage (Fig. 4, middle).
  - Proposed `SWIRES` (Stage-WIse REtracing Search) (Section 3.2.2; Algorithm in Appendix D)
    - At each stage:
      1. Generate `M` candidates.
      2. Score candidates with a multimodal reward model (InternLM-XComposer2.5-Reward; Section 5; Appendix D).
      3. If at least one candidate surpasses a threshold, keep the top `N` and proceed; each of the `N` seeds generates `M/N` candidates next stage.
      4. If none surpass the threshold, retrace one stage back, regenerate the previous stage, and try again (up to `C` times), then continue forward.
      5. After the final stage, select the overall best `CONCLUSION` by reward score.
    - Thresholding: a simple, distribution-aware cutoff
      - `backtrack_cutoff = reward_mean + Z * reward_std`
      - Empirical values from MMStar scores: mean ‚Äì0.77, std 2.08, `Z=0.2533` (‚âàtop 40% pass), with `M=4`, `N=2`, `C=3` (Appendix D, Table 8).
    - Intuition: enforce quality at each stage; if the caption is bad, do not let reasoning run forward‚Äîgo back and fix perception first. This addresses local optima and error cascading (Fig. 4, right).

- Implementation details worth noting
  - Stage granularity is semantic rather than by tokens or sentences, solving the ‚Äúwhen to branch/beam‚Äù problem in text decoding (Section 3.2.1).
  - Empirically, the `SUMMARY` stage is often solid, so retracing typically starts at `CAPTION` (Section 3.2.2).

## 4. Key Insights and Innovations
- A concrete, four-stage reasoning protocol learned end-to-end
  - Novelty: explicit tags make ‚Äúreasoning stages‚Äù first-class citizens the model must produce; the tags act as guardrails and anchors for test-time search. Ablations show the tags themselves matter (Table 2, ‚Äúw/o Structured Tags‚Äù drops average performance from 62.4 to 60.9).
  - Significance: clear gains on reasoning-heavy tasks and improved faithfulness vs. free-form CoT (Section 4.3; Table 3).

- A stage-wise retracing algorithm for test-time scaling
  - Difference from prior search: rather than sampling full outputs or searching every fixed k tokens, `SWIRES` evaluates completed stages, can backtrack, and then proceed (Fig. 4; Section 3.2.2).
  - Impact: stronger scaling with compute than best-of-N or plain stage-wise beam (Fig. 5). At ~10k‚Äì15k seconds on one A800, `SWIRES` continues to improve while the others plateau.

- A compact but effective dataset for structured reasoning
  - Contribution: `LLaVA-CoT-100k` focuses on general and science-targeted VQA, each annotated with the four stages via GPT-4o + strict filtering (Fig. 3; Table 1).
  - Why it matters: with only ~100k examples and plain SFT (no RL, no massive pretraining), the model shows sizeable gains (Table 2) and competitive results vs. much larger systems (Table 5).

- Evidence that ordering and structure‚Äînot just more supervision‚Äîdrive the gains
  - Controlled analyses (Appendix F; Table 7):
    - Training on the same sources but without the CoT structure (multi-task caption/summarize without stages) underperforms (average 57.7 vs. 63.1).
    - Shuffling stage order (‚Äúreorder‚Äù) also hurts (58.2 average).
    - Structured CoT prompting helps a strong teacher (GPT-4o) and does not automatically help a weaker base model without SFT, suggesting the benefit comes from learning the structure, not simply copying teacher content.

## 5. Experimental Analysis
- Evaluation setup (Section 4.1)
  - Benchmarks:
    - General VQA: MMStar, MMBench V1.1, MMVet
    - Reasoning-focused: MathVista (testmini), AI2D, HallusionBench
  - Metrics and tools: standardized through VLMEvalKit (footnote [1]).
  - For a reasoning-only comparison, the paper creates reduced versions‚ÄîMMStar-R, MMBench-R, MMVet-R‚Äîby removing perception/OCR-centric items (Appendix E; Table 5).

- Main quantitative results
  - Post-training gains over base (no test-time scaling) (Table 2):
    > Base `Llama-3.2-11B-Vision-Instruct`: average 56.6  
    > `LLaVA-CoT`: average 62.4 (+5.8 points).  
    - Per-benchmark highlights:
      - MMBench: 65.8 ‚Üí 75.0 (+9.2)
      - MathVista: 48.6 ‚Üí 54.8 (+6.2)
      - HallusionBench: 40.3 ‚Üí 47.8 (+7.5)
      - MMVet: 57.6 ‚Üí 60.3 (+2.7)
  - Where the gains come from (skill breakdown on MMStar; Table 3):
    > Instance reasoning 57.6 ‚Üí 63.2, logical reasoning 50.8 ‚Üí 58.0, math 45.2 ‚Üí 64.0, science & tech 32.8 ‚Üí 44.8.  
    - Improvements are largest in reasoning-intensive skills; perception-only categories improve marginally (coarse 66.0 ‚Üí 68.8; fine-grained 46.4 ‚Üí 46.8).
  - Test-time scaling adds more (Table 4):
    > `LLaVA-CoT` ‚Üí `LLaVA-CoT (w/ scaling)`: average 62.4 ‚Üí 65.5 (+3.1).  
    - On MMStar specifically: 57.6 ‚Üí 62.5 (+4.9).
    - Scaling trends in Fig. 5 show `SWIRES` outperforming both best-of-N and stage-wise beam as compute increases; the others plateau near 10k seconds, while `SWIRES` continues to climb.
  - State-of-the-art comparison on reasoning-centric subsets (Table 5):
    > `LLaVA-CoT (w/ scaling, 11B)`: average 66.3.  
    - Beats several larger open-source models (e.g., `Llama-3.2-90B-Vision-Instruct` average 62.3; `Deepseek-VL2 (MoE, 27B)` average 66.0).
    - Competitive with strong 7‚Äì11B open-source models: e.g., `Qwen2-VL-7B` at 65.9.
    - Surpasses some closed-source mid-tier APIs on this selection (e.g., `Gemini-1.5-Pro` 63.6, `GPT-4o-mini-0718` 63.8), while remaining below the very top APIs (`GLM-4v-Plus` 72.5, `GPT-4o-0806` 71.8).

- Ablations and diagnostics
  - Dataset/formats matter (Table 2):
    - Training on original QA pairs without structure (‚ÄúDirect Training‚Äù) yields 59.0 average‚Äîsome improvement but clearly below the full structured version (62.4).
    - Removing tags drops to 60.9‚Äîtags provide measurable benefit.
  - CoT design effectiveness (Appendix F; Table 7):
    - ‚ÄúMulti-task‚Äù dense supervision without CoT structure (captioner/summarizer, etc.) underperforms (57.7 average).
    - Shuffled stage order (‚Äúreorder‚Äù) also underperforms (58.2).
  - Qualitative comparisons (Fig. 2, Fig. 7, Fig. 8) show fewer reasoning lapses and more consistent task decomposition.

- Do the experiments support the claims?
  - Yes, for the targeted setting: On six diverse benchmarks, especially the four reasoning-heavy skills in MMStar (Table 3) and the reasoning-focused subsets (Table 5), structured stages plus test-time `SWIRES` provide consistent gains. The scaling curve (Fig. 5) demonstrates a clear compute‚Äìaccuracy trade-off where `SWIRES` dominates the other scaling strategies.

- Mixed results and trade-offs
  - On AI2D, the non-scaling `LLaVA-CoT` is slightly below the ‚ÄúDirect Training‚Äù variant (78.7 vs. 81.2; Table 2), though with `SWIRES` it reaches 81.0 (Table 4). This suggests structure sometimes trades off with raw memorization-style gains, and scaling helps recover them.
  - MMVet improvements without scaling are modest (+2.7), which may reflect the benchmark‚Äôs emphasis on free-form responses and multiple capabilities beyond reasoning.

## 6. Limitations and Trade-offs
- Dependence on synthetic supervision
  - The four-stage annotations are generated by GPT-4o (Appendix B). While filtered for correctness, this can import teacher biases or style artifacts.

- Reward-model dependence and threshold tuning
  - `SWIRES` relies on a multimodal reward model to score stages (InternLM-XComposer2.5-Reward). Thresholds are dataset-derived (Appendix D), which may need re-estimation for new domains and could mis-rank good-but-rare reasoning paths.

- Compute overhead at inference
  - Stage-wise search plus retracing is slower. Fig. 5 reports up to 10k‚Äì15k seconds per full MMStar evaluation setting on a single A800 node. Gains plateau with diminishing returns, so deploying `SWIRES` requires careful latency/accuracy trade-offs.

- Scope of robustness
  - The method assumes that ‚Äúgood‚Äù partial stages are detectable by the reward model; adversarial cases where the reward model is misled can still pass. Appendix J notes cases where the model ‚Äúgets lost during retracing or starts hallucinating,‚Äù especially on complex images.

- Fixed stage design
  - The four-stage structure is hand-specified. Some tasks might benefit from a different number or type of stages (e.g., explicit tool-use or spatial grounding steps), which the current framework does not learn automatically.

## 7. Implications and Future Directions
- How this work shifts the landscape
  - It operationalizes stage-structured reasoning for VLMs and shows that ‚Äúreasoning at the level of stages‚Äù is a better unit for search and self-correction than tokens or whole outputs. This suggests a path to more reliable, auditable multimodal reasoning systems.

- Follow-up research enabled or suggested
  - Learnable stage structures: automatically discover task-appropriate stages or adaptively skip/insert stages.
  - Stage-level reinforcement learning: replace heuristic thresholds with RLHF/RLAIF that directly optimizes stage quality and final task reward.
  - Stronger reward models: train multimodal verifiers specialized by stage type (e.g., a perception verifier for `CAPTION`, a logic verifier for `REASONING`).
  - Tool- and retrieval-augmented stages: allow `CAPTION` to invoke detectors/OCR, or `REASONING` to call solvers or calculators.
  - Faithfulness checks: incorporate formal consistency checks (e.g., unit consistency, object counts) to reduce hallucinations.

- Practical applications
  - Education and tutoring on diagrams and scientific content (AI2D, MathVista-style tasks).
  - Enterprise document and chart QA where separating perception from reasoning improves reliability.
  - Safety-critical review workflows: the stage traces are interpretable logs that can be audited; `SWIRES` can be run in high-stakes settings where extra compute is acceptable.

> Representative takeaways supported by the paper‚Äôs results and figures:
> - ‚ÄúStructured tags + four-stage CoT improves average accuracy from 56.6 to 62.4 over the base model across six benchmarks‚Äù (Table 2).
> - ‚ÄúStage-wise retracing search yields the strongest scaling curve and raises the average further to 65.5‚Äù (Fig. 5; Table 4).
> - ‚ÄúGains concentrate on reasoning-heavy skills (instance/logical/math/science) rather than perception alone‚Äù (Table 3).
> - ‚ÄúOn reasoning-focused subsets, the 11B model with scaling (66.3) rivals or surpasses some much larger or closed models‚Äù (Table 5).

Overall, LLaVA-CoT demonstrates that giving VLMs a disciplined internal workflow and a stage-aware way to revise their steps is an effective recipe for stronger multimodal reasoning.

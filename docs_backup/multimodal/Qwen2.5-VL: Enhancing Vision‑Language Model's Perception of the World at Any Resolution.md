# Qwen2.5-VL: Enhancing Visionâ€‘Language Model's Perception of the World at Any Resolution

**ArXiv:** [2502.13923](https://arxiv.org/abs/2502.13923)
**Authors:** Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin
**Institutions:** Qwen Team (Alibaba)

## ðŸŽ¯ Pitch

Qwen2.5â€‘VL revolutionizes vision-language models by integrating windowed attention in a Vision Transformer, a compact vision-language merger, and a novel absolute-time positional encoding. This innovation not only enhances processing efficiency for native-resolution and long-video content but also significantly advances applications in object localization, document parsing, and video understanding, paving the way for improved UI automation, robotics, and surveillance systems.

---

## 1. Executive Summary
Qwen2.5â€‘VL is a family of visionâ€‘language models that re-engineers the visual stack and temporal encoding to handle nativeâ€‘resolution images and long videos efficiently, while adding precise object localization and strong document parsing. It introduces windowed attention in a newly trained Vision Transformer, a compact visionâ€‘language merger, and a positional encoding that aligns tokens to absolute time, yielding stateâ€‘ofâ€‘theâ€‘art or competitive results across grounding, OCR/doc parsing, and longâ€‘video understanding (see Figure 1 and Tables 3, 5, 6, 7, 8, 9).

## 2. Context and Motivation
- Problem addressed
  - LVLMs struggle with fineâ€‘grained perception (precise localization, counting), computational blowâ€‘ups at native resolution, and inconsistent performance when sequence length or frame rate varies; temporal reasoning is often tied to frame counts rather than real time (Introduction; Sections 2, 2.1.1â€“2.1.3).
- Why it matters
  - Real applications demand accurate object grounding (e.g., UI automation, robotics), robust document parsing (invoices, forms, charts), and reliable longâ€‘video understanding (surveillance, meeting analysis) with secondâ€‘level timestamping (Abstract; â€œsparkling characteristicsâ€ bullets).
- Prior approaches and gaps
  - Standard LVLM design = visual encoder + projector + LLM (Introduction). Many models normalize coordinates and/or downsample inputs, losing scale fidelity; temporal position is usually tied to frame index, which fails to capture absolute timing across variable FPS (Sections 2.1.2â€“2.1.3).
  - Fineâ€‘grained perception has often relied on specialized detectors (e.g., Grounding DINO, SAM) or auxiliary heads; many LVLMs lack native point grounding and fineâ€‘grained spatial outputs in absolute image coordinates (Section 2.2.1 â€œGrounding Data with Absolute Position Coordinatesâ€).
- Positioning of this work
  - Qwen2.5â€‘VL proposes a nativeâ€‘resolution ViT with window attention for linearâ€‘scaling compute, a multimodal rotary positional embedding aligned to absolute time for stable temporal reasoning across variable FPS, and a light, trainable visionâ€‘language merger to compress tokens before the LLM (Sections 2.1â€“2.1.3).
  - It scales training to ~4.1T tokens with curated multimodal data, including an HTMLâ€‘based omniâ€‘document format that unifies layout, OCR, charts, formulas, and images (Sections 2.2â€“2.2.1; â€œDocument Omniâ€‘Parsing Dataâ€).

## 3. Technical Approach
Stepâ€‘byâ€‘step architecture and training pipeline (Figure 1; Table 1; Sections 2.1â€“2.3.4):

- Highâ€‘level architecture
  - Components: a redesigned `Vision Encoder` (ViT), an `MLPâ€‘based Visionâ€‘Language Merger`, and a `Qwen2.5` LLM with a multimodal rotary positional embedding aligned to absolute time (`MRoPEâ€‘Time`) (Section 2.1).
- Vision Encoder (fast, native resolution; Section 2.1.1)
  - Input handling
    - Images are resized only so height/width become multiples of 28, preserving aspect ratio (avoids heavy normalization).
    - Split into 14Ã—14 patches (stride 14) to produce patch tokens.
    - For videos, â€œ3D patchingâ€: two consecutive frames are grouped to reduce token count while retaining temporal continuity.
  - Positional encoding
    - Uses 2Dâ€‘RoPE for spatial relations; for videos, becomes 3D with temporal IDs (Section 2.1.1).
  - Windowed attention for efficiency
    - Most layers use local windows (max 112Ã—112 pixels = 8Ã—8 patches) so attention cost scales roughly linearly in number of patches; four layers keep full selfâ€‘attention to pass global information (fullâ€‘attention layers at indices {7, 15, 23, 31}, Table 1).
    - This design maintains native resolution without padding small regions (Section 2.1.1).
  - Alignment with LLM stack
    - Replaces typical ViT norms/activations with `RMSNorm` and `SwiGLU` to match LLM design choices and efficiency (Section 2.1.1).
  - Training
    - ViT is trained from scratch in stages (CLIPâ€‘style pretraining, alignment, endâ€‘toâ€‘end finetuning) with dynamic sampling over native resolutions (Section 2.1.1).
- MLPâ€‘based Visionâ€‘Language Merger (Section 2.1)
  - Problem: Visual feature sequences can be long and costly for the LLM.
  - Solution: Group each 2Ã—2 spatial neighborhoodâ€™s patch features; concatenate and pass through a 2â€‘layer MLP to project into the LLMâ€™s embedding size (compresses tokens while preserving local detail).
  - Why an MLP: A simple trainable projection avoids heavy crossâ€‘modal attention before the LLM and allows dynamic compression that scales with image resolution.
- LLM and multimodal positional encoding (Sections 2.1, 2.1.3)
  - Base LLM: The `Qwen2.5` family (e.g., 7B and 72B; Table 1), initialized from preâ€‘trained Qwen2.5.
  - `MRoPEâ€‘Time` (Multimodal Rotary Position Embedding aligned to absolute time)
    - Background: MRoPE decomposes positions into temporal, height, and width components; text uses identical IDs (acts like 1D RoPE). For images: temporal ID is constant; height/width reflect spatial location. For videos: temporal ID increases with frames (Section 2.1.3).
    - Innovation: Instead of indexing by frame count, temporal IDs align directly to absolute timestamps. The time interval between IDs reflects real time, allowing the model to learn tempo and localize events at second resolution across variable FPS (Sections 2.1.2â€“2.1.3; Figure 1).
- Dynamic resolution and FPS (Section 2.1.2)
  - Spatial: Token sequence length directly follows native image size; spatial outputs (e.g., bounding boxes, points) are expressed in absolute pixel coordinates of the input image (no normalization), preserving scale awareness.
  - Temporal: Training uses dynamic FPS sampling, and temporal IDs are tied to absolute time, so videos with different frame rates map consistently in time.
- Training data and recipe (Sections 2.2â€“2.2.2; Table 2)
  - Scale and coverage: Scales pretraining tokens from ~1.2T to ~4.1T, mixing image captions, interleaved multimodal streams, OCR, grounding (boxes and points), document parsing, video captioning/grounding, agent interaction data, and pure text.
  - Data quality control
    - Interleaved data are cleaned with a 4â€‘stage scoring pipeline focusing on text quality, imageâ€“text relevance and complementarity, and information balance.
    - Grounding uses absolute pixel coordinates; datasets include >10k categories and synthetic â€œnonâ€‘existentâ€ categories to stress openâ€‘vocabulary detection (Section 2.2.1).
    - Omniâ€‘document data: a unified `QwenVL HTML` format encodes layout boxes and modality content (tables, charts, formulas, music sheets, chemical formulas) with bounding boxes inside HTML tags (Section 2.2.1).
  - Three training phases (Table 2)
    - Phase 1 (1.5T tokens, seq len 8192): Train ViT alone on visionâ€‘centric data (captioning, knowledge, OCR) to align with the LLM interface.
    - Phase 2 (2T tokens, 8192): Unfreeze all parameters; train on interleaved data, VQA, video, math, agent tasks, and pure text.
    - Phase 3 (0.6T tokens, 32768): Longâ€‘context training with long videos, long agents, and long documents.
  - Efficiency techniques
    - Window attention reduces ViT costs; dynamic â€œpackingâ€ balances LLM sequence lengths per GPU to equalize load (Section 2.2.2).
- Postâ€‘training alignment (Sections 2.3â€“2.3.4)
  - SFT (Supervised Fineâ€‘Tuning): ~2M entries (50% text, 50% multimodal), using ChatML formatting for structured multimodal dialogue and careful domain coverage (e.g., OCR/Doc, Grounding, Video, Agent) with both single/multiâ€‘turn and single/multiâ€‘image contexts (Section 2.3.1).
  - Data filtering: Twoâ€‘stage pipelineâ€”domain classification into 8 domains/30 subdomains, then domainâ€‘tailored rule/model-based filtering to remove noise, truncation, or harmful/irrelevant entries; reward models score correctness, completeness, clarity, and visual grounding quality (Section 2.3.2).
  - Rejection sampling for reasoning: Build datasets with verified chainâ€‘ofâ€‘thought outputs that match ground truth; further filter out codeâ€‘switching, over-long or repetitive outputs, and ensure visual evidence is used properly in intermediate steps (Section 2.3.3).
  - DPO (Direct Preference Optimization): Preference-based alignment with imageâ€‘text and pure text examples; ViT is frozen during both SFT and DPO (Section 2.3.4).

## 4. Key Insights and Innovations
- Windowed attention in a nativeâ€‘resolution ViT (Section 2.1.1; Table 1)
  - Whatâ€™s new: Most ViT layers attend within local windows (112Ã—112 px), with only four global layers.
  - Why it matters: Reduces quadratic attention cost to nearâ€‘linear in tokens while preserving critical global routing through selected layers, enabling nativeâ€‘resolution inference without aggressive downsampling.
  - Difference from prior work: Many LVLMs downsample or normalize inputs, losing scale cues; this design keeps native scale and controls compute.
- Absoluteâ€‘time `MRoPEâ€‘Time` for videos (Sections 2.1.2â€“2.1.3; Figure 1)
  - Whatâ€™s new: Temporal position IDs encode real time rather than frame count, so the same 3â€‘second event aligns regardless of FPS.
  - Why it matters: Enables robust timestamp grounding, tempo awareness, and secondâ€‘level localization in long videos with variable sampling rates, without extra heads or textual timestamps.
- Simple, effective `MLPâ€‘based Visionâ€‘Language Merger` (Section 2.1)
  - Whatâ€™s new: A 2â€‘layer MLP compresses 2Ã—2 patch neighborhoods into LLMâ€‘sized embeddings before the LLM.
  - Why it matters: Cuts sequence length and LLM compute, while preserving local structure; avoids complex crossâ€‘modal attention or heavy pooling schemes.
- Unified omniâ€‘document representation and largeâ€‘scale grounding with absolute coordinates (Section 2.2.1 â€œDocument Omniâ€‘Parsing Dataâ€ and â€œGrounding Dataâ€¦â€)
  - Whatâ€™s new: A standardized HTML format stores layout, tables, charts, formulas, images, with `data-bbox` attributes. Grounding data uses absolute pixel coordinates across multiple formats (JSON/XML/custom), and includes >10k categories plus synthetic â€œnonâ€‘existentâ€ categories.
  - Why it matters: Trains a single model to parse diverse document types endâ€‘toâ€‘end and to ground objects precisely in absolute coordinatesâ€”key for UI agents and realâ€‘world measurements.
- Dynamic FPS training and 3D patch grouping (Sections 2.1.1â€“2.1.2)
  - Whatâ€™s new: During training, videos are sampled at varying FPS; two sequential frames are grouped at the patch level.
  - Why it matters: Improves robustness to frameâ€‘rate variation and reduces token count without losing shortâ€‘range temporal signalsâ€”important for long video processing.

## 5. Experimental Analysis
- Evaluation setup and breadth
  - Benchmarks span general VQA, math, OCR/docs/charts, spatial grounding (boxes, points, counting), video understanding/grounding (short to hours), agents (mobile/desktop/web GUIs), and pure text tasks (Tables 3â€“9; Section 3).
  - Model sizes: `Qwen2.5â€‘VLâ€‘3B`, `7B`, `72B` (Table 1). Results reported against strong baselines including GPTâ€‘4o, Claude 3.5 Sonnet, Gemini 1.5/2.0, InternVL2.5.
- Headline results (selected)
  - General VQA (Table 3)
    - > â€œ`MMBenchâ€‘EN` test: 88.6 (72B), slightly exceeding prior best 88.3; `MMStar`: 70.8 (72B)â€â€”competitive at highâ€‘level visual QA and multiâ€‘image understanding (MuirBench: 70.7).
  - Mathâ€‘inâ€‘vision (Table 3)
    - > â€œ`MathVista`: 74.8 (72B), surpassing the previous openâ€‘source SoTA 72.3; `MATHâ€‘Vision`: 38.1; `MathVerse`: 57.6.â€
  - OCR / Document / Charts (Table 5)
    - > â€œ`TextVQA` val: 84.9 (72B); `DocVQA` test: 96.4 (72B); `OCRBench`: 885 (InternVL2.5) vs 864 (72B).â€
    - > â€œ`OCRBench_v2` (comprehensive): 61.5/63.7 en/zh (InternVL2.5) vs 56.3/57.2 (72B).â€ Results are strong overall; some mixed relative to best proprietary baselines depending on track.
    - > â€œ`CCâ€‘OCR`: 79.8 (72B), above GPTâ€‘4o 66.9 and Claude 62.5.â€
    - OmniDocBench edit distance (lower is better) shows competitive but not top numbers (e.g., `0.226/0.324` InternVL2.5 vs `0.275/0.324` GPTâ€‘4o vs `0.308/0.398` 7B; Table 5).
  - Spatial grounding and counting (Tables 6â€“7)
    - > â€œRefCOCO/RefCOCO+/RefCOCOg: 72B reaches 92â€“95% on multiple splits; ODinWâ€‘13 openâ€‘vocab detection: 43.1 mAP (72B), surpassing most LVLMs, narrowing gap to specialist detectors.â€
    - > â€œPoint grounding: 67.5 (72B), near Molmoâ€‘72Bâ€™s 69.2.â€
    - > â€œ`CountBench`: 93.6 (72B), higher than GPTâ€‘4o 87.9 and Claude 89.7.â€
  - Video understanding and grounding (Table 8)
    - > â€œ`MVBench`: 70.4 (72B), above GPTâ€‘4o 64.6; `LVBench` (long video): 47.3 (72B) vs GPTâ€‘4o 30.8; `MLVU` Mâ€‘Avg: 74.6 (72B) vs GPTâ€‘4o 64.6.â€
    - > â€œ`Charadesâ€‘STA` temporal localization: mIoU 50.9 (72B) vs GPTâ€‘4o 35.7.â€ The setup caps frames at 768 and video tokens at 24,576 (Section 3.3.4).
    - Results indicate strong timestamp grounding and longâ€‘video QA consistent with the absoluteâ€‘time MRoPE design.
  - Agents and GUI grounding (Table 9)
    - > â€œ`ScreenSpot` (GUI element grounding): 87.1 (72B), competitive with Gemini 2.0â€™s 84.0; `ScreenSpot Pro`: 43.6 (72B), far above Aguvisâ€‘72B 23.6 and `Qwen2â€‘VLâ€‘72B` 1.6.â€
    - > â€œAndroid Control HighEM/LowEM: 67.36/93.7 (72B), leading among reported baselines.â€
    - > â€œOnline: AndroidWorld SR 35% (72B) vs GPTâ€‘4o 34.5% (SoM); MobileMiniWob++ 68% (72B) vs GPTâ€‘4o 61%.â€ The model performs without Setâ€‘ofâ€‘Mark (SoM) visual hints, while some baselines require SoM.
  - Pure text tasks (Table 4)
    - > â€œ`LiveBenchâ€‘0831`: 57.0 (72Bâ€‘VL) vs Qwen2.5â€‘72B 52.3; `MMLUâ€‘Pro`: 71.2; `HumanEval`: 87.8; `MultiPLâ€‘E`: 79.5; `MATH`: 83.0.â€ The VL models retain strong language and coding capability.
- Do the experiments support the claims?
  - Fineâ€‘grained perception: Yesâ€”box/point grounding and counting show strong gains (Tables 6â€“7).
  - Document parsing: Strong across multiple OCR/doc tasks; not uniformly best on every benchmark, but the breadth (TextVQA, DocVQA, CCâ€‘OCR, OCRBench variants, OmniDocBench) indicates robust generality (Table 5).
  - Longâ€‘video and temporal grounding: Clear improvements where absolute time matters (e.g., Charadesâ€‘STA mIoU 50.9; LVBench and MLVU; Table 8).
  - Agentic functionality: Marked advances in GUI grounding and downstream device control (Table 9).
- Missing ablations or diagnostics
  - No explicit ablation quantifying the contribution of windowed attention vs. full attention, of the MLP merger vs. alternatives, or of absoluteâ€‘time MRoPE vs. frameâ€‘indexed MRoPE.
  - Robustness to extreme resolutions and to window partition choices is not separately reported.
  - Training compute and wallâ€‘clock efficiency gains are described conceptually, not benchmarked against strong open baselines.

## 6. Limitations and Trade-offs
- Compute and data scale
  - Training uses ~4.1T tokens and a redesigned ViT trained from scratch, which implies significant compute and data demands (Table 2). This limits reproducibility for smaller labs.
- Windowed attention tradeâ€‘offs
  - Local windows reduce cost but can restrict longâ€‘range spatial interactions; four global layers mitigate but may not fully capture global patterns in edge cases (Section 2.1.1; Table 1).
- Token budget for videos
  - Despite absoluteâ€‘time encoding and dynamic FPS, experiments cap to â‰¤768 frames and â‰¤24,576 video tokens (Table 8), which may constrain truly â€œhoursâ€‘longâ€ detailed analysis without careful frame sampling.
- Absolute coordinate dependence
  - Using absolute pixels improves scale fidelity but can be brittle when downstream systems rescale or crop inputs unpredictably; precision relies on consistent handling of native resolution (Section 2.1.2).
- Limited transparency on error modes
  - The report emphasizes wins; systematic failure analyses (e.g., complex diagrams that require multiâ€‘step textual reasoning plus longâ€‘range spatial context) are not detailed.
- Postâ€‘training choices
  - ViT is frozen during SFT/DPO (Section 2.3.4). While efficient, this can limit lastâ€‘mile adaptation in domains where vision features need slight taskâ€‘specific adjustment.

## 7. Implications and Future Directions
- Landscape impact
  - Demonstrates that careful architectural surgery on the visual stackâ€”windowed attention with selective global layers, absoluteâ€‘time positional encoding, and a minimal mergerâ€”can unlock nativeâ€‘resolution perception and longâ€‘video grounding in a generalist LVLM (Figure 1; Sections 2.1â€“2.1.3).
  - Establishes a practical path to unify doc parsing, grounding, video understanding, and agent control within one model family (Tables 5â€“9).
- Followâ€‘up research enabled
  - Ablations on:
    - Window size vs. number of global layers vs. accuracy/latency tradeâ€‘offs.
    - Alternative mergers (e.g., crossâ€‘modal adapters, learned pooling, token pruning) vs. the 2â€‘layer MLP.
    - Absoluteâ€‘time MRoPE vs. hybrid timestamp tokens or learned time bases across diverse FPS distributions.
  - Scaling longâ€‘video without a hard frame cap, possibly via hierarchical memory, retrievalâ€‘augmented video tokens, or compressive streaming.
  - Adaptive coordinate systems that preserve absolute fidelity yet remain robust to unknown rescaling/cropping in deployment.
  - Extending the `QwenVL HTML` document format to native PDF/Office converters and roundâ€‘trip editing; richer chart/diagram semantics (e.g., program extraction).
- Practical applications
  - Enterprise document workflows (endâ€‘toâ€‘end conversion, verification, and extraction with layout grounding).
  - Autonomous UI assistants on desktop/mobile/web (reliable element grounding, stepâ€‘wise reasoning, and action planningâ€”Table 9).
  - Video analytics for surveillance, sports, meetings, and industrial inspection with secondâ€‘level event localization and multiâ€‘format timestamp outputs (Section 2.2.1 â€œVideo Dataâ€; Table 8).
  - Openâ€‘vocabulary visual search and counting in retail, logistics, and quality control (Tables 6â€“7).

> Core takeaway: The combination of nativeâ€‘resolution visual processing, absoluteâ€‘time positional encoding, and a compact visionâ€‘language mergerâ€”backed by largeâ€‘scale, carefully curated multimodal dataâ€”yields a single LVLM that is competitive across fineâ€‘grained grounding, document understanding, longâ€‘video reasoning, and agentic interaction (Figure 1; Tables 3â€“9).

# Higher-order Linear Attention

**ArXiv:** [2510.27258](https://arxiv.org/abs/2510.27258)

## üéØ Pitch

Introduces Higher-order Linear Attention (HLA), a causal, streaming attention-like mechanism that realizes exact higher-order (second- and third-order) interactions via compact prefix sufficient statistics, enabling per-token O(1) state updates and linear-time inference without forming n√ón affinity matrices. This makes attention-style, data-dependent mixing scalable and strictly autoregressive for long-context models, and supports exact chunk-parallel training via associative scans‚Äîbringing higher expressivity to efficient long-sequence autoregressive architectures.

---

## 1. Executive Summary (2-3 sentences)

The paper introduces `Higher-order Linear Attention (HLA)`, a strictly causal, streaming attention-like mechanism that captures higher-order (e.g., second- and third-order) interactions while avoiding the `O(n^2)` time/memory cost of standard scaled dot-product attention on sequence length `n` (Abstract; Section 1). Its core significance is an *exact* (not approximate) set of streaming identities for masked (autoregressive) higher-order operators‚Äîplus an associative-scan training scheme that reproduces serial recurrent activations exactly‚Äîmaking higher-order mixing feasible for long-context autoregressive models (Theorem 3.1; Section 4; Theorem 4.1).

## 2. Context and Motivation

- **Problem / gap addressed.**
  - Standard Transformer attention uses a dense `n√ón` affinity matrix (scaled dot-product attention), leading to quadratic compute and memory in the context length `n` (Section 1; Section 2.1).
  - Linear-time alternatives exist (linear attention via kernel/feature maps; recurrent/SSM-style models), but the paper targets a gap: many linear attentions are effectively *first-order* (maintaining only first-order sufficient statistics like sums of `œï(k)v·µÄ`) and may lose expressivity relative to attention (Abstract; Section 2.2; Section 1).

- **Why this is important.**
  - Long-context autoregressive modeling is constrained by quadratic attention; getting *streaming* inference with `O(1)` state update per token is a key scaling requirement for long sequences (Abstract; Section 1).
  - The paper emphasizes *strict causality* (no future-token leakage) and *exact* parallel training that matches serial recurrence, which are practical correctness requirements for autoregressive LMs (Section 3.1; Section 4; Theorem 4.1).

- **Prior approaches and shortcomings (as positioned in the paper).**
  - `Linear attention` replaces softmax attention with feature-map kernels to enable streaming via running sums (Section 2.2), but typically stays first-order and/or approximate with respect to softmax attention.
  - `SSMs` and `modern RNNs` provide constant per-token state updates, but their mixing is not ‚Äúattention-like‚Äù in the same data-dependent query/key sense (Section 1; Section 8).
  - The paper positions HLA as a complementary approach: preserve attention-style data-dependent mixing, but realize richer interactions via higher-order prefix moments (sufficient statistics) while remaining streaming and mask-correct (Abstract; Section 3; Section 8).

- **How this paper positions itself.**
  - HLA is presented as a ‚Äúdrop-in, attention-like mixer‚Äù that:
    1. Streams with constant-size state per head (second order: `S_K, C_QV, m_Q`, plus masked corrections `G, h`) (Section 3; Section 3.1).
    2. Enforces strict autoregressive masking exactly via additional summaries (Theorem 3.1).
    3. Supports chunk-parallel training via associative scans that reproduce serial activations exactly (Section 4; Theorem 4.1).
  - It deliberately emphasizes **algorithmic structure and implementation** rather than end-to-end model quality experiments (Section 1; Section 9).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **replacement for the attention ‚Äúmixer‚Äù** in a Transformer block that computes each token‚Äôs output using a small set of **streaming prefix summaries** (Section 5.2).
- It solves the ‚Äúlong context‚Äù cost problem by turning attention-like pairwise (and higher-order) interactions into **linear-time per token** updates that never build an `n√ón` attention matrix, while still supporting strict causal masking and parallel training (Abstract; Sections 3‚Äì4).

### 3.2 Big-picture architecture (diagram in words)

- **Inputs per token**: query `q_t`, key `k_t`, value `v_t` (Section 2; Section 3).
- **Streaming state (second order)**:
  - Core prefix moments: `S_K,t`, `C_QV,t`, `m_Q,t` (Section 3).
  - Masking correction moments: `G_t`, `h_t` (Section 3.1).
- **Output computation**:
  - Compute a bilinear form from the state and current query to produce `o_t` (Eq. (3.1), (3.3), (3.4)).
- **Training parallelization**:
  - Represent per-token updates as ‚Äúsegments‚Äù and combine them with an **associative operator** (`‚äï` / semidirect-product form) to enable chunk-wise and inter-chunk scans (Section 4; Eq. (4.1); Theorem 4.1).

### 3.3 Roadmap for the deep dive

- First, define baseline attention and linear attention to clarify what ‚Äúlinear-time streaming‚Äù means (Section 2).
- Second, derive **second-order HLA** in the unmasked (simpler) form and show its normalized option (Section 3; Eq. (3.1), (3.2)).
- Third, explain the key technical challenge‚Äî**strict causal masking**‚Äîand how extended summaries (`G_t`, `h_t`) fix it exactly (Section 3.1; Theorem 3.1).
- Fourth, explain how training becomes parallel via **associative scans**, including the masked semidirect operator and decay (Section 4; Eq. (4.1); Theorem 4.1).
- Fifth, cover the **asymmetric variant (AHLA)** and the **third-order extension**, highlighting what changes in state and complexity (Section 6; Theorem 6.1; Section 7; Theorem 7.1).

### 3.4 Detailed, sentence-based technical breakdown

This is an **algorithmic contribution** paper: it gives exact algebraic reformulations of higher-order attention-like operators into streaming recurrences with constant-size state, plus associative-scan training operators that exactly match serial computation (Sections 3‚Äì4; Theorems 3.1 and 4.1).

#### 3.4.1 Background: what ‚Äúquadratic attention‚Äù computes

- Scaled dot-product attention takes `Q ‚àà R^{n√ód}`, `K ‚àà R^{n√ód}`, `V ‚àà R^{n√ód_v}` and forms `QK·µÄ` (an `n√ón` matrix), applies a causal mask, softmaxes, and multiplies by `V` (Section 2.1).
- The quadratic bottleneck comes from constructing/manipulating `n√ón` structures in time and memory as `n` grows (Section 1).

#### 3.4.2 Linear attention recap: streaming via first-order sufficient statistics

- Linear attention replaces the softmax kernel with a feature map `œï`, letting attention be approximated using running sums like `‚àë œï(k_j)v_j·µÄ` and `‚àë œï(k_j)` (Section 2.2).
- This is streaming because the state is just those prefix sums, which are `O(1)` in `n` memory, but it is typically **first-order** in the sense of what statistics it keeps (Section 2.2; Section 8).

#### 3.4.3 Second-order HLA: unmasked factorization via prefix moments

**Core idea.** Instead of a first-order kernel, second-order HLA uses **second-moment structure** from keys to define a data-dependent metric over query space (Section 3).

- The paper introduces a second-order ‚Äútensor attention‚Äù weight matrix
  - `T2 := (QK·µÄ)(QK·µÄ)·µÄ = Q (K·µÄK) Q·µÄ` (Section 3).
  - This reveals dependence on `K·µÄK`, a **second moment** of keys, suggesting streaming computation via prefix moments.

- It maintains these prefix summaries up to time `t` (Section 3):
  - `S_{K,t} := ‚àë_{i‚â§t} k_i k_i·µÄ ‚àà R^{d√ód}` (a running key second moment),
  - `C_{QV,t} := ‚àë_{i‚â§t} q_i v_i·µÄ ‚àà R^{d√ód_v}` (a query-weighted value accumulator),
  - `m_{Q,t} := ‚àë_{i‚â§t} q_i ‚àà R^{d}` (a running ‚Äúquery mass‚Äù vector).

- **Default (unnormalized) second-order HLA output** at time `t` is the bilinear form (Eq. (3.1)):
  - `o_t := q_t·µÄ S_{K,t} C_{QV,t}`.
  - Mechanistically: `S_{K,t}` acts like a learned, data-dependent metric on query space; `C_{QV,t}` stores value information indexed by queries; multiplying by `q_t·µÄ` selects a mixture (Section 3).

- **Optional normalized variant** divides by a scalar denominator built from the same state (Eq. (3.2)):
  - `num_t = q_t·µÄ S_{K,t} C_{QV,t}`,
  - `den_t = q_t·µÄ S_{K,t} m_{Q,t}`,
  - `o_t = num_t / (den_t + Œµ)`.
  - The paper frames this as scale control/comparability with linear attention, while the unnormalized operator avoids length-dependent renormalization (Section 3).

- **Connection to (first-order) linear attention.**
  - If `S_{K,t} = I`, then `num_t = ‚àë_{i‚â§t} (q_t·µÄ q_i) v_i·µÄ` and `den_t = ‚àë_{i‚â§t} q_t·µÄ q_i`, which matches a linear-attention-like kernel `K(q_t, q_i) = q_t·µÄ q_i` (Section 3).
  - Without tying queries and keys (`q ‚â° k`), HLA differs from identity-feature linear attention because `S_{K,t}` depends on keys (Section 3).

**Complexity (second order, per token, per head).**
- Updating `S_{K,t}` costs `O(d^2)` and updating `C_{QV,t}` costs `O(d d_v)` (Section 3).
- Computing output can be done without explicitly forming `S_{K,t} C_{QV,t}` by first computing `u_t = q_t·µÄ S_{K,t}` (a matrix‚Äìvector multiply) and then `u_t C_{QV,t}` (a row-vector‚Äìmatrix multiply) (Section 5).

#### 3.4.4 Strict causal masking for second-order HLA via extended summaries

**The masking problem.**
- In an autoregressive model, token `t` must not use information from future tokens `> t`. In attention, this is done by a causal mask; for higher-order operators like `(L ‚äô QK·µÄ)(L ‚äô QK·µÄ)·µÄ`, the masking interacts with the ‚Äúsquare‚Äù and is nontrivial to implement streaming without `n√ón` intermediates (Section 3.1).

**Key move: add correction summaries that subtract ‚Äúillegal‚Äù terms.**
- Define the binary lower-triangular mask `L` and let `W = L ‚äô (QK·µÄ)` (Section 3.1).
- The paper introduces two additional prefix summaries (Section 3.1):
  - `G_t := ‚àë_{i‚â§t} (k_i k_i·µÄ) C_{QV,i‚àí1} ‚àà R^{d√ód_v}`,
  - `h_t := ‚àë_{i‚â§t} (k_i k_i·µÄ) m_{Q,i‚àí1} ‚àà R^{d}`.
  - Intuition: these capture exactly the contributions that would otherwise arise from using `S_{K,t}` with ‚Äútoo-large‚Äù time boundaries when enforcing the nested `min(t,j)` structure inherent in masked second-order weights.

**Masked streaming identity (second order).**
- Theorem 3.1 defines masked numerator/denominator (Section 3.1):
  - `num_t^mask = q_t·µÄ ( S_{K,t} C_{QV,t} ‚àí G_t )`,
  - `den_t^mask = q_t·µÄ ( S_{K,t} m_{Q,t} ‚àí h_t )`.
- The resulting strictly causal outputs are (Eq. (3.3), (3.4)):
  - Unnormalized: `o_t = q_t·µÄ ( S_{K,t} C_{QV,t} ‚àí G_t )`.
  - Optional normalized: `o_t = [q_t·µÄ (S_{K,t} C_{QV,t} ‚àí G_t)] / ([q_t·µÄ (S_{K,t} m_{Q,t} ‚àí h_t)] + Œµ)`.

**Online updates remain streaming and efficient.**
- Theorem 3.1 also provides updates (Section 3.1):
  - `S_{K,t} = S_{K,t‚àí1} + k_t k_t·µÄ`,
  - `C_{QV,t} = C_{QV,t‚àí1} + q_t v_t·µÄ`,
  - `m_{Q,t} = m_{Q,t‚àí1} + q_t`,
  - `G_t = G_{t‚àí1} + k_t (k_t·µÄ C_{QV,t‚àí1})`,
  - `h_t = h_{t‚àí1} + k_t (k_t·µÄ m_{Q,t‚àí1})`.
- The implementation uses the identity `(k k·µÄ) X = k (k·µÄ X)` to avoid cubic `O(d^3)` work when applying outer products to matrices (Section 3.1; Section 5).

#### 3.4.5 Chunk-parallel training via associative scans (exactly matching serial recurrence)

**Why scans are needed.**
- A purely serial recurrence is inefficient on GPUs; the paper adopts chunking and parallel prefix-scan patterns used in linear attention and scan-friendly RNNs (Section 4).

**Core concept: an associative ‚Äúsegment concatenation‚Äù operator.**
- A `Blelloch scan` is a parallel prefix-sum algorithm over an associative operator (Section 4; Remark 4.2).
- The paper builds an operator that composes per-token ‚Äúdeltas‚Äù into segment summaries such that:
  - an exclusive scan gives correct per-position prefix states, and
  - adding the local delta reconstructs the same state as a serial left-to-right loop (Section 4; Theorem 4.1).

**Unmasked case is additive (a monoid).**
- State `S = (S, C, m)` with per-token deltas `ŒîS_t = k_t k_t·µÄ`, `ŒîC_t = q_t v_t·µÄ`, `Œîm_t = q_t` (Section 4.1).
- Segment composition is just addition:
  - `(S_A, C_A, m_A) ‚äï (S_B, C_B, m_B) = (S_A+S_B, C_A+C_B, m_A+m_B)` (Section 4.1).

**Masked case uses a semidirect-product-style operator (adds cross terms).**
- Masked state expands to `S = (S, C, m, G, h)` (Section 4.2).
- For adjacent segments `A` then `B`, concatenation is (Eq. (4.1)):
  - `(S_A,C_A,m_A,G_A,h_A) ‚äï (S_B,C_B,m_B,G_B,h_B)`
  - `= (S_A+S_B, C_A+C_B, m_A+m_B, G_A+G_B + S_B C_A, h_A+h_B + S_B m_A)`.
- The additional terms `S_B C_A` and `S_B m_A` are exactly the cross-boundary contributions needed so that masked correction summaries remain correct under concatenation.

**Decay is supported while preserving associativity.**
- The paper adds exponential decay `Œ≥ ‚àà (0,1)` into the recurrence updates (Section 4.3), and lifts it to segment composition by tracking segment length/attenuation `œÅ = Œ≥^{‚Ñì(segment)}` (Section 4.2).
- The decayed operator `‚äï_Œ≥` rescales earlier summaries by `œÅ_B` when appending segment `B` (Section 4.2), maintaining associativity by bilinearity and multiplicativity of `œÅ`.

**Exact equivalence guarantee.**
- Theorem 4.1 proves that scanning under `‚äï` (or `‚äï_Œ≥`) and then locally including the token deltas yields **the same inclusive state** as the serial recurrence on tokens `1:t`, hence identical masked outputs (Section 4.2; Theorem 4.1).

**Forward/backward notes.**
- The paper states a reverse-mode algebra approach: define `‚äï*` as the adjoint (vector‚ÄìJacobian) of `‚äï` and run a reverse scan with checkpointing to match serial gradients (Section 4.2, ‚ÄúBackward for gradients‚Äù). No explicit full gradient formulas are provided in the excerpt beyond this high-level description.

#### 3.4.6 Implementation details (second order)

- Algorithm 1 provides within-chunk scan pseudocode for masked second-order HLA (Section 5; Algorithm 1).
- Key practical computation choices (Section 5):
  - Compute `u = q_t·µÄ S_eff,t` (`S_eff,t = S_t + ŒªI` optional ridge; line 7‚Äì10 of Algorithm 1) and then `num = u C_t ‚àí q_t·µÄ G_t` to avoid explicitly forming `S_t C_t`.
  - Optional normalization uses `den = u m_t ‚àí q_t·µÄ h_t + Œµ` (Algorithm 1, lines 12‚Äì15).
- The paper notes that adding `ŒªI` is for stability and ‚Äúdoes not correspond to the exact masked bilinear form‚Äù (Section 5.1 Remark).

- Multi-query optimization:
  - If keys/values are shared across heads (multi-query attention style), `S_{K,t}` can be shared once per layer, reducing memory from `O(h d^2 + h d d_v)` to `O(d^2 + h d d_v)` (Section 5.2).

#### 3.4.7 Asymmetric second-order variant (AHLA)

**Definition: left-cascaded second-order operator.**
- AHLA is based on `AAV` where `A = L ‚äô (QK·µÄ)` (Section 6).
- The output is (Eq. (6.1)):
  - `o_t^AHLA = ‚àë_{j‚â§t} ‚àë_{i=j}^t (q_t·µÄ k_i)(q_i·µÄ k_j) v_j·µÄ`.
- Theorem 6.1 shows an exact masked streaming identity with prefix summaries (Section 6.1):
  - `P_{KV,t} = ‚àë_{j‚â§t} k_j v_j·µÄ`,
  - `m_{K,t} = ‚àë_{j‚â§t} k_j`,
  - `E_t = ‚àë_{i‚â§t} k_i ( q_i·µÄ P_{KV,i} )`,
  - `n_t = ‚àë_{i‚â§t} k_i ( q_i·µÄ m_{K,i} )`,
  - Output: `o_t^AHLA = q_t·µÄ E_t`, and optional normalization `q_t·µÄ n_t + Œµ` (Theorem 6.1).
- Algorithm 2 gives the streaming recurrence with optional decay (Section 6.3; Algorithm 2).

**Complexity contrast (as given).**
- Theorem 6.1 notes streaming/serial inference is dominated by `q_t·µÄ P_{KV,t}` and the outer product `k_t (¬∑)`, giving `O(d d_v)` time and `O(d d_v + d)` state per head in the streaming path (Section 6.1).
- For chunk-parallel scans, AHLA introduces a segment-level moment `R_{KQ} = ‚àë k_i q_i·µÄ` used only for concatenation cross terms (Section 6.2; Eq. (6.2)).

#### 3.4.8 Third-order HLA extension (masked streaming)

**Unmasked factorization.**
- Third-order uses `AA·µÄA` (with `A=QK·µÄ`) and derives a factorization through prefix moments (Section 7.1).
- Prefix summaries (Section 7.1):
  - `S_{K,t} = ‚àë k_i k_i·µÄ`,
  - `S_{Q,t} = ‚àë q_i q_i·µÄ`,
  - `P_{KV,t} = ‚àë k_i v_i·µÄ`,
  - `m_{K,t} = ‚àë k_i`.
- Default unnormalized third-order output:
  - `o_t^{(3)} = q_t·µÄ S_{K,t} S_{Q,t} P_{KV,t}` (Section 7.1).
- Optional normalized divides by `q_t·µÄ S_{K,t} S_{Q,t} m_{K,t} + Œµ` (Section 7.1).

**Masked corrections require more cross-summaries.**
- The paper introduces three correction tensors `G^{(1)}, G^{(2)}, G^{(3)}` and analogous vectors `h^{(1)}, h^{(2)}, h^{(3)}` (Section 7.1).
- Theorem 7.1 defines masked numerator/denominator by subtracting all three correction terms:
  - `num_t^{(3)mask} = q_t·µÄ ( S_{K,t} S_{Q,t} P_{KV,t} ‚àí G_t^{(1)} ‚àí G_t^{(2)} ‚àí G_t^{(3)} )`,
  - `den_t^{(3)mask} = q_t·µÄ ( S_{K,t} S_{Q,t} m_{K,t} ‚àí h_t^{(1)} ‚àí h_t^{(2)} ‚àí h_t^{(3)} )`,
  - and sets `o_t^{(3)} = num_t^{(3)mask}` (unnormalized) or divides for normalization (Theorem 7.1).

**Streaming kernel pseudocode.**
- Algorithm 3 provides a strictly causal streaming kernel with optional decay for third order, explicitly updating base moments and cross-summaries, then computing:
  - `termA = (S_K q_t)·µÄ S_Q·µÄ P_{KV}`-style product (as written in Algorithm 3 lines 13‚Äì14),
  - minus `q_t·µÄ G^{(i)}` correction terms (Algorithm 3).
- Important caveat: the paper explicitly says that designing a chunk-parallel scan operator for third order requires additional segment-level summaries beyond those listed, and it ‚Äúleave[s] this composition to future work‚Äù (end of Section 7.2).

#### 3.4.9 Worked micro-example (second-order masked HLA, tiny dimensions)

This example illustrates *how the streaming state produces masked outputs* without ever building an `n√ón` matrix.

- Choose dimensions `d=2`, `d_v=1`. Let two tokens arrive (`t=1,2`).
- Let:
  - `q1 = [1, 0]·µÄ`, `k1 = [1, 1]·µÄ`, `v1 = [2]`,
  - `q2 = [0, 1]·µÄ`, `k2 = [1, 0]·µÄ`, `v2 = [3]`.
- Initialize all summaries to zero: `S_K, C_QV, m_Q, G, h = 0`.

**At t=1 (strictly causal includes token 1 itself because mask includes diagonal in the paper‚Äôs `L` definition; Section 3.1).**
- Update base moments:
  - `S_K = k1 k1·µÄ = [[1,1],[1,1]]`,
  - `C_QV = q1 v1·µÄ = [[2],[0]]`,
  - `m_Q = q1 = [1,0]·µÄ`.
- Update masked corrections using previous-prefix moments (`C_QV,prev = 0`, `m_Q,prev=0`):
  - `G = G_prev + k1 (k1·µÄ C_QV_prev) = 0`,
  - `h = h_prev + k1 (k1·µÄ m_Q_prev) = 0`.
- Compute masked unnormalized output (Eq. (3.3)):
  - First compute `S_K C_QV = [[1,1],[1,1]] [[2],[0]] = [[2],[2]]`.
  - Then `o1 = q1·µÄ (S_K C_QV ‚àí G) = [1,0] [[2],[2]] = [2]`.

**At t=2.**
- Base updates:
  - `S_K ‚Üê S_K + k2 k2·µÄ = [[1,1],[1,1]] + [[1,0],[0,0]] = [[2,1],[1,1]]`,
  - `C_QV ‚Üê C_QV + q2 v2·µÄ = [[2],[0]] + [[0],[3]] = [[2],[3]]`,
  - `m_Q ‚Üê m_Q + q2 = [1,0] + [0,1] = [1,1]·µÄ`.
- Mask corrections:
  - Need `C_QV_prev` and `m_Q_prev` (prefix up to `t‚àí1`): `C_QV_prev = [[2],[0]]`, `m_Q_prev=[1,0]·µÄ`.
  - Compute `k2·µÄ C_QV_prev = [1,0] [[2],[0]] = [2]`.
  - So `G ‚Üê G_prev + k2 (k2·µÄ C_QV_prev) = 0 + [1,0]·µÄ * 2 = [[2],[0]]`.
  - Compute `k2·µÄ m_Q_prev = [1,0][1,0]·µÄ = 1`.
  - So `h ‚Üê 0 + k2 * 1 = [1,0]·µÄ`.
- Output:
  - `S_K C_QV = [[2,1],[1,1]] [[2],[3]] = [[7],[5]]`.
  - Subtract correction: `S_K C_QV ‚àí G = [[7],[5]] ‚àí [[2],[0]] = [[5],[5]]`.
  - `o2 = q2·µÄ([[5],[5]]) = [0,1] [[5],[5]] = [5]`.

This demonstrates how `G` subtracts cross-boundary terms so the masked operator is respected exactly (Theorem 3.1), while the state size stays constant in `n`.

#### 3.4.10 Configurations / hyperparameters (what is and isn‚Äôt specified)

The paper excerpt provides **algorithmic hyperparameters** for HLA itself, but **does not provide** full end-to-end LLM training hyperparameters.

- Provided in the paper:
  - Optional normalization uses `Œµ > 0` for numerical stability (Eq. (3.2), (3.4); Algorithm 1).
  - Optional ridge regularization `Œª` via `S_eff,t = S_t + ŒªI` (Algorithm 1; Section 5.1 remark).
  - Optional exponential decay `Œ≥ ‚àà (0,1)` (Section 4.3; Algorithm 1; Algorithm 2; Algorithm 3).
  - Chunk width `w` and number of chunks `B_c` in scan-based training description (Section 4).
- Not provided in the excerpt (so cannot be specified here without fabrication):
  - Optimizer type/settings, learning rate schedule, batch size, context window length used in experiments, tokenizer, number of layers/heads/hidden sizes in an evaluated model, total training tokens, compute budget, and hardware.

## 4. Key Insights and Innovations

- **(1) Exact masked streaming identity for second-order higher interactions.**
  - Innovation: introduce correction summaries `G_t` and `h_t` so the masked second-order operator can be computed exactly as `q_t·µÄ(S_K C_QV ‚àí G)` (and normalized variant) without `n√ón` intermediates (Section 3.1; Theorem 3.1; Eq. (3.3), (3.4)).
  - Why it matters: strict causal masking is often where ‚Äúnice algebra‚Äù breaks; this result preserves streaming, constant-state updates while being mask-correct.

- **(2) Associative scan operator that exactly matches serial recurrence (masked and decayed).**
  - Innovation: define a semidirect-product-like concatenation law that composes segment summaries with cross terms (`+ S_B C_A`, `+ S_B m_A`) and remains associative (Eq. (4.1)).
  - Theorem 4.1 proves that a standard exclusive scan yields activations identical to serial recurrence, including with exponential decay (Section 4.2).
  - Why it matters: this bridges the gap between ‚Äústreaming recurrence‚Äù (good for inference) and ‚ÄúGPU-parallel training‚Äù (good for throughput) without approximate backpropagation through time (Abstract; Section 4).

- **(3) Higher-order generalization beyond second order (third order masked algebra).**
  - Innovation: provide a complete masked third-order state with three correction tensors/vectors `(G^{(1..3)}, h^{(1..3)})` and a streaming kernel (Section 7.1; Theorem 7.1; Algorithm 3).
  - Significance: shows the approach scales conceptually to higher-order operators, though training-parallel scan composition for third order is left incomplete (Section 7.2).

- **(4) Asymmetric higher-order operator (AHLA) with different inductive bias and potentially cheaper streaming path.**
  - Innovation: define `AAV`-style second-order mixing and derive an exact masked streaming identity with summaries `(P_KV, m_K, E, n)` (Section 6.1; Theorem 6.1).
  - Significance: provides an alternative second-order mechanism with `O(d d_v)` dominant streaming cost as described, and a separate associative scan operator using a segment-level `R_KQ` (Section 6.1‚Äì6.2).

## 5. Experimental Analysis

- **What evaluation methodology is described?**
  - The provided content does **not** include datasets, metrics, baselines, or quantitative results tables/figures.
  - The text emphasizes algorithmic correctness (streaming identities, associativity proofs, scan equivalence) and implementation considerations (Sections 3‚Äì5), rather than empirical benchmarks.

- **What quantitative results are available?**
  - None are present in the excerpt: no accuracy/perplexity numbers, speed/throughput measurements, memory benchmarks, or ablations are provided here.
  - Because of this, it is not possible (from the provided content) to assess ‚Äúperformance vs. baselines‚Äù in the usual empirical sense.

- **Do the included analyses support the claims that are made?**
  - For the *correctness* claims:
    - Theorem 3.1 provides a derivation for masked second-order streaming and gives explicit online updates (Section 3.1).
    - Theorem 4.1 provides a proof that scan-based parallelization matches serial recurrence exactly, including with decay (Section 4.2).
    - Theorem 6.1 and Theorem 7.1 give analogous correctness identities for AHLA and third-order HLA (Sections 6‚Äì7).
  - For the *systems* claims (e.g., throughput characteristics, practical efficiency), the excerpt provides complexity reasoning and implementation notes (Section 5) but no measured benchmarks.

- **Ablations / failure cases / robustness checks?**
  - None are included in the provided content.
  - The paper does note numerical stabilization options (`Œµ`, optional ridge `ŒªI`, optional decay `Œ≥`) (Eq. (3.2)/(3.4); Section 4.3; Algorithm 1), but does not quantify their effect here.

## 6. Limitations and Trade-offs

- **Lack of empirical validation in the provided content.**
  - The excerpt contains no end-to-end results on language modeling or long-context tasks, so practical accuracy/efficiency trade-offs versus attention, linear attention, or SSMs cannot be evaluated here.

- **State size and compute can be heavy at second order.**
  - Second-order HLA maintains `S_K,t ‚àà R^{d√ód}` per head (unless multi-query sharing is used) and updates it at `O(d^2)` per token (Section 3; Section 5.2).
  - This trades sequence-length scaling for potentially substantial per-token quadratic-in-`d` cost, which may be nontrivial when `d` is large.

- **Normalization and stability are optional but not fully characterized.**
  - The normalized form divides by `q_t·µÄ(S_K m_Q ‚àí h) + Œµ` (Eq. (3.4)), which can be sensitive if the denominator is small or changes sign; the excerpt does not analyze conditioning beyond adding `Œµ` and optional ridge `ŒªI` (Algorithm 1; Section 5.1 remark).

- **Third-order training parallelization is incomplete.**
  - While Algorithm 3 gives a streaming inference kernel and Theorem 7.1 gives masked identities, the paper explicitly leaves the associative scan operator composition for third order to future work (end of Section 7.2).
  - This means the strongest ‚Äúdrop-in with exact scan-parallel training‚Äù story is fully developed for second order (and for AHLA), but not fully closed for third order in the provided content.

- **Mask convention and exact operator target.**
  - The paper uses a binary mask `L` (ones on and below diagonal) for algebraic manipulations outside softmax (Section 2.1; Section 3.1). This is a different object than the `‚àí‚àû` additive mask inside softmax attention, and HLA is not presented as an exact replacement for softmax attention; it is an attention-like mixer with different weighting structure.

## 7. Implications and Future Directions

- **How this work changes the landscape (based on the provided content).**
  - HLA shows that ‚Äúattention-like‚Äù higher-order interactions are not inherently tied to quadratic `n√ón` attention matrices: they can be realized via compact, streaming sufficient statistics with exact causal masking (Theorem 3.1; Theorem 7.1).
  - It strengthens the bridge between attention mechanisms and recurrent/scan-based architectures by providing a principled associative-scan formulation that matches serial recurrence exactly (Section 4; Theorem 4.1).

- **Follow-up research suggested by what‚Äôs included.**
  - **Complete chunk-parallel scan operators for third order and beyond.** The paper provides third-order masked streaming but leaves scan composition for future work (Section 7.2).
  - **Empirical studies.** Given the algorithmic focus, a natural next step is benchmarking perplexity/quality, long-context behavior, stability, and measured throughput/memory across sequence lengths and model sizes‚Äînone of which are in the excerpt.

- **Practical applications / downstream use cases (as implied).**
  - Long-context autoregressive language modeling where streaming inference is needed and quadratic attention is prohibitive (Abstract; Section 1).
  - Scenarios requiring strict causality correctness (Theorem 3.1) and GPU-friendly training via scans (Section 4).

- **Repro/Integration Guidance (when to prefer what), grounded in the paper.**
  - Prefer **second-order HLA** when:
    - You want richer-than-first-order mixing while staying strictly causal and streaming, and you can afford `O(d^2 + d d_v)` per-token compute/state (Section 3; Section 5).
    - You need exact scan-parallel training that matches serial activations (Section 4; Theorem 4.1).
  - Prefer **AHLA** when:
    - You want a second-order mechanism with the paper‚Äôs described `O(d d_v)`-dominated streaming path and are comfortable with its different inductive bias (`AAV` vs `AA·µÄV`) (Section 6; Theorem 6.1).
  - Use **decay `Œ≥`** if:
    - You want recency bias and to control growth while preserving scan associativity (Section 4.3; Section 4.2).
  - Use **normalization (`Œµ`) and optional ridge (`ŒªI`)** if:
    - You need scale control/stability, noting the ridge variant is not exactly the original masked bilinear form target (Eq. (3.4); Algorithm 1; Section 5.1 remark).

If you can provide the full PDF (or additional sections beyond what‚Äôs pasted), I can anchor more details to any missing figures/tables/sections‚Äîespecially any experiments that may exist outside the excerpt.

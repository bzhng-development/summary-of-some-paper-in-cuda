# INTERN-S1: A SCIENTIFIC MULTIMODAL FOUNDATION MODEL

**ArXiv:** [2508.15763](https://arxiv.org/abs/2508.15763)

## ğŸ¯ Pitch

Intern-S1 defines a new open-source standard for scientific AI by unifying a massive Mixture-of-Experts language model with specialized vision and time-series encoders, plus a novel dynamic tokenizer tailored for complex scientific data. Trained on an unprecedented scale of scientific tokens and refined with a Mixture-of-Rewards reinforcement learning framework, Intern-S1 dramatically narrows the performance gap with closed modelsâ€”delivering state-of-the-art reasoning and multimodal understanding across diverse scientific domains. This leap enables transformative applications in scientific discovery, from molecular design to materials science, democratizing advanced tools for researchers worldwide.

---

## 1. Executive Summary (2â€“3 sentences)
Internâ€‘S1 is an openâ€‘source, scientific multimodal foundation model that integrates a large Mixtureâ€‘ofâ€‘Experts language model with vision and timeâ€‘series encoders, plus a new dynamic tokenizer for scientific strings. Trained on 5T tokens (over 2.5T from science) and postâ€‘trained with a Mixtureâ€‘ofâ€‘Rewards online reinforcement learning framework, it achieves top openâ€‘source performance on general reasoning and stateâ€‘ofâ€‘theâ€‘art performance on many scientific text and imageâ€‘text benchmarks (Tables 2â€“4), substantially narrowing the gap with leading closedâ€‘source systems.

## 2. Context and Motivation
- Problem addressed
  - Progress in openâ€‘source models has been fast for popular domains (math, code, natural images), yet capability in scientific domains (chemistry, materials, life sciences, physics, earth science) lags behind and still often relies on expert systems or closed models (Introduction; Fig. 1â€“2).
  - Scientific data are lowâ€‘resource, diverse in modality (molecules, protein sequences, formulas, tables, figures, time series), and require long, rigorous reasoning (Introduction).

- Why it matters
  - Better scientific models can accelerate hypothesis testing, experimental design, and discovery across highâ€‘value domains like drug design, materials discovery, and climate/earth observation (Introduction).

- Shortcomings of prior approaches
  - Openâ€‘source multimodal models mainly target natural images and general VQA; they underperform on scienceâ€‘specific content (e.g., chemistry strings, document equations) and lowâ€‘resource modalities (Fig. 2).
  - Static tokenizers treat scientific strings like ordinary text, leading to poor compression and ambiguous embeddings (Sec. 2.2). PDF parsing and web data pipelines are not optimized for scientific structure (Sec. 4.1.1).
  - RL for reasoning is largely validated on dense models; applying GRPOâ€‘style methods to large MoE models is unstable due to expert routing mismatch between inference and training (Sec. 5.2.3).

- Positioning
  - Internâ€‘S1 tackles the full stack: data, architecture, training system, and RL, with a scienceâ€‘first orientation:
    - 2.5T+ scientific tokens via specialized data pipelines (Sec. 4.1; Fig. 6â€“9).
    - A dynamic tokenizer that recognizes scientific substrings (SMILES, FASTA) and assigns modalityâ€‘specific embeddings (Sec. 2.2; Fig. 4).
    - A multimodal architecture that adds vision and timeâ€‘series encoders (Sec. 2; Fig. 3).
    - A scalable online RL setup using a Mixtureâ€‘ofâ€‘Rewards across 1000+ verifiable tasks, stabilized for MoE (Sec. 5.2; Fig. 12).

## 3. Technical Approach
This section walks through the model, data, training, and RL components and why each choice was made.

- Overall architecture (Sec. 2; Fig. 3)
  - Backbone LLM: `Qwen3â€‘235B` MoE (Internâ€‘S1) and `Qwen3â€‘8B` (Internâ€‘S1â€‘mini).
    - `Mixtureâ€‘ofâ€‘Experts (MoE)`: a routing mechanism that activates a subset of expert subâ€‘networks per token to increase capacity without proportional compute.
  - Vision encoder: `InternViTâ€‘6B` (or `InternViTâ€‘300M` for mini), trained from contrastive pretrain to LLMâ€‘coupled nextâ€‘token prediction for stronger fineâ€‘grained features (Sec. 2.1).
    - Uses dynamic resolution and `pixel unshuffle` to reduce visual tokens 4Ã—; a 448Ã—448 image becomes 256 visual tokens; an MLP projector aligns them to the LLM embedding space (Sec. 2.1).
  - Dynamic tokenizer for scientific strings (Sec. 2.2; Fig. 4).
  - Timeâ€‘series encoder with adaptive downsampling + Transformer blocks for long scientific signals (seismic, gravitational waves, EEG) (Sec. 2.3).

- Dynamic tokenizer: how it works and why it matters (Sec. 2.2; Fig. 4)
  - Problem: static tokenizers use one split strategy and one embedding set for all text. This:
    - Wastes tokens on rare formats (e.g., `SMILES` for molecules).
    - Forces the same symbol (e.g., â€œCâ€) in English, DNA, and molecules to share one embedding, biasing toward frequent usages.
  - Mechanism:
    1. Detect scientific substrings either by explicit tags (e.g., `<FASTA>`, `<SMILES>`) or rule/tool detectors (e.g., RDKit) (Fig. 4, left).
    2. Segment the input into modality spans (e.g., general text vs. SMILES vs. FASTA).
    3. Tokenize each span with a strategy tailored to that modality.
    4. Map each span into its own embedding subspace â€œorthogonalâ€ to others (i.e., independent embeddings); concatenate into a single sequence for the Transformer (Fig. 4, left).
  - Outcome:
    - Much higher compression for scientific strings; `compression ratio` (characters per token) improves up to ~70% vs. OpenAI GPTâ€‘OSSâ€‘120B, DeepSeekâ€‘R1, and Qwen3 tokenizers on SMILES (Fig. 4, right). The CR metric is formalized in Eq. CR(Ï„, D) (Sec. 2.2).
    - Reduces compute and avoids semantic interference across modalities.

- Timeâ€‘series encoder (Sec. 2.3)
  - Scientific signals vary in sampling rate and length; text tokenization is illâ€‘suited.
  - An adaptive downsampling module compresses long sequences, then Transformer blocks model temporal dependencies, producing representations that the LLM can reason over (Sec. 2.3).

- Data pipelines: scaling science data with quality control (Sec. 4.1; Fig. 6â€“9)
  - Scale and mix
    - Continued pretraining (CPT) on 5T text tokens; >2.5T are scientific (Fig. 6, left).
    - Imageâ€‘text CPT uses ~250B tokens: 70B text and 180B interleaved imageâ€‘text; ~30B tokens are multimodal scientific data (Sec. 4.1.2).
  - Pageâ€‘level PDF parsing (Sec. 4.1.1; Fig. 7)
    - PDFs are rich in equations/symbols. A twoâ€‘stage parser minimizes cost:
      - Lowâ€‘cost parser (MinerU) runs on all pages.
      - A detector flags pages with equations/symbolic markers for highâ€‘cost VLM parsing (e.g., InternVL, Qwenâ€‘VL), then postâ€‘processing and pageâ€‘level deduplication.
    - Only 5% (archived) / 3% (web) pages go through highâ€‘cost parsing, yet quality improves; 20â€“50% of lowâ€‘quality content is filtered (Sec. 4.1.1).
  - Domainâ€‘centric web parsing (Sec. 4.1.1; Fig. 8)
    - Treat each domain (hostname) as a unit; sample pages and use an LLM agent to decide perâ€‘domain actions (discard/retain/rewrite), capturing consistent parsing quirks at lower cost than pageâ€‘wise LLM parsing.
  - Scientific recall and filtering (Sec. 4.1.1; Fig. 9)
    - Build a taxonomy (six science domains: Math, Physics, Chemistry, Life, Earth, Materials).
    - Use a strong LLM to annotate a silver set â†’ train lightweight classifiers (fastText, 1.5B LLMs).
    - Optimize prompts using inâ€‘domain vs. outâ€‘ofâ€‘domain validation sets.
    - Result: targetâ€‘domain purity rises from ~2% to ~50% (Sec. 4.1.1).

- Training system and optimization (Sec. 3, 4.2)
  - Systems (Sec. 3.1â€“3.2)
    - `FSDP` for parameter sharding; `FP8` matmuls (DeepGEMM) with dynamic scaling; BF16 for the vision tower for stability.
    - MoE kernels: TMAâ€‘Adaptive FP8 Grouped GEMM for dynamic groups; fused loss kernels (Liger); FlashAttentionâ€‘3 for variable lengths.
    - Variableâ€‘Length Balanced Strategy (VLBS): bucket + slidingâ€‘window sort to equalize perâ€‘rank lengths, giving ~2Ã— speedup at scale (Sec. 3.1).
    - RL deployment: colocated training + inference meshes; FP8 inference; EP8 rollout via LMDeploy; continuous batching and onâ€‘theâ€‘fly slot rebalancing (Sec. 3.2).
  - Multiâ€‘stage training (Fig. 5)
    1. Text CPT (unimodal).
    2. Imageâ€‘text CPT (joint).
    3. Imageâ€‘text SFT (offline RL with bestâ€‘ofâ€‘N).
    4. Imageâ€‘text Online RL (Mixtureâ€‘ofâ€‘Rewards).
  - Batchâ€‘size warmup and LR via scaling laws
    - Observation: small batches train better early; large batches are more efficient later (Fig. 10).
    - Use WSD (Warmupâ€‘Stableâ€‘Decay) LR scheduler and connect batch size B to gradient noise `B_simple` (Eq. 1): as loss falls, the effective critical batch size rises (Sec. 4.2.3).
    - In Xtuner, batch grows from 66M to 132M tokens, with switch after ~400B tokens processed (Sec. 4.2.3).
    - Learningâ€‘rate schedule is chosen by fitting lossâ€‘vsâ€‘LR scaling laws and solving a constrained optimization over LR per step â„¦ (Eq. 2), yielding accurate loss prediction: predicted ~1.16 vs. actual 1.17â€“1.18 (Sec. 4.2.3).
  - Start from base vs. instruction checkpoints (Sec. 4.2.2; Fig. 11)
    - Empirically similar final performance post SFT+RL; instruct has an edge where postâ€‘training introduced genuinely new capability (coding), while elsewhere it mainly activates latent skills.
    - Base model shows slightly higher initial entropy (0.19 vs. 0.15 on a math subset), but this can be compensated by RL hyperparameters (Sec. 4.2.2).
  - Multimodal CPT loss (Sec. 4.2.4)
    - Standard causal objective on text tokens only (visual tokens are context), with squareâ€‘averaging token weights to reduce gradient bias (Eq. 3â€“4).

- Postâ€‘training: Offline RL (SFT) then Online RL (Sec. 5)
  - Offline RL / SFT (Sec. 5.1)
    - Filtered, labeled, and enhanced instruction data across domains; bestâ€‘ofâ€‘N sampling ensures highâ€‘reward responses.
    - For multimodal, augment with science diagrams, OCR, charts, and strengthened longâ€‘thinking data (SOPHIAâ€‘style with strict quality filters) (Sec. 5.1.1).
    - Mixture selection via stepwise ablations and composition validation (Sec. 5.1.2).
  - Online RL with Mixtureâ€‘ofâ€‘Rewards (Sec. 5.2; Fig. 12)
    - `Mixtureâ€‘ofâ€‘Rewards (MoR)`: unify verifiable rewards across >1000 task types (logic puzzles, algorithmic tasks, domain exams; InternBootCamp provides synthetic generators) and nonâ€‘verifiable openâ€‘ended prompts via a learned preference model.
    - Verifiers (Sec. 5.2.2):
      - Easyâ€‘toâ€‘verify tasks: ruleâ€‘based checkers + `CompassVerifier` (a lightweight generative verifier) to reduce false negatives.
      - Openâ€‘ended chat/writing: `POLARâ€‘7B` policy discriminator produces relativeâ€‘quality reward signals.
    - Hybrid data filtering (Sec. 5.2.4; Fig. 13)
      - Offline prune tooâ€‘easy (pass@8=1.0) and tooâ€‘hard/noisy (pass@8â‰¤0.25) items using both a dense SFT and a MoE SFT model.
      - Online drop groups where all 8 rollouts are identical (allâ€‘correct or allâ€‘wrong), and remove garbled/infiniteâ€‘loop generationsâ€”empirically stabilizes training and speeds gains on AIME2024 (Fig. 13).
    - RL algorithm for MoE stability (Sec. 5.2.3; Eq. 6)
      - Direct GRPOâ€‘style tokenâ€‘ratio clipping is brittle for MoE due to expert routing divergence between inference and training.
      - Use `OREAL`: behavior cloning (SFT loss) on positive samples + policy gradient on negatives; avoid tokenâ€‘level importanceâ€‘ratio clipping (Eq. 6).
      - Remove OREALâ€™s tokenâ€‘level reward model for throughput, then prevent entropy collapse via a selective KL regularizer on highâ€‘covariance tokens (`KLâ€‘Cov`; Eq. 5). With k=0.2, Î²=0.01, entropy holds near ~0.2 and validation accuracy keeps rising (Fig. 14).
    - Training details (Sec. 5.2.4)
      - FP8 for rollout and training; 8 rollouts/prompt; batch 4096 (8 miniâ€‘batches), AdamW lr=5eâ€‘7, wd=0.1, Î²=(0.9, 0.95); ViT and router frozen; 600 steps; drop 3% batches with gradâ€‘norm>0.3; final checkpoint averaging.

## 4. Key Insights and Innovations
- Dynamic, modalityâ€‘aware tokenization for science (Sec. 2.2; Fig. 4)
  - Novelty: perâ€‘span tokenization and perâ€‘modality embeddings prevent semantic interference; scientific strings get much better compression.
  - Significance: up to ~70% higher charactersâ€‘perâ€‘token on SMILES (Fig. 4, right) reduces compute and lets the model attend across longer scientific contextâ€”this is a fundamental capability, not a small tweak.

- Pageâ€‘level, costâ€‘aware PDF parsing with VLM fallbacks (Sec. 4.1.1; Fig. 7)
  - Novelty: a hybrid low/highâ€‘cost pipeline at page granularity, guided by equation/symbol detectors, plus pageâ€‘graph deduplication.
  - Significance: cheaply recovers highâ€‘quality text/equations from PDFs, crucial for science where formulas and figures carry the core knowledge.

- Domainâ€‘centric web parsing + recall/filtering (Sec. 4.1.1; Fig. 8â€“9)
  - Novelty: LLM agents make perâ€‘domain decisions (discard/retain/rewrite) and a taxonomyâ€‘guided recall/filter loop with inâ€‘domain vs. OOD prompt optimization.
  - Significance: boosts science purity from ~2% to ~50% (Sec. 4.1.1), solving the lowâ€‘resource bottleneck at scale.

- MoR: a unified, scalable online RL framework for 1000+ tasks (Sec. 5.2; Fig. 12)
  - Novelty: mixes ruleâ€‘based verifiers (exactness) with learned verifiers (`CompassVerifier`) and preference reward (`POLAR`) in one training loop; hybrid offline/online filtering balances task difficulty and sample quality (Sec. 5.2.4).
  - Significance: enables sustained gains across heterogeneous tasks while keeping training stable and efficientâ€”reported â€œ10Ã— less RL timeâ€ vs. comparable public work (Abstract; Sec. 5 overview).

- MoEâ€‘stable RL via OREAL + KLâ€‘Cov (Sec. 5.2.3; Eq. 5â€“6)
  - Novelty: avoids tokenâ€‘ratio clipping instability in MoE; adds selective KL on highâ€‘cov tokens to keep entropy healthy without collapsing exploration (Fig. 14).
  - Significance: a practical recipe to bring online RL to very large MoE VLMs.

## 5. Experimental Analysis
- Evaluation setup (Sec. 6.1; Table 1)
  - Tooling: VLMEvalKit and OpenCompass; â€œthinking modeâ€ enabled; sampling with temperature 0.7 (Internâ€‘S1) / 0.8 (mini), topâ€‘p 0.95, topâ€‘k 50; max tokens 65,536 (Table 1).
  - Scope: textâ€‘only and multimodal general reasoning; scienceâ€‘specific text and imageâ€‘text.

- Benchmarks (Sec. 6.2)
  - General reasoning: MMLUâ€‘Pro, GPQA (Diamond), AIMEâ€‘2025, IFEval; and multimodal MathVista, MMMU, MathVision, MMStar (Sec. 6.2.1).
  - Scientific reasoning (text): SmolInstruct (chemistry), ChemBench, MatBench (materials), ProteinLMBench (Sec. 6.2.2).
  - Scientific reasoning (multimodal): SFE, Physics (PhD qualifying problems), MicroVQA (microscopy), MSEarthâ€‘MCQ, XLRSâ€‘Bench (ultraâ€‘highâ€‘res remote sensing) (Sec. 6.2.2).

- Main quantitative results
  - General (Table 2)
    - Internâ€‘S1 leads among openâ€‘source multimodal models on all eight tasks.
    - Examples:
      - â€œMathVistaâ€: 81.5 vs. 79.0 (InternVL3â€‘78B) and 74.8 (Qwen2.5â€‘VLâ€‘72B).
      - â€œMathVisionâ€: 62.5 vs. 43.1 and 38.1 for the two openâ€‘source baselines.
    - It remains competitive but not best vs. APIs on some textâ€‘only tasks (e.g., GPQA: 77.3 vs. Grokâ€‘4 at 87.5).
  - Science textâ€‘only (Table 3)
    - Internâ€‘S1 tops 3/4 benchmarks:
      - â€œSmolInstructâ€: 51.0 (best overall; APIs: 40.4â€“47.3).
      - â€œChemBenchâ€: 83.4 (tied with/better than APIs).
      - â€œMatBenchâ€: 75.0 (far ahead of openâ€‘source MLLMs/VLMs by +23â€“26 points).
    - â€œProteinLMBenchâ€: 63.1â€”strong but below o3 (67.7) and Kimiâ€‘K2 (66.7).
  - Science multimodal (Table 4)
    - Internâ€‘S1 ranks first on 4/5:
      - â€œSFEâ€: 44.3 (best; Geminiâ€‘2.5 Pro at 43.0).
      - â€œMicroVQAâ€: 63.9 (best).
      - â€œMSEarthâ€‘MCQâ€: 65.7 (best).
      - â€œXLRSâ€‘Benchâ€: 55.0 (best).
    - â€œPhysicsâ€ (qualifying exams): 44.0â€”second to o3 (47.9).
  - Internâ€‘S1â€‘mini (Tables 5â€“7)
    - Textâ€‘only general: new openâ€‘source SOTA on MMLUâ€‘Pro 74.8, GPQA 65.2, AIMEâ€‘2025 80.0 (Table 5).
    - Science textâ€‘only: leads on all four vs. similarly sized openâ€‘source models (Table 6).
    - Science multimodal: best on 4/5, but behind on SFE (35.8 vs. ~43.5 for others) (Table 7).

- Ablations and diagnostics
  - Tokenization compression: Fig. 4 (right) quantifies the 70% CR improvement on SMILES.
  - Batchâ€‘size warmup: Fig. 10 shows early advantages with small batches and overall benefits of switching to large batches midâ€‘training.
  - Startâ€‘point choice: Fig. 11 shows marginal differences between base vs. instruct after CPT+SFT+RL, with instruct preferred when postâ€‘training introduced new skills (coding).
  - RL data filtering: Fig. 13 shows faster AIME2024 accuracy gains vs. DAPO filtering on a 32B model.
  - Entropy control: Fig. 14 demonstrates stabilized entropy (~0.2) and rising validation accuracy with KLâ€‘Cov vs. collapse without it.

- Do the experiments support the claims?
  - Yes for the core claims:
    - Strong general reasoning among openâ€‘source VLMs (Table 2).
    - Significant gains on scientific text and multimodal tasks, including wins over APIs on several science benchmarks (Tables 3â€“4).
    - Tokenization, data, and RL ablations provide mechanistic evidence for why performance improves (Figs. 4, 10â€“14).
  - Mixed areas:
    - ProteinLMBench lags behind some closedâ€‘source systems (Table 3).
    - Physics (multimodal) is close but behind o3 (Table 4).
    - Internâ€‘S1â€‘mini underperforms on SFE vs. other small VLMs (Table 7).

> â€œInternâ€‘S1 â€¦ outperforms both openâ€‘source and closeâ€‘source models on imageâ€‘text or textâ€‘only scientific tasks.â€ (Introduction; Fig. 1; detailed in Tables 3â€“4)

> â€œInternâ€‘S1 achieved topâ€‘tier general reasoning capability among openâ€‘source modelsâ€ (Fig. 1; detailed in Table 2).

## 6. Limitations and Trade-offs
- Assumptions and dependencies
  - The dynamic tokenizer relies on correct detection/tags for scientific substrings; misâ€‘tagging could degrade compression or semantics (Sec. 2.2).
  - Verifiable RL hinges on the quality of ruleâ€‘based checkers and learned verifiers; for openâ€‘ended tasks, POLAR provides relative rewards, not absolute correctness (Sec. 5.2.2).

- Scope not fully evaluated
  - A timeâ€‘series encoder is introduced (Sec. 2.3), but the benchmark suite does not directly evaluate timeâ€‘series tasks (e.g., seismology, EEG); realâ€‘world performance on such data remains to be shown.

- Computational complexity
  - Training uses 5T tokens and a very large MoE LLM; even with FP8 and FSDP, compute and engineering complexity are high (Sec. 3â€“4). The RL stage uses sizeable batches and multiâ€‘rollout sampling (Sec. 5.2.4).

- MoE RL stability and sensitivity
  - While OREAL + KLâ€‘Cov improves stability, it required tuning (k=0.2, Î²=0.01) because Internâ€‘S1 started with low entropy (Sec. 5.2.4). Sensitivity to these hyperparameters and to the proportion of positive/negative examples may persist.

- Data transparency
  - Some multimodal RL data are from â€œprivate collectionsâ€ and â€œanonymized, realâ€‘world user queriesâ€ (Sec. 5.2.2), which can limit perfect reproducibility and external auditing.

- Frozen components during RL
  - The RL stage freezes the vision tower and router (Sec. 5.2.4), potentially capping multimodal adaptation during online learning.

## 7. Implications and Future Directions
- Fieldâ€‘level impact
  - Demonstrates that scienceâ€‘specialized, openâ€‘source multimodal models can challenge or surpass closed systems on several domain benchmarks by investing in modalityâ€‘aware tokenization, scienceâ€‘centric data pipelines, and verifierâ€‘driven RL at scale (Tables 3â€“4).
  - Provides a concrete blueprint for stabilizing online RL in large MoE VLMs (OREAL + KLâ€‘Cov) and for unifying heterogeneous tasks under a single reward framework (MoR).

- Research directions enabled
  - Tokenization: extend dynamic, perâ€‘modality tokenization to other structured formats (e.g., crystallographic files, graph encodings, domainâ€‘specific markup) and study its interaction with retrieval and longâ€‘context memory.
  - Timeâ€‘series: design standardized, verifiable timeâ€‘series science benchmarks (astronomy, geophysics, biomed) to evaluate the dedicated encoder introduced in Sec. 2.3.
  - Verifiers: improve crossâ€‘domain verifiers (e.g., programmatic checkers for chemistry/materials, automatic derivation checking in physics) and study how verifier bias propagates through RL.
  - MoE RL theory: formalize expertâ€‘routing drift between inference and training, and design objectives explicitly robust to expert mismatches.

- Practical applications
  - Chemistry and materials: molecular synthesis planning, reaction condition prediction, and crystal stability estimation (claimed in Abstract), supported by strong ChemBench and MatBench results (Table 3).
  - Documentâ€‘heavy science: better parsing and reasoning over papers, lab protocols, and figureâ€‘rich documents (PDF pipeline, Sec. 4.1.1).
  - Earth observation and microscopy: strong performance on XLRSâ€‘Bench and MicroVQA (Table 4) suggests utility for remote sensing analysis and bioimaging assistants.

Overall, Internâ€‘S1 shows that systematic investment in scienceâ€‘specific tokenization, data, and RL yields large, transferable gains across both text and multimodal scientific reasoning. The paperâ€™s detailed pipelines (Fig. 6â€“9), stabilized MoE RL (Sec. 5.2.3â€“5.2.4), and comprehensive evaluations (Tables 2â€“7) make it a practical template for future openâ€‘source, domainâ€‘specialized generalist models.

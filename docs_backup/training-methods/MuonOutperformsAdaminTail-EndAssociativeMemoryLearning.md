# Muon Outperforms Adam in Tail-End Associative Memory Learning

**ArXiv:** [2509.26030](https://arxiv.org/abs/2509.26030)

## ðŸŽ¯ Pitch

This paper reveals that the Muon optimizerâ€™s core advantage over Adam stems from its alignment with the associative memory structure of transformer models. By leveraging matrix-normâ€“based updates, Muon enables more balanced and isotropic learning of both common and rare (tail) knowledge, leading to markedly improved performance on heavy-tailed, real-world data. These insights not only demystify Muon's empirical success but also highlight where and why optimizer choice is critical for robust, equitable knowledge acquisition in large language models.

---

## 1. Executive Summary
This paper explains why the Muon optimizer trains transformers faster and better than Adam by tying Muonâ€™s update rule to how transformers store knowledge as â€œassociative memories.â€ Through targeted ablations, spectrum analyses, a heavyâ€‘tailed knowledge task, and a stylized theory, it shows that Muonâ€™s matrixâ€‘norm update aligns with the outerâ€‘product structure of these memories, yielding more isotropic (evenly distributed) learning and markedly better performance on rare (â€œtailâ€) facts.

## 2. Context and Motivation
- Problem addressed
  - Many groups report that Muon trains Large Language Models (LLMs) faster than Adam, but the mechanism remains unclear. Prior interpretations view Muon as steepest descent under a matrix operator norm, yet this alone does not explain the consistent empirical advantage over Adam (Introduction; discussion around steepest descent in Appendix A).
- Why this matters
  - Practical: Choosing optimizers and where to apply them is crucial for training efficiency and generalization in large models.
  - Scientific: Understanding how an optimizer interacts with internal â€œmemoryâ€ structures (like Feedâ€‘Forward Networks and attention projections) clarifies what LLMs learn and how they learn it.
- Prior approaches and limitations
  - Adam is wellâ€‘studied (Related Works, Â§2), with convergence analyses and featureâ€‘learning perspectives. But none explain why Muon, which normalizes matrix gradients by their spectral structure, beats Adam inside transformer layers.
  - Existing Muon analyses focus on optimization geometry (steepest descent in spectral norm, operatorâ€‘norm constraints) and convergence (Related Works, Â§2), but not on how Muon interacts with the specific memoryâ€‘storing parts of transformers.
- Positioning of this paper
  - The paper reframes the question through the lens of associative memory in transformers (Preliminaries, Â§3): Valueâ€“Output (VO) attention weights and Feedâ€‘Forward Networks (FFNs) behave like linear associative memories that store facts as sums of outer products. The core thesis is that Muonâ€™s update (orthogonal, spectrumâ€‘normalized) perfectly matches this outerâ€‘product structure, balancing learning across frequent and rare facts.

## 3. Technical Approach
The paper builds a multiâ€‘part argument combining implementation details, empirical tests, and theory.

- What is Muon?
  - For a matrix parameter W with gradient G, Muon maintains a momentum `B_t = Î¼ B_{t-1} + G_t` and updates using the orthogonal factor of B_t: compute SVD `B_t = U_t S_t V_t^T`, then set `O_t = U_t V_t^T` and update `W_{t+1} = W_t âˆ’ Î·_t O_t` (Preliminaries, Â§3). In practice it approximates `O_t` with Newtonâ€“Schulz iterations to avoid full SVD (Practical note in Â§3).
  - Intuition: Muon normalizes away singular values and keeps only the orthogonal â€œdirections,â€ so each singular direction contributes equally to the update. In steepestâ€‘descent terms, it performs descent under the matrix operator (spectral) norm (Appendix A).
- Associative memory view of transformer components
  - The paper treats `WO`, `WV` (the output and value projections in attention) and FFN weights (`W_in`, `W_out`, optional `W_gate`) as â€œassociative memory parameters.â€ A linear associative memory stores facts as `W = Î£_i e_o,i e_s,i^T` (outer products of â€œvalueâ€ and â€œkeyâ€ embeddings) so that `e_o = W e_s` (Preliminaries, Â§3; references to Geva et al., Bietti et al., etc.).
  - Key observation: If gradients for such memories decompose into outer products, Muonâ€™s `O = U V^T = Î£_i u_i v_i^T` updates each orthogonal component equally, counteracting imbalances in the singular values `S` that often encode frequency skew (Main Results, Â§4.1 end).
- Twoâ€‘stage ablation design to locate where Muon helps most (Main Results, Â§4.1; Fig. 1; Table 1)
  1) Independent Blocks: apply Muon to one block at a time (Q, K, V, O, and FFN parts), keep others on Adam.  
  2) Combined Configurations: combine the most promising blocks (e.g., VO+FFN) under Muon, leave the rest on Adam, and compare to â€œfull Muon.â€
  - Models and data: 160M NanoGPT on FineWeb; both nonâ€‘gated and gated FFN variants (details in Â§4.1 and Appendix B.1).
- Spectral analysis to test the isotropy hypothesis (Main Results, Â§4.2; Fig. 2)
  - Define four spectrum metrics for each weight matrix W with singular values Ïƒ:
    - Normalized SVD entropy (uniformity of energy),  
    - Effective rank (entropy perplexity),  
    - Topâ€‘k energy fraction (energy concentration),  
    - Q75/Q25 eigenvalue ratio (spread robust to outliers) (Definitions in Â§4.2).
  - Compare Muon vs Adam over training and across seeds to assess isotropy and stability.
- Heavyâ€‘tailed knowledge task to test tail learning (Main Results, Â§4.3; Fig. 3; Appendix B.2, C.4, C.5)
  - Synthetic QA over ~32,768 â€œclassesâ€ (individuals), with a powerâ€‘law frequency of training samples per class (Fig. 3a; parameter m=15; 6 QA pairs per selection).
  - Metric: First Token Accuracy (FTA) on answers. Optimizers: Muon, Adam, and SGD+Momentum. Also, hybrids: Muon on VO+FFN but Adam on QK, and vice versa (Fig. 3eâ€“f).
- Theory on a oneâ€‘layer linear associative memory (Case Study, Â§5; Theorems 5.3â€“5.4)
  - Setup: K triplets (subjectâ€“relationâ€“object) with orthonormal key/value embeddings `E` and `Ä’` (`E^T E = Ä’^T Ä’ = I`, Assumption 5.1), class imbalance split across two frequency groups (Assumption 5.2). Initialize `W_0 = 0`.
  - Compare oneâ€‘step (and multiâ€‘step) updates for three optimizers:
    - GD (`W_{t+1} = W_t âˆ’ Î· âˆ‡W L(W_t)`),
    - Adam without moving averages, which reduces to `SignGD` (elementâ€‘wise sign of gradient) for analysis (Preliminaries Â§5, â€œAdam â†’ SignGDâ€),
    - Muon without momentum (`W_{t+1} = W_t âˆ’ Î· U norm(Î£) V^T`).
  - Main questions: With at least one class achieving high probability (â‰¥ 1 âˆ’ Îµ), how low can the worst class probability be under each optimizer? How isotropic are the updates?

## 4. Key Insights and Innovations
- Insight 1 â€” Where Muon actually helps in transformers
  - Claim: Applying Muon to VO and FFN yields almost all the gain; applying it to QK gives little benefit.
  - Evidence: In 160M NanoGPT (nonâ€‘gated and gated FFN), the â€œIndependent Blocksâ€ experiment shows larger validationâ€‘loss gains for `WV`/`WO` and FFN than for `WQ`/`WK` (Fig. 1aâ€“b; Table 1). In â€œCombined Configurations,â€ Muon on VO+FFN nearly recovers the fullâ€‘Muon curve (Fig. 1câ€“d; Table 1).  
  - Quote:
    > Observation 1: Muon is most effective when applied to VO and FFN; in particular, applying Muon to only VO+FFN almost recovers the fullâ€‘Muon trajectory. (end of Â§4.1)
- Insight 2 â€” Muon induces more isotropic weight spectra, stably
  - Difference from prior work: The analysis focuses on associativeâ€‘memory parameters (VO, FFN) rather than aggregating all weights. Isotropy is tied to balanced learning of facts.
  - Evidence: Across training and random seeds, Muon shows higher normalized SVD entropy and effective rank, lower Topâ€‘k energy and Q75/Q25 ratios than Adam for VO and `W_out` (Fig. 2aâ€“d). Error bars are small for Muon and large for Adam, indicating stability (Main Results, Â§4.2).
  - Quote:
    > Observation 2: Muon consistently yields more isotropic weight matrices â€¦ throughout training and across random initializations. (Â§4.2)
- Insight 3 â€” Muon learns tail facts better on heavyâ€‘tailed data
  - Result: Muon matches Adam on head classes and exceeds it on tail classes, reducing the headâ€“tail gap and accelerating convergence (Fig. 3bâ€“d; Appendix C.4â€“C.5 tables). The VO+FFN hybrid reproduces most of this effect; QKâ€‘only does not (Fig. 3eâ€“f).
  - Quote:
    > Observation 3: In heavyâ€‘tailed, knowledgeâ€‘intensive tasks, Muon â€¦ substantially improving learning on tail classes. (Â§4.3)
- Insight 4 â€” Theory: Muonâ€™s balanced learning is intrinsic to its update rule
  - Theorem 5.3 (oneâ€‘step) and Theorem 5.4 (multiâ€‘step) show Muon achieves nearly equal correctâ€‘class probabilities across items once any item is nearâ€‘correct, regardless of embeddings (orthonormal), whereas GD and SignGD (Adam without EMA) can be highly imbalanced.  
  - Mechanism: Muonâ€™s update is the (almost) uniform orthogonal factor of the gradient (`U V^T`), so its singular values are nearly equalâ€”matching the isotropy seen empirically (see the gradient form and SVD reasoning in Â§5.2 and Appendix D/E).

## 5. Experimental Analysis
- Evaluation methodology
  - Where to apply Muon (ablation on transformer components):
    - 160M NanoGPT on FineWeb; both nonâ€‘gated and gated FFN. Two stages: Independent Blocks and Combined Configurations (Main Results, Â§4.1; Fig. 1; Table 1).
  - Spectrum analysis:
    - Track four isotropy metrics over training steps and random seeds for VO and FFN parameters (Main Results, Â§4.2; Fig. 2). Repeated at 0.7B scale with consistent trends (Appendix C.2; Figs. 6â€“7).
  - Heavyâ€‘tail knowledge task:
    - Synthetic QA dataset with powerâ€‘law class frequencies (Fig. 3a; Appendix B.2). Measure First Token Accuracy (FTA). Compare Muon, Adam, SGD+Momentum, and hybrids (Fig. 3bâ€“f; Appendix C.4â€“C.5 tables).
  - Additional checks:
    - Logitâ€‘explosion control for attention via RMSNorm applied to Q/K; no instability is observed (Appendix C.1, Table 2).
    - Orthogonality plausibility of embeddings in real models: average angles between FFN embeddings are near 90Â° in Llamaâ€‘3â€‘8Bâ€‘Instruct (Â§5.1; Fig. 4a; Appendix B.3, C.6).
- Main quantitative results
  - Ablations (160M, 10k steps; Table 1):
    - Full Muon lowers validation loss to 3.565 (nonâ€‘gated) vs Adam 3.924.
    - Muon only on VO+FFN reaches 3.586 (nonâ€‘gated) and 3.531 (gated), â€œnearly recoveringâ€ full Muon (Fig. 1câ€“d).
    - Muon on QK only: much weaker (3.893 nonâ€‘gated; 3.852 gated).
    - Within FFN, `W_out` benefits strongly; in ungated FFN, VO+`W_out` is close to full Muon (Fig. 1c).
  - Spectral isotropy (Fig. 2):
    - For VO and `W_out`, Muon shows higher SVD entropy and effective rank, lower Topâ€‘10 energy and Q75/Q25 ratio across 10k steps and across seeds. Adamâ€™s curves fluctuate more with seed choice.
  - Heavyâ€‘tail learning (Fig. 3; tables in Appendix C.4â€“C.5):
    - At 10k steps (nonâ€‘gated FFN, tail group 15): Muon FTA 0.976 Â± 0.006 vs Adam 0.264 Â± 0.048; VO+FFN hybrid: 0.954 Â± 0.021; QKâ€‘only hybrid: 0.286 Â± 0.039 (Table 5).
    - Head groups: all optimizers reach ~1.0 FTA by 10k steps (Fig. 3bâ€“d).
    - Trends replicate with gated FFN (Appendix C.5, Tables 6â€“8).
  - Scaling to 0.7B:
    - Muon > Adam in validation loss; VO+FFN hybrid ~ full Muon; QKâ€‘only weak (Fig. 5). Spectral isotropy advantages persist (Figs. 6â€“7).
- Do the experiments support the claims?
  - Yes, in four ways that interlock:
    1) Targeted ablations localize Muonâ€™s value to associativeâ€‘memory parameters (VO, FFN) (Fig. 1; Table 1).
    2) Spectrum metrics show Muon equalizes singular valuesâ€”consistent with the associative memory hypothesis (Fig. 2).
    3) Heavyâ€‘tail QA shows Muonâ€™s balanced learning translates to tailâ€‘class gains (Fig. 3; tables).
    4) Theory matches both isotropy and tail balance (Theorems 5.3â€“5.4).
- Notes on robustness and edge cases
  - Adamâ€™s instability across seeds is visible in spectrum metrics (Fig. 2). Muon is stable.
  - The â€œMaxLogit explosionâ€ is not a confound in this setup (Appendix C.1).
  - Similar findings appear at larger scale (0.7B) and with gated FFNs (Appendix C.2, C.5).

## 6. Limitations and Trade-offs
- Theoretical assumptions simplify reality
  - Orthonormal embeddings (`E^T E = Ä’^T Ä’ = I`) and oneâ€‘layer linear associative memory (Â§5.2, Assumption 5.1). This aligns with measured nearâ€‘orthogonality (Fig. 4a), but real models are deeper and nonlinear.
  - Twoâ€‘group class imbalance (Assumption 5.2). It captures the headâ€“tail effect but is simpler than a true power law. The proof techniques can extend, but the paper shows the twoâ€‘group case explicitly.
  - Adam is analyzed in the â€œnoâ€‘EMAâ€ limit as SignGD (Â§5.2). This isolates the elementâ€‘wise normalization mechanism but does not capture the full Adam dynamics with moving averages.
- Scope of empirical evidence
  - Main training curves use 10k steps on 160M and 0.7B NanoGPT; the work does not report endâ€‘toâ€‘end pretraining of very large LLMs nor taskâ€‘rich evaluations.
  - Heavyâ€‘tail task is synthetic (biographical QA) by design (Appendix B.2). It convincingly isolates the tailâ€‘learning effect, but broader generalization to diverse knowledge tasks remains to be shown.
- Computational considerations
  - Muon requires computing an orthogonal factor per matrix per step; the paper uses an efficient Newtonâ€“Schulz approximation (Preliminaries, Â§3), but realâ€‘world throughput and memory tradeâ€‘offs versus Adam are not benchmarked here.
- Architectural nuance
  - QK weights also become more isotropic under Muon (noted in Â§4.2), yet this does not translate to clear validationâ€‘loss gains in ablations (Fig. 1aâ€“d). When and how QK benefits behaviorally remains an open question.

## 7. Implications and Future Directions
- How this changes the fieldâ€™s understanding
  - It reframes optimizer choice as a question of aligning updates with model internals. For transformers, VO and FFN implement associative memories; Muonâ€™s orthogonal updates match these memoriesâ€™ outerâ€‘product structure, yielding balanced, tailâ€‘friendly learning.
  - Practically, one can deploy Muon selectively: using Muon for VO+FFN while keeping Adam on QK nearly recovers fullâ€‘Muon gains (Fig. 1câ€“d; Fig. 5), simplifying adoption and reducing overhead.
- Followâ€‘up research enabled
  - Largerâ€‘scale pretraining studies with selective Muon on VO+FFN to quantify wallâ€‘clock and energy savings while tracking tailâ€‘knowledge performance.
  - Extending the theory beyond orthonormal embeddings, multiple layers, and beyond twoâ€‘group imbalance; analyzing Muon with momentum and Adam with EMA.
  - Investigating how isotropy interacts with other desiderata (e.g., sparsity, lowâ€‘rank structure, retrieval augmentation).
  - Generalizing the â€œouterâ€‘product alignmentâ€ idea to higherâ€‘order tensor memories (Conclusion, Â§6).
- Practical applications
  - Knowledgeâ€‘intensive LLMs in domains with heavyâ€‘tailed data (rare entities, longâ€‘tail relations): Muon on VO+FFN should improve recall of rare facts without sacrificing head performance (Fig. 3; Tables 3â€“5, 6â€“8).
  - Knowledge editing and safety: isotropic, balanced memories may yield more predictable edits in `W_out`/`WO` (ties to prior work cited in Â§2â€“Â§3).
  - Model scaling and efficiency: selective Muon can be a dropâ€‘in optimizer policy for specific parameter groups to improve sample efficiency early in training (Fig. 1, Fig. 5).

> Core takeaway: Muonâ€™s orthogonal, spectrumâ€‘normalized updates align with the associativeâ€‘memory structure of transformers. This alignment equalizes learning across singular directions, keeps spectra isotropic and stable, andâ€”on heavyâ€‘tailed dataâ€”translates into superior tail learning while maintaining strong head performance (Observations 1â€“3; Theorems 5.3â€“5.4; Figs. 1â€“3).

# OPT: Open Pre-trained Transformer Language Models

**ArXiv:** [2205.01068](https://arxiv.org/abs/2205.01068)

## ðŸŽ¯ Pitch

OPT delivers an unprecedented open suite of decoder-only transformer models ranging from 125M to 175B parameters, matching GPT-3's performance while releasing comprehensive training code, model weights (â‰¤66B openly, 175B via request), and detailed training documentation. This openness empowers the research community to systematically study, reproduce, and improve large language modelsâ€”making progress on robustness, bias, and efficiency possible for many, and ensuring the next generation of language AI is both scientifically rigorous and socially responsible.

---

## 1. Executive Summary
OPT introduces an open suite of decoderâ€‘only transformer language models from 125M to 175B parameters that closely tracks GPTâ€‘3 performance while releasing training code, models (up to 66B openly, 175B by request), and a detailed training logbook. It matters because it turns a largely closed research area into one that many groups can reproduce and scrutinize, and it shows that a GPTâ€‘3â€‘class model can be trained with substantially lower energy and compute overhead.

## 2. Context and Motivation
- Problem addressed:
  - The strongest large language models (LLMs) require vast compute and are largely inaccessible; researchers typically cannot inspect weights or training procedures. This hinders reproducibility and limits community understanding of robustness, bias, and safety issues.
- Importance:
  - Realâ€‘world: LLMs are central to search, assistants, dialogue, and downstream NLP. Responsible deployment needs community auditing, which requires access to models and training details.
  - Scientific: Reproducible baselines at multiple scales enable controlled studies of scaling laws, data curation, safety interventions, and decoding strategies.
- Prior approaches and gaps:
  - GPTâ€‘3 and successors are available primarily through paid APIs with hidden weights and safety layers.
  - Some open initiatives exist (e.g., EleutherAI up to 20B parameters, BigScience/BLOOM; see Introduction and Related Work) but not a GPTâ€‘3â€‘class suite with matching evaluation setup plus training process transparency.
- Positioning:
  - OPT offers a GPTâ€‘3â€‘style suite (125Mâ†’175B) with nearâ€‘parity performance on standard prompts, full training details, code (â€œmetaseqâ€), a release of all models â‰¤66B, and research access to 175B (Abstract; Sections 1, 2). It emphasizes efficient training and full documentation of midâ€‘flight interventions and failures.

## 3. Technical Approach
This is a practical recipe for training GPTâ€‘3â€‘style models efficiently and transparently.

- Model family and architecture (Section 2.1; Table 1):
  - Nine decoderâ€‘only transformers from `125M` to `175B` parameters.
  - For each size, Table 1 specifies layers (`#L`), attention heads (`#H`), hidden size (`dmodel`), peak learning rate, and global batch (tokens). Example: the `175B` model uses `96` layers, `96` heads, `dmodel=12288`, peak LR `1.2eâˆ’4`, batch `2M` tokens.
  - Design choices largely mirror GPTâ€‘3 to reduce instability risk but adjust batch sizes for throughput.
  - Activation: ReLU (not GELU); sequence length 2048; dropout 0.1 except embeddings (Section 2.2).

- Optimization and initialization (Section 2.2):
  - Weight init: normal with std 0.006; final output layer std scaled by `1/âˆš(2L)` where `L` is layers.
  - Optimizer: AdamW with Î²1=0.9, Î²2=0.95, weight decay 0.1.
  - LR schedule: linear warmâ€‘up then decay; 175B warms up over first 2000 steps, smaller models over 375M tokens; decay to 10% of peak by 300B tokens. Midâ€‘flight manual LR reductions are applied for stability (Section 2.5; Figure 1).
  - Gradient clipping initially at 1.0, later lowered to 0.3 to stabilize training spikes (Section 2.5).
  - Gradient preâ€‘divide factor: split global gradient division across two operations by `âˆšN` to reduce numerical under/overflows when aggregating across `N` processes (Section 2.2).
  - Dynamic loss scaling: scale loss during mixedâ€‘precision training to avoid underflow; a â€œloss scalarâ€ is adaptively adjusted (Section 2.4).

- Training data and preprocessing (Section 2.3):
  - Corpus combines: RoBERTa components (BookCorpus, Stories, CCNews v2), a curated subset of The Pile (CommonCrawl, OpenWebText2, Wikipedia, Project Gutenberg, DM Mathematics, HackerNews, OpenSubtitles, USPTO), and a subset of Pushshift Reddit transformed to linear threads by selecting each threadâ€™s longest comment chain.
  - Total size: ~180B tokens, tokenized with GPTâ€‘2 byteâ€‘level BPE.
  - Deâ€‘duplication with `MinHashLSH` using a Jaccard similarity threshold â‰¥0.95 to remove nearâ€‘duplicate documents across datasets. MinHashLSH is a localityâ€‘sensitive hashing technique that quickly estimates set similarity; here it helps prevent the model from overfitting repeated text.

- Systems and parallelism (Section 2.4):
  - Uses `Fully Sharded Data Parallel (FSDP)` combined with `Megatron-LM Tensor Parallelism`.
    - FSDP shards model parameters, gradients, and optimizer state across GPUs to fit larger models in memory.
    - Tensor parallelism splits large matrix multiplications within layers across GPUs to increase compute throughput.
  - Mixed precision: parameters in FP16, optimizer states in FP32 (for numerical stability).
  - Scale and efficiency:
    > â€œWe trained OPTâ€‘175B on 992 80GB A100 GPUs â€¦ reaching 147 TFLOP/s utilization per GPU.â€ (Section 2.4)
  - Failure handling and restarts:
    > â€œat least 35 manual restarts and an estimated 70+ automatic restarts due to hardware failures â€¦ over 2 monthsâ€ (Section 2.5). Nodes were diagnosed and cordoned; training resumed from checkpoints.

- Stabilizing instabilities (Section 2.5; Figures 1â€“2):
  - Observed correlation between divergences, the dynamic loss scalar collapsing to 0, and spikes in finalâ€‘layer activation L2 norms.
  - Strategy:
    - Reduce LR and restart from a checkpoint where the loss scalar is â€œhealthyâ€ (â‰¥1.0).
    - Lower gradient clip threshold from 1.0 to 0.3 early in training.
    - Reset loss scalar; test switching to SGD (did not help); upgrade Megatron version (reduced activation norm pressure).
  - Evidence:
    > Figure 1 shows the â€œEmpirical Learning Rateâ€ that was lowered multiple times midâ€‘flight, and Figure 2 shows the corresponding effects on validation perplexity curves.

- Decoding and evaluation protocols:
  - For standard NLP tasks, reuse GPTâ€‘3 prompts and settings; formulate WSC as multiple choice as in GPTâ€‘3 (Section 3.1).
  - For dialogue generation, use greedy decoding up to 32 tokens and a minimal dialogue prompt structure (â€œPerson 1:â€ / â€œPerson 2:â€) for OPTâ€‘175B; compare against supervised BlenderBot variants that use tuned decoding (Section 3.2).

## 4. Key Insights and Innovations
- Opening a GPTâ€‘3â€‘class suite with process transparency (fundamental contribution):
  - OPT provides models across scales, code (â€œmetaseqâ€), and a unique training logbook with dayâ€‘toâ€‘day decisions and failures (Sections 1, 6). This kind of transparencyâ€”e.g., explicit LR changes, gradient clipping adjustments, and hardware failure rates (Section 2.5)â€”has been missing in prior megaâ€‘model releases.
- Efficient largeâ€‘model training without pipeline parallelism (systems innovation):
  - Demonstrates that combining FSDP with tensor parallelism suffices to train a 175B decoderâ€‘only model on Nvidia GPUs without pipeline parallelism (Section 6). This simplifies training orchestration and is the â€œonly known openâ€‘source implementation â€¦ â‰¥175B â€¦ without the use of pipeline parallelismâ€ at publication time.
- Lower energy footprint for GPTâ€‘3â€‘class training (practical and environmental significance):
  - OPTâ€‘175Bâ€™s estimated CO2eq is 75 tons versus GPTâ€‘3â€™s reported ~500 tons and Gopherâ€™s ~380 tons (Section 6), roughly 1/7th the footprint.
- Thorough, mixed-result safety/ethics evaluation at 175B scale (important capability and caution):
  - OPTâ€‘175B shows stronger hateâ€‘speech detection in ETHOS fewâ€‘shot settings (Table 3) but higher toxicity generation on RealToxicityPrompts and more stereotypical bias on CrowSâ€‘Pairs (Table 4; Figure 5). The paper connects these outcomes to data composition choices (e.g., Reddit inclusion).

These are more than incremental tweaks: they change who can study LLMs at scale and how such models are trained and evaluated in the open.

## 5. Experimental Analysis
- Evaluation methodology (Section 3):
  - Standard NLP prompting on 16 tasks: HellaSwag, StoryCloze, PIQA, ARCâ€‘E, ARCâ€‘C, OpenBookQA, Winograd, WinoGrande, and SuperGLUE tasks (BoolQ, CB, COPA, WSC, MultiRC, WiC, RTE, ReCoRD). Accuracy is the primary metric. WSC is multipleâ€‘choice formatted as in GPTâ€‘3 (which affects scores).
  - Averages in Figures 3â€“4 exclude MultiRC and WiC because they â€œsystematically favorâ€ one family.
  - Dialogue evaluation on ConvAI2, Wizard of Wikipedia (WoW), Empathetic Dialogues (ED), Blended Skill Talk (BST), and Wizard of Internet (WoI), reporting perplexity and Unigram F1 (UF1) token overlap (Section 3.2; Table 2).
    - Unigram F1: harmonic mean of precision/recall on unigrams; a measure of lexical overlap with reference responses.
  - Safety/bias evaluations: ETHOS hateâ€‘speech detection (F1), CrowSâ€‘Pairs (percentage preferring stereotypical sentences; lower is better), StereoSet with LMS, SS, and ICAT (higher ICAT is better), RealToxicityPrompts (toxicity probability vs prompt toxicity), and Dialogue Safety Benchmarks (Section 4; Tables 3â€“6; Figure 5).
    - LMS (Language Modeling Score) captures how well a model assigns probability to sensible sentences; SS (Stereotype Score) measures preference for stereotypical content; ICAT balances both.

- Main quantitative results:
  - NLP zeroâ€‘shot averages (Figure 3):
    > â€œAcross a variety of tasks and model sizes, OPT largely matches the reported averages of GPTâ€‘3.â€ The trendlines for OPT and GPT are very close across 125Mâ†’175B.
  - Oneâ€‘shot and 32â€‘shot (Figure 4):
    > OPT lags slightly behind GPTâ€‘3 on average fewâ€‘shot accuracy, with substantial perâ€‘task variance (Appendix A, Figures 6â€“7). Performance is â€œsimilarâ€ on 10 tasks but consistently underperforms GPTâ€‘3 on MultiRC; several tasks exhibit scaleâ€‘instability (BoolQ, CB, WSC, RTE).
  - Dialogue (Table 2):
    - On ConvAI2, unsupervised OPTâ€‘175B achieves ppl 10.8 and UF1 0.185 vs supervised BlenderBot1 ppl 10.2, UF1 0.183; Reddit 2.7B unsupervised is much worse (ppl 18.9, UF1 0.126).
    - On WoI (unsupervised for all models), OPT has the best perplexity (12.0) but lower UF1 (0.147) than supervised models (0.154â€“0.160).
    - Generalization check: on ConvAI2 hidden test (not in pretraining), OPTâ€‘175B reaches ppl 10.7, UF1 0.185; on MSC (ConvAI2â€‘like), ppl 9.7, UF1 0.177â€”suggesting genuine skill rather than leakage (Section 3.2).
  - Hateâ€‘speech detection (Table 3):
    > OPTâ€‘175B outperforms the Davinci API across setups: zeroâ€‘shot F1 0.667 vs 0.628; oneâ€‘shot 0.713 vs 0.616; fewâ€‘shot binary 0.759 vs 0.354; fewâ€‘shot multiclass 0.812 vs 0.672.
  - CrowSâ€‘Pairs (Table 4):
    > Overall stereotypical preference is higher (worse) for OPTâ€‘175B (69.5) than GPTâ€‘3 (67.2); worse in most categories except religion.
  - StereoSet (Table 5):
    > Overall ICAT is similar (60.0 OPT vs 60.8 Davinci); OPT shows lower SS (less stereotypical preference) but also lower LMS (worse language modeling score), balancing out.
  - RealToxicityPrompts (Figure 5):
    > OPTâ€‘175B has higher toxicity probability than Davinci and PaLM across promptâ€‘toxicity bins, and toxicity rises with prompt toxicity for all models.
  - Dialogue safety (Table 6):
    > OPTâ€‘175Bâ€™s unitâ€‘test safety scores are comparable to Reddit 2.7B and worse than supervised BlenderBot variants, especially on â€œUnsafeâ€ prompts (0.567 vs 0.250â€“0.289; lower is better).

- Do the experiments support the claims?
  - Matching GPTâ€‘3 class: Figures 3â€“4 and Appendix plots show OPT roughly tracks GPTâ€‘3 averages across sizes; perâ€‘task deviations and the MultiRC discrepancy are acknowledged.
  - Efficient training: Sections 2.4â€“2.5 document utilization (147 TFLOP/s/GPU), largeâ€‘scale training without pipeline parallelism, and careful handling of instabilities; Section 6 contrasts estimated carbon footprints.
  - Safety and bias: Mixed results support the conclusion that training on broad, minimally moderated corpora yields strong awareness of toxic language (good detection) but higher propensity to generate it and to encode some stereotypes (Tables 3â€“4; Figure 5).

- Ablations/robustness/failure cases:
  - Midâ€‘flight interventions (Section 2.5) function as â€œablationâ€‘likeâ€ observations: LR reductions and clipâ€‘norm changes quell divergence; switching to vanilla SGD did not help; upgrading Megatron improved activationâ€‘norm behavior.
  - Known pitfalls: inconsistent scale effects on small validationâ€‘set tasks (CB, BoolQ, WSC), and underperformance on MultiRC despite attempts to replicate GPTâ€‘3â€™s setup (Section 3.1).
  - Promptâ€‘sensitivity remains; WSC reformulation is known to alter difficulty (Section 3.1).

## 6. Limitations and Trade-offs
- Data composition and safety (Sections 4, 5 Limitations):
  - Inclusion of Reddit and other web corpora likely increases exposure to toxic and stereotyped text; this correlates with higher CrowSâ€‘Pairs bias and RealToxicityPrompts toxicity generation.
  - The models can produce incorrect facts, repetitive loops, and fail at direct instruction following; instruction tuning or RLHF is not applied.
- Prompting sensitivity (Section 5 Limitations):
  - â€œDeclarative instructions or pointâ€‘blank interrogativesâ€ often elicit metaâ€‘dialogue rather than task execution; performance varies with prompt wording and fewâ€‘shot ordering.
- Compute and engineering assumptions (Sections 2.4â€“2.5):
  - Requires access to ~1,000 A100â€‘80GB GPUs, robust checkpointing, and the metaseq stack; training is nonâ€‘trivial due to frequent hardware failures and numerical instabilities.
- Evaluation ambiguities (Section 3.1; Appendix A):
  - Some tasks have very small validation sets; OPT cannot replicate GPTâ€‘3â€™s MultiRC and WiC results under the same public prompts; Davinci API results may reflect undocumented safety layers.
- Not productionâ€‘ready (Section 5 and D.2):
  - The release intentionally avoids safety fineâ€‘tuning; the license is nonâ€‘commercial; the authors explicitly caution against deployment without mitigations.

## 7. Implications and Future Directions
- How this changes the field:
  - Provides a reproducible, GPTâ€‘3â€‘class baseline with code and weights for the research community; enables systematic studies of scaling, data curation, safety mitigations, and decoding beyond APIâ€‘bound experiments.
  - Demonstrates that FSDP+tensor parallelism can train 175B models efficiently without pipeline parallelism, simplifying future largeâ€‘model training stacks.
- Followâ€‘up research enabled:
  - Instruction tuning and RLHF on OPTâ€‘175B to address instructionâ€‘following weaknesses; Section 5 suggests InstructGPTâ€‘style approaches.
  - Retrievalâ€‘augmented generation to improve factuality (Section 5 cites several retrievalâ€‘based methods).
  - Safety mitigations and audits: targeted data filtering, debiasing methods (e.g., selfâ€‘debiasing, unlikelihood training), and postâ€‘training safety layers; dialogueâ€‘specific safety finetuning improves toxicity (Table 6).
  - Data governance and carbon accounting: the logbook and footprint estimates (Section 6) invite standardized reporting, failureâ€‘overhead accounting, and embodiedâ€‘carbon analysis of hardware.
- Practical applications and use cases:
  - Researchâ€‘only use in: dialogue, fewâ€‘shot prompt design, safetyâ€‘tech evaluation, model analysis (e.g., scaling laws, activation norm monitoring), and systems research on distributed training.
  - As an open baseline, OPTâ€™s smaller models (â‰¤66B) are directly usable for downstream finetuning studies; the 175B research access supports controlled experiments at stateâ€‘ofâ€‘theâ€‘art scale.

> Key numbers and artifacts to remember:
> - Training: 992 A100â€‘80GB GPUs; 147 TFLOP/s/GPU; dynamic loss scaling; numerous restarts (Sections 2.4â€“2.5).
> - Data: ~180B tokens; GPTâ€‘2 BPE; heavy deâ€‘dup with MinHashLSH at Jaccard â‰¥0.95; Reddit longestâ€‘chain transformation (Section 2.3).
> - Performance: OPT â‰ˆ GPTâ€‘3 averages (Figures 3â€“4); competitive unsupervised dialogue perplexity and UF1 vs supervised models on ConvAI2 (Table 2).
> - Safety: Higher hateâ€‘speech detection F1 in fewâ€‘shot (Table 3) but higher toxicity continuation rates (Figure 5) and more bias on CrowSâ€‘Pairs (Table 4).
> - Footprint: ~75 tons CO2eq vs ~500 for GPTâ€‘3 (Section 6).

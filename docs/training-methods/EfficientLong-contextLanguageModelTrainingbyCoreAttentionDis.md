# Efficient Long-context Language Model Training by Core Attention Disaggregation

**ArXiv:** [2510.18121](https://arxiv.org/abs/2510.18121)

## ğŸ¯ Pitch

This paper introduces Core Attention Disaggregation (CAD), a novel system that isolates and distributes the parameter-free, computationally intensive core attention operation across a dedicated pool of 'attention servers.' By dynamically partitioning attention computation into fine-grained, token-level tasks and overlapping communication with computation, CAD eliminates training slowdowns caused by load imbalance in long-context language models. This approach, implemented in the DistCA system, enables up to 1.35Ã— faster end-to-end training for extremely long contexts (up to 512K tokens on 512 H200 GPUs), providing scalable throughput improvements critical for next-generation LLM workloads that demand both long-range reasoning and efficient hardware utilization.

---

## 1. Executive Summary
This paper introduces core attention disaggregation (CAD), a training-time system that separates the heavy, parameter-free part of attentionâ€”softmax(QKáµ€)V, called core attention (CA)â€”from the rest of the transformer and executes it on a shared pool of â€œattention servers.â€ By dynamically partitioning sequences into token-level tasks and aggressively overlapping communication with computation, the system (DistCA) removes data-parallel and pipeline-parallel stragglers and speeds up long-context LLM training by up to 1.35Ã— on 512 H200 GPUs and 512K context length (Abstract; Â§6.2â€“6.3, Fig. 10).

## 2. Context and Motivation
- Problem addressed
  - Long-context training suffers from severe load imbalance because the computation in CA grows quadratically with sequence length while other layers grow roughly linearly (Â§1, Table 1). When documents of different lengths are packed together into fixed-size chunks, microbatches (or pipeline stages) with longer documents spend much more time in attention, causing stragglers that idle other devices (Â§1, Fig. 1).
- Why it matters
  - Real workloads increasingly demand long contexts (reasoning chains, multi-file code repositories) and training must expose models to long documents to generalize at inference (Â§1). Without addressing the imbalance, large clusters waste compute and memory, and speedups degrade at scale (straggler amplification in DP and PP; Â§1â€“Â§2.2).
- Prior approaches and their shortcomings
  - Variable-length data chunks: redistribute documents so each replicaâ€™s total attention FLOPs matches. This balances compute but causes memory imbalance because activation memory scales with total tokens, not attention FLOPs (Â§3.2). At 512K context with Llamaâ€‘8B, idle time from attention imbalance still rises to 19% at DP=4 and 55% at DP=8 once memory caps prevent further rebalancing (Fig. 4b); memory divergence also grows with DP size (Fig. 4a).
  - Per-document context parallelism (CP): shard each document across GPUs and all-gather K/V for CA (Â§2.2). At scale it suffers (i) underfilled kernels for short documents (FA throughput drops when shard <128 tokens; Fig. 5), (ii) communication dominated by K/V all-gathers that grow with global tokens (from ~3% of step time on 2 nodes to ~40% on 32 nodes; Fig. 3a), and (iii) rising KV memory pressure on the last CP rank (from ~3% to ~30% memory; Fig. 3b). Fig. 6 shows that trading DP and CP cannot escape this overhead/imbalance trade-off.
- Positioning
  - The paper separates the parameter-free CA from the rest of the model to decouple quadratic and linear work (Â§3.3). Unlike CP, which enforces uniform sequence splits and per-layer all-gathers, CAD partitions at token granularity, rebatches shards into high-occupancy kernels, and schedules them across a dedicated compute poolâ€”while fully hiding extra communication via a pingâ€‘pong scheme (Â§4.1, Â§6.3, Fig. 11).

## 3. Technical Approach
High-level idea: treat core attention as a stateless, compute-bound service that can be scheduled independently of the rest of the model.

A. What is being separated?
- Distinguish two parts of a transformer layer (Â§2.1, Fig. 1):
  - `Context-independent layers`: token-wise operations (QKV projection, output projection, FFN, layer norm) whose compute and activation memory scale ~linearly with tokens. Dominated by GEMMs.
  - `Core attention (CA)`: the parameter-free computation `P = softmax(QKáµ€)` followed by `O = PV`. Modern kernels (e.g., FlashAttention) avoid materializing `P`, recomputing it in backward; CA has negligible transient state and no trainable parameters (Â§2.1).
- Complexity model (Â§3.1): for a document of length `l`, FLOPs(l) = Î± lÂ² + Î² l and activations M(l) = Î³ l (Table 1 and Â§3.1). The quadratic Î± lÂ² term is CA; the linear Î² l term is everything else.

B. How CAD executes a batch (Â§4.1; Fig. 2)
- Token-level partitioning into `CA-tasks`
  - Each document is split into non-overlapping token â€œquery shards.â€ A `CA-task` t is defined by a pair `(q(t), kv(t))`â€”the query shard tokens and the K/V range needed as context (Â§4.1). A documentâ€™s CA equals the collection of its tasks.
- Attention servers
  - A pool of GPUs that accept `CA-tasks`, batch them, and run a single high-occupancy attention kernel (e.g., a single FlashAttention call) across fused shards. Because modern kernels sustain throughput based on total fused tokens rather than their document of origin, shards from different documents/stages can be recombined without loss as long as each shard is â‰¥ the kernel tile (128 tokens in FA2; Fig. 5 and Â§3.3).
- In-place attention servers
  - Instead of dedicating separate GPUs (which would leave memory underutilized because CA is stateless while FFN dominates memory; Fig. 3b), each GPU time-shares: it alternates between computing context-independent layers and serving CA for others (Â§4.1).
- Pingâ€‘pong overlap to hide communication
  - Each microbatch is split into two equal-size â€œnano-batches,â€ Ping and Pong. The runtime fuses post-CA of layer i with pre-CA of layer i+1 (both context-independent) and pipelines them so that inter-node communication for CA of one nano-batch overlaps with compute on the other (Fig. 7; Â§4.1). It also overlaps intra-node TP traffic (NVLink) with inter-node CA traffic (InfiniBand).
- Integration with pipeline parallelism (PP)
  - Because CA has no weights, `CA-tasks` from different PP stages are indistinguishable from DP shards, enabling global load balancing across the attention server pool (Â§4.1). To avoid idling when swapping roles, the schedule synchronizes the phase (all forward or all backward) across stages within a tick and defers some backward microbatches into the pipeline bubbles at the end (Fig. 8). Warmup/drain bubbles are also repurposed for CA work.

C. Communication and scheduling (Â§4.2; App. Aâ€“B)
- Communication pattern
  - Instead of CPâ€™s all-gather, CAD uses all-to-all to send only needed shards of Q/KV to the chosen attention server (Â§3.3). Later query shards (with more context) can be spread across servers to avoid bottlenecks (Â§3.3).
- How much communication can be hidden?
  - Upper bound derivation (App. A): if per-token time for context-independent layers is `t` and network bandwidth is `B`, a document can be evenly split into `s â‰¤ 2(tB âˆ’ hq)/hkv âˆ’ 1` shards while fully hiding comm under compute. For Llamaâ€‘34B on H200s with 50 GB/s IB, this yields `s â‰ˆ 31` (App. A), and the bound increases for larger models (because `t` grows with hidden size).
- Communication-aware greedy scheduler (Â§4.2)
  - Profiler: build a latency/throughput table over a grid of `(query length, kv length)` to predict runtime per `CA-task` by interpolation, saturated at peak throughput when applicable.
  - Scheduling unit: an `Item` is either a complete document or one shard; each `Item` maps to one `CA-task`. Scheduler input is batch `B` and number of attention servers `n`.
  - Target load and source/target sets: compute ideal per-server load FÌ„ (sum of `Item` FLOPs divided by `n`) and label servers as `surplus` (load > FÌ„) or `deficit` (load < FÌ„).
  - Migration selection: iterate deficit servers; for each, consider candidate `Items` on surplus servers. For each candidate, compute the maximum transferable FLOPs `Î”Fmax = min(FItem, Ssource, Ddest)` and estimate bytes `Vcomm` to move that shard (App. B gives the closed-form that minimizes bytes for a target `Î”Fmax`, with headâ€“tail constraints for accurate FLOP/time prediction). Rank candidates by efficiency `E = Î”Fmax / Vcomm` and move the best. If only part is needed, split the `Item` accordingly.
  - Stop when each server is within `Îµ FÌ„` FLOPs of the target or further moves have poor `E`. The `Îµ` tolerance trades some imbalance for less communication (Â§4.2; ablation in Fig. 12).

D. System implementation (Â§5)
- ~2K lines of Python for DistCA runtime, plus ~1K CUDA/C++ lines to implement a fast all-to-all using NVSHMEM (Â§5). Integrated into Megatron-LM for the rest of the stack (token-independent layers, 4D parallelism, pipeline engine).

## 4. Key Insights and Innovations
- Disaggregating only core attention (CA) is both safe and powerful (Â§3.3; Â§2.1)
  - Novelty: prior systems kept attention co-located with other layers or used CP to shard whole sequences. CAD splits only CA because it is stateless and composable; training weights never cross the boundary, and the only state is ephemeral softmax stats. This makes the problem a pure scheduling of compute-bound tasks.
  - Significance: enables independent scaling of the quadratic work (CA) separate from linear work (FFN/linears), eliminating DP/PP stragglers without creating activation-memory imbalance (Abstract; Â§1, Â§3.1).
- Token-level partitioning with kernel composability (Â§3.3; Fig. 5)
  - Novelty: arbitrary token shards from multiple documents are rebatched into one high-occupancy FA kernel, provided shards are â‰¥ tile size (128 tokens). This departs from CPâ€™s uniform, per-document splits and allows flexible balancing that preserves MFU.
  - Significance: balances CA load near-perfectly without sacrificing kernel efficiency (Fig. 5 shows saturated throughput above 128 tokens).
- In-place attention servers + pingâ€‘pong overlap (Â§4.1; Fig. 3b, Fig. 7)
  - Novelty: instead of a separate CA cluster (which would waste memory), each GPU time-shares between context-independent compute and serving CA, while the pingâ€‘pong scheme overlaps inter-node comm with compute.
  - Significance: virtually hides CADâ€™s communication (Fig. 11 shows DistCA matches a â€œSignal-onlyâ€ control where each transfer is 1 byte), and keeps memory utilization high (FFN dominates memory; Fig. 3b).
- Communication-aware greedy scheduler with closed-form bytes minimization (Â§4.2; App. B)
  - Novelty: a pragmatic balancing algorithm that maximizes â€œbytes-per-FLOP movedâ€ by choosing shard sizes that minimize communication for a target `Î”Fmax`.
  - Significance: reduces traffic by 20â€“25% at similar latency when tuning the imbalance tolerance from 0 to 0.15 (Fig. 12), and supports large-scale all-to-all without exposing comm on the critical path.

## 5. Experimental Analysis
- Setup (Â§6.1; Tables 2â€“4)
  - Models: Llamaâ€‘3â€‘8B and Llamaâ€‘34B (Table 2).
  - Hardware: DGX H200 nodes, 8Ã—140GB H200 per node.
  - Parallelism: TP fixed to 8; PP grid-searched; DP/CP swept for baselines. DistCA replaces CP (Â§6.1).
  - Datasets: two synthetic distributionsâ€”â€œPretrainâ€ (upsampled long docs) and â€œProLongâ€ (public mixture of long/short; more long docs) (Â§6.1).
  - Baseline: WLBâ€‘LLM reimplemented with adaptive CP and variable-length packing (without deferred execution; Â§6.1). Denoted â€œWLBâ€‘ideal.â€
- Evaluation methodology
  - 3D (no PP) and 4D (with PP) experiments across context windows up to 512K and up to 512 GPUs (Tables 3â€“4). Report average throughput over 30 sampled batches per setting (Â§6.1â€“6.2).
- Main results
  - 3D (no PP): DistCA outperforms WLBâ€‘ideal by 1.07â€“1.20Ã— on Pretrain and 1.05â€“1.12Ã— on ProLong across batch sizes and GPU counts (Fig. 9). Gains are larger when document length diversity is higher (e.g., 34B at larger MaxDocLen; Â§6.2 discussion).
  - 4D (with PP): DistCA achieves 1.15â€“1.30Ã— speedup (8B, Pretrain) and 1.10â€“1.35Ã— (8B, ProLong), and up to 1.15Ã— (34B, Pretrain) and 1.25Ã— (34B, ProLong), with sustained scaling across 16/32/64 (8B) and 128/256/512 GPUs (34B) (Fig. 10). DistCA also leverages pipeline bubbles for CA work (Fig. 8), removing PP stragglers (Â§4.1, Â§6.2).
  - Ablations (Fig. 11, Fig. 12; plus Fig. 5)
    - Communication overlap: DistCAâ€™s latency closely matches a â€œSignal-onlyâ€ control (1-byte transfers), indicating comm is nearly fully hidden. Removing pingâ€‘pong (â€œSingle Streamâ€) raises latency by 10â€“17%.
    - Scheduler tolerance: raising `Îµ` from 0 to ~0.15 reduces communication by 20â€“25% with negligible latency change for 8B/8â€“16 nodes; for 34B, too small `Îµ` (<0.10) forces excess communication that can no longer be hidden (latency increases), while too large `Îµ` causes load imbalance (Fig. 12).
    - Kernel composability: FA throughput saturates when shard length â‰¥128 tokens (Fig. 5), justifying shard sizing.
- Do the experiments support the claims?
  - The combination of throughput gains, straggler elimination in both DP and PP (design analysis Â§4.1, empirical scaling Fig. 10), and ablations isolating communication overlap make a consistent case that CADâ€™s communication is hidden and compute is balanced.
  - CADâ€™s advantages grow with longer contexts and more diverse lengths (Fig. 9â€“10), precisely the regime where prior methods struggle (Fig. 3â€“6), supporting the paperâ€™s motivation.
- Notable observations and caveats
  - Memory fragmentation from dynamic tensor shapes causes CPU-side allocator overhead in 34B/4D runs (delays kernel launches), slightly limiting gains; the paper proposes static allocation and CUDA Graphs as future remedies (Â§6.2).

> â€œDistCA improves end-to-end training throughput by up to 1.35Ã—, eliminates DP/PP stragglers, and maintains near-perfect compute and memory balance.â€ (Abstract; supported by Fig. 9â€“10 and Â§6.2â€“6.3)

## 6. Limitations and Trade-offs
- Assumptions enabling CAD
  - Modern attention kernels achieve high MFU when shards are at least the tile size (128 tokens here; Fig. 5). Very short documents or fragmentary sharding below this size hurts efficiency (Â§3.3).
  - Sufficient network bandwidth so that CA communication can be overlapped. The bound in App. A indicates â€œfreeâ€ partitioning up to ~31 shards for Llamaâ€‘34B on 50 GB/s IB; weaker networks would lower this bound.
- Unaddressed scenarios
  - The scheduler currently restricts each `CA-task` to a Q shard with the full K/V context range; it does not split the K/V range itself. Allowing partial K/V ranges could improve flexibility (Â§8).
  - Communication modeling pessimistically assumes all tokens are transferred and ignores K/V already present on the destination, potentially overestimating bytes (Â§8).
  - The method targets self-attention in transformer decoders during training. Cross-attention or encoderâ€“decoder variants may require additional engineering.
- System-level constraints
  - Dynamic shapes create memory fragmentation and PyTorch GC overhead at large scale (34B, 4D), reducing peak achievable speedups; requires allocator/graph capture improvements (Â§6.2).
  - Complexity: the runtime introduces a central scheduler, profiling tables, and an all-to-all communication substrate (NVSHMEM). Operational simplicity may be lower than baseline training stacks.
- Trade-offs controlled by hyperparameters
  - Balance vs. bytes: the schedulerâ€™s imbalance tolerance `Îµ` trades FLOP balance for communication volume. As shown in Fig. 12, the sweet spot depends on model/cluster scale.

## 7. Implications and Future Directions
- How this changes the landscape
  - Establishes a new decomposition axis for training: instead of sharding sequences or models uniformly, separate the â€œstateless, quadraticâ€ component (CA) and schedule it like a pooled service. This is a conceptual shift away from CP and toward compute disaggregation guided by kernel properties (Â§3.3; Â§4.1).
- Practical applications
  - Training LLMs at 100Kâ€“1M context with fewer stragglers and better GPU utilization; integrating with existing DP/TP/PP pipelines (Megatron-compatible, Â§5).
  - Potential to reduce cost for long-context pretraining and domain-specific fine-tuning (e.g., code, retrieval-augmented settings) by maintaining linear memory use while balancing quadratic compute.
- Follow-up research
  - Dedicated attention pools for higher throughput and better failure isolation once memory capacity allows (Â§8).
  - More expressive schedulers: allow Q shards with partial K/V ranges; incorporate cache-awareness so already-present K/V is not re-sent; multi-objective optimization with topology awareness (Â§4.2, Â§8).
  - Memory/system improvements: static memory planning, CUDA Graph capture for dynamic shapes, and allocator changes to remove fragmentation (Â§6.2).
  - Extending disaggregation beyond CA, or combining with MoE routing or retrieval pipelines, where per-layer transfers can be amortized or co-scheduled (related work Â§7).

Overall, CAD reframes long-context training as a scheduling problem for stateless attention compute. With in-place servers, pingâ€‘pong overlap, and communication-aware tasking, it demonstrates consistent 1.1â€“1.35Ã— throughput gains at scale while eliminating DP and PP stragglers (Fig. 9â€“10), and offers a pragmatic path to training at ever-longer context windows.

# Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads

**ArXiv:** [2401.10774](https://arxiv.org/abs/2401.10774)
**Authors:** Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao
**Institutions:** Princeton University (Electrical and Computer Engineering; Center for Statistics & Machine Learning; Princeton Language and Intelligence (PLI)); potentially affiliations of all authors (not fully enumerated in abstract)

## ğŸ¯ Pitch

MEDUSA introduces a groundbreaking approach to speeding up large language model inference by integrating lightweight decoding heads, enabling parallel token prediction and verification. This innovation significantly reduces latency and energy consumption in interactive applications, offering a substantial 2.3â€“2.8x speedup without compromising on output quality, thus making it a compelling choice for efficient AI deployments.

---

## 1. Executive Summary
MEDUSA accelerates inference in large language models (LLMs) by adding a handful of lightweight â€œdecoding headsâ€ that predict several future tokens in parallel from the last hidden state, then verifying those predictions at once using a tree-structured attention mask. With two training modesâ€”drop-in heads on a frozen model (MEDUSA-1) and joint training with the backbone (MEDUSA-2)â€”the method achieves 2.3â€“2.8Ã— wallâ€‘time speedups on multiple Vicuna and Zephyr models without hurting output quality (Figure 3a, Table 1).

## 2. Context and Motivation
- Problem/gap:
  - Autoregressive LLMs generate one token at a time; each step repeatedly streams full model weights from high-bandwidth memory to on-chip cache. This makes inference â€œmemoryâ€‘bandwidthâ€‘bound,â€ not computeâ€‘bound, so accelerators are underutilized (Introduction; A.1 â€œLLM Inference Accelerationâ€).
- Why it matters:
  - Latency limits interactive applications (chatbots, coding assistants), cost, and energy usage. Increasing â€œarithmetic intensityâ€ (doing more computation per memory transfer) per decoding step can reduce latency and energy (Introduction).
- Prior approaches and shortcomings:
  - Speculative decoding uses a small â€œdraft modelâ€ to propose several tokens that the large model then accepts/rejects (Leviathan et al., 2022; Chen et al., 2023). Challenges: training or obtaining a well-matched draft model, distribution shift between draft and target, and serving complexity when two models must be orchestrated in distributed systems (Introduction; A.1).
- This paperâ€™s positioning:
  - Replace the separate draft model with a few singleâ€‘layer heads attached to the backboneâ€™s final hidden state. Predict multiple future tokens in parallel and verify them with a custom attention maskâ€”no extra model to serve, and far simpler to integrate (Abstract; Sections 2.1â€“2.3; Figure 1).

## 3. Technical Approach
At each decoding step MEDUSA performs three sub-stepsâ€”generate candidates, process them, and accept a continuationâ€”mirroring speculative decoding but without a separate draft model (Section 2; Figure 1).

1) How the â€œMEDUSA headsâ€ generate parallel predictions (Section 2.1.1)
- Idea: Attach K extra â€œdecoding headsâ€ on top of the backboneâ€™s last hidden state `h_t`. The `k`â€‘th head predicts the token at position `t + k + 1` (the base LM head still predicts position `t + 1`).
- Architecture: each head is a single-layer feed-forward block with residual connection:
  - For head `k`, the predictive distribution is
    - `p_t^(k) = softmax(W2^(k) * (SiLU(W1^(k) * h_t) + h_t))`, with `W2^(k) âˆˆ R^(dÃ—V)`, `W1^(k) âˆˆ R^(dÃ—d)` (Section 2.1.1).
  - Initialization aligns the heads with the base LM: `W2^(k)` copied from the original LM head; `W1^(k)` initialized to zero. This means initially the heads mimic the main headâ€™s distribution at `t+1`.
- Why this design:
  - Parameterâ€‘efficient (just head layers, no new transformer blocks).
  - Modelâ€‘serving simplicity: still only one model to host; heads share all backbone computation and KV cache with the LM (Sections 2.1.1, 2.2).

2) How multiple candidate continuations are formed and processed at once (Section 2.1.2; Figure 2)
- Candidate construction:
  - For each head `k`, take its top-`s_k` tokens. The Cartesian product across heads gives up to `âˆ_{i=1..K} s_i` candidate branches of length `K` (plus the first token from the base head).
  - Example: if head1 top-2 and head2 top-3, there are `2 Ã— 3 = 6` branches (Figure 2).
- Tree attention mask:
  - Treat the candidates as branches in a tree. A token can only attend to predecessors on its own branch, enforced by a custom attention mask and adjusted positional indices (Figure 2).
  - This allows the backbone to score all branch tokens in one forward pass without increasing batch size. Total tokens processed in parallel per step equals `Î£_{k=1..K} âˆ_{i=1..k} s_i` (Section 2.1.2).
- Why this matters:
  - Many proposed continuations are evaluated simultaneously, increasing the chance of accepting multiple tokens each step, which boosts â€œacceleration rateâ€ (average tokens accepted per step).

3) Accepting a continuation (Section 2.3.1)
- Two options:
  - Rejection sampling (RS): accept only if the large model would sample those tokens under the same sampling scheme; preserves the exact output distribution but suffers efficiency loss at higher temperatures (Section 2.3.1).
  - Typical acceptance (new): accept any candidate prefix whose per-token probability under the large model exceeds a threshold that adapts with the distributionâ€™s entropy:
    - For token `x_{n+k}`, accept if `p_original(x_{n+k} | x_{1..n+k-1}) > min(Îµ, Î´ * exp(-H(p_original)))`, where `H(Â·)` is entropy, `Îµ` a hard threshold, `Î´` a scaling factor (Section 2.3.1).
    - To guarantee progress, the first token (greedy from the base head) is always accepted; beyond that, pick the longest candidate prefix that passes the threshold (Section 2.3.1).
- Rationale:
  - RS ensures distribution fidelity but adds overhead and degrades as temperature rises. The typical acceptance rule keeps outputs â€œtypicalâ€ for the large model, preserving quality while improving acceptance length and speed (Section 2.3.1; Figure 5).

4) Training strategies (Section 2.2)
- MEDUSAâ€‘1 (frozen backbone; Section 2.2.1):
  - Loss: crossâ€‘entropy between the `k`â€‘th head and ground truth token `y_{t+k+1}` with a decay weight `Î»_k` (to downweight higherâ€‘k heads that are harder to predict):
    - Equation (1): `L_MEDUSA-1 = Î£_{k=1..K} -Î»_k * log p_t^(k)(y_{t+k+1})`, with `Î»_k â‰ˆ 0.8^k` in practice.
  - Practical note: can train with a quantized backbone (QLoRA-like) on a single GPU; e.g., 5 hours for Vicunaâ€‘7B using 60k ShareGPT samples (Section 2.2.1).
- MEDUSAâ€‘2 (joint training; Section 2.2.2):
  - Preserve nextâ€‘token quality while improving head accuracy via three tactics:
    - Combined loss: Equation (2) `L_MEDUSA-2 = L_LM + Î»0 * L_MEDUSA-1`, adding the base LMâ€™s next-token loss to stabilize its behavior.
    - Differential learning rates: larger LR for heads than for the backbone.
    - Heads warmup / twoâ€‘stage schedule: start by training heads (MEDUSA-1), then joint training with a warmup for the backbone or by gradually increasing `Î»0` (Section 2.2.2).
  - Can be integrated into standard SFT so the released model â€œnativelyâ€ supports MEDUSA (Section 2.2.2).
- Selfâ€‘distillation when no SFT data is available (Section 2.3.2):
  - Generate conversations from seed prompts using the target model itself; for joint training use a KL loss that matches the backbone logits to the original model (teacher) while training heads:
    - `L_LM-distill = KL(p_original^(0) || p^(0))` (Section 2.3.2).
  - Memoryâ€‘efficient trick: implement the backbone as a LoRA adapter; the teacher is the same network with the adapter turned off, so no second model copy is needed (Section 2.3.2).

5) Optimizing the candidate tree (Section 2.3.3; Appendix C)
- Goal: with a fixed token budget (number of tree nodes), choose which topâ€‘i predictions per head to include.
- Method: on a calibration set, estimate the accuracy `a_k^(i)` of the iâ€‘th top token at head `k`. Approximate a candidate prefix `[i1..ik]` accuracy as `âˆ_j a_j^(i_j)` and greedily add nodes with highest marginal contribution to the expected accepted length until the token budget is reached (Section 2.3.3).

6) Hardware characterization (Appendix G)
- Roofline analysis shows that standard decoding is memoryâ€‘bandwidthâ€‘bound for attention and most linear layers. MEDUSA increases â€œoperational intensityâ€ (FLOPs per byte moved) by processing many candidate tokens at once, shifting parts of the workload toward computeâ€‘bound regimes (Figures 18â€“20; Tables 6â€“8). However, too many candidates create compute bottlenecks, so there is an optimal range (Figures 4b, 21â€“23).

## 4. Key Insights and Innovations
- Extra decoding heads instead of a draft model
  - Whatâ€™s new: A single backbone with a few oneâ€‘layer heads predicts multiple future tokens from the same hidden state (Section 2.1.1).
  - Why it matters: Avoids the engineering and alignment burden of serving/training a separate draft model; reduces distribution mismatch and infrastructure complexity (Introduction; Section 2.1.1).
- Tree attention for concurrent verification of many candidates
  - Whatâ€™s new: A topâ€‘down tree mask lets tokens attend only to their branch predecessors, enabling many candidate continuations to be scored in one pass without increasing batch size (Section 2.1.2; Figure 2).
  - Why it matters: Increases accepted tokens per step while keeping memory movement per step similar; boosts arithmetic intensity.
- Typical acceptance rule
  - Whatâ€™s new: A distributionâ€‘aware thresholding based on entropy chooses â€œtypicalâ€ candidate prefixes instead of strict rejection sampling (Section 2.3.1). The rule is
    - `p_original(x) > min(Îµ, Î´ * exp(-H(p_original)))`, applied tokenâ€‘wise within candidates after greedily accepting the first token.
  - Why it matters: Especially at higher temperatures, typical acceptance yields longer accepted prefixes and higher speed than RS while maintaining similar quality (Figure 5).
- Two training modes and selfâ€‘distillation
  - MEDUSAâ€‘1 enables â€œboltâ€‘onâ€ speedups to existing models, even with quantized backbones (Section 2.2.1).
  - MEDUSAâ€‘2 coâ€‘trains heads with backbone using a combined loss and warmup; selfâ€‘distillation removes the need for original SFT data (Sections 2.2.2, 2.3.2).
  - Significance: Offers both a lowâ€‘resource adoption path and a higherâ€‘performance â€œnativeâ€ path.

## 5. Experimental Analysis
- Evaluation setup (Sections 3, B)
  - Models: Vicunaâ€‘7B/13B/33B (v1.5; Llamaâ€‘2 base) and Zephyrâ€‘7B (Sections 3.1â€“3.2).
  - Data/training:
    - For 7B/13B: ShareGPT fineâ€‘tuning for heads (2 epochs; Section 3.1).
    - For 33B and Zephyrâ€‘7B: selfâ€‘distillation from ShareGPT and UltraChat seed prompts; ~100k samples (Section 3.2).
    - Common training choices: 5 heads, `Î»_k = 0.8^k` (B.2).
  - Metrics:
    - Speed: tokens per second; â€œacceleration rateâ€ (tokens accepted per step); â€œoverheadâ€ (perâ€‘step latency vs. vanilla); â€œspeedupâ€ = acceleration rate / overhead (B.1).
    - Quality: MTâ€‘Bench score (0â€“10) via GPTâ€‘4 judge (Sections 3, 3.2).
  - Baselines: Vanilla decoding; openâ€‘source speculative decoding with draft models (Appendix D; Table 1).
- Main quantitative results
  - Wallâ€‘time speedups (Figure 3a):
    - Vicunaâ€‘7B: MEDUSAâ€‘1 2.18Ã—; MEDUSAâ€‘2 2.83Ã—.
    - Vicunaâ€‘13B: MEDUSAâ€‘1 2.33Ã—; MEDUSAâ€‘2 2.83Ã—.
  - Categoryâ€‘level speedups (Figure 3b, MEDUSAâ€‘2, 7B):
    - Largest gains on Extraction (3.62Ã—) and Coding (3.29Ã—), indicating many deterministic spans can be accepted per step.
  - Selfâ€‘distillation setting (Table 1):
    - Acceleration rate (accepted tokens/step) â‰ˆ 3.0â€“3.5; overhead â‰ˆ 1.18â€“1.27; resulting speedups:
      - Vicunaâ€‘7B 2.83Ã—; Zephyrâ€‘7B 2.66Ã—; Vicunaâ€‘13B 2.83Ã—; Vicunaâ€‘33B 2.35Ã—.
    - Quality differences on MTâ€‘Bench are small: for example, Vicunaâ€‘7B change +0.01; Zephyrâ€‘7B âˆ’0.07; Vicunaâ€‘33B +0.05.
  - Comparison to speculative decoding with public drafts (Table 1; Appendix D):
    - Reported speedups for speculative decoding are ~1.47â€“1.60Ã— on the Vicuna lineup, lower than MEDUSAâ€™s 2.35â€“2.83Ã—.
- Ablations and robustness
  - Tree size vs. speed (Figure 4):
    - â€œSparseâ€ optimized trees with ~64 nodes achieve higher acceleration rates than much larger â€œdenseâ€ Cartesian trees; too many candidates reduce tokens/s due to compute overhead (Figure 4b).
  - Typical acceptance vs. RS (Figure 5):
    - As the probability threshold `Îµ` increases, quality rises and acceleration falls; with `T=0.7`, typical acceptance traces match RS quality while achieving higher acceleration (plot shows acceleration in the 3.0â€“3.5 range).
  - Twoâ€‘stage joint training is necessary (Table 2):
    - Direct joint fineâ€‘tuning lowers quality (MTâ€‘Bench 5.93 vs. baseline 6.17), whereas MEDUSAâ€‘2 keeps quality (6.18) and attains 2.83Ã— speedup.
- Hardware modeling (Appendix G):
  - Roofline plots (Figures 9â€“17) and simulations (Figures 21â€“23) show MEDUSA lifts operational intensity and FLOPs/s for attention and MLPs, but speedup saturates or declines beyond ~64 candidate tokens and with very large batches.

> Figure 3a: â€œMEDUSAâ€‘1 shows a 2.18Ã— speedup on Vicunaâ€‘7B and 2.33Ã— on 13B; MEDUSAâ€‘2 delivers 2.83Ã— on both.â€
>
> Table 1: â€œAcceleration rates â‰ˆ 3.01â€“3.51 with overhead â‰ˆ 1.18â€“1.27 yield speedups of 2.35â€“2.83Ã—; MTâ€‘Bench quality deltas are within Â±0.14.â€
>
> Table 2: â€œDirect fineâ€‘tuning hurts quality (5.93). MEDUSAâ€‘1 (6.23, 2.18Ã—) and MEDUSAâ€‘2 (6.18, 2.83Ã—) preserve quality and improve speed.â€

Assessment: The evidence is consistent and multiâ€‘sidedâ€”absolute speeds, acceptedâ€‘token analysis, ablations for tree design and acceptance scheme, and quality measurementsâ€”supporting the claim of large wallâ€‘time speedups without quality loss across models and datasets (Sections 3, 3.3; Figures 3â€“5; Tables 1â€“2).

## 6. Limitations and Trade-offs
- Distribution changes vs. exact fidelity:
  - Typical acceptance does not reproduce the exact sampling distribution of the original model as RS does (Section 2.3.1). This is a pragmatic tradeâ€‘off for speed; it may matter for tasks requiring strict statistical fidelity.
- Tuning and engineering knobs:
  - Performance depends on choices of number of heads K, tree size, topâ€‘k per head, and typicalâ€‘acceptance thresholds `Îµ, Î´`. Overly large trees can reduce tokens/s (Figure 4b). Thresholds affect the quality/speed tradeâ€‘off (Figure 5).
- Training data dependence:
  - MEDUSAâ€‘2 needs suitable data for joint training; when unavailable, selfâ€‘distillation is used. The 33B case shows smaller speedup, possibly due to mismatch between hidden SFT data and selfâ€‘distilled data (Section 3.2, Table 1).
- Focus on batch size 1:
  - Most experiments assume batch size 1 (Introduction; Discussion). While the authors mention broader applicability and note that later libraries support it, the empirical evidence here focuses on singleâ€‘request latency.
- Compute vs. bandwidth regimes:
  - MEDUSA shifts work toward computeâ€‘bound regimes; too many candidate tokens or very large batches can push into compute saturation, lowering net speedup (Appendix G; Figures 21â€“23).
- Scope:
  - The method assumes standard decoderâ€‘only transformer LLMs with causal attention and typical KVâ€‘cache serving; applicability to nonâ€‘standard architectures would require custom masking and may vary.

## 7. Implications and Future Directions
- Field impact:
  - MEDUSA reframes multiâ€‘token parallelization as a property of the backbone itself rather than a separate draft model. This simplifies deployment and encourages broader adoption of parallel decoding in production LLM systems.
- Practical applications:
  - Lowerâ€‘latency interactive assistants, onâ€‘device or edge deployment due to fewer memory transfers, faster coding assistance, and cheaper API serving given the 2.3â€“2.8Ã— speedups (Figure 3a; Table 1). Typical acceptance is a strong default for creative/temperatureâ€‘controlled generation (Figure 5).
- Research directions:
  - Adaptive trees: learn or dynamically select which head depths and topâ€‘i branches to expand based on context, latency targets, or confidence (Section 2.3.3; Appendix C).
  - Quality guarantees: hybrid acceptance schemes that interpolate between typical acceptance and RS to provide tunable fidelity vs. speed.
  - Training integration: joint SFT/RLHF recipes that natively include MEDUSA heads and entropyâ€‘aware acceptance during training.
  - Systems coâ€‘design: combine with KVâ€‘cache optimizations (PagedAttention), quantization, and attention sparsification for compounding gains (A.1; Appendix G).
  - Beyond chat: apply to code generation, extraction, and other domains where Figure 3b shows especially high gainsâ€”potentially with domainâ€‘specific head depths and thresholds.

In short, MEDUSA provides a clean, general mechanism to accept multiple tokens per decoding step without separate draft models. Its combination of lightweight heads, tree attention, and typical acceptance delivers substantial and repeatable speedups while preserving output quality across several model families and training regimes.

# Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels

**ArXiv:** [2510.06499](https://arxiv.org/abs/2510.06499)

## üéØ Pitch

Webscale-RL introduces a breakthrough automated pipeline that transforms massive pretraining corpora into millions of high-quality, diverse, and verifiable question‚Äìanswer pairs tailored for reinforcement learning with language models. This innovation closes the major data bottleneck in RL for LLMs, enabling reinforcement learning at unprecedented scale and diversity‚Äîand delivering dramatically improved reasoning and efficiency, with models achieving strong benchmark results using up to 100√ó fewer training tokens than traditional pretraining. By unlocking web-scale RL data, Webscale-RL paves the way for more robust, capable, and data-efficient language models fit for general-purpose AI applications.

---

## 1. Executive Summary
Webscale-RL introduces an automated data pipeline that converts large, raw pretraining text corpora into millions of diverse, verifiable question‚Äìanswer (QA) pairs suitable for reinforcement learning (RL) with language models. Trained on the resulting 1.2M-example Webscale-RL dataset, a 3B-parameter model outperforms continual pretraining and strong data-refinement baselines across general knowledge and reasoning benchmarks while matching continual pretraining quality with up to 100√ó fewer training tokens (Section 5.3; Figure 4).

## 2. Context and Motivation
- Problem/gap:
  - Large language models are mainly trained with imitation learning (next-token prediction and supervised fine-tuning, SFT), which uses ‚Äúteacher forcing‚Äù (feeding ground-truth tokens during training). This produces a training‚Äìinference gap: models never experience their own generation distribution during training, making them brittle to distribution shift and limiting robust reasoning (Section 1; Equation (1)).
  - RL can close that gap by optimizing for rewards on generated outputs (Equation (2)), but RL at LLM scale is bottlenecked by data. Existing RL datasets are tiny compared to pretraining corpora and skewed toward narrow domains, mainly math and code (Figure 1; Section 2).
- Why it matters:
  - Practically: Better reasoning and robustness are needed for general-purpose assistants across diverse domains (commerce, lifestyle, healthcare, etc.). RL is also much more data-efficient than imitation learning if provided with the right data and rewards (Abstract; Sections 1, 5.3).
  - Scientifically: Demonstrates a path to scale RL to the same order of magnitude as pretraining, potentially changing how we train LLMs (Abstract; Section 6).
- Prior approaches and limitations:
  - Post-training RL datasets have been small and domain-limited (e.g., math reasoning sets like DeepScaler and OpenR1-Math). SFT/RL data often rely on distillation from stronger teachers, tying quality to teacher capability and limiting scale (Section 2; Table 1).
  - Data refinement for pretraining (e.g., QuRating, ProX, GDR) improves corpus quality but still trains with teacher forcing and does not provide verifiable rewards needed by RL (Section 5.1).
- Positioning:
  - Webscale-RL converts pretraining corpora directly into verifiable QA pairs, preserving breadth and scale while enabling RL with a binary correctness reward (Sections 3.2, 4.1). It therefore addresses both the data scarcity and diversity problems that have constrained RL for LLMs.

## 3. Technical Approach
The paper‚Äôs method has two intertwined parts: (A) a data pipeline that turns pretraining text into RL-ready, verifiable QA pairs; and (B) an RL training setup that exploits these pairs efficiently.

A. Data pipeline (Section 3.2; Figure 2)
- Definitions
  - Verifiable QA pair: a question and short answer whose correctness can be checked unambiguously from supplied context (often a number, date, name, or short phrase).
  - Persona: a viewpoint or role (e.g., ‚Äúmedical expert‚Äù, ‚Äúpatient‚Äù, ‚Äúhealth journalist‚Äù) guiding question style and information needs to increase diversity.
- Four stages end-to-end:
  1) Data Filtering (Stage 1; Section 3.2; Appendix B.1.1)
     - Goal: keep only informative, self-contained documents that can yield verifiable questions.
     - How: simple heuristics remove obvious low-quality pages; an LLM-based filter then excludes (i) boilerplate-heavy pages (menus, headers) and (ii) fragments lacking sufficient context to verify answers.
  2) Domain Classification and Persona Assignment (Stage 2)
     - Goal: tag each document with a domain (e.g., commerce, healthcare, social science) and attach multiple personas likely to be interested in the content (Section 3.2).
     - Why: domain tags select appropriate few-shot exemplars; personas drive diversity by eliciting different question types from the same document.
  3) Verifiable QA Generation (Stage 3)
     - Inputs: source document, domain-specific few-shot examples (a demonstration library), and an assigned persona (Section 3.2).
     - Instructions:
       - Generate self-contained questions (include any necessary context in the prompt so the model under RL later cannot ‚Äúpeek‚Äù at the source document).
       - Require short, verifiable answers (numbers/dates/names/short phrases). This lowers generation cost and simplifies automated checking.
     - Mechanism: a prompt template integrates domain examples and the persona; an LLM produces one or more QA pairs per persona (Appendix B.1.1). A concrete example from Wikipedia about ‚ÄúAlterna Bank‚Äù shows two persona-driven QA pairs (Appendix B.2.1).
  4) Quality Check and Leakage Control (Stage 4)
     - Correctness verification: confirm the answer is supported by the source document.
     - Leakage prevention: ensure the question does not trivially reveal the answer (e.g., embed the answer directly).
     - Implementation: an LLM-based verifier conducts multi-stage checks and filters out failures (Section 3.2; Appendix B.1.1).
- Contamination control
  - After verification, they decontaminate against evaluation sets using lm-eval-harness to remove overlaps (Section 3.2).
- Implementation specifics (Section 4.1; Appendix B.1, B.2)
  - LLMs used: GPT-4.1 for data filtering and QA generation; GPT-4.1-mini for domain classification and final quality checks.
  - Up to three personas per document to widen coverage.
  - Sources: DCLM, Wikipedia, MegaMath, Stack-v2, plus selective reasoning-focused sources, yielding ~1.2M QA pairs across 9+ domains (Section 4.1; Appendix B.2, Table 3).
  - Diversity analysis: domain distribution in Figure 3 (left) shows notable coverage beyond STEM‚Äîe.g., Lifestyle > 8.6%, Commerce > 3.3%. Embedding visualization with UMAP indicates more uniformly scattered topics than Nemotron (Figure 3, right).

B. RL formulation and training (Sections 3.1, 5.1; Equations (1‚Äì2))
- Pretraining vs RL (Section 3.1)
  - Pretraining objective (Equation (1)): minimize negative log-likelihood (imitation learning).
  - RL objective (Equation (2)): maximize expected reward over generated answers to queries.
- Reward design
  - Binary reward: 1 if the generated final answer matches the ground-truth short answer; 0 otherwise (Section 3.1). Short answers make automatic checking reliable and cheap.
- Base model and algorithm
  - Base: `Qwen2.5-3B` (for main comparisons), with `Qwen2.5-7B` as a reference larger model (Section 5.1).
  - RL algorithm: `GRPO` (Group Relative Policy Optimization), a sample-efficient PPO variant tailored to LLM RL (Table 4; Section 2).
- Training procedure (Section 5.1; Appendix B.3; Table 4)
  - Warm-up SFT: a small 10K-example dataset improves instruction following and reduces evaluation bias. For these, a short chain-of-thought is distilled using GPT-4.1 given the ground-truth answer (Appendix B.3.1).
  - RL on Webscale-RL: sample 150K QA items; train with GRPO. Key hyperparameters include batch size 256, learning rate 5e-6, 16 samples per query, and max rollout length 2560 (Table 4).
  - Token accounting for scaling comparisons: when comparing RL vs continual pretraining, they count RL tokens as the size of the original pretraining source documents from which those RL QA pairs were derived (Section 5.3).

Design choices and why:
- Short, verifiable answers: reduce cost and error rates in automatic checking and enable a simple, stable binary reward (Section 3.2).
- Persona-driven question generation: elicits multiple valid question styles from the same document, expanding diversity without new sources (Section 3.2; Figure 3).
- LLM verifier for correctness and leakage: protects reward quality by ensuring answers are grounded and questions aren‚Äôt trivial (Section 3.2).
- Domain-specific few-shot library: enforces consistent quality and question types appropriate to each domain (Figure 2; Section 3.2).

## 4. Key Insights and Innovations
- Data engine that scales RL data to pretraining levels (fundamental innovation)
  - Webscale-RL systematically converts raw web-scale corpora into RL-ready QA pairs (Figure 2; Sections 3.2, 4.1). This sidesteps costly human labeling and avoids dependence on teacher models solving tasks during generation.
  - Significance: opens a path to RL at pretraining scale, addressing the core bottleneck of RL data scarcity and narrow coverage (Figure 1; Table 1).
- Persona- and domain-driven generation for diversity (substantial innovation)
  - Attaching multiple personas per document and using domain-specific few-shot exemplars produces a broad distribution of question styles and topics from the same source (Section 3.2; Figure 3).
  - Significance: yields better generalization to heterogeneous benchmarks, not just math/coding (Table 2; Section 5.2).
- Verifiability-first design enabling simple binary rewards (substantial innovation)
  - By forcing self-contained questions with short answers and verifying them against sources, the method ensures clean, automatically checkable rewards (Section 3.2).
  - Significance: allows stable RL training with correctness-based binary reward, avoiding complex reward modeling and making scaling feasible.
- Demonstrated data efficiency of RL over continual pretraining (empirical insight)
  - Scaling study shows RL reaches continual pretraining performance with ~100√ó fewer tokens on MMLU-pro (Figure 4; Section 5.3).
  - Significance: empirically supports a strategic shift‚Äîinvest in RL with verifiable data rather than ever larger imitation datasets.

## 5. Experimental Analysis
- Evaluation setup (Section 5.1; Appendix B.3; Table 5)
  - Models: `Qwen2.5-3B` finetuned with either (a) continual pretraining variants followed by SFT or (b) RL on Webscale-RL (with the same SFT warm-up). `Qwen2.5-7B` used as a larger reference baseline.
  - Baselines:
    - Continual pretraining on the original corpus used to create Webscale-RL.
    - Data-refinement + continual pretraining: QuRating, ProX, GDR.
    - All continual pretraining variants get the same 10K SFT buffer to mitigate instruction-following bias.
  - Benchmarks and protocols:
    - General knowledge and reasoning: MMLU-pro (5-shot), Big-Bench (0-shot), GPQA-diamond (0-shot).
    - Math/STEM: GSM8K (8-shot), MATH500 (0-shot).
    - Code: MBPP and EvalPlus (HumanEval, MBPP and their + versions; 0-shot average).
- Main results (Table 2; Section 5.2)
  - Average score (macro across all benchmarks):
    - Best 3B baseline (GDR): 48.7
    - Webscale-RL (3B): 52.1 (+3.4 over GDR)
    - Qwen2.5-7B base: 58.2
  - Task-wise highlights:
    - MMLU-pro: 43.7 (Webscale-RL) vs 39.9‚Äì40.0 (best baselines), approaching 7B‚Äôs 48.3.
    - Big-Bench: 48.3 (Webscale-RL) vs ~46.0 (best baselines).
    - GPQA-diamond: 23.2 (Webscale-RL) vs 20.8 (best baseline).
    - MATH500: 58.0 (Webscale-RL) vs 44.0‚Äì44.6 (continual pretraining variants) and 47.6 (base 3B), close to 7B‚Äôs 60.8.
    - GSM8K: modest gain‚Äî78.5 (Webscale-RL) vs ~77.4 (baselines), likely due to saturation because the base model is already strong here.
    - Code (MBPP/EvalPlus): improvements are small; coding is underrepresented in the pretraining sources that were converted (Section 5.2; Section 6).
- Diversity analysis (Section 4.2; Figure 3)
  - Domain coverage includes non-STEM areas such as Lifestyle (>8.6%) and Commerce (>3.3%), which are underrepresented in other RL datasets.
  - Question embedding visualization (UMAP) shows Webscale-RL spreads more uniformly across the space than Nemotron, indicating greater diversity.
- Scaling/efficiency study (Section 5.3; Figure 4)
  - Token-based comparison (counting RL tokens by the size of the original documents from which QA pairs were derived):
    - On MMLU-pro and on the macro average, RL with ~10M tokens achieves performance comparable to continual pretraining with ~1B tokens (>100√ó efficiency).
    - With 100M tokens, RL improves the average by +4.4 points over the base model, while continual pretraining at the same budget is roughly flat.
  - Trend lines in Figure 4 show steeper gains for RL as training scale increases.
- Support strength and caveats
  - The broad, multi-benchmark improvements and clear scaling curves convincingly support both effectiveness and efficiency claims (Sections 5.2‚Äì5.3).
  - No detailed ablations isolate each pipeline component (e.g., personas vs domain library vs verifier), which would clarify where the biggest gains originate.

> Table 2 shows `Webscale-RL (3B)` achieves 52.1 average vs 48.7 for the strongest 3B data-refinement baseline (GDR), with large gains on MATH500 (58.0 vs 44‚Äì45).

> Figure 4 shows that on MMLU-pro, RL with roughly 10M tokens matches continual pretraining with 1B tokens, demonstrating over 100√ó token efficiency.

## 6. Limitations and Trade-offs
- Domain coverage and balance
  - Coding is relatively underrepresented in the converted sources used here, leading to smaller gains on code benchmarks (Section 6). Results might change with repository-scale code integration.
- Reward design constraints
  - Binary match on short answers is simple and stable but limits applicability to tasks requiring long-form answers, multi-step proofs, or subjective judgments. It also presumes the final answer alone is sufficient to evaluate reasoning quality (Sections 3.1, 6).
- Computational cost on the reward side
  - The current RL setup relies on a generative reward model to compare final answers, imposing nontrivial inference cost and becoming a scaling bottleneck (Section 6).
- Dependence on LLMs for data generation and verification
  - GPT-4.1/4.1-mini are used for filtering, classification, generation, and verification (Section 4.1; Appendix B.1). This may embed biases or errors from those models and can be costly at very large scales.
- Generality of demonstrated results
  - Main experiments use a 3B model (Qwen2.5-3B) with one larger reference (7B). Effects on much larger base models, or across different architectures, are not reported (Section 5).
- Limited ablations
  - The paper does not dissect how much each component (personas, few-shot library, leakage filter, decontamination) contributes to final gains, leaving optimization opportunities unquantified.

## 7. Implications and Future Directions
- How this changes the landscape
  - Establishes a practical, scalable route to RL training using web-scale corpora: instead of collecting expensive human-labeled rewards or distilling from stronger teachers, convert existing pretraining texts into verifiable QA with reliable automatic rewards (Sections 3‚Äì4).
  - Empirically supports prioritizing RL (with the right data) over simply increasing imitation-learning tokens, given the 100√ó token efficiency demonstrated (Section 5.3; Figure 4).
- Follow-up research enabled
  - Reward modeling:
    - Develop lighter, faster verifiers (e.g., non-generative classifiers), or learned reward models trained on the verified QA pairs to reduce RL overhead (Section 6).
    - Extend beyond exact-match rewards to handle longer answers while keeping verification robust.
  - Data pipeline extensions:
    - Domain rebalance (e.g., scale code repositories) to lift coding performance (Section 6).
    - Multi-turn QA and tool-use tasks: adapt the pipeline to generate verifiable multi-step interactions and tool-grounded answers.
    - Component ablations and improvements: quantify the effect of personas, domain libraries, and leakage checks to refine cost‚Äìbenefit.
  - Scaling studies:
    - Evaluate on larger base models and different families to assess universality of the efficiency gains.
    - Explore integrating RL earlier in the training lifecycle (‚Äúreinforcement pretraining‚Äù; Section 2 cites related directions).
- Practical applications
  - Building small yet capable assistants: results show a 3B model narrows the gap to a 7B model across diverse tasks when trained with Webscale-RL (Table 2).
  - Enterprise and domain-specific assistants: persona- and domain-controlled generation can tailor RL data to target industries (healthcare, finance, education) while retaining verifiability.
  - Continuous data engine: organizations can convert their proprietary corpora into RL-ready data at scale, enabling ongoing reinforcement improvements without extensive human labeling.

In short, Webscale-RL reframes the data problem for RL with LLMs: instead of collecting narrow, expensive, task-specific rewards, it mines the latent supervision already present in pretraining corpora. The pipeline‚Äôs verifiability-first design makes RL both feasible and efficient, and the experiments substantiate meaningful gains across broad benchmarks with far fewer tokens than continual pretraining.

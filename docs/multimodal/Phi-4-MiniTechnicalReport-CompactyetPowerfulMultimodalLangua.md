# Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs

**ArXiv:** [2503.01743](https://arxiv.org/abs/2503.01743)

## üéØ Pitch

This paper introduces Phi-4-Mini and Phi-4-Multimodal, two compact language models that deliver state-of-the-art text, vision, and speech understanding through an innovative Mixture-of-LoRAs architecture. By keeping the core language model frozen and adding modality-specific adapters and routers, these models achieve top-tier reasoning and multimodal capability without sacrificing language performance‚Äîsignificantly simplifying deployment and enabling highly efficient, unified AI systems even on resource-constrained devices.

---

## 1. Executive Summary (2‚Äì3 sentences)
This paper introduces two compact models: `Phi-4-Mini` (a 3.8B-parameter language model) and `Phi-4-Multimodal` (a 5.6B-parameter unified model for text, vision, and speech/audio) that extend small language models with strong reasoning and multimodal abilities. The core advance is a Mixture-of-LoRAs design that adds modality-specific adapters and routers on top of a frozen language backbone, enabling multiple inference modes (text-only, image+text, speech-only, speech+image) without degrading language performance, while achieving state-of-the-art results for models in this size class (Sections 1‚Äì2; Figure 1; Sections 2.2‚Äì2.2.2).

## 2. Context and Motivation
- Gap addressed
  - Most multimodal systems fine-tune the base language model to add vision/speech, which often degrades text-only quality and forces separate model deployments per modality (Section 2.2). This is problematic for resource-constrained or on-device scenarios.
  - Preserving a strong small language model (SLM) while adding robust multimodal abilities within a single checkpoint remains challenging.

- Why this matters
  - Practical: One compact model that handles text, images, and speech cuts memory/latency and simplifies deployment and updates on edge devices.
  - Scientific: Demonstrates that careful data and adapter design can deliver high multimodal performance without full model fine-tuning.

- Prior approaches and their shortcomings
  - Full fine-tuning of the language backbone for multimodality (e.g., LLaVA, Qwen-VL, InternVL; Section 2.2) risks eroding text skills.
  - Cross-attention ‚Äúpreserve-the-LM‚Äù designs (Flamingo-style; Llama-Vision) avoid touching LM weights but trail fully fine-tuned models on vision benchmarks (Section 2.2).
  - Hybrid joint-SFT approaches (NVLM) close some gaps but show limited breadth of post-training stages and benchmarks (Section 2.2).

- Positioning of this work
  - Introduces a unified Mixture-of-LoRAs (modality-specific Low-Rank Adaptations plus routers) that attaches to a frozen language model to add vision and speech/audio (Figure 1; Sections 2.2‚Äì2.2.2), aiming to keep language quality intact while matching or surpassing fine-tuned multimodal systems of similar or larger size.

## 3. Technical Approach
This section explains how the models are built and trained end-to-end, including why specific design choices were made.

- Language backbone (Section 2.1)
  - Architecture
    - Decoder-only transformer with 32 layers, hidden size 3072, and tied input/output embeddings to save memory (Section 2.1).
    - Uses `Group Query Attention (GQA)` with 24 query heads and 8 key/value heads, cutting key-value cache memory to one-third for long-context generation (Section 2.1).
    - `LongRoPE` provides a 128K token context window; a ‚Äúfractional RoPE‚Äù leaves 25% of attention head dimensions position-agnostic to ease very long-context handling (Section 2.1).
    - Tokenizer: `o200k_base` with a 200,064 vocabulary to better cover multilingual and multimodal tokens (Section 2).
  - Training detail
    - Peak learning rate follows LR*(D) = B¬∑D^(-0.32), with B tuned across 12.5B‚Äì50B token regimes (Section 2.1). This stabilizes scaling across different token budgets.

- Mixture-of-LoRAs multimodality (Sections 2.2, 2.2.1; Figure 1)
  - Concept: `LoRA` (Low-Rank Adaptation) injects small, low-rank weight updates into specific linear layers to adapt the model; the base weights remain frozen.
  - `Mixture-of-LoRAs`: the model holds distinct LoRA adapters for different modalities (vision, audio) and uses modality-specific routers to apply the appropriate adapter during inference. The frozen base LM ensures the text-only capability is preserved (Abstract; Section 2.2; Figure 1).
  - Why this over alternatives?
    - Compared to full fine-tuning: avoids catastrophic forgetting of language skills and reduces training cost/instability.
    - Compared to cross-attention-only methods: achieves higher vision-language performance while still preserving LM weights (Section 2.2).

- Vision pathway (Section 2.2.1; Figure 1)
  - Encoder: `SigLIP-400M`, further tuned with `LLM2CLIP` for richer image-text alignment; input resolution up to 448√ó448 during vision pretraining (Section 2.2.1).
  - Projector: a 2-layer MLP maps vision feature dimension to the LM‚Äôs 3072-d embedding space (Section 2.2.1).
  - LoRA for vision (`LoRA_V`): applied to all linear layers in the language decoder during vision-language SFT (Section 2.2.1).
  - Parameter budget: vision encoder + projector ‚âà 440M; `LoRA_V` ‚âà 370M (Section 2.2.1).
  - Dynamic multi-crop strategy: generates image crops based on original H/W vs crop size and caps the number of crops (‚â§16 for pretraining, ‚â§36 for SFT). If a naive grid would exceed caps, it switches to an aspect-ratio matching strategy (from InternVL2) but avoids upscaling very small images to unreasonable sizes (Section 2.2.1). This balances detail retention with compute.

- Speech/audio pathway (Section 2.2.1; Figure 1)
  - Inputs: 80-dim log-Mel filterbanks at 10 ms frames.
  - Encoder: 3 convolutions + 24 `Conformer` blocks (attention dim 1024; FFN 1536; 16 heads). The CNN front-end subsamples by 8√ó, yielding an 80 ms ‚Äútoken rate‚Äù for the LLM‚Äîabout 750 tokens per minute of audio (Section 2.2.1).
  - Projector: 2-layer MLP maps 1024-d audio features to 3072-d LM embedding (Section 2.2.1).
  - `LoRA_A`: rank-320 LoRA applied to all attention and MLP layers of the LM (Section 2.2.1).
  - Parameter budget: audio encoder + projector ‚âà 460M; `LoRA_A` ‚âà 460M (Section 2.2.1).

- Training pipeline (Section 2.2.2; Sections 3.1‚Äì3.4)
  - Language (Sections 3.1.1‚Äì3.1.2)
    - 5T-token pretraining corpus with improved quality filtering across languages, curated math/coding data, high-quality synthetic data (Phi-4 synthetic), and a rebalanced mixture emphasizing reasoning (Section 3.1.1).
    - Post-training: large, diverse instruction-following and function-calling data; extensive code completion including ‚Äúfill-in-the-middle‚Äù (Section 3.1.2).
  - Vision training (four stages; Section 2.2.2; Section 3.2)
    1) Projector alignment: train only the projector on captions to align vision and text spaces.
    2) Joint vision pretraining: train projector + vision encoder for OCR and dense understanding.
    3) Generative V-L SFT: activate `LoRA_V` and train with curated single-frame SFT for generative QA/summarization.
    4) Multi-frame SFT: extend to multi-image/video contexts (up to 64k tokens for visual context), freezing the vision encoder (Section 2.2.2).
  - Speech/audio training (two stages; Sections 2.2.2, 3.4)
    - Pretraining: 2M hours of anonymized ASR-style speech-text pairs to align audio encoder with LM, training encoder + projector for 50k steps at LR 4e-5 while freezing the LM (Sections 2.2.2, 3.4.1).
    - Post-training: ~100M weighted SFT samples spanning ASR, AST, SQA, SQQA (spoken query QA), summarization, and audio understanding. Freeze audio encoder; train audio projector + `LoRA_A` for 50k steps at LR 1e-4 (Section 3.4.2). 
    - Maximum audio lengths in post-training: up to 30 minutes (‚âà22.5k tokens) for summarization; 30 seconds (‚âà375 tokens) otherwise. With a 128k LM context, the theoretical max is ~2.8 hours, though long-form fine-tuning is not performed (Section 2.2.2).
  - Vision-speech joint training
    - After separate post-training, freeze the language base, audio encoder, and audio projector; fine-tune the vision encoder + projector + `LoRA_V` on joint vision-speech SFT, mixing in language and vision data to maintain performance (Section 2.2.2; Section 3.3).

- Reasoning enhancement for `Phi-4-Mini` (experimental; Section 2.2.2; Section 3.1.3; Table 9)
  - 3-stage recipe:
    1) Distill pretraining on ~60B CoT tokens from frontier reasoning LLM outputs, using rejection sampling to filter incorrect chains.
    2) Fine-tune on ~200k high-quality CoT samples across domains and difficulties.
    3) Roll-Out DPO (Direct Preference Optimization): construct ~300k preference pairs from filtered wrong vs corrected outputs and train with DPO.
  - This yields a reasoning-optimized 3.8B model competitive with larger 7B‚Äì8B distilled reasoning models (Table 9).

- How a multimodal turn works (simplified walk-through; Figure 1)
  - The input stream contains text plus `<|image_i|>` and/or `<|audio_j|>` placeholders. Images go through the vision encoder, get projected, then token-merged; audio frames are encoded and projected to LM-embedding tokens.
  - During decoding, the router selects `Original W + LoRA_V` for visual tokens and `Original W + LoRA_A` for audio tokens; for plain text-only spans the model can run effectively with the base (frozen) weights. This selective activation reduces interference across modalities while leveraging the same LM state (Figure 1).

## 4. Key Insights and Innovations
- Mixture-of-LoRAs for unified multimodality without LM fine-tuning (Abstract; Section 2.2; Figure 1)
  - What‚Äôs new: Modality-specific LoRA adapters plus routers attached to a frozen LM, supporting text-only, V+L, A+L, and V+A scenarios in a single checkpoint.
  - Why it matters: Retains language performance while achieving near fully fine-tuned vision-language quality, avoiding the typical text-performance regressions seen in many VLMs (Section 4.1.1, bullets under Table 1).

- Data and training pipelines tuned for compact models (Sections 3.1‚Äì3.4; 2.2.2)
  - 5T high-quality text data with improved filtering and curated math/code; staged multimodal training with careful projector alignment; large-scale speech alignment (2M hours) followed by broad SFT tasks (~100M weighted examples).
  - Significance: Drives outsized gains in math, coding, and ASR/AST at small model sizes (Tables 3, 7, 8).

- Dynamic multi-crop and token-efficient long-context design (Sections 2.1‚Äì2.2.1)
  - `GQA` reduces KV cache to 1/3 for long sequences; `LongRoPE` extends context to 128k; fractional RoPE dims improve long-context stability (Section 2.1).
  - Dynamic multi-crop avoids pathological resizing while accommodating higher-res images efficiently (Section 2.2.1).
  - Impact: Enables long transcripts, multi-image/video inputs, and resource-aware deployment.

- First open model of this size with strong speech summarization in a unified stack (Tables 3, 6)
  - Despite speech summarization comprising only ~1% of speech SFT data, the model achieves near GPT-4o quality on two datasets (Golden3 and AMI; Table 6), while also ranking first on OpenASR as of 2025-01-14 (Sections 4.1.2, Table 3‚Äì4).

## 5. Experimental Analysis
- Evaluation setup
  - Vision: 13 single-image, 2 multi-image/video benchmarks (Table 1). Uniform internal evaluation pipeline across models; for server-scored sets (DocVQA, InfoVQA), they report server results (Section 4.1.1).
  - Vision-speech: Four benchmarks adapted from InternOmni where inputs are image+speech; to make closed models evaluable, additional text instructions enforce multiple-choice/concise formats (Table 2; Section 4.1.1).
  - Speech/audio: ASR (CommonVoice v15, FLEURS, OpenASR leaderboard), AST (CoVoST2, FLEURS with both X‚ÜíEN and EN‚ÜíX), SQQA (MT-Bench turns 1‚Äì2 via synthetic spoken questions; MMMLU in 8 languages), speech summarization (Golden3 and AMI), and audio understanding (AIR-Bench-chat and MMAU) (Section 4.1.2; Tables 3‚Äì6).
  - Language & coding: Broad battery including MMLU, MMLU-Pro, BBH, GPQA, GSM8K, MATH, IFEval; coding benchmarks such as BigCodeBench (instr. and completion), HumanEval(+), MBPP(+), LiveBench (Tables 7‚Äì8).

- Main quantitative results
  - Vision-language (Table 1)
    - Average over 15 visual tasks: 
      > `Phi-4-Multimodal` achieves 72.0 vs Qwen2.5-VL-3B 68.7, InternVL2.5-4B 68.8, and is close to Qwen2.5-VL-7B 73.3; it exceeds GPT-4o-mini (72.4 average) and slightly trails Gemini-2.0-Flash (74.3).
    - Highlights:
      > MathVista test-mini: 62.4 (beats InternVL2.5-4B 51.2 and GPT-4o-mini 56.1; Table 1).  
      > DocVQA: 93.2 (comparable to top open models; Qwen2.5-VL-7B is 95.7).  
      > BLINK (multi-image): 61.3 (beats Qwen2.5-VL-7B 55.3 and GPT-4o-mini 62.4 is slightly higher).  
      > VideoMME: 55.0 (on par with ~4‚Äì8B open models; GPT-4o-mini 68.2 and Gemini-2.0-Flash 65.5 are higher).
  - Vision-speech (Table 2)
    - Average across four sets:
      > 72.2 for `Phi-4-Multimodal` vs 62.6 for InternOmni (8.7B) and 66.2 for Gemini-2.0-Flash; shows notable gains in mixed modality understanding.
  - Speech and audio (Table 3; Tables 4‚Äì6)
    - ASR (WER/CER):
      > CommonVoice v15 (8 languages): 6.80 average vs WhisperV3 8.13 and SeamlessM4T-v2 8.46 (Table 3‚Äì4).  
      > FLEURS ASR: 4.00 vs WhisperV3 4.58 and SeamlessM4T-v2 7.34 (Table 3‚Äì4).  
      > OpenASR: 6.14 vs nvidia/canary-1B 6.50 and WhisperV3 7.44 (Table 4). The paper reports ranking No. 1 on Hugging Face OpenASR leaderboard as of 1/14/2025 (Section 4.1.2).
    - AST (BLEU; Table 5):
      > CoVoST2 X‚ÜíEN: 39.33 (0-shot) / 40.76 (with CoT decoding), outperforming WhisperV3 (33.26) and SeamlessM4T-v2 (37.54).  
      > FLEURS X‚ÜíEN: 29.86 / 32.35 (CoT), on par with GPT-4o (30.69) and above SeamlessM4T-v2 (28.87).
    - Spoken QA (Table 6):
      > MT-Bench (turn-1/2 average score): 7.05 vs Qwen2-audio 4.92; still below Gemini-2.0-Flash 8.07 and GPT-4o 8.11.  
      > MMMLU (8 languages): 38.50% vs Qwen2-audio 15.53%, but far behind GPT-4o 72.56%.
    - Speech summarization (Table 6):
      > Golden3 overall: 6.28 (close to GPT-4o 6.76; Qwen2-audio 2.25).  
      > AMI overall: 6.29 (vs GPT-4o 6.53; Qwen2-audio 1.34).  
      > Hallucination is low (e.g., 0.13‚Äì0.14 on AMI/Golden3).
    - Audio understanding (Table 6):
      > AIR-Bench-chat avg: 6.98 vs Qwen2-audio 6.93 and GPT-4o 6.54.  
      > MMAU accuracy: 55.56 vs Qwen2-audio 52.50; Gemini-2.0-Flash is higher at 61.23.
  - Language & coding (Tables 7‚Äì8)
    - Language (Table 7, average across diverse tasks):
      > `Phi-4-Mini (3.8B)` averages 64.9, surpassing Llama-3.2-3B (58.0), Ministral-3B (58.3), Qwen2.5-3B (61.4), and close to or slightly below Qwen2.5-7B (67.9) and Gemma2-9B (66.0).
      - Strong on reasoning/math: e.g., GSM8K 88.6 (CoT, 8-shot), MATH 64.0 (0-shot CoT), MMLU-Pro 52.8 (0-shot CoT) (Table 7).
    - Coding (Table 8, average):
      > `Phi-4-Mini` averages 49.0, better than Llama-3.2-3B (39.5), Qwen2.5-3B (42.6), Ministral-3B (45.9), and close to Qwen2.5-7B (52.2).  
      > HumanEval: 74.4; HumanEval+: 68.3; BigCodeBench-Instruct: 33.8 (big jump over 3B peers).

- Do experiments support claims?
  - Evidence is strong that the Mixture-of-LoRAs approach achieves top-tier performance for its size across modalities (Tables 1‚Äì6), with particularly convincing ASR/AST results and competitive V+L performance vs larger open models.
  - The ‚Äúlanguage performance remains unchanged‚Äù claim for the multimodal model is asserted (Section 4.1.1, bullets after Table 2), but the paper does not present a side-by-side comparison of text-only scores between `Phi-4-Mini` and the full `Phi-4-Multimodal` checkpoint to conclusively quantify ‚Äúno degradation.‚Äù
  - The reasoning-enhanced ablation (Table 9) is persuasive: stepwise gains from distill pretraining ‚Üí distill fine-tuning ‚Üí Roll-Out DPO demonstrate clear benefits (e.g., AIME 10.0 ‚Üí 30.0 ‚Üí 43.3 ‚Üí 50.0).

- Caveats and robustness
  - Several speech and summarization evaluations rely on GPT-4 as a judge (Appendix A; Tables 3, 6), which is standard but introduces judge-model bias and prompt-sensitivity.
  - For closed models in vision-speech (Table 2), additional textual prompts were necessary to force extractable outputs; the comparability is reasonable but not perfect (Section 4.1.1).
  - Long-audio capacity (2.8 hours theoretical) is not empirically validated; the paper warns further fine-tuning may be needed for extremely long inputs (Section 2.2.2).

## 6. Limitations and Trade-offs
- Assumptions and scope
  - Base LM is frozen during multimodal training; performance gains depend on LoRAs and projectors being sufficient to cover modality gaps. Tasks requiring deep cross-modal fusion inside the LM may benefit from partial unfreezing, which this design avoids by intent (Sections 2.2‚Äì2.2.2).
  - Speech interface officially supports eight languages (Chinese, English, French, German, Italian, Japanese, Portuguese, Spanish; Section 3.4.1). Broader multilingual coverage is not addressed.

- Data and evaluation constraints
  - Many benchmarks are judged by GPT-4 (Appendix A) or use internal pipelines; while widely accepted, this can introduce evaluation bias and prompt-dependence (Tables 3, 6; Section 4.1.2).
  - Vision pretraining uses 0.5T tokens and SFT ~0.3T (Section 3.2); scaling beyond this is not studied.

- Performance trade-offs
  - SQQA/MMMLU performance lags behind large proprietary models (Table 6); the model is tuned more toward speech understanding/summarization than general knowledge chat with spoken queries (Section 4.1.2).
  - The paper acknowledges a multilingual trade-off: emphasizing coding reduced non-English language strength compared to English (Section 6).

- Practical constraints
  - Total parameter count of the multimodal model (~5.6B) is still non-trivial for edge deployment with all adapters loaded; however, LoRA adapters can be activated on-demand per modality (Figure 1; Section 2.2.1).
  - The theoretical long-audio limit (~2.8 hours) is not trained or stress-tested; latency/memory for such extremes is unknown (Section 2.2.2).

## 7. Implications and Future Directions
- How this changes the landscape
  - Demonstrates that a single, compact checkpoint can deliver state-of-the-art small-model performance across text, vision, and speech without sacrificing language skills, by freezing the LM and layering modality-specific LoRAs (Abstract; Sections 2.2, 4.1). This provides a viable blueprint for unified on-device assistants.

- Follow-up research enabled
  - New modalities via additional LoRAs and projectors (e.g., sensors, structured data) with low interference risk (Abstract; Section 2.2).
  - Systematic studies of router designs, LoRA ranks/placements, and multi-adapter composition in complex mixed-modality dialogs.
  - Extending the reasoning-enhancement pipeline to multimodal CoT (e.g., visual or audio chain-of-thought) and testing whether Roll-Out DPO similarly scales.

- Practical applications
  - Edge assistants that can see, listen, and converse: meeting transcription + summarization, image-grounded help, hands-free Q&A with spoken queries.
  - Developer tooling: strong code understanding/generation for small models (Tables 7‚Äì8), useful in IDE copilots and local CI systems.
  - Enterprise automation: document OCR + VQA + speech notes in one model; multilingual ASR/AST pipelines with top-tier quality for the supported languages (Tables 3‚Äì5).

- Safety practices and remaining needs
  - Safety alignment and testing across text, audio, and vision are extensive (Tables 10‚Äì15), including jailbreak robustness improvements (Table 11).  
  - The paper documents low defect rates for the size class and multilingual coverage (Table 13), but also notes susceptibility to persuasive/context attacks and the need for app-level safeguards (Section 5; Section 6).

Overall, the paper presents a coherent, well-engineered path to small, unified multimodal models. The Mixture-of-LoRAs design plus high-quality data recipes deliver unusually strong performance for the parameter budgets, especially in speech (ASR/AST) and coding. The most important open questions are: (1) quantifying language preservation with more direct head-to-head text-only comparisons of the LM vs multimodal checkpoint; (2) scaling router/adapters to more modalities and longer mixed-modality contexts; and (3) stress-testing the theoretical long-audio limits and streaming/latency behavior in real deployments.

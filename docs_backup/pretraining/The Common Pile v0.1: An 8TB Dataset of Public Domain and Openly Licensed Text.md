# The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text

**ArXiv:** [2506.05209](https://arxiv.org/abs/2506.05209)
**Authors:** Nikhilâ€¯Kandpal, Brianâ€¯Lester, Colinâ€¯Raffel, Sebastianâ€¯Majstorovic, Stellaâ€¯Biderman, Baberâ€¯Abbasi, Lucaâ€¯Soldaini, Enricoâ€¯Shippole, A.â€¯Federâ€¯Cooper, Aviyaâ€¯Skowron, Shayneâ€¯Longpre, Lintangâ€¯Sutawika, Alonâ€¯Albalak, Zhenlinâ€¯Xu, Guilhermeâ€¯Penedo, Loubnaâ€¯Benâ€¯Allal, Elieâ€¯Bakouch, Johnâ€¯Davidâ€¯Pressman, Hongluâ€¯Fan, Dashiellâ€¯Stander, Guangyuâ€¯Song, Aaronâ€¯Gokaslan, Johnâ€¯Kirchenbauer, Tomâ€¯Goldstein, Brianâ€¯R.â€¯Bartoldson, Bhavyaâ€¯Kailkhura, Tylerâ€¯Murray
**Institutions:** EleutherAI, University of Toronto, Hugging Face

## ğŸ¯ Pitch

The paper introduces 'Common Pile v0.1,' an 8TB legally shareable corpus of publicâ€‘domain and openly licensed text, demonstrating that quality language model pretraining can be achieved without relying on copyrighted sources. This innovation addresses significant legal and ethical challenges in AI development, paving the way for transparent, reproducible, and compliant machine learning research and applications.

---

## 1. Executive Summary (2-3 sentences)
This paper introduces `Common Pile v0.1`, an 8TB corpus of publicâ€‘domain and openly licensed text spanning 30 sources, and validates its usefulness by training two 7Bâ€‘parameter language models (`Comma v0.1-1T` and `Comma v0.1-2T`). The models achieve performance competitive with similarly budgeted models trained on unlicensed web data, showing that highâ€‘quality LLM pretraining is feasible without relying on copyrighted sources (Figures 3â€“4; Tables 10â€“11).

## 2. Context and Motivation
- Problem addressed
  - Can performant LLMs be trained using only publicâ€‘domain and openly licensed text, with fully shareable data and reproducible pipelines? This question arises because most modern pretraining relies on large amounts of unlicensed web data (Section 1).
- Why it matters
  - Legal and ethical stakes: web text is often copyrighted; compensating rights holders would cost billions and has triggered lawsuits and takedowns (Section 1, refs. [24, 40, 57, 96, 159, 191]). Consent concerns grew as websites began blocking AI crawlers in midâ€‘2023 (Section 1, ref. [107]).
  - Scientific stakes: open, shareable pretraining datasets enable research on learning dynamics, auditing, and memorization, which is limited when data cannot be redistributed (Section 1; refs. [18, 47, 50, 57, 76, 83, 128, 145]).
- Prior approaches and their gaps
  - Prior â€œopenâ€ corpora exist but are small, licenseâ€‘ambiguous, or narrow:
    - `OLC`: similar scope but ~0.85 TB and includes sources like Hacker News which has no open license (Section 2.2).
    - `Common Corpus`: size comparable but less English text and includes OpenAlex, which is known to mislabel licensing (Section 2.2).
    - `KL3M`: strictly excludes CC BYâ€‘SA, therefore mostly government text and smaller (3 TB) with less diversity (Section 2.2).
  - Existing highâ€‘quality web datasets (e.g., FineWeb, OSCAR) are licensed at the collection level (e.g., ODCâ€‘By) and include copyrighted documents, so the underlying text is not openly licensed (Section 2.1, â€œUse of collection licensesâ€).
- Positioning
  - The paper defines â€œopenly licensedâ€ per the Open Knowledge Foundationâ€™s Open Definition 2.1; accepts CC BY, CC BYâ€‘SA, CC0, Blue Oakâ€“approved software licenses; excludes CCâ€‘NC and CCâ€‘ND (Section 2; Appendix C).
  - It sets strict provenance standards to avoid â€œlicense launderingâ€ (reâ€‘posting under incorrect terms) and avoids synthetic LLMâ€‘generated data whose licensing is unsettled (Section 2.1).

## 3. Technical Approach
This section explains how the dataset is built, cleaned, mixed for training, and how models are trained and evaluated.

- Data sourcing under explicit licensing constraints (Sections 2, 2.1; Appendix B)
  - 30 sources spanning: research papers, government/legal text, wikis, publicâ€‘domain books, forums, openly licensed web pages, code, open educational resources, and CC BY YouTube transcripts (Figure 1; Appendix B).
  - Dueâ€‘diligence choices:
    - Require licensing provided by the rights holder; exclude sources with unreliable metadata (e.g., OpenAlex, YouTube â€œCommonsâ€ at large, some Kaggle sets) to mitigate license laundering (Section 2.1 â€œLicense due diligenceâ€).
    - Treat collectionâ€‘level licenses as nonâ€‘sufficient for underlying documents (Section 2.1).
    - Avoid LLMâ€‘generated synthetic datasets given unsettled licensing status (Section 2.1).

- Dataset construction for the â€œCommaâ€ training mixture (Sections 4.1â€“4.2; Tables 5â€“7; Appendix Jâ€“K)
  - Preprocessing and filtering (Section 4.1; Table 5):
    - Language ID via `FastText` to retain English; quality filtering for web pages with a DataCompâ€‘LM classifier using a low threshold for noise removal.
    - OCR error removal via a likelihood filter on a unigram model built from the Trillion Word Corpus; toxicity filtering using two `FastText` classifiers trained on Jigsaw Toxic Comment data.
    - PII redaction for emails, phone numbers, and IP addresses replaced with `<EMAIL_ADDRESS>`, `<PHONE_NUMBER>`, `<IP_ADDRESS>`.
    - Sourceâ€‘specific regex cleanup (e.g., boilerplate, license headers).
  - Global deduplication (Section 4.1):
    - Crossâ€‘source, documentâ€‘level nearâ€‘duplicate removal using a Bloomâ€‘filter approach; duplicates are those sharing >90% of 20â€‘grams. A Bloom filter is a spaceâ€‘efficient data structure that quickly tests set membership with low memory, suitable for deâ€‘dup at web scale.
  - Code curation (Section 4.1):
    - Start from the openly licensed subset of `The Stack v2` (license detection by BigCode/Software Heritage).
    - Apply RedPajama V1 heuristics (e.g., max line length, character ratios), restrict to a language set (Python, C/C++, SQL, Java, PHP, Rust, JS/TS, Go, Ruby, Markdown, C#, Swift, shell).
    - Use languageâ€‘specific quality classifiers to keep wellâ€‘documented, educational code; HTML files extracted with Trafilatura and passed through the same text filters.
  - Openâ€‘web text (`CCCC`) with license verification (Appendix G; Section B.10):
    - Scan 52 Common Crawl snapshots using CC regex as a first pass, then manually verify the top 1000 domains by volume; retain only 537 domains where the CC license covers all text, not just embedded media.
    - Extract main content and remove boilerplate with Resiliparse; apply exact and nearâ€‘duplicate removal and additional heuristics (C4 and Gopher rules).
  - Resulting raw vs. filtered sizes (Table 6):
    - From 7.56 TB raw text to 1.84 TB after filtering and deduplication across sources.

- Data mixing for pretraining (Section 4.2; Table 7)
  - Rationale: sources vary in quality and domain match; patents (USPTO) are huge but stylistically narrow, so size alone is a poor proxy for quality.
  - Procedure:
    - Train perâ€‘source 1.7B models for 28B tokens (Section 4.2; 4.3) and use their performance to set heuristic mixing weights.
    - Target at most 6 passes over each source during a 1Tâ€‘token run; assume small sources are high quality and also repeat up to 6 times.
    - Attempted `MixMin` (an automatic mixture optimizer) but it did not beat the heuristics (Section 4.2).
  - The resulting 1T mixture (â€œ`Comma dataset`â€) is detailed in Table 7, including each sourceâ€™s repetition count and token share (e.g., `peS2o` â‰ˆ27.4%, `StackExchange` â‰ˆ13.5%, `Stack v2` â‰ˆ13.0%, `CCCC` â‰ˆ8.7%).

- Model training and tokenizer (Section 4.4)
  - Tokenizer: 64kâ€‘vocabulary BPE trained on a 600GB sample of the Comma dataset, using Llamaâ€‘3.2â€‘style splitting regex and byteâ€‘level preprocessor (Section 4.4 â€œTokenizationâ€). BPE (byteâ€‘pair encoding) merges frequent byte sequences to form subword tokens that compress well across domains.
  - Architecture and setup: Llamaâ€‘style 7B decoderâ€‘only Transformer implemented in `lingua` (Section 4.4 â€œTraining setupâ€).
    - `Comma v0.1-1T`: effective batch 512Ã—4096 tokens, AdamW, weight decay 0.2; cosine schedule with warmup; twoâ€‘stage curriculum finishing with a â€œcoolâ€‘downâ€ phase on a highâ€‘quality subset (Table 8), linearly decaying LR to 0, then average 10 checkpoints (Section 4.4).
    - `Comma v0.1-2T`: same mixture repeated to reach 2T tokens; increase batch to 2048Ã—4096; same coolâ€‘down and averaging (Section 4.4).
    - Note: some sources are repeated up to 16Ã— at 2T (Section 4.4 â€œResultsâ€), which is known to risk diminishing returns.

- Evaluation protocols (Sections 4.3â€“4.4; Figures 2â€“4; Tables 9â€“11)
  - Controlled dataset quality study (1.7B models on 28B tokens): evaluate â€œearlyâ€‘signalâ€ tasksâ€”`ARC`, `MMLU`, `HellaSwag`, `OpenBookQA`, `CommonSenseQA`, `PIQA`, `SIQA`â€”to compare datasets on equal footing (Section 4.3; Figure 2; Table 9).
  - Largeâ€‘scale model benchmarks (`Comma v0.1-1T` and `-2T`): `ARC-C/E`, `MMLU` (5â€‘shot), `BoolQ`, `HellaSwag`, `OBQA`, `CSQA`, `PIQA`, `SIQA`, plus code (`HumanEval`, `MBPP` pass@10), using OLMES evaluation formats (Section 4.4 â€œEvaluationâ€; Figures 3â€“4; Tables 10â€“11).

Definitions introduced only where uncommon:
- `License laundering`: redistribution of copyrighted work under an incorrect/unauthorized open license (Section 2.1).
- `PII` (personally identifiable information): data that can identify individuals; here, emails, phone numbers, IPs are redacted (Section 4.1).
- `Bloom filter`: probabilistic data structure used for fast membership tests, enabling memoryâ€‘efficient nearâ€‘deduplication (Section 4.1).

## 4. Key Insights and Innovations
- A legally shareable, multiâ€‘domain, openâ€‘license corpus at scale, with documented provenance
  - Whatâ€™s new: `Common Pile v0.1` aggregates 8TB across 30 sources with perâ€‘source licensing checks and manual verification for web content (Figure 1; Section 2.1; Appendix G).
  - Why it matters: It directly addresses reproducibility and legal barriers that prevent sharing of most pretraining datasets (Section 1, Section 2.1).
- Rigorous licensing stance and antiâ€‘laundering practices integrated into curation
  - Whatâ€™s new: Excludes CCâ€‘NC/CCâ€‘ND; filters out sources with unreliable license metadata; avoids synthetic LLM outputs; manually audits top web domains for CC coverage of textual content (Sections 2, 2.1; Appendix C, G).
  - Why it matters: This is a principled blueprint for curating â€œcopyrightâ€‘cleanâ€ corpora at web scale.
- Evidenceâ€‘driven data mixing for pretraining with repetition caps
  - Whatâ€™s new: Build a 1T mixture by training perâ€‘source proxy models, then assign heuristic weights to upâ€‘/downâ€‘weight sources; cap repetitions to 6Ã— at 1T (Section 4.2; Table 7).
  - Why it matters: Source size does not equal quality; mixing improves efficiency and performance (Section 4.2). The team also shows that offâ€‘theâ€‘shelf mixture optimization (MixMin) did not beat their informed heuristics in this setting.
- Endâ€‘toâ€‘end validation: release of data, code, tokenizer, mixtures, and checkpoints
  - Whatâ€™s new: Beyond the corpus, the paper releases the `Comma v0.1` training recipe, mixtures (Tables 7â€“8), and checkpoints, enabling full reproducibility (Section 5 Conclusion).
  - Why it matters: Transparent, open artifacts are rare in pretraining and enable community study of data effects.

## 5. Experimental Analysis
- Evaluation design
  - Datasetâ€‘quality comparison (Section 4.3; Figure 2; Table 9)
    - Train identical 1.7B models for 28B tokens on different corpora: `Common Pile` (as the â€œComma datasetâ€), `OLC`, `Common Corpus`, `KL3M`, `The Pile` (unlicensed blend), `OSCAR` and `FineWeb` (modern web curation).
    - Metrics: zeroâ€‘shot accuracy on `ARC`, `MMLU`, `HellaSwag`, `OBQA`, `CSQA`, `PIQA`, `SIQA` (Winogrande omitted to avoid data leakage from DPI content; Section 4.3).
  - Largeâ€‘scale model evaluation (Section 4.4; Figures 3â€“4; Tables 10â€“11)
    - Compare `Comma v0.1-1T` to 7B/1T models trained on unlicensed data (LLaMAâ€‘1 7B, MPTâ€‘7B, RPJâ€‘INCITEâ€‘7B, StableLMâ€‘7B, OpenLLaMAâ€‘7B).
    - Compare `Comma v0.1-2T` to 7B/2T models (OLMoâ€‘Twin, LLaMAâ€‘2 7B, DeepSeekLLM).
    - Also show a higherâ€‘budget reference point: `Qwen3 8B` trained on 36T tokens (Figure 3â€“4).

- Main quantitative findings
  - Datasetâ€‘quality study (Figure 2; Table 9):
    - The `Comma` dataset beats all openâ€‘license baselines (`OLC`, `Common Corpus`, `KL3M`) on every benchmark and surpasses `The Pile` on five of seven tasks.
    - Average accuracies (Table 9):
      - `Comma`: 40.8 average vs. `OLC` 37.3, `Common Corpus` 37.6, `KL3M` 36.2, `The Pile` 39.6.
      - `FineWeb` is highest overall at 43.7, but `Comma` leads on scientific knowledge: it is best on `MMLU` (29.5 vs. 29.1 FineWeb) and ties top on `ARC` (38.0, same as FineWeb).
    - Notable weakness: `HellaSwag`, `PIQA`, `CSQA` are lower than top web datasetsâ€”consistent with underâ€‘representation of personal blogs, hobbies, and sports (Section 4.3 citing [188]).
    - DPI (taskâ€‘like) data ablation (Table 9): removing DPI has minor effect; the average drops from 40.8 to 40.0. A small drop on `HellaSwag` suggests DPI contains some relevant content.
  - `Comma v0.1-1T` vs computeâ€‘matched 7B/1T models (Figure 3; Table 10):
    - Standout strengths: knowledge (`ARCâ€‘C` 52.8 vs LLaMA 44.5; `MMLU` 42.4 vs MPT 30.2; vs LLaMA 34.8), and coding (`HumanEval` 36.5; `MBPP` 35.5), often the best among 1T peers.
    - Weaker on `HellaSwag` (62.6), where webâ€‘heavy baselines reach midâ€‘70s.
    - Overall average in Table 10 is top among 1T computeâ€‘matched baselines (54.7), narrowly ahead of OpenLLaMA and MPT averages (â‰ˆ54â€“55).
  - `Comma v0.1-2T` vs 7B/2T models (Figure 4; Table 11):
    - Competitive with OLMoâ€‘Twin and LLaMAâ€‘2: strong on `MMLU` (49.8 vs LLaMAâ€‘2 45.8), `ARCâ€‘E` (71.8 vs 69.5), `SIQA` (52.3 vs 50.8), and notably strong on coding (`HumanEval` 44.2; `MBPP` 41.5).
    - Still weaker on `HellaSwag` (64.4) relative to LLaMAâ€‘2 (76.2) and DeepSeekLLM (74.1).
- Ablations and robustness (Appendix O; Table 12)
  - Two additional 1T runs with different batch sizes and a threeâ€‘stage curriculum yield averages ~53.5â€“53.8 vs the main runâ€™s 53.6 (preâ€‘averaging). Coding sometimes benefits further (Table 12), suggesting moderate robustness to hyperparameter choices and curricula.
- Do results support the claims?
  - Yes for the central claim: training on exclusively openâ€‘license text can match older, computeâ€‘matched unlicensed baselines on many tasks and excel at scientific knowledge and code (Figures 3â€“4; Tables 10â€“11).
  - The controlled 1.7B comparison isolates dataset effects and shows `Common Pile`â€™s relative data quality vs other open corpora (Figure 2; Table 9).
  - Caveat: `Comma v0.1-2T` repeats some sources up to 16Ã— (Section 4.4 â€œResultsâ€), so its 2T result is not a clean â€œbestâ€‘caseâ€ for scaling; authors note diminished returns are expected with heavy repetition.

> â€œComma v0.1-1T outperforms budgetâ€‘matched baseline models on over half of the benchmarks testedâ€¦ and is particularly strong at codeâ€‘related tasksâ€ (Section 4.4; Figure 3; Table 10).

> â€œComma v0.1-2T is competitive with OLMo, Llama 2, and DeepSeekLLMâ€¦ with especially strong performance on MMLU, SIQA, ARCâ€‘E, and the coding tasksâ€ (Figure 4; Table 11).

## 6. Limitations and Trade-offs
- Residual licensing risk
  - Even with strict standards, license laundering and metadata drift are hard to eliminate completely; rights holders may later change license terms; publicâ€‘domain texts may quote copyrighted content (Section 2.1 â€œCaveatsâ€).
- Coverage gaps that show up as performance tradeâ€‘offs
  - The corpus underâ€‘represents casual, blogâ€‘style, hobby, and sports content; this likely depresses commonsense benchmarks such as `HellaSwag` and `PIQA` (Section 4.3, with analysis informed by [188]).
- Englishâ€‘centric focus
  - Primary emphasis on English (Section 4.1), reducing multilingual generality relative to some modern pretraining corpora.
- Data repetition at scale
  - The 2T run repeats certain sources up to 16Ã— (Section 4.4 â€œResultsâ€), which prior work shows can cause diminishing returns and memorization risks (cited in 4.2, 4.4).
- Heuristic mixing
  - Mixture weights are based on perâ€‘source proxy training and heuristics; automated methods like `MixMin` did not help here (Section 4.2). There may be headroom in principled, dynamic mixture optimization.
- Benchmark scope
  - Evaluations focus on knowledge/reasoning and code; they do not cover safety, longâ€‘context modeling, multilinguality, or instructionâ€‘following/chat alignment. The paperâ€™s claim is about pretraining viability, not endâ€‘toâ€‘end deployment readiness (Sections 4.3â€“4.4).
- Compute and resource demands
  - Training 1Tâ€“2Tâ€‘token 7B models is still costly; while datasets are open, reproducing fullâ€‘scale training requires significant compute and storage (implied by Sections 4.4 and the sizes in Table 7).

## 7. Implications and Future Directions
- How this changes the field
  - Demonstrates that enforceably open, auditable corpora can produce competitive generalâ€‘purpose LLMs at billions of parameters (Figures 3â€“4). This lowers legal barriers for organizations that require clean provenance and fosters transparent data science.
  - Provides a reference pipelineâ€”including web verification, crossâ€‘source dedup, and perâ€‘source filtering thresholds (Table 5)â€”for building copyrightâ€‘compliant datasets at scale.
- What followâ€‘up research it enables
  - Data mixture optimization: systematic methods (e.g., online mixture learning, curriculum search) to replace heuristics (Section 4.2). The paperâ€™s negative result for MixMin in this setting is a useful data point for future work.
  - Scaling without repetition: targeted collection to grow underrepresented domains (blogs, tutorials, sports, hobbies) and languages; measuring the returns of fresh data vs repeats (Sections 4.2, 4.4).
  - Attribution research: methods to connect model outputs to training points to satisfy attribution obligations when applicable (Section 2.1 â€œCaveatsâ€, refs. [28, 129]).
  - Licensing toolchains: automated, reliable license detection across text and embedded assets (Appendix C); better standards for publishing licenses on the web.
- Practical applications
  - Organizations with strict compliance needs (government, healthcare, education) can pretrain or fineâ€‘tune on the released data, mixtures, and checkpoints without the legal uncertainty of webâ€‘scraped corpora (Section 5 Conclusion).
  - Domainâ€‘focused LLMs for science, government, and education may benefit from `Common Pile`â€™s strengths (e.g., `peS2o`, PubMed, USGPO, Hansard); code assistants benefit from curated `Stack v2` and `PEPs`.

Overall, `Common Pile v0.1` plus the `Comma v0.1` models supply both a blueprint and concrete evidence that openâ€‘license pretraining is viable at scale, with clear strengths in scientific knowledge and code. The most immediate frontier is broadening domain and language coverage while preserving license certainty, and improving data mixture strategies to further narrow gaps on commonsense benchmarks.

# Learning to (Learn at Test Time): RNNs with Expressive Hidden States

**ArXiv:** [2407.04620](https://arxiv.org/abs/2407.04620)

## ğŸ¯ Pitch

The paper introduces Test-Time Training (TTT) layers, which make the hidden state a small, learnable model (W_t) that is updated by gradient steps online as a sequence is processed, yielding sequence-modeling layers with linear per-token complexity. By compressing context via learned inner-loop updates (TTT-Linear and TTT-MLP), the approach preserves RNN-style efficiency while substantially improving long-context utilizationâ€”maintaining perplexity gains as context grows where modern RNNs plateauâ€”making it a practical path toward scalable long-context models.

---

## 1. Executive Summary (2-3 sentences)

This paper introduces `Test-Time Training (TTT) layers`, a sequence-modeling layer family with **linear-time per-token complexity** whose hidden state is not a fixed vector but the **weights `W_t` of a small model** that is **updated by gradient descent while processing the sequence**, even at test time (Eq. (2), Figure 1). The central significance is a practical framework that aims to combine the **hardware- and asymptotic-efficiency of RNN-style recurrence** with improved **long-context utilization**, addressing the observed plateau of modern RNNs (e.g., Mamba) beyond ~16k tokens in their evaluation (Figure 2 right).

## 2. Context and Motivation

- **Problem / gap**
  - Self-attention is effective in long-context modeling but has **quadratic cost** in context length because each token attends over all previous tokens (Figure 3, â€œSelf-attentionâ€ row; discussion in Â§1 and Â§2).
  - RNN layers have **linear cost** but must compress all past context into a **fixed-size hidden state**, which can limit long-context performance (Â§1, Â§2).

- **Why important**
  - The paper argues that the practical advantage of linear-time models only becomes meaningful at long context (they reference â€œafter 8kâ€ in their setting; Â§1, Figure 12 discussion).
  - In long contexts, one wants perplexity to keep improving as more conditioning information is available; their evaluation shows this happens for Transformers but not for Mamba after ~16k (Figure 2 right; Â§1).

- **Prior approaches and shortcomings (as positioned by the paper)**
  - Classic RNNs (e.g., LSTMs) were previously observed not to scale like Transformers (cited as Kaplan et al.; Â§1).
  - Modern RNNs (Mamba) improve scaling at moderate context, but still show a long-context plateau in their token-index perplexity diagnostic (Figure 2; Â§1).
  - Linear-attention / â€œfast weightâ€ style methods exist; the paper later shows a **formal equivalence** between a particular TTT instantiation and `linear attention` (Theorem 1, Â§2.6).

- **How this paper positions itself**
  - It reframes sequence modeling layers via a unifying â€œhidden state + update rule + output ruleâ€ lens (Figure 3).
  - It proposes a framework where the hidden state can be an **arbitrary learnable model** (not just a vector or matrix), trained online via a **self-supervised objective whose form is itself learned** in an outer loop (Â§2.1â€“Â§2.3).
  - It contributes systems techniques (mini-batch token updates and a â€œdual formâ€) to make this tractable on accelerators (Â§2.4â€“Â§2.5).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **new kind of recurrent sequence layer** whose internal memory is a **small model** (e.g., a linear map or a 2-layer MLP) that is **trained online** as the sequence is processed.
- It solves long-context modeling by compressing past tokens into the modelâ€™s weights using **self-supervised learning updates**, while keeping **per-token recurrence** rather than attentionâ€™s growing KV cache (Figure 1, Figure 3; Eq. (2)).

### 3.2 Big-picture architecture (diagram in words)

- **Input token representation `x_t`** enters a TTT layer.
- The layer maintains a **hidden state = learner state**, mainly the current **weights `W_t`** (and potentially optimizer state; Â§2.6).
- For each token (or token mini-batch):
  1. Compute a **self-supervised loss** (multi-view reconstruction) using learnable projections (`Î¸_K`, `Î¸_V`) (Eq. (4), Â§2.3).
  2. Take (mini-batch) **gradient-descent-style updates** to update `W` (Eq. (6), Â§2.4).
  3. Produce the layer output using a **test-view projection `Î¸_Q`** and the updated model `f(Â·; W_t)` (Eq. (5), Â§2.3).
- The TTT layer is used inside a standard LM training setup (next-token prediction outer objective), optionally with a **Mamba-style backbone** (temporal convolutions + gating) rather than a Transformer block (Â§2.7, Figure 13).

### 3.3 Roadmap for the deep dive

- First, define the **TTT hidden state as model weights** and the basic update/output rules (Eq. (1)â€“(3), Figure 1).
- Next, explain how the self-supervised task is made **learnable via multi-view projections** optimized in the outer loop (Eq. (4)â€“(5), Figure 5).
- Then cover the two efficiency techniques:
  - **Mini-batch TTT** (parallelize gradients within token blocks) (Â§2.4, Figure 6â€“7).
  - The **dual form** (rewrite computations to use matmuls, avoid materializing per-token gradients and intermediate weights) (Â§2.5, Eq. (7)â€“(8)).
- Finally, connect the framework to known mechanisms:
  - **Equivalence to linear attention** (Theorem 1, Table 1).
  - **Equivalence to self-attention** via a nonparametric learner (Theorem 2).
- Close with concrete instantiations: `TTT-Linear` and `TTT-MLP` and their implementation choices (Â§2.7).

### 3.4 Detailed, sentence-based technical breakdown

**Framing sentence (type of paper + core idea).**  
This is an **algorithm + systems + empirical scaling** paper whose core idea is to turn the recurrent hidden state into a **trainable model** updated by **self-supervised gradient steps during the forward pass**, so that the layer â€œlearns at test timeâ€ by construction (Eq. (2), Figure 1, Â§2.1).

#### 3.4.1 Unifying view: sequence layer = hidden state + update rule + output rule

- The paper treats any autoregressive sequence layer as maintaining a hidden state `s_t` that evolves via an update rule and produces outputs via an output rule (Figure 3 top).
- In this lens:
  - A â€œnaive RNNâ€ keeps a fixed-size vector state and updates it with a parametric recurrence (Figure 3 bottom).
  - Self-attention keeps an ever-growing list state (KV cache), making per-token cost grow with time index `t` (Figure 3 bottom).
  - A â€œnaive TTTâ€ keeps a fixed-size state tooâ€”but that state is the **parameter vector/matrix** of a model `f` (Figure 3 bottom).

#### 3.4.2 Core TTT mechanism: hidden state as weights `W_t`, update rule as learning

- **Hidden state definition.** The hidden state at time `t` is `W_t`, the parameters of a â€œlearner modelâ€ `f` (Figure 1; Â§2.1).
- **Output rule.** The layer output at token `t` is a prediction produced by the current learner:
  - Base form: `z_t = f(x_t; W_t)` (Eq. (1)).
  - After introducing views (below): `z_t = f(Î¸_Q x_t; W_t)` (Eq. (5)).
- **Update rule.** The recurrence update is a gradient step on a self-supervised loss:
  - `W_t = W_{t-1} - Î· âˆ‡â„“(W_{t-1}; x_t)` (Eq. (2)).
- **Interpretation.** Because this update happens while processing a sequence during inference too, the layer performs *training at test time* on the test sequence (Â§2.1).

#### 3.4.3 Self-supervised objective: from naive reconstruction to learned multi-view reconstruction

- **Naive reconstruction.** A straightforward self-supervised task is to reconstruct `x_t` from a corrupted version `xÌƒ_t`:
  - `â„“(W; x_t) = || f(xÌƒ_t; W) - x_t ||^2` (Eq. (3)).
- **Learned task via multi-view projections.**
  - The paper introduces *views*â€”learnable low-rank linear projections that define what the learner sees and what it must predict (Â§2.3).
  - A **training view**: `Î¸_K x_t`.
  - A **label view**: `Î¸_V x_t`.
  - A **test view**: `Î¸_Q x_t`.
  - The inner-loop loss becomes:
    - `â„“(W; x_t) = || f(Î¸_K x_t; W) - Î¸_V x_t ||^2` (Eq. (4)).
  - The output becomes:
    - `z_t = f(Î¸_Q x_t; W_t)` (Eq. (5)).
- **Outer vs inner parameters (critical distinction).**
  - `W_t` is **not** an outer-loop parameter; it is a sequence-specific hidden state updated per example/sequence.
  - `Î¸_K, Î¸_V, Î¸_Q` (and later `Î¸_init`, `Î¸_lr`) *are* **outer-loop parameters** trained with the standard LM objective (next-token prediction) (Â§2.2â€“Â§2.3, Table 2).
- **Code-level picture.**
  - Figure 5 shows a conceptual implementation where `Task` contains `Î¸_K, Î¸_V, Î¸_Q`, while `Learner` contains the inner-loop model and optimizer, and `TTT_Layer.forward()` iterates through tokens calling `train()` then `predict()`.

#### 3.4.4 Why learning updates are plausible as â€œcompressionâ€

- The paper motivates the update rule as a compression heuristic: tokens that generate large gradients cause larger updates, so the hidden modelâ€™s weights preferentially encode information that is â€œlearn-worthyâ€ under the self-supervised objective (Â§2.1).
- Empirically, they show the TTT loss improves over time within test sequences:
  - In Figure 4 (and expanded in Figure 14), `â„“(W_t; x_t)` is lower than `â„“(W_{t-1}; x_t)` after one gradient step, and performance relative to the initialization `W_0` improves further along the sequence (Figure 4 caption; Â§2.1).

#### 3.4.5 Making TTT trainable end-to-end: backprop through inner-loop updates

- Training a model containing TTT layers uses standard outer-loop next-token prediction, but gradients must flow through the inner-loop update computations.
- The paper notes that although the forward pass uses a gradient operator `âˆ‡` internally, this still corresponds to differentiable computations; backprop â€œthrough gradientsâ€ is described as taking â€œgradients of gradients,â€ connecting to meta-learning ideas (Â§2.2).
- The paperâ€™s terminology:
  - **Inner loop:** updates to `W` inside the TTT layer via `âˆ‡â„“` (Â§2.2, Table 2).
  - **Outer loop:** standard LM training updating `Î¸_rest` and also the TTT task parameters `Î¸_K, Î¸_Q, Î¸_V` (and others) (Â§2.2â€“Â§2.3, Table 2).

#### 3.4.6 Systems/efficiency technique #1: mini-batch TTT (token blocks)

- **Problem:** Online GD updates `W_t` strictly depend on `W_{t-1}` inside the gradient, which is sequential and hard to parallelize (Â§2.4).
- **Key observation:** Gradient descent variants can be written as `W_t = W_{t-1} - Î· G_t = W_0 - Î· Î£_{s=1}^t G_s` (Eq. (6)), so once `G_t` are available, `W_t` can be obtained via prefix sums (â€œcumsumâ€).
- **Mini-batch gradient descent across tokens.**
  - Divide the sequence into mini-batches of tokens of size `b`.
  - For tokens within a mini-batch, compute gradients with respect to the **same reference weights** (the weights at the start of the mini-batch), enabling parallel gradient computation (Â§2.4, Figure 6).
  - This yields a speedâ€“quality trade-off:
    - Smaller `b` is closer to online GD (more steps, better perplexity).
    - Larger `b` is closer to batch GD (more parallelism, worse perplexity).
  - They choose `b = 16` for all experiments (Figure 7; Â§2.4).

#### 3.4.7 Systems/efficiency technique #2: the dual form (matmul-friendly rewrite)

- **Problem:** Even with mini-batches, the â€œprimalâ€ computation involves many per-token outer products and materialization of `G_t` and `W_t`, which is inefficient on accelerators and heavy in memory I/O (Â§2.5).
- **Dual-form idea:** Do not materialize intermediate gradients `G_1â€¦G_b` or intermediate weights `W_1â€¦W_{b-1}` inside a mini-batch; instead compute:
  - The end-of-batch weights `W_b`, and
  - The batch of outputs `z_1â€¦z_b`,
  using a small number of large matrix multiplications (Â§2.5, Figure 6).
- **Concrete derivation in the simplified linear case (Â§2.5).**
  - With `f(x;W)=Wx` and a simple squared reconstruction loss, they show:
    - `W_b = W_0 - 2Î· (W_0 X - X) X^T` where `X=[x_1,â€¦,x_b]` (derivation around Eq. (7)).
  - For outputs, they derive a masked formulation:
    - Define a masked triangular accumulation using `mask(X^T X)` (Eq. (8)), analogous to causal masking in attention but with zeros instead of `-âˆ`.
  - Complexity trade-off:
    - Dual form uses `O(b d^2)` for end-of-batch weights and adds `O(b^2 d)` to compute all outputs in the batch (Â§2.5).
    - The paper argues this is acceptable because they pick small `b` (16) and typical `d` is a few hundred, and it improves wall-clock time substantially on TPU: â€œmore than 5Ã— fasterâ€ in JAX (Â§2.5).
- **Extension to MLP learners.**
  - Appendix A generalizes the dual form to multi-layer MLPs using standard forward/backprop quantities like `âˆ‡Z^k l` and masked matmuls.

#### 3.4.8 Theoretical equivalences: how TTT unifies RNN/attention constructions

- **Equivalence to linear attention (parametric learner).**
  - Theorem 1 (Â§2.6) states: with a linear inner model `f(x)=Wx`, batch GD, `Î·=1/2`, and `W_0=0`, the TTT output matches linear attention.
  - The proof rewrites the batch gradient update to show `W_t` becomes a running sum of outer products of projected values and keys, and the output becomes the standard linear-attention form (Â§2.6).
  - Table 1 empirically verifies the equivalence (â€œLinear attn. improvedâ€ and â€œTTT equivalenceâ€ both have perplexity `15.23`, diff `0`).
- **Equivalence to self-attention (nonparametric learner).**
  - Theorem 2 (Â§2.6) shows that if the learner is a Nadarayaâ€“Watson estimator with an exponential kernel `Îº(x,x') âˆ exp((Î¸_K x)^T Î¸_Q x')`, then the induced TTT layer corresponds to self-attention.
  - This reframes attention as a particular â€œlearnerâ€ that stores all past data points and predicts via kernel-weighted averaging (Â§2.6; Appendix B elaborates Nadarayaâ€“Watson).

#### 3.4.9 Final instantiations: TTT-Linear and TTT-MLP, plus stabilizers

- **Two proposed variants (Â§2.7).**
  - `TTT-Linear`: the learner `f_lin(x)=W x` with square `W`.
  - `TTT-MLP`: the learner is a 2-layer MLP with hidden dimension `4Ã—` input dimension and `GELU` activation (Â§2.7).
- **Stability structure inside `f`.**
  - They wrap learners with residual + LayerNorm:
    - `f(x) = x + LN(f_res(x))` where `f_res` is linear or MLP (Â§2.7).
  - Table 1 shows adding â€œLN and residual in `f`â€ yields a large perplexity improvement (`15.27 â†’ 14.05`, improvement `âˆ’1.22`) on their 125M ablation path.
- **Learnable initialization `W_0`.**
  - They learn `W_0` as an outer-loop parameter `Î¸_init` to improve training stability (Â§2.7).
  - Table 1 indicates it slightly hurts perplexity in isolation (`15.23 â†’ 15.27`) but is needed for stable training of later improvements.
- **Learnable inner learning rate `Î·`.**
  - They learn a token-dependent gate:
    - `Î·(x) = Î·_base Ïƒ(Î¸_lr Â· x)` with `Î·_base=1` (TTT-Linear) and `0.1` (TTT-MLP) (Â§2.7).
- **Backbone choice.**
  - Instead of directly swapping attention in a Transformer block, their strongest versions use a **Mamba-style backbone** with temporal convolutions and gating (Â§2.7, Figure 13).
  - They also report ablations using a Transformer backbone (Figures 10â€“11 show both â€œ(M)â€ and â€œ(T)â€ variants).

#### 3.4.10 Worked micro-example (single-step intuition without full LM scale)

To make the mechanism concrete, consider one time step of a simplified TTT layer.

- Let the token embedding be `x_t âˆˆ R^d`.
- Choose projections `Î¸_K, Î¸_V, Î¸_Q` so that:
  - training input: `xÌ‚_t = Î¸_K x_t`
  - training label: `y_t = Î¸_V x_t`
  - test input: `xÌ„_t = Î¸_Q x_t`
- Let the learner be linear: `f(u;W)=W u`.
- The self-supervised loss at time `t` is:
  - `â„“(W; x_t) = || W xÌ‚_t - y_t ||^2` (Eq. (4) specialized).
- A single inner-loop update (online GD form) is:
  - `W_t = W_{t-1} - Î· âˆ‡_W || W xÌ‚_t - y_t ||^2` (Eq. (2)).
- The output for the sequence model at this time is:
  - `z_t = W_t xÌ„_t` (Eq. (5) specialized).
- Intuition: if the current token has structure that helps predict `y_t` from `xÌ‚_t`, the update will adjust `W` so that future tokensâ€”processed with the updated `W`â€”can reuse that learned structure, acting like an adaptive memory.

## 4. Key Insights and Innovations

- **(1) Hidden state as an explicit learner trained online (TTT layers).**
  - Novelty: Instead of a vector/matrix state updated by a fixed recurrence, the hidden state is the **weights of a model** updated by **gradient-based learning** on self-supervised loss (Eq. (2), Figure 1).
  - Significance: This increases the *expressive capacity* of what an RNN-style layer can store, aiming to better compress long context (Â§1, Â§2.1).

- **(2) Learnable self-supervised task via multi-view reconstruction.**
  - Novelty: The reconstruction task is not handcrafted; it is parameterized by `Î¸_K, Î¸_V, Î¸_Q` and optimized in the **outer loop for next-token prediction** (Â§2.3, Eq. (4)â€“(5), Table 2).
  - Significance: This aligns the inner-loop adaptation with the final LM objective, rather than relying on human-designed corruptions/tasks (Â§2.3).

- **(3) Mini-batch token updates to trade off parallelism and adaptation quality.**
  - Novelty: The paper introduces mini-batch gradient computations across *tokens* inside the sequence to expose parallel computation while retaining multi-step adaptation across mini-batches (Â§2.4, Figure 6â€“7).
  - Significance: This is a practical bridge between purely online updates (slow, sequential) and batch updates (parallel but too limited), with an empirically chosen sweet spot `b=16` (Figure 7).

- **(4) Dual form rewrite to make TTT matmul-friendly on accelerators.**
  - Novelty: Within a token mini-batch, they derive an equivalent computation that avoids materializing per-token gradients and intermediate weights, using masked matmuls (Eq. (7)â€“(8), Â§2.5; Appendix A for MLPs).
  - Significance: The paper reports >5Ã— speedup in JAX on TPU for training with the dual form compared to primal (Â§2.5).

- **(5) Unifying theoretical lens: TTT spans linear attention and (via learners) self-attention.**
  - Novelty: Theorem 1 shows linear attention is a special case of TTT (and Table 1 verifies equal perplexity under an improved implementation). Theorem 2 shows self-attention corresponds to a nonparametric learner choice (Â§2.6).
  - Significance: This reframes common sequence layers as points in a broader design space, motivating â€œexpressive hidden statesâ€ as â€œricher learners.â€

## 5. Experimental Analysis

### Evaluation methodology

- **Primary metric:** `perplexity` (Ppl), reported on validation/evaluation following protocols aligned with the Mamba paper (Â§3; Figure 2 caption references Kaplan-style evaluation; Â§3).
- **Datasets:**
  - `The Pile` for standard 2k and 8k context evaluations (Â§3, Â§3.1).
  - `Books3` subset of the Pile for long-context evaluations from 1k to 32k in Ã—2 steps (Â§3, Â§3.2; Figures 11, 15, 16).
- **Baselines compared:**
  - A strong `Transformer` baseline (â€œTransformer++â€ style, based on Llama architecture) (Â§3, Appendix C).
  - `Mamba` as a modern RNN baseline (Â§3).
- **Model scales:** Four sizes are used: `125M`, `350M`, `760M`, `1.3B` parameters (Â§3 protocols).
  - Mamba sizes are slightly different: `130M`, `370M`, `790M`, `1.4B` (noted in Â§3 protocols).
- **Training setup (â€œChinchilla recipeâ€ as used in Mamba):**
  - Optimizer: `AdamW` with `Î²=(0.9, 0.95)` (Appendix C).
  - LR schedule: cosine decay to `1e-5`, with linear warmup for 10% of steps (Appendix C).
  - Weight decay `0.1`, grad clip `1.0`, no dropout, mixed precision (Appendix C).
  - Training tokens per model size (Table 3): `2.5B`, `7B`, `15B`, `26B`.
  - Peak LRs (Table 3): `3e-3`, `1.5e-3`, `1.25e-3`, `1e-3`.
  - Block counts / embed dims / heads (Table 3): e.g., 1.3B uses 24 blocks, `d=2048`, 32 heads.
- **Context/batch policy:** They keep total tokens per training batch fixed at `0.5M` tokens regardless of context length (Â§3.2 footnote 12; Appendix C reiterates).

### Main quantitative results (numbers grounded where available)

- **Long-context utilization diagnostic (token-index perplexity):**
  - Figure 2 (right) shows perplexity vs token index up to `32k`.
  - Reported qualitative outcome: `TTT-Linear` and `TTT-MLP` continue reducing perplexity as token index increases (similar to Transformer), while `Mamba` plateaus after `16k` (Figure 2 right; Â§1).

- **Scaling trends on the Pile (2k and 8k contexts):**
  - Figure 10 summarizes Ppl vs FLOPs for 2k and 8k.
  - Reported conclusions (Â§3.1):
    - At `2k`, `TTT-Linear (M)`, `Mamba`, and `Transformer` are comparable (lines overlap).
    - At `8k`, both `TTT-Linear (M)` and `TTT-MLP (M)` outperform `Mamba`, and the gap widens with longer context (Â§3.1).

- **Long-context on Books (up to 32k):**
  - Figure 11 (2k and 32k points) and Figure 15 (full set) show that at `32k`, `TTT-Linear (M)` and `TTT-MLP (M)` outperform `Mamba` (Â§3.2).
  - Figure 16 shows an alternate view: for models trained from scratch, perplexity can worsen when context gets â€œtoo large,â€ and the best context length increases with model size; this trend is less present with Transformer finetuning (â€œTF finetuneâ€) (Â§3.2, Figure 16 caption).

- **Ablation path from linear attention to TTT-Linear (concrete perplexities).**
  - Table 1 reports, for 125M models trained on Pile with their recipe:
    - `Linear attn. improved`: `15.23`.
    - `TTT equivalence`: `15.23` (diff `0`).
    - `+ LN and residual in f`: `14.05` (improvement `âˆ’1.22`).
    - `+ mini-batch TTT`: `12.35` (improvement `âˆ’1.70`).
    - `+ learnable Î·`: `11.99` (improvement `âˆ’0.36`).
    - `+ Mamba backbone`: `11.09` (improvement `âˆ’0.90`), which they identify as the final TTT-Linear result used in Figure 10 (Table 1 caption).

### Wall-clock / latency evaluation

- Figure 12 reports inference latency on `NVIDIA A100 80GB PCIe`:
  - Prefill (forward) latency for batch size 16 increases with context for Transformer (consistent with quadratic-ish attention costs), while TTT-Linear/TTT-MLP/Mamba are roughly constant per token as context grows (Figure 12; Â§3.3).
  - Decode (generation) uses the primal form because it is sequential (Â§3.3); the figure shows roughly constant per-token behavior for non-Transformer methods across tested context lengths, with Transformer growing (Figure 12).
- They also note TPU training iteration time on a v5e-256 pod at 2k context:
  - Transformer baseline: `0.30s/iter`, TTT-Linear: `0.27s/iter` (about 10% faster) without extra systems optimization (Â§3.3).
  - However, they do not provide full comparable TPU timing vs Mamba because Mamba implementation is GPU-focused (Â§3.3).

### Do experiments support the claims?

- **Supportive evidence:**
  - The long-context token-index perplexity diagnostic in Figure 2 right directly targets the claim that some RNNs fail to keep benefiting from additional context; TTT variants behave more like Transformer there.
  - Table 1 provides a grounded ablation chain showing which components matter most (mini-batch TTT and LN/residual inside `f`).
  - Multiple model sizes (125M to 1.3B) and two datasets (Pile, Books) provide breadth (Â§3).

- **Caveats evident in the paperâ€™s own reporting:**
  - The paper explicitly notes they do not see a clean scaling-law linear fit in their FLOPsâ€“perplexity plots (Figures 10â€“11; Â§3.1), limiting extrapolation claims.
  - TTT-MLP has wall-clock challenges due to memory I/O despite favorable FLOPs comparisons (Figure 12; Abstract; Â§4.2).

## 6. Limitations and Trade-offs

- **Wall-clock efficiency, especially for more expressive learners (`TTT-MLP`).**
  - The paper repeatedly emphasizes that while TTT-MLP may be â€œeffective in terms of FLOPs,â€ its structure increases wall-clock time much more than FLOPs would suggest, attributing this to memory I/O (Abstract; Â§4.2; Figure 12).

- **Speedâ€“quality trade-off controlled by token mini-batch size `b`.**
  - Mini-batching is necessary for parallelism but reduces the â€œgradient channelâ€ dependency within a mini-batch (Â§2.4).
  - Figure 7 shows perplexity degrades as `b` increases, and they fix `b=16` as a compromise (Â§2.4).

- **Training stability and reliance on particular stabilizers.**
  - Learning `W_0` (`Î¸_init`) is reported as crucial for stability even if it slightly hurts perplexity in isolation (Table 1 caption; Â§2.7).
  - The learner architecture requires LN + residual wrapper for stability and performance (Table 1; Â§2.7).

- **Backbone dependence / architectural confounding.**
  - Their strongest TTT results use a Mamba-style backbone with temporal convolutions and gating (Â§2.7, Figure 13).
  - This means improvements are not purely attributable to the TTT mechanism alone; Table 1 explicitly shows a gain from â€œ+ Mamba backboneâ€ (Table 1).

- **Long-context training-from-scratch difficulties.**
  - On Books, Figure 16 indicates that for all methods trained from scratch, perplexity can worsen once context becomes too large, and the optimal context depends on model size (Figure 16 caption).
  - This suggests that simply increasing context length in training is not uniformly beneficial under the fixed recipe.

- **Evaluation scope constraints acknowledged by the paper.**
  - No hybrid architectures (mixing attention and TTT) are explored, to keep baselines clean (Â§3 protocols).
  - They did not train at extremely long contexts (millions/billions of tokens of context), citing academic resource constraints (Â§5).

- **Systems implementation gaps.**
  - Training is run on TPUs in JAX, but their GPU kernel work is limited to inference; they explicitly did not build a full training kernel for GPUs (Â§3.3).

## 7. Implications and Future Directions

- **How this work changes the landscape (as implied by the paperâ€™s framework + results)**
  - It proposes that the design space of sequence layers can be expanded from â€œchoose a recurrence vs attentionâ€ to â€œchoose a `learner` (model + optimizer + task) as memory,â€ with attention and linear attention appearing as special cases (Figure 8â€“9; Theorems 1â€“2).
  - Empirically, it suggests that long-context weaknesses observed in modern RNNs (their Mamba plateau result) may be mitigated by making the hidden state more expressive and adaptively learned during inference (Figure 2 right).

- **Follow-up research directions explicitly suggested**
  - **Richer outer-loop task parameterizations:** explore other families of self-supervised tasks beyond linear multi-view projections (`Î¸_K, Î¸_V, Î¸_Q`) (Â§5).
  - **Systems optimization:** better kernels, pipeline parallelism through time, and multi-device processing for very long sequences (Â§5).
  - **Longer contexts + larger models:** explore regimes beyond 32k and potentially very long contexts, where the paper expects TTT advantages to grow (Â§5).
  - **More ambitious learners `f`:** larger inner models (possibly convolutional nets for video/agents) when context becomes extremely long (Â§5).
  - **Multi-level nested learning:** if `f` is itself attention, it can be seen as nesting additional inner loops (discussion in Â§5, connected to Â§2.6 Theorem 2).

- **Practical applications / downstream use cases (grounded in the paperâ€™s framing)**
  - Any setting needing **efficient long-context conditioning** where quadratic attention is costly, but where a fixed-size RNN state is too restrictive, could benefit if the wall-clock issues can be resolved (Abstract; Â§1; Â§4.2).
  - The paper hints at very long sequential domains like video streams and embodied agents as future targets, where online adaptation is natural (Â§5; related work in Â§4.1 references video-stream TTT motivation).

- **Repro/Integration Guidance (based on the paperâ€™s reported choices)**
  - If you want a **linear-time layer** with relatively mature efficiency, `TTT-Linear` is the more practical instantiation in this paper (Abstract; Figure 12 shows TTT-MLP heavier).
  - If your priority is **long-context utilization** and you can tolerate more systems complexity, `TTT-MLP` is positioned as having â€œlarger potential in long contextâ€ but currently bottlenecked by memory I/O (Abstract; Â§3.3; Â§4.2).
  - For matching the paperâ€™s training setup, use:
    - `AdamW(Î²=(0.9,0.95))`, cosine decay to `1e-5`, 10% warmup, weight decay `0.1`, grad clip `1.0`, no dropout, mixed precision (Appendix C),
    - model-scale hyperparameters as in Table 3 (blocks, embed dim, heads, steps, peak LR, tokens),
    - TTT token mini-batch size `b=16` (Figure 7; Â§2.4),
    - learner stabilizers: LN + residual inside `f`, learnable `W_0`, and learnable gated `Î·(x)` (Â§2.7; Table 1).
  - When comparing to Transformers in long context, the paper highlights that finetuning a short-context Transformer is a stronger real-world baseline than training from scratch at long context, and they include `TF finetune` following a described recipe (20% more tokens) for â‰¥4k contexts (Â§3.2; Appendix C).



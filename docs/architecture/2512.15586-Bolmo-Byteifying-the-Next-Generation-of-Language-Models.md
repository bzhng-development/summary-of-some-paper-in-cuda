# Bolmo: Byteifying the Next Generation of Language Models

**ArXiv:** [2512.15586](https://arxiv.org/abs/2512.15586)

## üéØ Pitch

Bolmo introduces a practical, low-cost method to convert strong subword-tokenized LMs into competitive byte-level LMs via a tailored LTLM architecture and a two-stage subword-to-byte distillation procedure. This byteification delivers models (1B and 7B) that match or exceed prior byte-level work, markedly improve character understanding, enable flexible efficiency‚Äìperformance tradeoffs through higher bytes-per-patch compression, and can leverage existing subword post-training (via task arithmetic), making byte-level LMs practical for real-world use.

---

## 1. Executive Summary (2-3 sentences)

`Bolmo` is a family of fully open byte-level language models at `1B` and `7B` parameters that is created by *byteifying* existing strong subword-tokenized LMs (specifically `OLMo 2 1B` and `Olmo 3 7B`) rather than training byte-level models from scratch. The core contribution is an LTLM-style architecture plus a two-stage distillation/training procedure that makes this conversion cheap (Stage 2 uses `39.3B` tokens) while producing byte-level models that are competitive with subword LMs overall and substantially better on character-understanding benchmarks (e.g., `CUTE`, `EXECUTE`) (Tables 1‚Äì2, Sections 3‚Äì6). The paper also shows byte-level models can reach competitive inference speed by increasing bytes-per-patch compression, and can reuse subword post-training checkpoints via task arithmetic (Sections 5.1‚Äì5.2, Figures 3‚Äì4, 7).

## 2. Context and Motivation

- **Problem/gap addressed.**
  - Most deployed LMs use **subword tokenization** (fixed vocabulary of 30k‚Äì300k tokens), which the paper argues creates several issues (Sections 1‚Äì2):
    - Weak **character understanding** because internal token IDs hide character composition (motivating tasks like `CUTE` / `EXECUTE`).
    - **Tokenization bias**: boundaries depend on future characters, which can leak information and cause odd behavior when prompts end mid-word (Section 1 footnote 1; Section 2).
    - **Vocabulary bottleneck**: fixed vocabulary cannot efficiently cover all languages/words.
    - **Compute allocation rigidity**: compute and KV cache scale per token uniformly, even when a different granularity might be better (Section 2).
  - Byte-level modeling (over `UTF-8 bytes`, i.e., 256 symbols) can address these, but in practice has not been widely adopted.

- **Why prior byte-level approaches fall short (paper‚Äôs framing).**
  - Much prior byte-level work focuses on **training from scratch** and compares to compute-matched subword baselines.
  - Meanwhile, strong subword LMs benefit from rapidly evolving **data curation**, **architecture**, and **post-training** ecosystems; keeping pace by training new byte LMs from scratch is costly (Section 1).

- **Paper‚Äôs positioning.**
  - Introduces **byteification**: converting an existing strong subword LM into a byte-level LM with modest additional training, aiming to ‚Äúinherit‚Äù the source LM‚Äôs strengths and ecosystem (Sections 1, 3.2).
  - Uses an LTLM family architecture (similar in overall shape to `DTP`, `BLT`, `H-Net`) but modifies it to make byteification via *exact distillation* feasible (Section 3.1; Figure 2).

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a **byte-level language model** that reads and generates text as `UTF-8 bytes`, but internally groups bytes into learned **patches** so the expensive ‚Äúmain Transformer‚Äù runs on a shorter sequence.
- It solves the ‚Äúbytes are too long/slow‚Äù problem by using a **local encoder + boundary predictor + pooling** to compress bytes into patches for the global Transformer, then **depooling + local decoder** to return to byte-level predictions (Figure 1; Section 3.1).

### 3.2 Big-picture architecture (diagram in words)

- **Input bytes `x`** ‚Üí
- `T` **Tokenization & Embedding**: map each byte to a vector (plus an added subword-suffix embedding) ‚Üí
- `E` **Local encoder** (mLSTM stack): contextualize bytes locally ‚Üí
- `B` **Boundary predictor** (non-causal in prefill): decide patch boundaries ‚Üí
- **Pool**: turn variable-length byte spans into one vector per patch (take last-byte vector) ‚Üí
- `M` **Global model** (deep Transformer copied from the source subword LM): process patch sequence ‚Üí
- **Depool**: broadcast patch outputs back to byte positions (with residual connection) ‚Üí
- `D` **Local decoder** (mLSTM stack): refine byte-level states ‚Üí
- `LMHead`: predict next byte (and whether a boundary follows), i.e., next-byte probabilities over a 512-sized fused vocabulary (Figure 1; Sections 3.1, 3.1.1).

### 3.3 Roadmap for the deep dive

- Explain (1) what an LTLM is and why patches matter for efficiency.
- Detail (2) Bolmo‚Äôs specific architectural choices (`T`, `E`, `B`, pooling/depooling, `D`, `LMHead`) and what is new vs prior LTLMs.
- Detail (3) the *non-causal boundary prediction* mechanism and why it matters for byteification.
- Detail (4) the two-stage byteification procedure, including the **exact distillation** objective and its efficiency.
- Explain (5) how higher compression and post-training transfer work (Sections 5.1‚Äì5.2).

### 3.4 Detailed, sentence-based technical breakdown

- **Framing.** This is primarily an **empirical systems + algorithm/architecture** paper: it introduces an architecture variant of byte-level LTLMs and a training procedure that converts (byteifies) a subword LM into a byte-level LM cheaply, and validates the result on broad benchmark suites (Sections 3‚Äì6).

#### 3.4.1 Core modeling setup: bytes, patches, and ‚Äúlatent tokenization‚Äù

- A standard subword LM consumes a sequence of subword tokens; a na√Øve byte LM would consume many more steps (bytes), increasing cost.
- An LTLM addresses this by learning a **latent tokenization** into **patches**:
  - A **boundary predictor** marks some byte positions as ‚Äúend of patch.‚Äù
  - The model **pools** bytes within each patch into one patch vector.
  - A deep **global model** (Transformer) runs on patch vectors (shorter sequence), dominating compute.
  - The model **de-pools** Transformer outputs back to bytes and predicts the next byte at byte granularity (Figure 1; Section 3.1).

#### 3.4.2 Component-by-component architecture (Bolmo)

**(A) `T`: Tokenization & Embedding over bytes, plus subword-suffix residual**

- Each byte `x_i ‚àà {0,‚Ä¶,255}` gets a learned embedding `TByte(x_i) ‚àà R^d`.
- Bolmo also adds a *residual* embedding derived from the source subword vocabulary: the embedding of the **longest subword token that ends at the current byte position**:
  - The paper defines (Section 3.1):
    - `e_i := TByte(x_i) + TSubwordSuffix(x_:i)`
  - Intuition: this keeps ‚Äúcheap, sparse parameters‚Äù from the subword embedding table to improve the performance‚Äìefficiency tradeoff without slowing inference much (Section 3.1).
- This is presented as *not strictly necessary*‚Äîone could instead increase local encoder capacity‚Äîbut helpful in practice (Section 3.1).

**(B) `E`: Local encoder**

- `E` contextualizes byte embeddings using an `mLSTM` layer (a linear-RNN-like module; the paper treats it as a fast local model) to produce shallowly contextualized byte states `√™` (Section 3.1).
- The paper uses `1` encoder layer in the final configuration (Table 7).

**(C) `B`: Boundary predictor (key novelty: non-causal for prefill)**

- Boundary predictor outputs a score `p_t ‚àà [0,1]` per byte; if above a threshold, a patch boundary is placed after that byte (Section 3.1).
- **Key mismatch the paper fixes (Section 3.1.1; Figure 2):**
  - Subword LMs are ‚Äúcausal‚Äù over subword tokens, but the **tokenizer itself uses future characters** to decide boundaries (tokenization bias).
  - Prior LTLMs predict boundaries **causally** (only past context), so they cannot replicate subword boundary behavior well.
- **Bolmo‚Äôs fix:** during **prefill** (processing the prompt), use *one byte of future context* for boundary prediction:
  - Prior LTLM form: `B(√™)_t = f(√™_0,‚Ä¶,√™_t)`
  - Bolmo form: `BBolmo(√™)_t = f(√™_0,‚Ä¶,√™_t, √™_{t+1})` (Section 3.1.1).
- **Concrete parametrization:** cosine distance between projections of current byte and the next byte (Section 3.1.1):
  - `BBolmo(√™)_t := 1/2 * (1 - cos( Wq √™_{t+1}, Wk √™_t ))`
  - This yields a value in `[0,1]`.

**Worked micro-example (why one-byte lookahead matters).**
- The paper‚Äôs example compares `_Hello_Wor!` vs `_Hello_World!` (Section 3.1.1).
- At the byte position after ‚Äú`r`‚Äù in ‚Äú`Wor`,‚Äù a subword tokenizer might:
  - Place a boundary in `_Hello_Wor!` (token `_Wor` exists),
  - But *not* place a boundary in `_Hello_World!` (token `_World` exists),
  - Even though the prefix up to that byte is identical.
- A causal boundary predictor cannot distinguish these cases at that point; Bolmo‚Äôs one-byte lookahead can use the next byte (`!` vs `l`) to decide.

**(D) Pooling and depooling**

- **Pooling:** for each patch, Bolmo takes the representation of the **last byte in the patch** as the patch vector `h` (Section 3.1). This introduces no extra parameters.
- **Global model `M`:** Bolmo reuses the source model‚Äôs Transformer backbone (e.g., Olmo 3‚Äôs decoder-only Transformer) to process patch vectors `h ‚Üí ƒ•` (Section 3.1).
- **Depooling:** for each byte position, Bolmo adds (i) a linear projection of the local encoder byte representation and (ii) the latest available patch representation from the global model, forming `z` (Section 3.1).

**(E) `D`: Local decoder and `LMHead`**

- `D` is a stack of `mLSTM` layers (the paper uses `4` layers) to contextualize the depool result `z ‚Üí ·∫ë` (Section 3.1; Table 7).
- `LMHead` projects `·∫ë` to next-byte probabilities via softmax (Section 3.1).
- **Output boundary prediction during decoding (Section 3.1.1; Figure 2):**
  - During decoding you cannot use future bytes, so Bolmo predicts whether a patch ends *as part of the next-byte prediction*.
  - Bolmo introduces a special boundary symbol `<b>` conceptually, but makes it ‚Äúeffectively zero-cost‚Äù via **boundary symbol fusion**:
    - It doubles the output vocabulary from `256` bytes to `512` symbols: for each byte, there is a version ‚Äúbyte-with-boundary.‚Äù
    - So each step predicts both the byte and whether a boundary follows, without increasing sequence length (Section 3.1.1).

**(F) Parameter accounting**

- The design aims to keep parameter count close to the source model by removing the subword output embedding matrix and adding local modules (Section 3.1).
- Reported totals:
  - `Bolmo 1B`: ~`10M` fewer parameters than `OLMo 2 1B` (`‚àí0.7%`) (Section 3.1).
  - `Bolmo 7B`: ~`330M` more than `Olmo 3 7B` (`+4.5%`) (Section 3.1).
  - Table 1 lists `Bolmo 7B` as `7.63B` params vs `Olmo 3 7B` `7.30B`.
  - Table 2 lists `Bolmo 1B` as `1.47B` params vs `OLMo 2 1B` `1.48B`.

#### 3.4.3 Two-stage byteification procedure (Section 3.2)

**Initialization (common to both stages).**
- Copy the **global Transformer `M`** parameters from the source subword checkpoint.
- Initialize local encoder/decoder, boundary predictor, and LM head randomly (Section 3.2).

---

### Stage 1: Subword-to-byte distillation (Section 3.2.1)

**Goal:** learn local modules so the byte-level system can *exactly recover the behavior* of the source subword LM, while keeping `M` frozen for efficiency.

**Why it is efficient (paper‚Äôs claim, Section 3.2.1).**
- Stage 1 requires:
  - One forward pass through all layers,
  - But only a backward pass through the *first `n` Transformer layers* (plus local modules), instead of backprop through the full Transformer.
- They choose `n = 4` layers as a balance (Section 3.2.1).

**Stage 1 loss = boundary imitation + encoder matching + decoder/LM matching.**

1) **Boundary predictor training** to match subword tokenizer boundaries:
- Define `Bsubword(x)_t = 1` if byte `t` is the **last byte of a subword token**, else `0`.
- Train with binary cross-entropy:
  - `LB = - Œ£_t [ Bsubword(x)_t log BBolmo(√™)_t + (1 - Bsubword(x)_t) log(1 - BBolmo(√™)_t) ]`
- The paper reports `>99%` boundary accuracy for prefill boundary prediction (Section 3.2.1).

2) **Local encoder training** to substitute for the subword embedding matrix.
- Prior work minimized L2 between pooled representations and subword embeddings directly (equivalent to `n=0`).
- Bolmo instead matches *representations after passing through the first `n` Transformer layers* (motivated by ‚Äúmodel stitching‚Äù concerns) (Section 3.2.1):
  - `LE = || M_:n( Pool(E(e), Bsubword(x)) ) - M_:n( Tsubword(x) ) ||`
- Notably, they pool using **true** subword boundaries during this objective to preserve alignment (footnote 14 in Section 3.2.1).

3) **Local decoder + LM head distillation** with an ‚Äúexact‚Äù patch likelihood matching enabled by output boundary prediction.
- The goal is to match the probability the subword model assigns to each subword token with the probability the byte model assigns to the corresponding sequence of bytes (including boundary emission), yielding an exact objective when boundaries align (Section 3.2.1).
- They use a temperature-modulated binary cross-entropy with `œÑ = 5` (Section 3.2.1).
- They optionally add standard next-byte cross-entropy `LD,CE` to begin exploiting byte-level information early (Section 3.2.1).

**Combined Stage 1 loss:**
- `LStage1 = ŒªB LB + ŒªE LE + ŒªD,Distill LD,Distill + ŒªD,CE LD,CE`
- Weights are set to:
  - `ŒªB = 4`, `ŒªE = 1`, `ŒªD,Distill = 1`, `ŒªD,CE = 1` (Section 3.2.1).

---

### Stage 2: End-to-end training (Section 3.2.2)

**Goal:** fine-tune the *entire* byteified model (including `M`) to adjust to imperfect boundary prediction and to exploit byte-level information.

- Stage 2 retains:
  - Boundary loss `LB`,
  - Standard next-byte cross-entropy (now computed on the model‚Äôs own outputs), denoted `LCE`.
- Combined:
  - `LStage2 = ŒªB LB + ŒªCE LCE` (Section 3.2.2).

#### 3.4.4 Training configuration / hyperparameters (from Tables 7‚Äì8)

Because the paper provides full run-level hyperparameters for Bolmo 7B and 1B, the key settings are:

- **Data mix:** `Bolmo Mix` = ~`172B` tokens from `Dolma 3` mix + `75M` tokens of `CUTE-style` synthetic character tasks; training is for less than one epoch (Section 4; Appendix C).
  - Tokens are counted using the ‚ÄúDolma2 Tokenizer‚Äù (footnote 16, Section 4).

- **Sequence lengths:**
  - Max length: `4096` tokens and `24576` bytes (Table 8).

- **Optimizer:** `AdamW` with `Œ≤1=0.9`, `Œ≤2=0.95`, weight decay `0.1`, max grad norm `0.5` (Table 8).

- **Learning-rate schedule:** Warmup + linear decay (Table 8).
  - Stage 1 peak LR:
    - `7B`: `5e-4`
    - `1B`: `7e-4`
  - Stage 2 peak LR:
    - Global model: `1.8e-5` (7B), `2.6e-5` (1B)
    - Local models: `3.7e-5` (7B), `5.2e-5` (1B)

- **Training volumes:**
  - Stage 1: `9.8B` tokens (~`43.1B` bytes), `75K` steps, batch size `32` (Table 8).
  - Stage 2: `39.3B` tokens (~`172.9B` bytes), `150K` steps, batch size `64` (Table 8).

- **Throughput (on H100 GPUs, per accelerator, final runs):**
  - Stage 1:
    - 7B: `9.9K` tokens/s and `59.4K` bytes/s
    - 1B: `34.5K` tokens/s and `207K` bytes/s
  - Stage 2:
    - 7B: `6.3K` tokens/s and `37.8K` bytes/s
    - 1B: `27.7K` tokens/s and `166.2K` bytes/s (Table 8)

- **Local module architecture (Table 7):**
  - 7B uses dimension `4096`; 1B uses `2048`.
  - Encoder: `1` layer of (`mLSTM + FFN`).
  - Decoder: `4` layers of (`mLSTM + FFN`).
  - `mLSTM` heads: `16`; QK dim `128`; V dim `256`; exponential nonlinearity; ‚ÄúGate Soft Cap‚Äù `15`; input gate bias init `-10`.
  - FFN expansion: `5504` (7B) / `2816` (1B); activation `SwiGLU`; normalization `RMSNorm`.

## 4. Key Insights and Innovations

- **(1) ‚ÄúByteification‚Äù as a practical development strategy (Sections 1, 3.2).**
  - Instead of training byte-level LMs from scratch, Bolmo converts a strong subword model into a byte-level model with relatively small additional training (`39.3B` tokens in Stage 2; plus Stage 1 distillation).
  - Significance: it leverages existing pretrained checkpoints and the broader ‚Äúecosystem‚Äù (evaluation, post-training, etc.) around subword LMs.

- **(2) Non-causal boundary prediction during prefill to match subword boundary expressivity (Section 3.1.1; Figures 2, 5; Table 3).**
  - Prior LTLMs predict patch boundaries causally, but subword tokenization uses future context.
  - Bolmo‚Äôs one-byte lookahead boundary predictor resolves a specific expressivity mismatch that otherwise harms the ability to emulate the teacher model during byteification.
  - The ablation in Figure 5 shows this materially affects boundary error and downstream performance.

- **(3) Output boundary prediction + boundary fusion enables an ‚Äúexact distillation‚Äù objective (Section 3.2.1).**
  - Predicting boundaries as part of decoding makes it possible to compute a patch-level likelihood matching objective that is *exact* rather than approximate, avoiding the need for heuristic likelihood alignment across different tokenizations.
  - Boundary fusion keeps this from inflating the sequence length at decoding time by expanding the output vocabulary from `256` to `512` instead.

- **(4) Stage 1 training that avoids full backprop through the global Transformer (Section 3.2.1; Figure 6).**
  - The paper‚Äôs Stage 1 setup backprops through only the first `n=4` Transformer layers, making it cheaper while still improving final performance compared to starting end-to-end immediately.

- **(5) Demonstrations unique to the byteified LTLM setting: higher compression and ecosystem reuse (Sections 5.1‚Äì5.2; Figures 3‚Äì4, 7).**
  - **Higher bytes-per-patch compression** provides a smooth performance‚Äìefficiency trade-off without a subword softmax bottleneck (Figure 3).
  - **Task arithmetic** can transfer instruction-following behavior from a post-trained subword checkpoint into Bolmo‚Äôs shared Transformer layers with *no additional training* (Figure 4).

## 5. Experimental Analysis

### Evaluation methodology (Section 4; Appendix B; Tables 5‚Äì6)

- **Baselines.**
  - Main baseline for 7B: the source `Olmo 3 7B` plus a **continued training** variant trained on the same data for the same number of global-model gradient updates, to separate ‚Äúbyteification effects‚Äù from ‚Äúcontinued training effects‚Äù (Section 4; Appendix A/Table 4).
  - Comparisons to prior byte-level models in Table 1 include `EvaByte 6.5B`, `TFree-Hat 7B`, `BLT 7B`. Table 2 includes `H-Net XL` and `BLT 1B`.

- **Benchmark suites.**
  - `Bolmo 7B` suite is based on `OlmoBaseEval` with added `CUTE` and `EXECUTE` for character understanding (Section 4).
    - Note: Section 4 says it skips GSM Symbolic and BigCodeBench ‚Äúdue to their size,‚Äù but Table 5 lists `BigCodeBench`. The provided excerpt contains this inconsistency; Table 5 is the only place with the full enumerated task list.
  - `Bolmo 1B` suite is based on `Base Easy Suite` plus `CUTE` (Section 4; Table 6).
  - They use `OLMES` for evaluations (Appendix B).

- **Metrics and settings.**
  - A mix of multiple-choice accuracy, rank-choice accuracy, generative QA F1, code execution `pass@k`, and CoT exact match for math tasks (Table 5).
  - Code sampling settings mentioned for Table 1 discussion: `temperature = 0.6`, `top_p = 0.6` with `pass@1` and `pass@16` (Section 5; Appendix B referenced).

### Main quantitative results

#### 5.1 Bolmo 7B vs other byte-level models and vs source subword LM (Table 1)

- **Overall claim supported by Table 1:** Bolmo 7B is best across nearly all listed categories among the compared byte-level models, with one exception: it is slightly lower on `GenQA` than `TFree-Hat 7B` (`70.9` vs `71.3`) (Section 5; Table 1).
- **Character understanding:**
  - `Char`: `75.1` (Bolmo) vs `56.0` (Olmo 3)
  - `CUTE`: `78.6` vs `56.9`
  - `EXECUTE`: `71.6` vs `55.1` (Table 1)
  - This is one of the strongest wins emphasized by the paper (Section 5).
- **Code aggregate category:**
  - `Code`: `40.7` (Bolmo) vs `39.5` (Olmo 3) (Table 1).
  - Individual code benchmarks show a mixed picture:
    - `HumanEval pass@1/@16`: `40.6 / 74.7` (Bolmo) vs `49.0 / 71.1` (Olmo 3).
    - This reflects the paper‚Äôs observation: higher `pass@16` but generally lower `pass@1`, interpreted as more diverse generations under the fixed sampling setup (Section 5).
- **Math category:** `48.9` (Bolmo) vs `55.3` (Olmo 3) (Table 1), suggesting some remaining gap on math-heavy evaluation.
- **Multiple-choice categories (examples):**
  - `ARC MC`: `88.5` (Bolmo) vs `89.2` (Olmo 3).
  - `MMLU STEM`: `57.0` vs `59.5`.
  - `MMLU Humanities`: `67.2` vs `69.2` (Table 1).

#### 5.2 Bolmo 1B vs other byte-level models and source subword LM (Table 2)

- `Bolmo 1B Suite` aggregate: `58.2` vs `58.3` (OLMo 2 1B), essentially matched overall (Table 2).
- Notable wins/losses:
  - `CUTE`: `60.0` (Bolmo 1B) vs `27.5` (OLMo 2 1B), a large gain in character understanding (Table 2).
  - `MMLU`: `37.2` vs `40.4` (drop).
  - `Lambada`: `65.2` vs `60.1` (gain).
  - `CoQA`: `81.7` vs `77.4` (gain) (Table 2).
- Comparison vs other byte-level models:
  - Bolmo 1B outperforms `H-Net XL` variants on many tasks and is close to `BLT 1B` on the suite aggregate (`58.2` vs `58.5`), though the paper notes `BLT 1B` parameter counting differs due to hash embedding parameters (Section 5; Table 2).

### Ablations / support for claims

- **Non-causal boundaries matter (Section 6.1; Figure 5; Table 3).**
  - Figure 5 summarizes that causal boundary predictors must trade off boundary accuracy vs representation matching; non-causal prediction enables both, improving downstream core task performance after Stage 1.
  - Table 3 further supports this by showing much worse outcomes for ‚Äúpatch end, causal‚Äù under learned boundary prediction (e.g., boundary accuracy `96.0` and large performance drop) compared to ‚Äúpatch end, non-causal fused‚Äù (boundary accuracy `99.2` and better average) (Table 3).

- **Stage 1 helps but isn‚Äôt strictly required (Section 6.2; Figure 6).**
  - Figure 6 shows training without Stage 1 can recover but tends to have worse bits-per-byte for much of training, especially for the 1B model.
  - The paper FLOP-matches by extending Stage 2-only training by ~`6.5B` tokens to approximate Stage 1 compute (Section 6.2).

### Efficiency / inference analysis (Sections 5.1, 6.3; Figures 3, 7)

- **Higher compression training (Section 5.1; Figure 3).**
  - They change boundary supervision to encourage *more bytes per patch* by merging subword tokens until a target compression ratio `t` is reached, using three strategies:
    - per-example BPE-style merges,
    - entropy-based merges using an auxiliary `370M` subword LM,
    - cross-entropy-based merges using the same auxiliary LM.
  - Figure 3 argues subword LMs eventually hit a softmax FLOP bottleneck as vocabulary grows (demonstrated using `SuperBPE` transfer to `200k` and `400k` vocabularies), while byte-level models can keep increasing compression (Section 5.1; Figure 3).

- **Wallclock speed measurements over FLOPs (Section 6.3; Figure 7).**
  - The paper measures on `H100` GPUs with `batchsize=1`:
    - At similar compression (`c=4.4 bytes/patch`), decoding throughput is ~`125 bytes/s` for Bolmo vs ~`150 bytes/s` for the subword model (Section 6.3; Figure 7).
    - Prefill for `72K` bytes: ~`1s` for Bolmo vs ~`0.8s` for the subword model at equivalent content length (Section 6.3; Figure 7).
    - Bolmo surpasses the subword model in inference efficiency at about `c ‚âà 6.6 bytes per patch` (Figure 7; Section 6.3).

### Do experiments convincingly support the claims?

- The results strongly support:
  - **Competitiveness vs other byte-level models** (Table 1 shows consistent improvements).
  - **Large gains on character understanding** (Tables 1‚Äì2).
  - **Importance of non-causal prefill boundaries** for byteification (Figure 5; Table 3).
  - **Feasibility of speed/quality trade-offs via compression** (Figures 3, 7).
- Some claims are appropriately qualified:
  - On code diversity (pass@16 vs pass@1), the paper notes it cannot conclude byte-level models are inherently better without exploring sampling trade-offs (Section 5).
  - On ‚Äúremaining gap to Olmo 3,‚Äù Appendix A/Table 4 argues much of it is due to continued training effects rather than byteification alone.

## 6. Limitations and Trade-offs

- **Boundary predictor is not perfect, and errors matter.**
  - The paper explicitly attributes part of the remaining gap to the subword model to <100% boundary accuracy (Section 6.1; Appendix A; Table 3 shows learned boundary accuracy ~`99.2%` in some settings).

- **Non-causal boundaries are not trained end-to-end here (Section 3.1.1).**
  - The paper argues end-to-end learning with non-causal boundaries risks ‚Äúdegenerate‚Äù leakage: boundary scores could transmit many bits about the next byte (e.g., via bfloat16 boundary scores) (Section 3.1.1).
  - As a result, Bolmo relies on **external supervision** for boundaries.

- **Continued-training degradation complicates attribution (Appendix A/Table 4).**
  - Continued training of `Olmo 3` on the Bolmo mix can degrade performance on various tasks; thus some observed gaps may be driven by the continued training setup rather than byteification per se (Appendix A).

- **Task arithmetic transfer depends on ‚Äúembedding resettability‚Äù (Section 5.2; Appendix D).**
  - Only the shared Transformer layers map 1-to-1 between source LM and Bolmo‚Äôs global model.
  - If post-training substantially changes the embedding interface, transferring only inner layers may fail; Appendix D shows it ‚Äúgenerally‚Äù works but not always.

- **Evaluation/serving scope limitations.**
  - Speed measurements are primarily `batchsize=1` and the paper notes batched inference for dynamic LTLMs needs more work (Future Direction Bit 7).
  - The atomic unit is `UTF-8 bytes`, which the paper calls ‚ÄúLatin-centric,‚Äù and biases may persist (Future Direction Bit 6).

- **Potential dependence on source model ecosystem and design choices.**
  - Retaining subword-suffix embeddings is a deliberate design choice to get a better efficiency trade-off; it is not strictly necessary but is part of the reported best-performing configuration (Section 3.1).

## 7. Implications and Future Directions

- **How this changes the landscape (within the paper‚Äôs scope).**
  - The paper reframes byte-level LMs from ‚Äútrain from scratch‚Äù to ‚Äúconvert strong existing LMs,‚Äù which could drastically lower iteration cost and make byte-level modeling more practical for real use cases (Sections 1, 7).
  - It suggests byte-level LMs can be competitive not only in robustness/character tasks but also in end-to-end performance while offering new efficiency knobs (compression) not available to subword LMs (Sections 5.1, 6.3).

- **Practical applications / downstream use cases suggested by results.**
  - Applications where **character-level fidelity** matters (spelling, exact string operations, multilingual token understanding) are directly targeted by `CUTE` / `EXECUTE` gains (Tables 1‚Äì2; Section 5).
  - Settings needing **flexible efficiency trade-offs** can tune bytes-per-patch compression (Section 5.1; Figure 7).

- **Repro/Integration Guidance (based on what the paper shows).**
  - Prefer **byteification** when you already have a strong subword checkpoint and want byte-level benefits without full retraining (Sections 1, 3.2).
  - Use **Stage 1** if you want faster iteration and generally better final bpb, especially at smaller scale (Section 6.2; Figure 6).
  - If you need speed, consider training with **higher compression factors** (Section 5.1) and measure **wallclock latency/throughput**, not just FLOPs (Section 6.3).

- **Explicit future directions listed by the paper (Section 8).**
  - Test whether byteification-optimized architecture choices (e.g., non-causal boundaries) also help when **training from scratch** (Bit 0).
  - Develop methods to **learn non-causal boundaries end-to-end** without leakage/degeneracy (Bit 1).
  - Jointly scale **patch size and local model capacity** for better speed/quality trade-offs (Bit 2).
  - Explore **multi-byte prediction** to reduce sequential overhead (Bit 3).
  - Reduce continued-training ‚Äúdestructiveness‚Äù via **PEFT-style methods** (Bit 4).
  - Create sampling methods specialized for LTLMs (Bit 5).
  - Replace UTF-8 bytes with more equitable atomic units (Bit 6).
  - Build **batched inference** infrastructure for dynamic patching (Bit 7).

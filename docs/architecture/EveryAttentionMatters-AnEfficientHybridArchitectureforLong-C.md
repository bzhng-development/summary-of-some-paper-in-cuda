# Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning

**ArXiv:** [2510.19338](https://arxiv.org/abs/2510.19338)

## ðŸŽ¯ Pitch

This paper introduces Ring-linear-2.0, a new family of large language models that combine linear attention for most layers with periodic softmax attention, dramatically slashing memory and compute costs for long-context reasoning. By further integrating advanced FP8 fused kernels and a robust trainingâ€“inference alignment procedure for reinforcement learning, these models achieve up to 10x faster inference and robust, stable training over exceptionally long contextsâ€”all while preserving state-of-the-art reasoning performance. This innovation makes practical, economical deployment of long-context LLMs vastly more feasible in real-world applications such as agents, code assistants, and knowledge retrieval.

---

## 1. Executive Summary
This paper proposes a long-context largeâ€‘languageâ€‘model family, `Ring-linear-2.0`, that mixes two kinds of attention: fast, constantâ€‘state â€œlinear attentionâ€ for most layers and standard softmax attention sparingly. It couples this architecture with FP8 kernel fusions and a trainingâ€“inference alignment procedure for reinforcement learning (RL), yielding large speedups and stable longâ€‘horizon training while maintaining competitive reasoning quality (Figures 6â€“8; Tables 2â€“3).

## 2. Context and Motivation
- Problem addressed
  - Longâ€‘context reasoning (agents, code generation, retrieval, testâ€‘time scaling) demands processing sequences well beyond 32K tokens. Standard softmax attention has quadratic compute with sequence length and a Keyâ€“Value (`KV`) cache that grows linearly with output length, inflating both compute and memory I/O during decoding (Section 2.2; Figure 4).
- Why it matters
  - Practical deployments increasingly rely on â€œtestâ€‘time scalingâ€â€”letting models â€œthink longerâ€ by generating more tokensâ€”and on long input contexts. Without better attention mechanisms and efficient kernels, cost and latency make these use cases uneconomical (Introduction; Section 2.2.4).
- Prior approaches and gaps
  - Pure linear-time sequence models (RetNet, Lightning Attention, Mamba, Gated Linear Attention, DeltaNetâ€”Section 1) reduce theoretical complexity and keep a constantâ€‘size state. However:
    - Pure linear models often underperform at large scale and on retrieval (Section 1; Section 2.2.2).
    - Their advantages tend to appear only beyond ~8K tokens, while much pretraining still sits in the 4â€“8K range; MoE computation can dominate and blunt efficiency gains during pretraining (Section 1).
    - Community kernels for linear attention were fragmented and lacked support for advanced inference features like speculative decoding with tree masks (Section 3.4).
  - Hybrid designs mixing linear and softmax attention exist (e.g., Minimax M1, GPTâ€‘OSS, Qwen3â€‘Next; Section 1) but lack a systematic exploration of the optimal ratio, fullâ€‘stack kernel fusions in FP8, and trainingâ€“inference alignment for longâ€‘horizon RL.
- Positioning
  - `Ring-linear-2.0` targets this gap with: (a) a tuned hybrid ratio found by scalingâ€‘law analysis (Section 2.2.2; Figure 3), (b) infrastructureâ€‘level FP8 kernel fusion (â€œlingheâ€) for training and inference (Section 3; Figure 6), and (c) a moduleâ€‘byâ€‘module alignment method that stabilizes RL for long Chainâ€‘ofâ€‘Thought outputs (Section 5.2; Figures 10â€“12).

## 3. Technical Approach
At a glance, the system has three pillars: a hybrid linearâ€“softmax architecture, compute kernels and FP8 training/inference optimizations, and a training pipeline (continued pretrain â†’ SFT â†’ RL) with trainingâ€“inference alignment.

- Model architecture (Section 2; Figure 2; Table 1)
  - The model is organized into layer groups. Each group has `M` linearâ€‘attention blocks followed by one softmax attention block (â€œGrouped Query Attentionâ€, `GQA`).
    - `GQA` reduces KV cache by sharing keys/values across multiple query heads.
  - The feedforward is a highâ€‘sparsity `MoE` (Mixtureâ€‘ofâ€‘Experts) with a 1/32 activation ratio (only ~3% of expert parameters are active per token), optimized with choices like shared experts, sigmoid routing without auxiliary loadâ€‘balancing loss, and Multiâ€‘Token Prediction (`MTP`) heads (Section 2.1; Figure 2).
  - Two open models:
    - `Ring-mini-linear-2.0`: 16.4B total parameters, 1.6B active (957M nonâ€‘embedding), `d_model=2048`, `n_layers=20`, `n_experts=256`, `top_k=8`, `n_heads=16`, `n_kv_heads=4`, hybrid ratio `1:4` (one softmax after 4 linear layers), 128K context (Table 1).
    - `Ring-flash-linear-2.0`: 104.2B total, 7.4B active (6.1B nonâ€‘embedding), `d_model=4096`, `n_layers=32`, same MoE settings, `n_heads=32`, hybrid ratio `1:7`, 128K context (Table 1).

- Linear attention core (Section 2.2.1; Equations 1â€“4)
  - Intuition: replace the pairwise attention computation with a recurrent accumulation of â€œkeyâ€“value outer products,â€ so decoding only needs a constantâ€‘size state per head.
  - Formalization:
    - Standard attention output: `O = Q (K^T V)` (Eq. 1).
    - With fixed decay `Î»` (Lightning Attention form), the tâ€‘th output is
      - `o_t = q_t âˆ‘_{sâ‰¤t} Î»^{t-s} k_s^T v_s` (Eq. 2),
      - maintained via a recurrent state `kv_t = Î» kv_{t-1} + k_t^T v_t`, `o_t = q_t (kv_t)` (Eq. 3).
    - The state `kv_t âˆˆ R^{dÃ—d}` is the constantâ€‘size KV cache for linear attention (Eq. 4). This makes memory independent of sequence length in decoding.

- Why a hybrid architecture (Section 2.2.2â€“2.2.3)
  - Pure linear attention underperforms retrieval/extrapolation; a small number of softmax layers repairs this (Section 2.2.2).
  - The paper fits computeâ€“loss scaling laws (Chinchillaâ€‘style) for different ratios (Figure 3).
    - > â€œthe hybrid linear architecture consistently outperforms the pure softmax architectureâ€¦ a large layer group size (e.g., `M=7`) performs well under high FLOP budgetsâ€ (Figure 3).
  - Final choices: `M=7` for the 104B model, `M=4` for the 16B model (Section 2.2.2; Table 1).

- Design choices inside linear attention (Section 2.2.3)
  - `Grouped RMSNorm`: normalize locally per tensorâ€‘parallel rank to avoid allâ€‘reduce (cuts communication).
  - `Partial RoPE` after `QK` normalization: RoPE on half the dimensions lowered LM loss by ~0.004 (Section 2.2.3; Figure 2).
  - `Head-wise decay`: a powerâ€‘law schedule across heads improved LM loss by ~0.04 over linear decay and helped downstream tasks (Section 2.2.3).

- Decoding cost model (Section 2.2.4; Figure 4)
  - Decoding speed is bound by memory bandwidth to the KV cache/state. Linear attentionâ€™s constantâ€‘size state plus `GQA` yields substantially smaller memory access growth than softmax, `GQA`, or `MLA` alone as sequence length increases (Figure 4).

- Kernel and FP8 training/inference optimizations (Section 3)
  - Fused kernels to minimize memory traffic and activation size (Figure 5), including:
    - Linear attention gate fusion (transpose + grouped norm + sigmoid gate).
    - `permute/unpermute` fused with padding by modifying the routing map.
    - `QK` normalization fused with split, partial RoPE, and transpose.
    - Router casting done inâ€‘kernel (BF16â†’FP32) to reduce I/O.
    - A single Triton prefill kernel for linear attention by reâ€‘partitioning Q/K and V (instead of 2â€“4 kernels; Section 3.1).
  - FP8 training (Section 3.2):
    - Quantization fusion: e.g., make `SiLU` output quantized tensors directly, halving I/O from ~`8MN` to `4MN` for input shape `[M, N]`.
    - Stateâ€‘aware recomputation: during backward, only produce quantized `x^T` needed for `dw = x^T y`; during forward (nonâ€‘recompute) only produce quantized `x`â€”reduces redundant compute and quantize ops.

- Inference system integration (Section 3.4; Figures 7â€“8)
  - Integrated fused linear attention kernels into SGLang and vLLM, expanding mode coverage and throughput.
  - Implemented the first linear attention kernel supporting tree masks, enabling speculative decoding for a hybrid linear model; available in the offline framework `Flood` and being ported to SGLang (Section 3.4).

- Training pipeline and schedulers (Section 4; Figure 9)
  - Start from softmaxâ€‘based `Ling-base-2.0-20T` checkpoints; convert each linear layerâ€™s `QKV` into MHA weights along head dimension and randomly initialize new gate/RMSNorm parameters (Section 4).
  - Twoâ€‘stage continued pretraining:
    1) â€œContinued trainingâ€ at 4K context to recover base capabilitiesâ€”600B tokens (`mini`), 1T tokens (`flash`).
    2) â€œMidâ€‘trainingâ€: extend contexts 4K â†’ 32K â†’ 128K while increasing highâ€‘quality reasoning data.
  - Use a `WSM` (Warmupâ€‘Stableâ€‘Merge) LR schedule (merge midâ€‘training checkpoints instead of explicit decay), recovering â‰¥98% of the base models across categories (Figure 9).

- Postâ€‘training: SFT and RL with alignment (Section 5)
  - SFT data mixes hard reasoning (math, code, science, logic) with general tasks and reâ€‘synthesized functionâ€‘calling to match broader calling patterns (Section 5.1).
  - RL training on carefully filtered, appropriately difficult data at long contexts (often 64K) to avoid truncation and reach a higher ceiling (Section 5.2).
  - Trainingâ€“inference alignment (Section 5.2.1; Figures 10â€“12):
    - Systematically feed the same inputs through training and inference engines and match activations layerâ€‘byâ€‘layer, fixing modules that drift.
    - Key fixes:
      - Use FP32 state for linearâ€‘attention KV accumulation at inference; BF16 causes error growth (Figure 11).
      - FP32 `lm_head` math via a custom GEMM that takes BF16 inputs but accumulates in registers to control cost.
      - Ensure identical `RMSNorm` eps, FP32 residuals, and unfused residualâ€‘norm in both stacks.
      - Match RoPE numerics between PyTorch training and inference operators.
      - Use the same attention backend (e.g., FlashAttention) and align prefillâ€“decode numerics.
      - Make MoE deterministic: stable `topk`, fixed permutation/summation orders, same operators.
    - RL objective: with alignment, PPO can safely use rolloutâ€‘engine probabilities for clipping (Eq. 5), instead of biasâ€‘inducing recomputed training probabilities (Eq. 6). This improves rewards and stabilizes the probability gap (Figure 12).

## 4. Key Insights and Innovations
- Hybrid ratio chosen by scaling laws (Figure 3)
  - Novelty: explicitly fit computeâ€“loss curves to select the number of linear layers per softmax layer, rather than adâ€‘hoc design. The optimal `M` depends on FLOP budget; they fix `M=7` (flash) and `M=4` (mini) to balance efficiency and quality (Section 2.2.2).
  - Significance: keeps most of the model in linear attention without losing retrieval/extrapolation, outperforming pure softmax in the scaling regime explored.

- Constantâ€‘state linear attention with targeted architectural tweaks (Section 2.2.1â€“2.2.3)
  - Novelty: a practical Lightningâ€‘style linear attention implementation with `Grouped RMSNorm`, `QK` norm, partial RoPE, and headâ€‘wise powerâ€‘law decay, each justified by measured LMâ€‘loss improvements.
  - Significance: delivers linear scaling in sequence length for compute and constant state for decoding, addressing the I/O bottleneck (Figures 4, 7â€“8).

- FP8 endâ€‘toâ€‘end kernel fusion and â€œstateâ€‘aware recomputeâ€ (Section 3; Figure 5)
  - Novelty: quantization fused into surrounding ops (e.g., `SiLU`), singleâ€‘kernel prefill for linear attention, and recomputation that emits different quantized tensors depending on whether a backward pass needs `x` or `x^T`.
  - Significance: large training throughput gainsâ€”up to +77% on the 16B model and +57% on the 104B model (Figure 6)â€”and higher inference throughput (Figures 7â€“8).

- Trainingâ€“inference alignment for longâ€‘horizon RL in hybrid linear MoE models (Section 5.2; Figures 10â€“12)
  - Novelty: a systematic, moduleâ€‘level alignment process (KV cache precision, lm_head precision, norms, RoPE, attention backend, MoE determinism) tailored to hybrid linear+MoE, coupled with using rollout probabilities in PPO after alignment (Eq. 5).
  - Significance: prevents RL collapse, yielding steadily increasing reward and test scores (Figure 13).

- First treeâ€‘maskâ€“compatible linearâ€‘attention kernel for speculative decoding (Section 3.4)
  - Novelty: enabling treeâ€‘based speculative decoding in a linearâ€‘attention setting, previously blocked by mask support.
  - Significance: reduces smallâ€‘batch latency for long-context decoding while keeping linearâ€‘attention efficiency.

## 5. Experimental Analysis
- Evaluation setup (Section 6.1; Tables 2â€“3; Figure 1)
  - 17 benchmarks covering:
    - Mathematical reasoning: AIMEâ€™24/â€™25, OlympiadBench, CNMOâ€™24, LiveMathBench, TheoremQA.
    - Coding/agents: HumanEval+, MBPP+, LiveCodeBench (LCB), Codeforces Elo, Spider, BFCLâ€‘Live.
    - General reasoning/knowledge: GPQAâ€‘Diamond, SciBench, DROP, MuSR, Multiâ€‘LogiEval.
  - Comparisons:
    - For `Ring-mini-linear-2.0`: vs `Ring-mini-2.0`, `Qwen3â€‘8Bâ€‘Thinking`, `GPTâ€‘OSSâ€‘20Bâ€‘Medium` (Table 2).
    - For `Ring-flash-linear-2.0`: vs `Ring-flash-2.0`, `Qwen3â€‘32Bâ€‘Thinking`, `Geminiâ€‘2.5â€‘Flash`, `GPTâ€‘OSSâ€‘120Bâ€‘Medium`, `Seedâ€‘OSSâ€‘36Bâ€‘Instruct`, `Qwen3â€‘Nextâ€‘80Bâ€‘A3Bâ€‘Thinking` (Table 3; Figure 1).

- Main quantitative results (Tables 2â€“3)
  - `Ring-mini-linear-2.0` (Table 2):
    - Math: matches or slightly trails `Ring-mini-2.0` by small margins (e.g., AIMEâ€™25 73.65 vs 74.06; TheoremQA 69.69 vs 70.09) while staying competitive with `Qwen3â€‘8Bâ€‘Thinking`.
    - Coding: wins on Codeforces Elo (83.84 vs Qwenâ€™s 73.31) and is competitive on HumanEval+ / MBPP+/LCB.
    - General reasoning: mixedâ€”e.g., GPQAâ€‘Diamond 65.69 (near GPTâ€‘OSSâ€‘20B 65.53), strong on DROP 83.20 but behind `Ring-mini-2.0` (88.55).
    - Takeaway: despite using mostly linear attention and only 1.6B active params, performance remains comparable to strong 8â€“20B baselines across tasks.
  - `Ring-flash-linear-2.0` (Table 3; Figure 1):
    - Math: very strong. AIMEâ€™25 86.51 (close to `Ringâ€‘Flashâ€‘2.0` 86.98; near top vs `Qwen3â€‘Nextâ€‘80Bâ€‘A3B` 87.80), OlympiadBench 87.36, CNMOâ€™24 84.98.
    - Coding: LCB 70.37 (better than `Ringâ€‘Flashâ€‘2.0` 70.76â‰ˆ, `Qwen3â€‘32B` 62.33, and `Geminiâ€‘2.5â€‘Flash` 61.40), Codeforces Elo 90.24 (high, similar to `Ringâ€‘Flashâ€‘2.0` 90.23).
    - General reasoning: GPQAâ€‘Diamond 74.49 (above `Qwen3â€‘32B` 68.40 but below `Geminiâ€‘2.5â€‘Flash` 82.80 and `Qwen3â€‘Nextâ€‘80Bâ€‘A3B` 77.20); DROP 89.66 (competitive).
    - Overall: broad competitiveness with larger models, with standout math/coding strengths (Figure 1 highlights AIMEâ€™25, LCB, Codeforces, GPQA).

- Efficiency results
  - Training throughput (Figure 6):
    - `Ring-mini-linear-2.0`: +21% with fused kernels vs baseline FP8; +77% when fused kernels allow `TP=1` (from `TP=2`) at same microâ€‘batch size.
    - `Ring-flash-linear-2.0`: +25% with fused kernels; +57% after also doubling microâ€‘batch (from 1â†’2) and adjusting pipeline parallelism.
  - Inference throughput (Figures 7â€“8):
    - Prefill: linear models overtake softmax models past 8K context and accelerate rapidly; beyond 128K they reach â€œ>2.5Ã—â€ vs `Ringâ€‘2.0` and â€œ>8Ã—â€ vs baseline dense models (Figure 7a; Figure 8a, text in Section 3.4).
    - Decode: past 4K generated tokens, linear models exceed `Ringâ€‘2.0`; at 64K they achieve â€œ>2Ã— vs Ringâ€‘2.0â€ and â€œ>10Ã— vs baselineâ€ (Figure 7b; Figure 8b, Section 3.4).
  - KV/state memory access scaling (Figure 4): hybrid linear grows much slower with sequence length than `GQA` or `MLA`, explaining decode throughput gains.

- Ablations and robustness
  - Hybrid ratio/scaling laws (Figure 3): larger `M` (more linear per softmax) performs well at higher FLOP budgets; hybrid curves dominate pure softmax.
  - Linearâ€‘module design tweaks (Section 2.2.3): partial RoPE (~0.004 LM loss improvement) and powerâ€‘law headâ€‘wise decay (~0.04) show measured benefits.
  - RL stability from alignment (Figures 10â€“13):
    - > Each added fix (KV cache, `lm_head`, `RMSNorm`, attention backend, RoPE) â€œcontributes to improved training efficiency and stabilityâ€ (Figure 10).
    - > After alignment, PPO with rollout probabilities yields higher rewards and keeps the trainingâ€‘inference probability gap small (Figure 12).
    - Training reward and test metrics (AIMEâ€™25, LCB) rise steadily over RL steps (Figure 13).

- Do the experiments support the claims?
  - Yes for efficiency: clear, repeated speedups in training throughput (Figure 6) and longâ€‘context prefill/decode (Figures 7â€“8), with a mechanistic explanation (Figure 4).
  - Yes for capability retention: continued pretraining restores â‰¥98% of base performance (Figure 9).
  - Yes for RL stability: alignment ablations and PPO probability choices directly correlate with reward stability (Figures 10â€“12).
  - Quality vs top competitors: strong but not always SOTA (e.g., GPQA vs Geminiâ€‘2.5, Table 3); strengths are most pronounced in math/coding and longâ€‘context efficiency.

## 6. Limitations and Trade-offs
- Softmax layers remain bottlenecks
  - Even though few, softmax layers still incur quadratic cost and KV cache growth, limiting the full potential of the hybrid approach (Conclusion).
- Memory overhead from attention heads
  - To maintain effectiveness, the linear module keeps the same head count for `Q`, `K`, and `V`, which â€œbrings heavy memory overheadâ€ (Conclusion).
- Benefits are contextâ€‘length dependent
  - Linear attentionâ€™s advantages emerge strongly past ~8K context (Section 1; Figures 7â€“8). In shorter contexts, dense/MoE compute may dominate, muting gains.
- Knowledge retention tradeâ€‘offs
  - Continued pretraining shows slight deficits in reasoning/professional knowledge vs the original base (Figure 9), likely due to knowledge forgetting.
- Engineering complexity and reproducibility
  - The gains rely on extensive kernel fusions, FP8 quantization strategies, and a careful alignment pipeline (Sections 3 and 5). Portability to all stacks and hardware may require significant effort.
- Evaluation breadth
  - While 17 benchmarks are covered, robustness to adversarial retrieval, multimodal longâ€‘context tasks, or nonâ€‘English domains is not detailed.

## 7. Implications and Future Directions
- Field impact
  - This work shows that longâ€‘context reasoning can be made economical by combining linear attention (for constantâ€‘state decoding) with just enough softmax layers to preserve retrieval/extrapolationâ€”chosen via scaling laws (Figure 3). The fullâ€‘stack engineering (FP8 kernels, inference integration, RL alignment) demonstrates that architectural ideas must be matched with systems work for impact.
- Followâ€‘up research enabled
  - Attention research:
    - Reduce memory by decoupling head counts for `Q`, `K`, `V` in linear modules (Conclusion).
    - Explore adaptive or learned headâ€‘wise decay schedules and partialâ€‘RoPE strategies.
    - Push toward even sparser softmax layers (or smarter placement) while preserving retrieval.
  - Systems:
    - Generalize treeâ€‘mask linear attention kernels and speculative decoding across toolchains (SGLang/vLLM).
    - Extend FP8 fusion patterns to more activations and attention variants; coâ€‘design with compilerâ€‘level fusion.
  - RL and alignment:
    - Standardize moduleâ€‘wise alignment tests and metrics; make rolloutâ€‘probability PPO the default once aligned.
    - Study alignment for multiâ€‘GPU/MoE routing nondeterminism at larger scales and for multimodal models.
- Practical applications
  - Longâ€‘document assistants, multiâ€‘step code agents (LCB/Codeforces results), retrievalâ€‘augmented generation with very long contexts (128K+), and testâ€‘time scaling settings that require thousands of â€œthinkingâ€ tokensâ€”all with substantially lower serving cost and higher throughput (Figures 7â€“8; Section 3.4).

> Bottom line: `Ring-linear-2.0` combines a scalingâ€‘lawâ€‘tuned hybrid architecture with deep systems optimizations and alignmentâ€‘aware RL to deliver longâ€‘context efficiency without sacrificing reasoning qualityâ€”showing a credible path to economical, capable longâ€‘context LLMs.

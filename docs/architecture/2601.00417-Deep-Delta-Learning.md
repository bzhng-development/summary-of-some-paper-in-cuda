# Deep Delta Learning

**ArXiv:** [2601.00417](https://arxiv.org/abs/2601.00417)

## üéØ Pitch

Introduces Deep Delta Learning (DDL), a residual block that replaces the fixed identity shortcut with a learnable, data-dependent rank-1 Delta Operator (A = I ‚àí Œ≤kk‚ä§) plus a synchronized rank-1 write, allowing each layer to interpolate between identity, projection, and reflection via a single gate Œ≤(X)‚àà[0,2]. This empowers networks to dynamically erase, rewrite, or reflect feature subspaces (including introducing negative eigenvalues) for richer, potentially oscillatory or non-monotonic layer-to-layer dynamics while retaining the stability benefits of gated residual learning‚Äîimproving expressivity and controlled forgetting in deep models.

---

## 1. Executive Summary (2-3 sentences)

Deep Delta Learning (DDL) replaces the fixed identity shortcut in a ResNet-style block with a *learnable, data-dependent* rank-1 geometric transformation‚Äîcalled the `Delta Operator`‚Äîplus a synchronized rank-1 ‚Äúwrite‚Äù update (Eq. (2.2)‚Äì(2.5), Figure 1). This lets each layer dynamically interpolate between doing nothing (identity), selectively erasing a feature subspace (projection), or flipping a feature subspace (reflection) via a single gate `Œ≤(X) ‚àà [0,2]`, while preserving the training-stability intuition of gated residual updates. The key significance is that this shortcut can realize *negative eigenvalues* along a learned direction `k(X)`, enabling more complex (including non-monotonic/oscillatory) layer-to-layer state transitions than strictly additive residual connections.

---

## 2. Context and Motivation

- **What specific problem/gap is addressed?**
  - Standard deep residual networks use an identity shortcut:  
    \[
    X_{l+1} = X_l + F(X_l)
    \quad\text{(Eq. (1.1))}
    \]
    The shortcut has a fixed Jacobian equal to the identity, so the layer-to-layer transition has a strong *additive / translation* inductive bias.
  - The paper argues this rigidity limits the kinds of state transitions a network can represent, especially when modeling dynamics that benefit from *negative eigenvalues* (mentioned in the Introduction with reference to oscillations/oppositional behavior).

- **Why is this important?**
  - Residual connections are central to training very deep networks because they help gradients propagate (mitigating vanishing gradients).
  - But if the shortcut is always ‚Äúadd the new features onto the old ones,‚Äù the network may lack an explicit mechanism to *erase* or *reorient* problematic components of the representation across depth (Section 4.1 discusses ‚Äúresidual accumulation‚Äù and interference).

- **What prior approaches existed, and where do they fall short (as positioned here)?**
  - **Standard ResNets:** stable training, but shortcut is fixed identity (Introduction).
  - **Gated residual / Highway-type gates:** gates interpolate between identity path and transform path, but do not *change the shortcut geometry itself* (Related Work).
  - **Orthogonal/unitary constraints and Householder parameterizations:** enforce orthogonality as a hard constraint; DDL instead uses a *soft, data-dependent* gate that can be identity, projection (singular), or reflection (Related Work, Section 3.2).
  - **Invertible residual networks (i-ResNets):** enforce conditions for invertibility; DDL allows the model to choose near-invertible vs intentionally projective transitions (Related Work).
  - **Delta-rule memories / DeltaNet:** apply a delta-rule update over time for associative memory / linear attention; DDL applies an isomorphic structure over *depth* (Section 4.2).

- **How this paper positions itself**
  - It reframes the residual shortcut as a *geometric operator with controllable spectrum* (Section 3), enabling dynamic control over contraction/erasure/reflection along a learned direction, while still keeping the ‚Äúskip‚Äù behavior when the gate vanishes (Section 3.3).

---

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a residual block that updates a hidden state matrix `X` using a learned *rank-1 geometric shortcut transform* plus a learned *rank-1 write*.
- It solves the problem of an overly rigid identity shortcut by letting each layer *learn how much to keep, erase, or reflect* the incoming state along a learned direction, controlled by a single scalar gate `Œ≤(X)`.

### 3.2 Big-picture architecture (diagram in words)

- **Input:** hidden state `X_l ‚àà ‚Ñù^{d√ód_v}`.
- **Branch 1 (direction):** computes a vector `k(X_l) ‚àà ‚Ñù^d` (Appendix A.1), typically normalized to unit length.
- **Branch 2 (gate):** computes a scalar `Œ≤(X_l) ‚àà [0,2]` (Eq. (2.6), Appendix A.2).
- **Branch 3 (value/write):** computes a value vector `v(X_l) ‚àà ‚Ñù^{d_v}` via a function `F: ‚Ñù^{d√ód_v} ‚Üí ‚Ñù^{d_v}` (Eq. (2.2), Appendix A.2).
- **Core update:** uses `k` and `Œ≤` to build a rank-1 operator `A(X_l)` acting on the *feature dimension* `d`, and applies:
  \[
  X_{l+1} = A(X_l)\,X_l + \beta(X_l)\,k(X_l)\,v(X_l)^\top
  \quad\text{(Eq. (2.2))}
  \]

### 3.3 Roadmap for the deep dive

- Explain the **state shape** (`X ‚àà ‚Ñù^{d√ód_v}`) and what it means for an operator to act ‚Äúspatially‚Äù on `d`.
- Define the **Delta Operator** `A(X)` and show how it generalizes a Householder reflection (Eq. (2.1)‚Äì(2.4)).
- Rewrite the update into the **Delta-rule form** that makes ‚Äúerase‚Äù vs ‚Äúwrite‚Äù explicit (Eq. (2.5), (4.1)).
- Describe the **spectral control**: eigenvalues/eigenvectors and how `Œ≤` induces identity/projection/reflection (Theorem 3.1, Section 3.2).
- Connect the mechanism to **depth-wise delta-rule memory updates** (Section 4.2).

### 3.4 Detailed, sentence-based technical breakdown

- **Framing sentence (type of paper + core idea).**  
  This is an algorithmic/architectural paper that introduces a new residual block whose shortcut path is no longer fixed identity, but a *learned rank-1 perturbation of identity* with an explicitly analyzable spectrum (Section 2‚Äì3).

- **Hidden state representation (`X` is a matrix, not just a vector).**
  - Each layer‚Äôs hidden state is `X ‚àà ‚Ñù^{d√ód_v}`, where `d` is the feature dimension and `d_v` is the number of ‚Äúvalue channels‚Äù (Section 2.2).
  - The shortcut operator `A(X)` multiplies `X` on the left, so it transforms the feature dimension `d` **the same way for every value column** (Section 3.1 ‚ÄúLifting to matrix-valued states‚Äù).

- **Preliminaries: Householder reflection (what it is and why it matters here).**
  - A `Householder matrix` reflects vectors across a hyperplane; it has the form:
    \[
    H_k = I - 2\frac{kk^\top}{\lVert k\rVert_2^2}
    \quad\text{(Eq. (2.1))}
    \]
  - Its key property for this paper is spectral: one eigenvalue is `-1` (along direction `k`) and the remaining `d-1` eigenvalues are `+1` (on the subspace orthogonal to `k`) (Section 2.1).

- **Delta Operator: a gated, rank-1 perturbation of identity.**
  - DDL replaces the fixed shortcut with:
    \[
    A(X)=I-\beta(X)\frac{k(X)k(X)^\top}{k(X)^\top k(X)+\epsilon}
    \quad\text{(Eq. (2.3))}
    \]
    where `Œµ>0` is for numerical stability.
  - For theoretical analysis the paper assumes `k` is unit-normalized (`k^\top k = 1`) and takes `Œµ‚Üí0`, yielding the simplified form:
    \[
    A(X) = I - \beta(X)\,k(X)k(X)^\top
    \quad\text{(Eq. (2.4))}
    \]
  - This is a **rank-1 update** because `k k^\top` is rank 1 (it projects onto the 1D subspace spanned by `k`), so `A` differs from identity only along that direction.

- **Full residual update: erase + write are synchronized by the same gate.**
  - The Delta residual block outputs:
    \[
    X_{l+1} = A(X_l)X_l + \beta(X_l)k(X_l)v(X_l)^\top
    \quad\text{(Eq. (2.2))}
    \]
    where `v(X_l) ‚àà ‚Ñù^{d_v}` is produced by a residual/value branch `F`.
  - A crucial design choice is that **the same scalar gate `Œ≤(X)` multiplies both**:
    - the shortcut‚Äôs rank-1 ‚Äúerase/transform‚Äù term inside `A(X)`, and
    - the rank-1 ‚Äúwrite‚Äù term `k v^\top`.
  - Under the unit-norm assumption, substituting Eq. (2.4) into Eq. (2.2) yields the equivalent ‚ÄúDelta-rule‚Äù style form:
    \[
    X_{l+1} = X_l + \beta(X_l)\,k(X_l)\Big(v(X_l)^\top - k(X_l)^\top X_l\Big)
    \quad\text{(Eq. (2.5))}
    \]
    which makes the decomposition explicit:
    - `k^\top X_l` is a `1√ód_v` row vector capturing the current projection of every value column onto direction `k` (Section 4.1).
    - `v^\top - k^\top X_l` is the ‚Äúcorrection‚Äù signal: what you want to write minus what is currently there along that direction.
    - Multiplying by `k` injects that correction back into the `d√ód_v` state *only along the direction `k`*.

- **‚ÄúOrdered operations‚Äù as in Figure 1 (interpreting the computation).**
  - Figure 1 describes the block as:
    1. **Project**: compute `k^\top X_l` (a projection of the current state onto `k`).
    2. **Compare**: compute `v^\top - (k^\top X_l)` (a difference between desired value and current projected value).
    3. **Gate**: multiply by `Œ≤`.
    4. **Inject**: write back along `k` via the outer product `k(¬∑)` and add to get `X_{l+1}`.
  - This is exactly what Eq. (2.5) encodes.

- **How `Œ≤(X)` is parameterized and why `[0,2]` matters.**
  - The gate is constrained to `[0,2]` by:
    \[
    \beta(X) = 2\cdot \sigma(\mathrm{Linear}(G(X)))
    \quad\text{(Eq. (2.6))}
    \]
    where `G(¬∑)` is a pooling/convolution/flattening operation (paper‚Äôs examples), and `œÉ` is the sigmoid.
  - Appendix A gives a concrete lightweight form:
    \[
    \beta(X)=2\cdot\sigma\big(w_\beta^\top\tanh(W\,\mathrm{Pool}(X))\big)
    \quad\text{(Eq. (A.2))}
    \]
  - The interval `[0,2]` is chosen because the shortcut operator‚Äôs eigenvalue along `k` becomes `1-Œ≤`, which sweeps the meaningful geometric regimes (Section 3.2).

- **How `k(X)` and `v(X)` are produced (what the branches do).**
  - **Direction branch `œï_k`:** maps `X` to `k(X) ‚àà ‚Ñù^d`. Appendix A.1 provides:
    - MLP approach: `\tilde{k} = \mathrm{MLP}(\mathrm{Pool}(X))`, then normalize  
      \[
      k=\frac{\tilde{k}}{\lVert \tilde{k}\rVert_2+\epsilon_k}
      \quad\text{(Eq. (A.1))}
      \]
    - Attention-based approach is mentioned but not fully specified in the provided text (Appendix A.1).
  - **Value/write branch `F`:** maps `X` to `v(X) ‚àà ‚Ñù^{d_v}`. The paper states it can mirror the backbone block type (e.g., FFN or multi-head attention if used inside a Transformer), but no fixed architecture is specified in the provided excerpt (Appendix A.2).

- **System/data ‚Äúpipeline diagram in words‚Äù (explicit first/second/third).**
  1. Start with `X_l ‚àà ‚Ñù^{d√ód_v}`.
  2. Compute pooled statistics `Pool(X_l)` (Appendix A.1/A.2) and use them to produce `k(X_l)` and `Œ≤(X_l)`.
  3. Run the value branch `F` on `X_l` to produce `v(X_l)`.
  4. Form the projection `k(X_l)^\top X_l` (a `1√ód_v` row vector).
  5. Compute the correction `v(X_l)^\top - k(X_l)^\top X_l`.
  6. Multiply the correction by `Œ≤(X_l)` and inject along `k(X_l)` to obtain the rank-1 update added to `X_l`, producing `X_{l+1}` (Eq. (2.5)).

- **Worked micro-example (single forward step with tiny dimensions).**  
  Consider `d=2`, `d_v=1` (so the state is a vector), unit direction \(k=\begin{bmatrix}1\\0\end{bmatrix}\), gate `Œ≤=2` (reflection regime), current state \(x=\begin{bmatrix}a\\b\end{bmatrix}\), and value scalar `v=c`.
  - Compute projection: \(k^\top x = a\).
  - Compute correction: \(v - k^\top x = c-a\).
  - Inject along `k` with gate: \(Œ≤(c-a)k = 2(c-a)\begin{bmatrix}1\\0\end{bmatrix}\).
  - Update (Eq. (3.7), which is Eq. (2.2) specialized to `d_v=1`):
    \[
    x_{l+1} = x + 2(c-a)\,k
            = \begin{bmatrix}a\\b\end{bmatrix} + \begin{bmatrix}2(c-a)\\0\end{bmatrix}
            = \begin{bmatrix}2c-a\\b\end{bmatrix}.
    \]
  Interpretation: the component along `k` (the first coordinate) is ‚Äúreflected-and-overwritten‚Äù toward the target `c`, while the orthogonal component is left unchanged. This illustrates the paper‚Äôs claim that the block can perform a reflection-like transformation along a learned direction while writing new content along that same direction (Section 3.2, Figure 1).

- **Spectral analysis: how the block controls eigenvalues (and thus dynamics).**
  - Theorem 3.1 analyzes \(A = I - Œ≤kk^\top\) with unit `k` and shows its eigenvalues are:
    \[
    \sigma(A)=\{1 \text{ (multiplicity } d-1),\; 1-Œ≤\}
    \quad\text{(Eq. (3.1))}
    \]
    with eigenvector `k` for eigenvalue `1-Œ≤`, and eigenspace `k^\perp` for eigenvalue `1`.
  - Consequences emphasized in Section 3:
    - Along most directions (orthogonal to `k`), the shortcut acts like identity.
    - Along `k`, the shortcut scales by `1-Œ≤`, enabling:
      - contraction (`0<Œ≤<1` gives `0<1-Œ≤<1`),
      - projection (`Œ≤=1` gives eigenvalue `0`),
      - sign flip (`Œ≤>1` gives negative eigenvalue, e.g., `Œ≤=2` gives `-1`).
  - The determinant is \( \det(A)=1-Œ≤ \) (Corollary 3.2, Eq. (3.4)), and when lifted to the full matrix state space it becomes \((1-Œ≤)^{d_v}\) due to broadcasting across `d_v` columns (Section 3.1).

- **Geometric regimes unified by one gate (`Œ≤ ‚àà [0,2]`).**
  - **Identity** (`Œ≤‚Üí0`): \(A‚ÜíI\) and the write term vanishes because it is also multiplied by `Œ≤`, so \(X_{l+1}‚âàX_l\) (Section 3.2).
  - **Orthogonal projection** (`Œ≤‚Üí1`): \(A = I - kk^\top\) projects onto `k^\perp`, explicitly removing the `k` component before the write injects a new `k` component (Section 3.2, ‚Äúreplace-along-k‚Äù interpretation).
  - **Full reflection / Householder** (`Œ≤‚Üí2`): \(A=I-2kk^\top\) becomes a standard Householder reflection (Eq. (2.1) with unit `k`), giving an orthogonal transformation along the shortcut (Section 3.2).

- **Diagonal matrix case: how feature coupling appears.**
  - Section 3.4 studies `X = diag(Œª_1,‚Ä¶,Œª_d)` and shows:
    \[
    (AX)_{ij} = \lambda_i\delta_{ij} - Œ≤\lambda_j k_i k_j
    \quad\text{(Eq. (3.6))}
    \]
  - This makes a specific mechanism clear: even if features are initially decoupled (diagonal), a non-zero `Œ≤` introduces off-diagonal interactions proportional to `k_i k_j`, i.e., controlled coupling determined by the learned direction.

- **Vector-state limit (`d_v=1`) and the induced gated update.**
  - When `d_v=1`, the update becomes (Section 3.5):
    \[
    x_{l+1} = x_l + Œ≤_l\,(v_l - k_l^\top x_l)\,k_l
    \quad\text{(Eq. (3.7))}
    \]
  - This highlights that DDL includes ordinary vector-based networks as a special case, but with an explicitly geometric ‚Äúerror-correcting‚Äù update along `k_l`.

- **Connection to Delta Rule and DeltaNet (depth-wise isomorphism).**
  - The paper rewrites the update as:
    \[
    X_{l+1} = X_l + Œ≤_l k_l\left(v_l^\top - k_l^\top X_l\right)
    \quad\text{(Eq. (4.1))}
    \]
    and interprets it as the Delta Rule: ‚Äúerase‚Äù old projection `k^\top X` and ‚Äúwrite‚Äù new value `v`.
  - It then matches this to the DeltaNet recurrence:
    \[
    S_t = (I-Œ≤_t k_t k_t^\top)S_{t-1} + Œ≤_t k_t v_t^\top
    \quad\text{(Eq. (4.2))}
    \]
    showing DDL applies the same structural update over *depth index `l`* instead of *time index `t`* (Section 4.2).

- **Core configurations and hyperparameters (what is and is not provided).**
  - The provided excerpt includes architectural formulas and some parameterization choices (e.g., `Œ≤` in `[0,2]` via sigmoid; normalization of `k`), but **does not provide**:
    - optimizer type/settings, learning rate schedule, batch size,
    - number of layers/heads/hidden dimensions,
    - tokenizer/context window (if any),
    - total training tokens/compute budget/hardware,
    - throughput/latency/memory benchmarks.
  - The only explicit ‚Äúconfiguration-like‚Äù constraints in the excerpt are:
    - `Œ≤(X) ‚àà [0,2]` via Eq. (2.6),
    - numerical stabilizers `Œµ` (Eq. (2.3)) and `Œµ_k` (Eq. (A.1)),
    - and the unit-norm assumption on `k` for the theoretical analysis (Section 2.2, Theorem 3.1).

---

## 4. Key Insights and Innovations

- **1) A shortcut that is a learnable rank-1 geometric operator rather than fixed identity.**
  - Novelty: Standard residual shortcuts are fixed `I`; DDL uses \(A(X)=I-Œ≤(X)k(X)k(X)^\top\) (Eq. (2.3)/(2.4)), which changes the shortcut‚Äôs geometry in a data-dependent way.
  - Significance: This makes the *shortcut path* expressive (not only the residual branch `F`), but still analytically simple (rank-1).

- **2) A single scalar gate `Œ≤(X)` that continuously unifies identity, projection, and reflection.**
  - Novelty: One gate controls a spectrum of behaviors, not just ‚Äúhow much of identity vs transform to mix.‚Äù
  - Significance: The gate gives explicit control over whether the layer is effectively skipped (`Œ≤‚âà0`), forgets a component (`Œ≤‚âà1`), or flips orientation along a subspace (`Œ≤‚âà2`) (Section 3.2).

- **3) Synchronized ‚Äúerase and write‚Äù with the same step size (Delta-rule structure across depth).**
  - Novelty: The update couples subtraction of the old projection `k^\top X` and injection of `v` under the same multiplicative `Œ≤` (Eq. (2.5), (4.1)).
  - Significance: This provides an explicit mechanism to avoid uncontrolled ‚Äúresidual accumulation‚Äù by allowing selective replacement along a learned direction (Section 4.1).

- **4) Spectral characterization of the shortcut operator with dynamic negative eigenvalues.**
  - Novelty: Theorem 3.1 fully characterizes the eigensystem of the shortcut operator.
  - Significance: Because the eigenvalue along `k` is `1-Œ≤`, setting `Œ≤>1` yields a *negative* eigenvalue along that direction, which the paper motivates as useful for richer dynamics (Introduction, Section 3.2).

- **5) Depth-wise isomorphism to DeltaNet memory updates.**
  - Novelty: The paper provides an explicit correspondence between the DDL depth update (Eq. (4.3)) and DeltaNet‚Äôs time recurrence (Eq. (4.2)).
  - Significance: This positions DDL as importing ‚Äúfast-weight / associative memory‚Äù style updates into the architecture of deep residual networks (Section 4.2).

---

## 5. Experimental Analysis

- **Evaluation methodology (datasets, metrics, baselines, setup).**
  - The provided content includes architectural definitions, theory, and implementation parameterization options, but **does not include any experimental section** with datasets, metrics, baselines, or protocols.
  - A project page link is given in the abstract, but no experimental results are contained in the provided text, and I do not assume external content.

- **Main quantitative results with specific numbers and comparisons.**
  - **Not available in the provided excerpt.** There are no accuracy/F1/loss curves, tables, or benchmark comparisons.

- **Do experiments convincingly support the claims?**
  - Based on the provided content alone, the paper‚Äôs claims are supported primarily by:
    - exact algebraic equivalences (Eq. (2.2) ‚Üî Eq. (2.5); Eq. (4.2) ‚Üî Eq. (4.3)),
    - and spectral analysis (Theorem 3.1, Corollary 3.2).
  - Empirical validation (e.g., improved performance, better training stability, or specific tasks benefiting from negative eigenvalues) cannot be assessed from the provided text.

- **Ablations, failure cases, robustness checks.**
  - **Not provided** in the excerpt.

---

## 6. Limitations and Trade-offs

- **No empirical evidence in the provided content.**
  - Without experiments, it remains unclear (from this excerpt alone) when DDL improves performance, when it harms it, and what task regimes benefit most.

- **Potential instability or ill-conditioning around the projection regime (`Œ≤‚âà1`).**
  - The paper notes that at `Œ≤‚Üí1`, `A` becomes singular (projection) with `det(A)‚Üí0` (Section 3.2, Corollary 3.2).
  - This can be desirable for ‚Äúforgetting,‚Äù but it also implies that information along `k` is destroyed by the shortcut unless reintroduced via the write term, which could be risky if `v` is poorly estimated.

- **Reliance on the unit-norm assumption for clean theory vs practical implementation.**
  - The spectral results assume `k^\top k = 1` (Section 2.2, Theorem 3.1).
  - Implementation uses normalization with `Œµ_k` (Eq. (A.1)) and uses `Œµ` in Eq. (2.3) for stability, meaning the exact theoretical operator may be approximated rather than exact.

- **Expressivity is focused on rank-1 modification per layer (per block).**
  - Because `A(X)` differs from identity only along a single direction `k(X)`, each layer can only directly control one ‚Äúspecial‚Äù subspace direction at a time.
  - The paper does not discuss (in the provided excerpt) stacking multiple directions per layer or higher-rank generalizations, so any need for multi-direction control must come from composing many layers.

- **Architectural degrees of freedom are underspecified in the excerpt.**
  - Appendix A notes multiple options for generating `k(X)` and `v(X)` (MLP vs attention-based, value branch mirroring backbone), but the excerpt does not specify concrete choices for any particular domain (vision, language, etc.).
  - This makes it hard to infer compute cost, memory overhead, or best-practice defaults purely from the provided text.

---

## 7. Implications and Future Directions

- **How this work changes the landscape (conceptually).**
  - It reframes residual learning as not only ‚Äúlearn an additive correction `F(X)`,‚Äù but ‚Äúlearn a *controlled state transition operator* with analyzable spectrum,‚Äù where the shortcut can contract, erase, or reflect along a learned direction (Section 3).
  - This explicitly introduces a mechanism for *negative eigenvalues* in layer-to-layer transitions via `1-Œ≤` (Section 3.2), which the introduction motivates as potentially important for certain dynamic patterns.

- **Follow-up research suggested by the paper‚Äôs structure.**
  - **Empirical validation across domains:** The theory suggests benefits for modeling oscillatory/oppositional dynamics; experiments could test tasks where negative eigenvalues are hypothesized to matter (the excerpt itself does not provide such tests).
  - **Design-space exploration for `œï_k` and `F`:** Appendix A outlines MLP vs attention-based `k(X)` generation and flexible `v(X)` branches; a natural direction is studying which parameterizations work best and at what compute cost.
  - **Understanding gating behavior:** Because `Œ≤(X)` controls identity/projection/reflection, analyzing learned `Œ≤` distributions across depth/data could clarify when the model chooses to skip, forget, or reflect.

- **Practical applications / downstream use cases (based on provided content).**
  - Any deep architecture using residual connections could, in principle, swap in a Delta-Res block to gain explicit ‚Äúforget/overwrite‚Äù control along learned subspaces (Sections 2 and 4).
  - The DeltaNet correspondence (Section 4.2) suggests applicability in settings where delta-rule memory updates are beneficial, but DDL applies them over depth rather than time.

- **Repro/Integration Guidance (when to prefer this over alternatives, based only on the excerpt).**
  - Prefer DDL when you want residual blocks that can:
    - behave like identity (skip) when `Œ≤‚âà0` (Section 3.3),
    - perform controlled forgetting/replacement along a learned direction when `Œ≤‚âà1` (Section 3.2),
    - or introduce reflection/negative-eigenvalue behavior when `Œ≤>1` (Section 3.2).
  - Prefer a standard ResNet-style block when you want the simplest additive dynamics and do not need the explicit erase/write coupling of Eq. (2.5)/(4.1).
  - The excerpt does not provide training recipes (optimizer/LR/batch) or performance trade-offs, so any integration choice beyond these functional behaviors would require additional information not present in the provided text.

# TRANSFORMER-SQUARED: SELF-ADAPTIVE LLMS

**ArXiv:** [2501.06252](https://arxiv.org/abs/2501.06252)

## üéØ Pitch

The paper introduces Transformer¬≤, a two-pass self-adaptation framework that adapts frozen LLMs at inference by scaling singular values of weight matrices via compact per-layer ‚Äúexpert‚Äù vectors (SVF) trained with RL, enabling dynamic task-specific behavior without full fine-tuning. This yields orders-of-magnitude fewer parameters than common PEFT methods, improves out-of-distribution and cross-modal performance, and enables scalable, composable on-demand specialization‚Äîreducing training cost and supporting continual adaptation in deployed LLMs.

---

## 1. Executive Summary (2-3 sentences)

`Transformer¬≤` (Transformer-Squared) is a self-adaptation framework that lets a frozen large language model (LLM) change its behavior at inference time by *selectively scaling the singular values* of its existing weight matrices, using compact per-layer ‚Äúexpert vectors‚Äù trained offline with reinforcement learning (RL) (Figure 1; Section 3.2; Eq. (1)). It performs adaptation in a two-pass inference procedure: a first pass infers task properties and selects/mixes experts, and a second pass produces the final answer with the adapted weights (Figure 1; Section 3.2). Across multiple base models and tasks, the paper reports that `SVF` (Singular Value Fine-tuning) is more parameter-efficient than LoRA and that `Transformer¬≤` improves performance on several *unseen* tasks via test-time expert selection/mixing (Tables 1‚Äì2, 4).

---

## 2. Context and Motivation

- **Problem / gap addressed.**
  - Standard post-training (fine-tuning) of LLMs is described as (i) **computationally expensive**, (ii) **static** (one tuned model configuration is not flexible at runtime), and (iii) prone to **trade-offs** when trying to cover many tasks at once (Introduction, page shown near Figure 1).
  - A modular ‚Äúexpert‚Äù approach (train multiple small task/domain modules, then choose them on demand) is attractive, but existing ways to do this (e.g., LoRA experts) can become **storage-heavy** across many tasks, can **overfit** on small narrow datasets, and are hard to **compose** reliably (Introduction; Section 3.2 ‚ÄúNegligible parameters / High compositionality / Principled regularization‚Äù).

- **Why it matters.**
  - If a system can adapt at inference time, it can:
    - reduce repeated fine-tuning cost by reusing a bank of experts (Introduction; Section 3.2),
    - support continual learning by adding experts without ‚Äúcatastrophic forgetting‚Äù (Introduction),
    - better handle unknown/shifted tasks at deployment (Section 3.2 two-pass mechanism; Table 2 on unseen tasks).

- **Prior approaches and their shortcomings (as positioned here).**
  - **LoRA / PEFT family**: Efficient compared to full fine-tuning, but per-task modules can accumulate, can overfit on small data, and are not inherently compositional (Introduction; Section 3.2 ‚ÄúHigh compositionality‚Äù; Appendix A.2; Table 4).
  - **MoE systems**: Typically do token-level routing and train experts differently (Section 2 ‚ÄúMicroview‚Äù). The paper positions `Transformer¬≤` as closer to *sample-level* selection/mixing rather than per-token routing, and it emphasizes RL-trained experts rather than experts trained from scratch without explicit specialization signals (Section 2).
  - **SVD-based PEFT variants**: Existing SVD/low-rank methods often truncate to top-`r` components and can lose information when singular values are not extremely skewed (Section 2; Appendix C with Figures 10‚Äì11).

- **How this paper positions itself.**
  - It proposes `SVF` as a *principled PEFT parameterization* (scale singular values only) intended to be (i) extremely small, (ii) regularized, and (iii) compositional (Section 3.2).
  - Then it proposes `Transformer¬≤` as an inference-time *self-adaptation* recipe that uses these experts in a two-pass dispatch-and-answer process (Figure 1; Section 3.2), with three increasingly informed adaptation strategies (Section 3.2 A/B/C).

---

## 3. Technical Approach

### 3.1 Reader orientation (approachable technical breakdown)

- The system is a way to **modify a frozen LLM‚Äôs weights at inference time** using tiny learned vectors (‚Äúexperts‚Äù) instead of doing conventional fine-tuning.
- It solves ‚Äúhow do we adapt to an unknown task at runtime?‚Äù by running the model twice: **first to decide how to adapt**, and **second to answer with adapted weights** (Figure 1; Section 3.2).

### 3.2 Big-picture architecture (diagram in words)

- **Offline training stage (build experts).**
  1. Take the base model‚Äôs weight matrices and compute an `SVD` factorization per matrix: `W = U Œ£ V·µÄ` (Section 3.1).
  2. Freeze `U` and `V` and learn only a vector `z` that scales the diagonal singular values in `Œ£` to produce `Œ£‚Ä≤`, yielding adapted weights `W‚Ä≤ = U Œ£‚Ä≤ V·µÄ` (Section 3.2).
  3. Train `z` with RL to make an expert specialized on a domain/task (Section 3.2; Eq. (1)).

- **Online inference stage (self-adapt per prompt/task).**
  1. **First pass**: run a dispatch/adaptation mechanism to choose an expert (or mixture of experts) ‚Üí produce an adapted vector `z‚Ä≤` (Figure 1; Section 3.2 A/B/C).
  2. **Second pass**: run the base LLM again, but with weights modified via `z‚Ä≤`, to generate the final answer (Figure 1).

### 3.3 Roadmap for the deep dive

- I will explain:
  1. `SVD` and what it means to scale singular values in a transformer weight matrix (Section 3.1).
  2. `SVF`: the parameterization `W ‚Üí W‚Ä≤` via a learned `z` (Section 3.2) and why it is claimed to be efficient/compositional/regularized.
  3. How the experts are trained with RL (objective, reward, KL penalty; Eq. (1); Appendix A.1).
  4. How `Transformer¬≤` performs test-time adaptation via three strategies (Section 3.2 A/B/C), including `CEM` for mixture search.
  5. What is (and is not) specified about implementation, hyperparameters, and runtime overhead (Appendix A; Table 3).

### 3.4 Detailed, sentence-based technical breakdown

This is an **algorithmic + empirical systems** paper: it introduces a new PEFT parameterization (`SVF`) and an inference-time adaptation recipe (`Transformer¬≤`), and then evaluates them across models/tasks (Sections 3‚Äì4; Tables 1‚Äì5).

#### 3.4.1 Core math building block: SVD and ‚Äúsingular components‚Äù
- For any weight matrix `W ‚àà ‚Ñù^{n√óm}`, the paper uses singular value decomposition (SVD) to write it as:
  - `W = U Œ£ V·µÄ` (Section 3.1),
  - where `U` and `V` contain orthogonal directions (‚Äúsingular vectors‚Äù) and `Œ£` is diagonal with nonnegative singular values `œÉ_i` sorted descending (Section 3.1).
- The paper interprets the linear map `y = W x` as a sum of independent rank-1 contributions:
  - `y = ‚àë_{i=1}^r œÉ_i u_i v_i·µÄ x` (Section 3.1),
  - meaning each component `u_i v_i·µÄ` processes the input separately, while `œÉ_i` controls how strong that component is.

**Micro-example (to make the mechanism concrete).**
- Suppose a 2√ó2 weight matrix `W` has SVD with singular values `[œÉ‚ÇÅ, œÉ‚ÇÇ] = [10, 1]` and corresponding singular vectors. If we learn `z = [0.5, 2.0]`, then SVF forms `œÉ‚ÇÅ‚Ä≤ = 10¬∑0.5 = 5` and `œÉ‚ÇÇ‚Ä≤ = 1¬∑2 = 2`, so the ‚Äústrong‚Äù direction is damped and the weaker direction is amplified. The singular vectors stay the same; only the strength of each rank-1 pathway changes (Section 3.2 definition of `Œ£‚Ä≤ = Œ£ ‚äó diag(z)`).

#### 3.4.2 SVF: Singular Value Fine-tuning (how weights are adapted)
- **Parameterization.**
  - For each matrix `W` being adapted, `SVF` learns a vector `z ‚àà ‚Ñù^r` (with `r = min(m,n)` as defined in the text) that scales singular values independently (Section 3.2).
  - The adapted weight is `W‚Ä≤ = U Œ£‚Ä≤ V·µÄ`, where `Œ£‚Ä≤ = Œ£ ‚äó diag(z)` (Section 3.2).
  - Intuitively, `z` is a ‚Äúper-singular-component volume knob‚Äù for the pre-trained matrix.

- **Where SVF is applied in a transformer.**
  - The model has `N` layers and the authors choose `M` matrices per layer to fine-tune: `Œ∏_W = {W‚ÇÅ, ‚Ä¶, W_{N√óM}}` (Section 3.2).
  - They learn `Œ∏_z = {z‚ÇÅ, ‚Ä¶, z_{N√óM}}`‚Äîone `z` per selected matrix (Section 3.2).
  - An ablation explicitly varies whether SVF is applied to **MLP**, **attention**, or both (Table 4, rows 1‚Äì3).

- **Why the paper claims SVF is attractive (mechanistic explanations tied to the parameterization).**
  - **Negligible parameters**: because each matrix only gets a vector `z`, not a low-rank pair of matrices as in LoRA (Section 3.2 ‚ÄúNegligible parameters‚Äù). Empirically, one configuration uses `0.16M‚Äì0.58M` SVF parameters vs `6.82M‚Äì35.13M` for LoRA in their ablation setting (Table 4).
  - **Principled regularization**: scaling existing singular components restricts the space of changes (you cannot rotate into new directions by changing `U,V`), which is argued to reduce overfitting and collapse on small datasets (Section 3.2 ‚ÄúPrincipled regularization‚Äù).
  - **Compositionality**: because experts are ‚Äúaligned‚Äù in the same coordinate system (the singular component index `i`), algebraic operations like interpolation are claimed to preserve behavior better than interpolating LoRA factors, which can have many equivalent factorizations (Section 3.2 ‚ÄúHigh compositionality‚Äù).

  Note: The paper asserts SVF ‚Äúaffect[s] the weight matrix in a full-rank manner‚Äù and thus can be ‚Äúmore information than low-rank approaches‚Äù (Section 3.2). Strictly, the update is constrained to rescaling the existing singular directions, but it does produce a dense full-rank modification when applied across all singular values.

#### 3.4.3 Training experts with RL (how `z` is learned)
- **Objective function.**
  - SVF experts are trained with REINFORCE (policy gradient) to directly optimize task performance (Section 3.2).
  - For each prompt `x_i` with target `y_i`, the model samples an answer `≈∑_i` and gets a **unit reward** `r ‚àà {‚àí1, 1}` based on correctness (Section 3.2).
  - The optimization includes a KL regularizer that penalizes deviation from the base model distribution `œÄ_{Œ∏_W}` (Section 3.2; Eq. (1)):
    > `J(Œ∏_z) = E[ log œÄ_{Œ∏_W‚Ä≤}(≈∑_i | x_i) ¬∑ r(≈∑_i, y_i) ‚àí Œª D_KL(œÄ_{Œ∏_W‚Ä≤} || œÄ_{Œ∏_W}) ]` (Eq. (1))

- **What happens first/second/third in the training pipeline (diagram in words).**
  1. First, select which transformer matrices `W` will be adapted (e.g., attention projections and/or MLP matrices; Table 4 ‚ÄúModule‚Äù).
  2. Second, compute SVD for each selected `W` to obtain frozen `U, Œ£, V·µÄ`, initialize `z` (Appendix A.1; Table 6 gives initialization mean `0.1` and variance `1√ó10^{-3}`).
  3. Third, for each epoch, generate model answers for training prompts, compute rewards from correctness, and update `z` using AdamW to maximize Eq. (1) (Section 3.2; Appendix A.1; Figure 4 shows learning curves).

- **Optimization hyperparameters actually provided.**
  - For SVF RL training (Appendix A.1; Table 6):
    - Optimizer: `AdamW`
    - Learning rate: `2 √ó 10^{-3}` with **cosine decay**
    - Global batch size: `256`
    - Gradient clipping: ‚Äúclip max norm‚Äù `1 √ó 10^{-3}` (Appendix A.1; Table 6)
    - KL coefficient `Œª`: swept over `{0.0, 0.1, 0.2, 0.3}`, chosen by validation performance (Appendix A.1; Table 6)
    - Data splits: each dataset is split into equal-sized train/validation (Appendix A.1)
    - Early stopping: used (Appendix A.1)
  - **Important missing training details (not specified in the provided content).**
    - How correctness is judged (exact match? a verifier? unit tests for code?) is not fully specified in the excerpt, beyond ‚Äúunitary reward based on correctness‚Äù (Section 3.2).
    - Number of epochs/total RL steps per task is not specified in the excerpt; Figure 4 plots epochs but does not give a universal stopping rule beyond early stopping and some tasks stopped early (Figure 4 caption).
    - Hardware/compute budget (GPUs, hours, tokens) is not reported here; the only resource note is that for LLAMA3-70B and vision tasks they fine-tune **half of the layers** ‚Äúdue to limited GPU resources‚Äù (Appendix A.1; Section 4.2 text).

- **Why RL is used here (as argued).**
  - The paper argues RL is better when datasets lack ‚Äúexplaining texts‚Äù (chain-of-thought-like solutions), because next-token prediction needs detailed target sequences (Section 3.2, paragraph comparing RL vs LoRA on GSM8K without reasoning text).
  - An ablation claims next-token prediction training for SVF can even hurt performance compared to RL (Table 4, row 4 vs rows 1‚Äì3).

#### 3.4.4 Transformer¬≤ inference: two-pass self-adaptation and three strategies
- **Two-pass inference mechanism (core idea).**
  - Pass 1: run an adaptation/dispatch mechanism to decide which expert capability is needed (or to compute mixing weights), producing `z‚Ä≤` (Figure 1; Section 3.2).
  - Pass 2: re-run the LLM with weights modified by `z‚Ä≤`, and generate the final answer (Figure 1).

- **Experts available at inference.**
  - The framework assumes `K` pre-trained SVF experts `z_{1:K}` (Section 3.2).
  - In the main language-only experiments, experts are trained on `GSM8K` (math), `MBPP-pro` (coding), and `ARC-Easy` (reasoning) (Section 4.1). So in those settings, effectively `K = 3`.

- **Strategy A: Prompt-based adaptation (prompt engineering dispatch).**
  - The system constructs a classifier prompt that asks the model to label the incoming question into `{code, math, reasoning, others}` (Figure 3; Section 3.2 A).
  - It then picks the corresponding expert vector `z‚Ä≤` from `z_{1:K}`, or ‚Äúothers‚Äù triggers using base weights (Section 3.2 A).

- **Strategy B: Classification expert (an SVF-trained dispatcher).**
  - The paper trains a separate SVF expert `z_c` to improve task classification (Section 3.2 B).
  - Training data is built from the same expert-training tasks: `D = {(x_{i,1}, 1), ‚Ä¶, (x_{i,k}, k), ‚Ä¶}` where each example is labeled by the expert/task identity (Section 3.2 B).
  - At inference pass 1, load `z_c` to classify the prompt, then choose the task expert `z‚Ä≤` (Section 3.2 B).

- **Strategy C: Few-shot adaptation via mixture search (CEM).**
  - Instead of choosing one expert, it constructs a *mixture*:
    - `z‚Ä≤ = ‚àë_{k=1}^K Œ±_k z_k` (Section 3.2 C; Appendix A.4).
  - It uses the `CEM` (cross-entropy method) to search over `Œ± ‚àà ‚Ñù^K` based on performance on a small set of held-out ‚Äúfew-shot prompts‚Äù from the target task (Section 3.1 on CEM; Section 3.2 C; Appendix A.4).
  - If multiple Œ±-samples tie in score, ties are broken by higher average log-likelihood on the generated correct answers (Section 3.2 C; Appendix A.4).
  - Reported few-shot settings:
    - Typically reserve **10 samples** for adaptation and run up to **100 CEM iterations** (Appendix A.4).
    - They also explore ‚Äúper-layer‚Äù vs ‚Äúper-vector‚Äù adaptation and Œ± normalization vs unconstrained Œ±, and then report the best of these test configurations (Appendix A.4).  
      This implies the selection of configuration is itself based on test behavior; the excerpt does not describe a separate validation set for this meta-choice (Appendix A.4).

- **Runtime overhead of two-pass inference.**
  - For prompt-based adaptation (Strategy A), Table 3 reports total time on entire problem sets, split into pass 1 (adaptation) vs pass 2 (solving):
    - `MATH`: 42.64s (pass 1) vs 321.19s (pass 2); pass1/pass2 ‚âà 13% (Table 3)
    - `HumanEval`: 2.76s vs 14.28s; ‚âà 19% (Table 3)
    - `ARC-Challenge`: 13.40s vs 28.51s; ‚âà 47% (Table 3)
  - The paper explains the ratio depends on the number of tokens generated; ARC has short answers so pass 2 is relatively cheap, making pass 1 more prominent (Section 4.2 discussion near Table 3).

#### 3.4.5 LoRA baseline training details (what is actually compared)
- LoRA is trained with next-token prediction (Section 4.2; Appendix A.2).
- Implementation choices given:
  - Apply LoRA to **query and value projection** layers (Appendix A.2).
  - Learning rates ‚Äúaround `5 √ó 10^{-5}`‚Äù and **200 total iterations** with global batch size 256 (Appendix A.2).
  - Requires collecting full solutions and appending them to prompts (Figure 8; Appendix A.2), which the paper argues is a heavier dataset requirement than RL-on-correctness.
- Hyperparameters listed (Table 6):
  - Rank `16`, LoRA alpha `32`, dropout `0.05`
  - Learning rate sweep includes `{2√ó10^{-4}, 5√ó10^{-4}, 2√ó10^{-5}, 5√ó10^{-5}, 2√ó10^{-6}, 5√ó10^{-6}}` (Table 6; note the table text formatting is slightly garbled but these values are visible)
  - Clip max norm `{1√ó10^{-3}, 1.0}` (Table 6)

#### 3.4.6 Critical missing ‚Äúcore model configuration‚Äù details
The instructions ask for model architecture hyperparameters (layers, hidden size, heads, tokenizer, context window, etc.). The provided excerpt **does not** specify these for `LLAMA3-8B-INSTRUCT`, `MISTRAL-7B-INSTRUCT-V0.3`, or `LLAMA3-70B-INSTRUCT`, nor for the vision-language backbone beyond naming `LLAMA3-LLAVA-NEXT-8B` (Section 4.1; Figure 5 text). I therefore cannot report:
- number of layers / heads / hidden dimension,
- context window length,
- tokenizer type,
- total training tokens / compute budget,
without inventing details not present in the provided paper content.

---

## 4. Key Insights and Innovations

- **(1) SVF: scaling singular values as a PEFT parameterization.**
  - What is new here is the decision to learn only `z` that rescales *all* singular components of each chosen weight matrix: `W‚Ä≤ = U Œ£‚Ä≤ V·µÄ` with `Œ£‚Ä≤ = Œ£ ‚äó diag(z)` (Section 3.2).
  - Significance claimed/observed:
    - Very small trainable parameter count compared to LoRA (Table 4 shows `0.16M‚Äì0.58M` vs `6.82M‚Äì35.13M` in an ablation setting).
    - Better stability/generalization on small datasets due to the constrained update space (Section 3.2 ‚ÄúPrincipled regularization‚Äù; Table 4 row 4 shows next-token SVF can degrade badly).

- **(2) RL-trained ‚Äúexpert vectors‚Äù as modular skills.**
  - Experts are trained with a correctness reward plus KL penalty (Eq. (1)), directly optimizing task success rather than next-token likelihood (Section 3.2).
  - This is positioned as especially useful when training data does not include full reasoning traces/solutions (Section 3.2 discussion).

- **(3) Two-pass self-adaptation at inference time.**
  - The framework formalizes a practical mechanism: observe behavior / classify task in pass 1, then answer with adapted weights in pass 2 (Figure 1; Section 3.2).
  - This differs from token-level MoE routing; it is sample-level dispatch/mixing (Section 2 ‚ÄúMicroview‚Äù).

- **(4) Three adaptation strategies with increasing access to test-time information.**
  - Strategy A (prompt classification), B (learned classification expert), and C (few-shot mixture search via CEM) provide a spectrum of deployment options (Section 3.2 A/B/C).
  - The experiments suggest ‚Äúmore involved strategies‚Äù tend to help more, with few-shot mixture often best (Section 4.2 discussion; Table 2).

- **(5) Cross-model transfer of SVF vectors (a compositionality-related property).**
  - The paper reports that SVF vectors trained on one model (`LLAMA3-8B`) can sometimes improve another (`MISTRAL-7B`) if the singular-value ordering is preserved, and that shuffling the vector degrades performance (Table 5; Section 4.3 Analysis 4).
  - This is presented as evidence that the SVF coordinate system (singular component index order) carries meaning across similar architectures.

---

## 5. Experimental Analysis

### Evaluation methodology (datasets, metrics, setup)

- **Base models evaluated** (Section 4.1):
  - `LLAMA3-8B-INSTRUCT`
  - `MISTRAL-7B-INSTRUCT-V0.3`
  - `LLAMA3-70B-INSTRUCT`

- **Tasks used to train SVF experts** (Section 4.1):
  - `GSM8K` (math word problems)
  - `MBPP-pro` (coding)
  - `ARC-Easy` (reasoning/science QA)
  - Additionally, for VLM evaluation: `TextVQA` is used to train an SVF set for `LLAMA3-8B-INSTRUCT` as a language backbone (Section 4.1; Figure 5 context).

- **Unseen tasks for self-adaptation evaluation** (Section 4.1):
  - `MATH`
  - `HumanEval`
  - `ARC-Challenge`
  - `OKVQA` (vision QA)
  - Importantly, for adaptation experiments (including OKVQA), they ‚Äúonly consider experts obtained in the pure-language settings‚Äù (Section 4.1).

- **Metrics**
  - The paper reports ‚Äúscores‚Äù as percentages for many tasks and also normalized scores in parentheses (Tables 1‚Äì2, 4‚Äì5). It does not define the normalization formula in the provided excerpt, but it appears to be relative to the base model (base = 1.00).

- **Training setup**
  - SVF RL training hyperparameters: AdamW, LR `2e-3` cosine decay, batch size 256, clip `1e-3`, Œª sweep (Appendix A.1; Table 6).
  - LoRA training: next-token prediction with solution-augmented prompts, applied to q/v projections, ~`5e-5` LR, 200 iterations, batch size 256 (Appendix A.2; Table 6).

### Main quantitative results

#### 5.1 SVF fine-tuning vs LoRA on training tasks (Table 1)
Table 1 reports test-split performance after fine-tuning on each training task.

- **LLAMA3-8B-INSTRUCT** (Table 1):
  - Base: GSM8K `75.89`, MBPP-Pro `64.65`, ARC-Easy `88.59`
  - +LoRA: GSM8K `77.18`, MBPP-Pro `67.68`, ARC-Easy `88.97`
  - +SVF: GSM8K `79.15`, MBPP-Pro `66.67`, ARC-Easy `89.56`
  - Interpretation: SVF improves GSM8K and ARC-Easy vs base; on MBPP-Pro, LoRA is slightly higher than SVF in this row, though both improve over base.

- **MISTRAL-7B-INSTRUCT-V0.3** (Table 1):
  - Base: GSM8K `42.83`, MBPP-Pro `49.50`, ARC-Easy `81.65`
  - +LoRA: GSM8K `44.66`, MBPP-Pro `51.52`, ARC-Easy `81.19` (a drop on ARC-Easy)
  - +SVF: GSM8K `49.74`, MBPP-Pro `51.52`, ARC-Easy `85.14`
  - Interpretation: SVF gives the largest gains here, notably GSM8K +6.91 absolute over base.

- **LLAMA3-70B-INSTRUCT** (Table 1):
  - Base: GSM8K `85.29`, MBPP-Pro `80.81`, ARC-Easy `89.10`
  - +LoRA: GSM8K `77.26`, MBPP-Pro `68.69`, ARC-Easy `88.55` (large degradations on GSM8K/MBPP)
  - +SVF: GSM8K `88.32`, MBPP-Pro `80.81`, ARC-Easy `88.47`
  - Interpretation: For this model, LoRA as trained here harms performance substantially on two tasks, while SVF improves GSM8K and matches MBPP-Pro.

#### 5.2 Self-adaptation on unseen tasks (Table 2)
Table 2 evaluates adaptation strategies on tasks not used to train the experts.

- **LLAMA3-8B-INSTRUCT** (Table 2):
  - Base: MATH `24.54`, HumanEval `60.98`, ARC-Challenge `80.63`
  - +LoRA (best checkpoint among trained LoRAs): MATH `24.12`, HumanEval `52.44`, ARC-Challenge `81.06`
  - +Transformer¬≤ (Prompt): MATH `25.22`, HumanEval `61.59`, ARC-Challenge `81.74`
  - +Transformer¬≤ (Cls-expert): MATH `25.18`, HumanEval `62.80`, ARC-Challenge `81.37`
  - +Transformer¬≤ (Few-shot): MATH `25.47`, HumanEval `62.99`, ARC-Challenge `82.61`
  - Pattern: all three Transformer¬≤ strategies improve over base on all three tasks; few-shot is best on all three for this model.

- **MISTRAL-7B-INSTRUCT-V0.3** (Table 2):
  - Base: MATH `13.02`, HumanEval `43.29`, ARC-Challenge `71.76`
  - +LoRA: MATH `13.16`, HumanEval `37.80` (drop), ARC-Challenge `75.77` (gain)
  - +Transformer¬≤ (Prompt): MATH `11.86` (drop), HumanEval `43.90` (tiny gain), ARC `72.35`
  - +Transformer¬≤ (Cls-expert): MATH `11.60` (drop), HumanEval `43.90`, ARC `74.83`
  - +Transformer¬≤ (Few-shot): MATH `13.39` (gain), HumanEval `47.40` (gain), ARC `75.47` (gain)
  - Pattern: for Mistral, only the few-shot mixture produces consistent gains across all three unseen tasks.

- **LLAMA3-70B-INSTRUCT** (Table 2):
  - Base: MATH `40.64`, HumanEval `78.66`, ARC-Challenge `87.63`
  - +LoRA: MATH `25.40`, HumanEval `73.78`, ARC `83.70` (large degradations)
  - +Transformer¬≤ (Prompt): MATH `40.44` (about equal), HumanEval `79.88` (gain), ARC `88.48` (gain)
  - The text notes MATH is an exception where they tuned only half the layers for this model ‚Äúdue to limited GPU resources‚Äù (Section 4.2 discussion), which may cap gains.

#### 5.3 Ablations that probe ‚Äúwhy‚Äù (Table 4; Section 4.3)
On `LLAMA3-8B-INSTRUCT` trained on GSM8K:

- **Module sensitivity** (Table 4):
  - SVF RL MLP only (`0.39M` params): GSM8K `78.62`, MATH transfer `24.20`
  - SVF RL attention only (`0.16M`): GSM8K `76.19`, MATH `24.20`
  - SVF RL MLP+attention (`0.58M`): GSM8K `79.23`, MATH `25.04`
  - Interpretation: MLP contributes more than attention here; combining both gives best MATH transfer.

- **Objective function matters** (Table 4):
  - SVF next-token prediction (attention, `0.16M`): GSM8K `60.50`, MATH `18.52` (large degradation vs base GSM8K `75.89`, MATH `24.54`)
  - This supports the paper‚Äôs claim that RL is crucial in their SVF setting (Section 4.3 Analysis 3).

- **SVF vs LoRA under RL** (Table 4):
  - LoRA policy gradient (attention, `6.82M`): GSM8K `57.92`, MATH `15.72` (collapse)
  - Appendix B.4 Figure 9 shows LoRA with policy gradient collapses early and does not recover.
  - This supports the claim that LoRA is unstable under their RL recipe, while SVF remains stable (Section 4.3 Analysis 3; Figure 9).

#### 5.4 Dispatch accuracy (Figure 6)
- Confusion matrices show prompt engineering and classification-expert dispatch are mostly correct (high diagonal mass), and classification expert improves accuracy over prompt-only for LLAMA3-8B and Mistral-7B (Section 4.3 Analysis 1; Figure 6).
- Some mass goes to ‚ÄúOthers‚Äù (rows not summing to one), meaning the system sometimes declines to use experts (Figure 6 caption).

#### 5.5 Vision-language results (Figure 5)
- The text claims that applying SVF/Transformer¬≤ in the VLM domain improves performance ‚Äúby over 39%‚Äù for `LLAMA3-LLAVA-NEXT-8B` (Section 4.2; Figure 5).
- Exact numeric scores for TextVQA/OKVQA are not legible in the provided excerpt beyond the qualitative bar plot; I therefore only restate the ‚Äúover 39%‚Äù claim that is explicitly written.

### Do experiments support the claims?

- **Supportive evidence**
  - SVF often improves on training tasks across models, while LoRA sometimes degrades badly (Table 1).
  - Transformer¬≤ improves on unseen tasks, especially with few-shot CEM mixing, and tends to outperform ‚Äúbest-of LoRA checkpoints‚Äù on MATH/HumanEval for LLAMA3-8B and LLAMA3-70B (Table 2).
  - Ablations isolate that RL objective + SVF parameterization seems key; naive next-token SVF training can hurt (Table 4).

- **Where evidence is weaker / unclear from the excerpt**
  - The mechanism relies on correctness rewards, but the excerpt does not specify exact reward computation or evaluation protocol details (e.g., how HumanEval correctness is checked, generation settings), which affects reproducibility confidence.
  - Few-shot adaptation reports ‚Äúbest sample from test configurations‚Äù without a validation set (Appendix A.4), which can blur the line between adaptation and evaluation if not carefully separated.

---

## 6. Limitations and Trade-offs

- **Two-pass inference overhead.**
  - There is a real latency/compute cost because inference is run twice; Table 3 shows pass 1 costs 13%‚Äì47% of pass 2 depending on task (Table 3; Section 4.2 discussion).

- **CEM-based few-shot adaptation cost scales with adaptation budget.**
  - Few-shot mixing uses up to 10 adaptation samples and up to 100 CEM iterations (Appendix A.4), which is a one-time per-task overhead but may be heavy for tasks with very few prompts (Appendix D discussion).
  - The paper provides ‚ÄúCEM light‚Äù variants (Appendix D; Table 10) showing ARC-Challenge improvement remains with reduced prompts/generations, but these are still additional runs compared to single-pass inference.

- **Dependence on base model strength and reward sparsity.**
  - The paper notes SVF+RL can suffer from sparse rewards if the base model is weak (Section 3.2: ‚ÄúOne possible caveat‚Ä¶‚Äù), though detailed mitigation is not included in the provided excerpt.

- **SVD computation/storage is not fully specified.**
  - SVF conceptually needs `U, Œ£, V·µÄ` for each tuned matrix (Section 3.2). The excerpt does not state whether SVD is computed once and cached, how it affects memory, or how it is integrated efficiently into inference kernels.

- **Expert coverage and taxonomy limitations (Strategies A/B).**
  - Prompt-based and classifier-based dispatch require a predefined set of categories (Figure 3; Section 3.2 A/B). If the taxonomy is misaligned with real tasks, the system may select suboptimal experts (Section 4.3 Analysis 1 hints that similarity is not the only relevant metric).

- **Cross-model transfer is promising but not guaranteed.**
  - Transfer from LLAMA3-8B SVF vectors to Mistral-7B helps in 2/3 tasks in Table 5, but it also hurts MATH (Table 5: 13.02 ‚Üí 11.96). The paper itself cautions that compatibility may depend on architectural similarity and may not generalize to different scales (Section 4.3 Analysis 4).

- **Model architecture/training compute details are missing in the excerpt.**
  - Without details like decoding parameters, context length, and hardware, it is hard to judge scalability and deployability precisely.

---

## 7. Implications and Future Directions

- **How this changes the landscape (within the paper‚Äôs scope).**
  - The work suggests an alternative axis for modular adaptation: instead of adding low-rank adapters, one can reuse the base model‚Äôs internal directions (singular vectors) and learn only multiplicative scalings, yielding very small ‚Äúskill‚Äù objects (`z` vectors) (Section 3.2; Table 4 parameter counts).
  - The two-pass ‚Äúself-adapt then answer‚Äù design provides a concrete blueprint for sample-level expert routing/mixing without training a full MoE architecture (Figure 1; Section 2 vs MoE; Table 2).

- **Follow-up research directions suggested by the provided content.**
  - **Better dispatch/routing signals**: The paper notes domain similarity may not be sufficient and suggests future heuristics like ‚Äúpast expert performance‚Äù or ‚Äútoken-level analysis‚Äù to improve scalability (Section 4.3 Analysis 1).
  - **Scaling few-shot adaptation efficiency**: Appendix D suggests reducing number of few-shot samples and generations, and exploring alternative evolutionary algorithms beyond CEM (Appendix D).
  - **Model merging + SVF**: The conclusion points to model merging as a way to address the limitation that SVF experts are tied to base model latent components (Section 5 Conclusion).

- **Practical applications / downstream use cases (implied by experiments).**
  - Deployments where one wants:
    - a single base model plus a library of tiny experts,
    - runtime switching between math/coding/reasoning behaviors,
    - adaptation to unseen tasks using a small number (e.g., 3‚Äì10) labeled examples via mixture search (Appendix A.4; Table 8).

- **Repro/Integration Guidance (based on the paper‚Äôs described recipes).**
  - Prefer **SVF + RL** when:
    - you can define a correctness-based reward (unit tests, exact answers, etc.), and
    - you lack rich supervised solution traces (Section 3.2; Table 4 shows next-token SVF can degrade).
  - Prefer **Transformer¬≤ (Prompt or Cls-expert)** when:
    - you need low adaptation overhead per prompt and can tolerate a coarse taxonomy (Section 3.2 A/B; Table 3 overhead).
  - Prefer **Transformer¬≤ (Few-shot / CEM)** when:
    - you can afford a one-time per-task adaptation cost and have a small set of examples from the target task, since it is typically strongest in Table 2 and scales well with as few as 3‚Äì10 examples on ARC-Challenge (Table 8; Section 4.2 monotonic trend discussion).
  - If you want cross-model reuse, preserve the SVF vector ordering; shuffling harms performance (Table 5), but expect mixed outcomes and treat it as experimental (Section 4.3 Analysis 4).

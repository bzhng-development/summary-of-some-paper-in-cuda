# Group Sequence Policy Optimization

**ArXiv:** [2507.18071](https://arxiv.org/abs/2507.18071)
**Authors:** Chujie Zheng, Shixuan Liu, Mingze Li, Xiongâ€‘Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, Junyang Lin
**Institutions:** Alibaba or affiliated (not explicitly listed on arXiv metadata)

## ğŸ¯ Pitch

The paper introduces Group Sequence Policy Optimization (GSPO), a reinforcement learning algorithm that enhances training stability and efficiency by utilizing sequence-level weighting and clipping. This innovation resolves the instability and noise issues faced by existing methods like GRPO, particularly in long responses and Mixture-of-Experts (MoE) models, marking a significant advancement for scalable and robust language model training.

---

## 1. Executive Summary
This paper proposes Group Sequence Policy Optimization (GSPO), a reinforcement learning (RL) algorithm for training large language models that replaces token-level importance weighting with sequence-level weighting, clipping, and optimization. The core significance is training stability and efficiency: GSPO avoids the collapse and noise issues observed with GRPO on long responses and Mixtureâ€‘ofâ€‘Experts (MoE) models, while simplifying RL infrastructure and improving benchmark performance (see Â§5.1â€“Â§5.4, Fig. 1â€“3).

## 2. Context and Motivation
- Problem addressed
  - Large-scale RL for language models needs to remain stable as models grow, responses get longer, and training becomes more offâ€‘policy due to mini-batching. The paper targets the instability and inefficiency of the widely used GRPO algorithm when scaling RL, especially for MoE models (Â§1, Â§3).
- Why it matters
  - Instability â€œoften result[s] in catastrophic and irreversible model collapseâ€ during RL of large models (Â§1). Stable RL is a prerequisite for scaling model capability (reasoning, long-context tasks) through extensive compute and data.
- Prior approaches and their gaps
  - PPO (Â§2): Uses token-level ratios and a separate value model to estimate advantages. Problems: heavy memory/compute for a value model of similar size to the policy; difficulty obtaining reliable value estimates for long responses and hard tasks.
  - GRPO (Â§2): Removes the value model by computing a group-based normalized advantage (z-score) across multiple responses to the same query, but still optimizes with token-level importance ratios and token-level clipping.
  - Gap: GRPOâ€™s token-level importance weighting misapplies importance sampling (Â§3). Weighting each token by a single sample from its next-token distribution introduces high-variance noise that grows with sequence length; clipping then amplifies these effects, pushing training toward collapse (Â§3, Â§4.2).
- Positioning
  - The paper reframes offâ€‘policy correction and clipping at the same unit as reward assignment: the sequence. It defines importance ratios by sequence likelihood, performs sequence-level clipping, and shows both theoretical and empirical benefits (Â§4.1â€“Â§4.2; Fig. 1â€“3).

## 3. Technical Approach
This section explains each algorithmic component and how it departs from prior RL training for language models.

- Preliminaries (Â§2)
  - The policy `Ï€_Î¸` is an autoregressive language model. For a query `x` and response sequence `y = (y_1, ..., y_|y|)`, the sequence likelihood is
    - Ï€_Î¸(y|x) = Î _t Ï€_Î¸(y_t | x, y_<t).
  - A verifier (reward model or evaluator) scores the pair (x, y) with reward r(x, y) âˆˆ [0, 1].
  - Offâ€‘policy setting: Data is generated by the â€œoldâ€ policy `Ï€_{Î¸_old}` while we optimize the â€œcurrentâ€ policy `Ï€_Î¸`. This necessitates importance weighting and clipping.
  - Clipping: A mechanism that caps how much an offâ€‘policy sample can influence the update. In PPO/GRPO this is implemented as min(wÂ·A, clip(w)Â·A), where `w` is an importance ratio and `A` an advantage.

- What PPO and GRPO do (Â§2)
  - PPO (Eq. 1): For each token, uses token-level ratio w_t(Î¸) = Ï€_Î¸(y_t|x, y_<t)/Ï€_{Î¸_old}(y_t|x, y_<t) and token advantage A_t (from a value model) with token-level clipping.
  - GRPO (Eq. 2â€“3): Replaces the value model with a group-based advantage `AÌ„_i` computed as a zâ€‘score over G responses `{y_i}` to the same query:
    - AÌ„_i = (r(x, y_i) âˆ’ mean_i r)/(std_i r). Every token in y_i shares this same AÌ„_i.
    - Still uses token-level ratios w_{i,t}(Î¸) with token-level clipping.

- Why token-level importance weighting is ill-posed here (Â§3)
  - Importance sampling principle (Eq. 4) estimates an expectation under a target distribution by reweighting many samples from a behavior distribution. Crucially, the importance weight corrects a distribution mismatch when averaging across many samples.
  - GRPO applies a token-level weight using only a single next-token sample at each position t, w_{i,t} = Ï€_Î¸(y_{i,t}|...)/Ï€_{Î¸_old}(y_{i,t}|...). This does not correct a distribution mismatch; instead it injects high-variance noise, which accumulates over tokens and is further distorted by clipping (Â§3).
  - Empirically, this can lead to â€œcatastrophic and irreversible model collapseâ€ on large models and long responses (Â§1, Â§3).

- GSPO: Group Sequence Policy Optimization (Â§4.1)
  - Core idea: Make the unit of offâ€‘policy correction and clipping match the unit of rewardâ€”the entire sequence.
  - Sequence-level importance ratio (Eq. 7):
    - s_i(Î¸) = [Ï€_Î¸(y_i|x)/Ï€_{Î¸_old}(y_i|x)]^{1/|y_i|}
    - This is the geometric mean of token-wise ratios; the exponent 1/|y_i| normalizes by length to stabilize the scale across different response lengths and limit volatility.
  - Objective (Eq. 5â€“6):
    - J_GSPO(Î¸) = E_{x, {y_i}âˆ¼Ï€_{Î¸_old}} [ (1/G) Î£_i min( s_i AÌ„_i , clip(s_i, 1âˆ’Îµ, 1+Îµ) AÌ„_i ) ]
    - Same group-based advantage AÌ„_i as in GRPO (zâ€‘score within a group of G responses).
    - Sequence-level clipping is now applied to entire responses via s_i, not to tokens.
  - Intuition:
    - The ratio s_i compares how likely the whole sequence y_i is under the new vs old policy, matching how rewards are applied (to whole sequences). This reduces the mismatch that produced noise in GRPO.

- Gradient analysis: how GSPO changes the update (Â§4.2)
  - Ignoring clipping for clarity, GSPOâ€™s gradient (Eq. 10) is
    - âˆ‡J_GSPO = E [ (1/G) Î£_i s_i AÌ„_i Â· (1/|y_i|) Î£_t âˆ‡ log Ï€_Î¸(y_{i,t}|x,y_{i,<t}) ].
    - All tokens in y_i are weighted equally by s_i AÌ„_i/|y_i|.
  - In contrast, GRPOâ€™s gradient (Eq. 12) is
    - âˆ‡J_GRPO = E [ (1/G) Î£_i AÌ„_i Â· (1/|y_i|) Î£_t w_{i,t} âˆ‡ log Ï€_Î¸(y_{i,t}|x,y_{i,<t}) ].
    - Each token is reweighted by its own w_{i,t}, which varies across tokens and steps. These per-token fluctuations are non-negligible and accumulate unpredictably (Â§4.2).
  - Takeaway: GSPO removes the unstable, token-specific weighting and aligns optimization with sequence-level rewards and offâ€‘policy correction.

- GSPO-token: finer-grained advantage without token-level ratios (Â§4.3)
  - Motivation: In settings like multi-turn RL, we may want different advantages per token (e.g., credit assignment to specific steps).
  - Objective (Eq. 13â€“17):
    - Define s_{i,t}(Î¸) = sg[s_i(Î¸)] Â· Ï€_Î¸(y_{i,t}|...)/sg[Ï€_Î¸(y_{i,t}|...)], where sg[Â·] stops gradients. Numerically s_{i,t} equals s_i, so clipping and optimization remain sequence-level in magnitude, but you can multiply by token-specific advantages AÌ„_{i,t}.
    - The resulting gradient equals GSPOâ€™s when AÌ„_{i,t} = AÌ„_i. Hence GSPO-token preserves the stability of sequence-level ratios while allowing token-wise advantage design.

- Design choices and why
  - Sequence-level ratio: matches reward granularity and reduces variance (Â§4.1â€“Â§4.2).
  - Length normalization s_i = (ratio)^{1/|y|}: prevents a few token changes from exploding the overall ratio and keeps clipping ranges consistent across lengths (Â§4.1).
  - Group-based normalized rewards: keeps the method â€œvalue-freeâ€ like GRPO while aligning the advantage with sequence-level optimization (Â§4.1, Eq. 6).

## 4. Key Insights and Innovations
- Sequence-level importance weighting and clipping (fundamental innovation)
  - Whatâ€™s new: Replace token-level w_{t} with sequence-level s = [Ï€_Î¸(y|x)/Ï€_{Î¸_old}(y|x)]^{1/|y|} and apply clipping on s (Eq. 5â€“7).
  - Why it matters: Aligns the unit of offâ€‘policy correction with reward application, reduces gradient noise accumulation, and improves stability and efficiency (Â§3â€“Â§4.2). This directly addresses the root cause of GRPOâ€™s instability.
- Theoretical diagnosis of GRPOâ€™s failure mode (conceptual contribution)
  - Whatâ€™s new: Formal argument via importance sampling (Eq. 4) and gradient comparison (Eq. 10 vs. 12) showing token-level weighting with single samples is ill-posed and high-variance (Â§3â€“Â§4.2).
  - Why it matters: Explains frequent collapses when scaling RL to long responses and MoE, guiding a principled fix rather than adâ€‘hoc stabilization.
- Stability for MoE training without Routing Replay (practical advance)
  - Whatâ€™s new: GSPO converges for MoE without extra mechanisms; GRPO typically requires â€œRouting Replayâ€ that caches and reuses old expert routes to stabilize token-level ratios (Â§5.3, Fig. 3).
  - Why it matters: Removes memory/communication overhead and capacity restrictions from MoE RL pipelines while improving stability (Â§5.3).
- Infrastructure simplification: use inference-engine likelihoods (systems contribution)
  - Whatâ€™s new: Because GSPO relies on sequence-level likelihoods, it tolerates precision discrepancies between engines and can avoid recomputing token-level likelihoods with the training engine (Â§5.4).
  - Why it matters: Reduces engineering complexity and cost in disaggregated training-inference setups and in partial rollout or multi-turn RL.

## 5. Experimental Analysis
- Evaluation methodology (Â§5.1)
  - Setup: Start from a cold-start model fine-tuned from `Qwen3-30B-A3B-Base`. Each rollout batch is split into 4 mini-batches (making the optimization offâ€‘policy).
  - Algorithms and hyperparameters:
    - GSPO: sequence-level clipping ranges set to left=3eâˆ’4, right=4eâˆ’4 (Eq. 5).
    - GRPO baseline: token-level clipping tuned to left=0.2, right=0.27 (Eq. 2).
    - Note the very different magnitudes reflect different ratio definitions (Â§4.1).
  - Baseline requirements: GRPO requires the â€œRouting Replayâ€ strategy to converge on MoE (Â§5.1, Â§5.3). GSPO does not.
  - Benchmarks and metrics:
    - AIMEâ€™24: average Pass@1 over 32 samples.
    - LiveCodeBench (202410â€“202502): average Pass@1 over 8 samples.
    - CodeForces: Elo rating.
    - Also monitor training reward (y-axis between 0.50â€“0.70 in Fig. 1).

- Main results
  - Stability and efficiency (Fig. 1):
    - Training reward under GSPO rises smoothly with compute and continues to improve as the query set is updated and generation length extended.
    - Quoted claim: â€œGSPO possesses remarkably higher training efficiency than GRPO.â€ (Fig. 1 caption).
    - On AIMEâ€™24, LiveCodeBench, and CodeForces tracks, GSPO reaches higher performance at the same training compute compared to GRPO (curves consistently above).
  - Clipping fractions (Fig. 2):
    - GSPO clips a far larger fraction of tokens on average (~0.15) than GRPO (~0.0013), a 100Ã— difference, yet still trains more efficiently.
    - Quote (Â§5.2): this â€œcounter-intuitive findingâ€ implies GRPOâ€™s token-level gradient estimates are â€œinherently noisy and inefficient,â€ whereas GSPOâ€™s sequence-level signal is more reliable.
  - MoE training and Routing Replay (Fig. 3; Â§5.3):
    - With GRPO, training â€œconverges properlyâ€ only when Routing Replay is used; without it, reward stagnates far below the â€œwith replayâ€ curve.
    - They observe roughly â€œ10% of the expertsâ€ activated differ after each RL update for the same sample in a 48â€‘layer `Qwen3-30B-A3B-Base` MoE model, which destabilizes token-level ratios (Â§5.3).
    - GSPO removes the need for Routing Replay and still converges (see Fig. 1 and Â§5.3 discussion).
  - Deployment relevance (Â§5.4):
    - Infrastructure simplification: sequence-level likelihoods can be taken directly from the inference engine without recomputation, making GSPO practical in disaggregated systems.

- Are the experiments convincing?
  - Evidence strongly supports stability and efficiency claims:
    - Multiple metrics and tasks show GSPOâ€™s curves consistently surpass GRPO at equal compute (Fig. 1).
    - The routing replay ablation isolates a concrete failure mode of GRPO in MoE (Fig. 3).
    - The clipping fraction analysis (Fig. 2) substantiates the variance/efficiency argument.
  - Gaps and caveats:
    - Results are shown as trends over training compute without detailed numeric tables for exact Pass@1/Elo at fixed budgets; quantitative deltas are visually clear but not tabulated.
    - Hyperparameters like group size G are not reported in Â§5.1; sensitivity analyses (e.g., Îµ ranges, group size, reward normalization choices) are not provided.
    - While they note GSPO contributed to â€œexceptional performance improvementsâ€ in the latest Qwen3 models (Â§1, Â§5.1), the paper does not present end-to-end SOTA comparisons within this manuscript.

## 6. Limitations and Trade-offs
- Remaining offâ€‘policy bias/variance
  - GSPO still relies on offâ€‘policy samples from `Ï€_{Î¸_old}`; while sequence-level ratios reduce variance relative to token-level ratios, they do not eliminate it. Extreme distribution shifts between old and new policies could still cause many sequences to be clipped, slowing learning (Â§4.1, Â§5.2).
- Choice of clipping ranges
  - GSPO uses very small Îµ (e.g., 3eâˆ’4/4eâˆ’4 in Â§5.1) because s_i is a length-normalized geometric mean; these ranges are not directly comparable to PPO/GRPO. Selecting Îµ may require tuning, and no sensitivity study is provided.
- Equal token weighting within sequences
  - In base GSPO, all tokens in a response receive equal weight in the gradient (Eq. 10). This can be suboptimal when rewards depend on specific spans or steps. GSPO-token (Â§4.3) mitigates this by enabling token-wise advantages, but the paper does not present empirical results for GSPO-token.
- Group-based advantage normalization
  - Advantages are z-scored within a group (Eq. 6). When the group reward variance is tiny (std near zero) or group size G is small, normalization could be unstable. The paper does not discuss safeguards (e.g., std floors) or report G and its sensitivity.
- Reward model and task scope
  - Rewards r(x, y) come from verifiers and are bounded in [0,1] (Â§2). The paper does not explore noisy or adversarial reward models, multi-objective reward compositions, or strict safety constraints, which might interact with clipping and sequence-level ratios.
- Generalization beyond the tested settings
  - Results focus on a large Qwen3 MoE model setup and three benchmarks. While the failure mode in GRPO is general, broader validation (e.g., diverse domains, multilingual data, extremely long contexts) is not shown here.

## 7. Implications and Future Directions
- How this changes the landscape
  - Sequence-level offâ€‘policy correction and clipping provide a robust foundation for RL with LLMs, especially at scale and with MoE. It clarifies a root cause of instability in prior practice and offers a principled, simpler replacement that aligns optimization with how rewards are given.
- Practical applications
  - Large-scale RL training (reasoning, math, coding) where stability previously limited scaling.
  - MoE-based systems, where avoiding Routing Replay reduces memory/communication overhead and unlocks full capacity.
  - Disaggregated training-inference pipelines, partial rollouts, and multi-turn RL, where sequence-level likelihoods from an inference engine can be used directly (Â§5.4).
- Research directions enabled or suggested
  - Theoretical analysis: biasâ€“variance trade-offs of sequence-level clipping; convergence guarantees; optimal Îµ schedules.
  - Credit assignment: Empirical evaluation of GSPO-token and other ways to combine sequence-level ratios with fine-grained advantages; exploration for multi-turn dialogues and tool use.
  - Robustness: Behavior under severe offâ€‘policy drift; adaptive clipping; trust-region interpretations at the sequence level.
  - Reward shaping: Interplay of sequence-level ratios with diverse reward models (safety, factuality, style), and with multi-objective or constrained optimization.
  - Infrastructure: Standardizing inference-engine likelihood APIs; mixed-precision impacts on sequence-level likelihoods; reducing recomputation in distributed RL training.

Quote highlights from the paper
- â€œWe propose Group Sequence Policy Optimization (GSPO) â€¦ [that] defines importance ratios based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization.â€ (Â§1, Â§4.1)
- â€œGSPO possesses remarkably higher training efficiency than GRPO.â€ (Fig. 1 caption)
- â€œAverage fractions of clipped tokens â€¦ GSPO: 0.15, GRPO: 0.0013.â€ (Fig. 2)
- â€œWith the 48-layer Qwen3-30B-A3B-Base model â€¦ roughly 10% of the experts â€¦ are different â€¦ after each RL gradient update.â€ (Â§5.3)
- â€œGSPO eliminates the dependency on Routing Replay â€¦ converging normally, and optimizing stably.â€ (Â§5.3)

In sum, GSPO replaces a noisy token-level mechanism with a sequence-level approach that matches the unit of reward, producing stable and efficient RL training for modern LLMs and MoE architectures.

# Scaling Preâ€‘training to One Hundred Billion Data for Vision Language Models

**ArXiv:** [2502.07617](https://arxiv.org/abs/2502.07617)
**Authors:** Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, Xiaohua Zhai
**Institutions:** Google DeepMind

## ğŸ¯ Pitch

This paper reveals a significant shift in vision-language model benefits at an unprecedented scale of 100 billion image-text pairs from the web, demonstrating that while classic benchmarks saturate, there is a marked improvement in cultural inclusivity and multilingual coverage. This finding is crucial for developing equitable AI systems, emphasizing the need to prioritize data diversity and low-resource language support over traditional Western-centric metrics.

---

## 1. Executive Summary
This paper studies what changes when you pre-train visionâ€“language models (VLMs) on an unprecedented scale of 100 billion imageâ€“text pairs drawn from the raw web. The main finding is counterintuitive: classic â€œWesternâ€‘centricâ€ benchmarks (e.g., ImageNet zeroâ€‘shot and COCO retrieval) largely saturate at this scale, but diversity-focused capabilitiesâ€”cultural coverage, lowâ€‘resource languages, and crossâ€‘group parityâ€”improve markedly (Sections 4.1â€“4.4; Tables 2â€“5; Figure 1).

## 2. Context and Motivation
- Problem addressed
  - Whether pushing VLM preâ€‘training data from â‰ˆ10B to 100B unique imageâ€“text pairs yields meaningful benefits, and in what dimensions those benefits appear.
- Why this matters
  - VLMs underpin retrieval, captioning, and multimodal assistants. If additional webâ€‘scale data only yields diminishing returns on standard benchmarks, resources could be better spent elsewhere. Conversely, if scaling uniquely improves inclusivity (e.g., cultural and linguistic coverage), it is vital for building equitable systems (Abstract; Figure 1; Sections 1 and 7).
- Prior landscape and gaps
  - Dataset growth: from curated millions (Conceptual Captions) to webâ€‘scale billions (LAIONâ€‘5B, WebLIâ€‘10B) using filters such as CLIP to â€œimprove qualityâ€ (Section 2; citations [59, 15, 60]).
  - Scaling laws: error often follows a power law with data size, implying diminishing but nonâ€‘zero returns; however, effects at 100B for VLMs were unknown (Section 1; scaling law references).
  - Inclusivity concerns: filtering and Englishâ€‘centric pipelines can suppress cultural diversity and multilingual coverage (Section 2; e.g., [53]).
- Positioning of this work
  - Introduces `WebLIâ€‘100B` (100B unique pairs) with minimal filtering and evaluates models in a computeâ€‘matched regime across 1B, 10B, and 100B scales (Sections 3.1â€“3.2).
  - Broadens evaluation beyond traditional metrics to cultural diversity, multilinguality, and fairness (Section 3.3), showing where the 100B scale pays off (Sections 4.2â€“4.4).

## 3. Technical Approach
This is an empirical scaleâ€‘up study. The core questionâ€”what changes at 100B data?â€”is isolated by holding training compute roughly fixed across data scales and by evaluating many capability axes.

- Data construction
  - `WebLIâ€‘100B`: 100B imageâ€“text pairs scraped from the web; minimally filtered (remove harmful images and PII) to preserve breadth of languages and cultures; use both `alt-text` and page `title` as paired text; remove nearâ€‘duplicates with overlap to >90 evaluation tasks to avoid leakage (Section 3.1, Raw Datasets).
  - Subsets: `1B` and `10B` are uniform random samples of `WebLIâ€‘100B` (1% and 10%) (Section 3.1).
  - Language attribution: use the pageâ€™s `content-language` HTML meta tag rather than noisy onâ€‘theâ€‘fly language detection (Section 3.1).
  - Qualityâ€‘filtered sets for analysis: three 5Bâ€‘pair English datasetsâ€”(i) â€œCLIPâ€‘filteredâ€ using a `CLIP-L/14` alignment score, (ii) â€œClassifierâ€‘filteredâ€ using a VLM trained to predict alignment, and (iii) â€œBaseline (en)â€ by sampling English pairs without filtering (Section 3.1; Figure 4; Appendix D).
  - Language rebalancing (for a specific study): upsample 7 lowâ€‘resource languages in the `Crossmodal-3600` benchmark (bn, fil, hi, iw/he, mi, sw, te) to 1% each of training examples, with the remaining 93% drawn from the original mix (Section 3.1 Languageâ€‘rebalanced Datasets; Section 5.2; Appendix F lists language shares).

- Models and training
  - VLM type: `SigLIP` contrastive models with ViT backbones (`ViT-B/16`, `ViT-L/16`, `ViT-H/14`) for both image and text encoders (Section 3.2).
    - Contrastive learning aligns matched imageâ€“text pairs in a shared embedding space and pushes apart mismatched pairs; `SigLIP` uses a sigmoid loss rather than softmax/InfoNCE (reference [78]).
  - Computeâ€‘matched protocol:
    - Fix the total number of seen examples to 100B for every condition. Therefore, `1B` data runs 100 epochs, `10B` runs 10 epochs, and `100B` runs 1 epoch (Section 3.2: â€œAll models are trained on a maximum of 100 billion examplesâ€).
    - Batch size 32K; inverse squareâ€‘root learning rate schedule with 200M warmup and cooldown examples; LR 0.001; weight decay 1eâ€‘4 (Section 3.2).
    - Inputs: images resized to 224Ã—224; texts tokenized with multilingual `mt5` tokenizer up to 64 tokens (Section 3.2).
    - Periodic checkpoints when models have seen 3, 7, 10, 17, 26, 33, 49, 66, and 100B examples (Section 3.2).
  - Transfer to generative VLMs:
    - Initialize `PaliGemma` (a compact, instructionâ€‘tunable VLM) with these vision encoders; pretrain on 50M seen examples following its Stageâ€‘1 recipe at 224Ã—224. Evaluate two scenarios: vision frozen vs unfrozen during PaliGemma pretraining/finetuning (Section 3.3 â€œTransfer to Generative Modelsâ€; Table 6; Appendix C).

- Evaluations and metrics (Section 3.3)
  - Westernâ€‘centric tasks:
    - Zeroâ€‘shot classification: ImageNet, CIFARâ€‘100, Oxfordâ€‘IIIT Pets (Table 2).
    - 10â€‘shot classification: CUBâ€‘Birds, Caltechâ€‘101, Cars196, Colorectal Histology, DTD (Table 2).
    - Zeroâ€‘shot retrieval: COCO Captions and Flickr30k (imageâ†’text and textâ†’image) (Table 2).
  - Cultural diversity:
    - Zeroâ€‘shot: Dollar Street (DS), GeoDE, Google Landmarks v2 (GLDv2).
    - 10â€‘shot geolocalization: DS and GeoDE (Table 3).
    - â€œGeolocalizationâ€ here means predicting an imageâ€™s country/region category with few labeled examples per class.
  - Multilinguality:
    - `Crossmodalâ€‘3600`: 3600 images with human captions in 36 languages; measure zeroâ€‘shot retrieval (imageâ†’text and textâ†’image) per language. Report averages for lowâ€‘resource vs highâ€‘resource groups (Section 3.3; Figure 3; Appendix B).
  - Fairness:
    - Representation bias (RB): tendency to associate random images with label â€œMaleâ€ vs â€œFemaleâ€ (values >50% mean a male preference). Values reported as the percentage of times â€œMaleâ€ wins (Table 4).
    - Association bias (AB): for pairs of occupation labels (e.g., â€œsecretaryâ€ vs â€œmanagerâ€), measure how often a gendered image steers the model to specific occupations using `FairFace` images (Figure 2; Section 4.4).
    - Performance disparity: maximum gap across subgroupsâ€”by income level on DS (four income bins) and by region on GeoDE (Africa, Americas, East Asia, Europe, SE Asia, West Asia) (Table 5).
  - Statistics and scaling fits:
    - Use Wilcoxon signedâ€‘rank tests to compare conditions (Sections 4.1 and 4.2).
    - Fit powerâ€‘law scaling laws for error vs data size and report exponents and asymptotic error limits (Tables 2â€“3), following [2].

- Qualitative analysis
  - Attentionâ€‘map visualizations show where models focus in images across scales (Tables 1 and 7). These illustrate better localization of culturally specific elements at 100B.

## 4. Key Insights and Innovations
1) Scaling to 100B yields little on classic benchmarks but large gains in inclusivity axes
- What is new: Prior work emphasized better classic scores with better filtering and moderate scale-ups. This study isolates dataset size (1Bâ†’10Bâ†’100B) under fixed compute and shows a split outcome across task families (Sections 4.1â€“4.3).
- Evidence:
  - Western benchmarks saturate. For example, `ViT-L/16` ImageNet zeroâ€‘shot error changes 29.7%â†’28.5% going 10Bâ†’100B, a modest 1.2â€‘point drop; COCO I2T@1 error increases 47.2%â†’45.3% (Table 2). A signedâ€‘rank test yields p=0.9 (Section 4.1).
  - Cultural diversity improves considerably. Example: Dollar Street 10â€‘shot error for `ViT-L/16` improves 64.1%â†’58.3% (âˆ’5.8 points), and for `ViT-H/14` 59.1%â†’53.7% (âˆ’5.4) when scaling 10Bâ†’100B; improvements are statistically significant (p=0.002) (Table 3; Section 4.2).
  - Lowâ€‘resource languages benefit more. Figure 3 shows larger error reductions for lowâ€‘resource languages than for highâ€‘resource ones across all model sizes (Section 4.3).

2) Filtering improves classic scores but harms cultural and fairness metrics
- What is new: A systematic, sideâ€‘byâ€‘side comparison of CLIPâ€‘based filtering vs raw English sampling vs a classifierâ€‘based filter, all at the same 5B size (Section 5.1; Figure 4; Appendix D).
- Evidence:
  - Western tasks: CLIP filter shows consistent error reductions (e.g., ImageNet 0â€‘shot error at 30B seen examples: 23.9% CLIP vs 24.3% baselineâ€‘en; Table in Appendix D).
  - Cultural diversity and fairness: all filtered sets perform worse. Figure 4 (middle/right) shows higher error on cultural tasks and fairness aggregates across training trajectories; Table 10 details perâ€‘benchmark degradations (Appendix D).

3) Language rebalancing is a cheap, targeted fix for lowâ€‘resource languages
- What is new: Upsampling seven lowâ€‘resource languages to 1% each yields substantial retrieval error drops for those languages, with minor regressions on highâ€‘resource languages; overall multilingual average improves (Section 5.2; Figure 5; Table 11).
- Evidence:
  - Lowâ€‘resource average error decreases markedly after rebalancing across data scales (Figure 5, topâ€‘left). For example, at `ViT-L/16` and 100B seen examples, lowâ€‘resource average goes from 75.01% to 70.10% (Table 11, â€œAverage Multilingual: Lowâ€‘Resource Langâ€).

4) Bias persistence vs parity improvements at 100B
- What is new: More unfiltered data does not fix gender label/occupation associations, but it narrows crossâ€‘group performance gaps (Section 4.4).
- Evidence:
  - Persistent representation bias: models prefer â€œMaleâ€ over â€œFemaleâ€ â‰ˆ85% of the time; this remains high at 100B (Table 4).
  - Association bias: heatmaps in Figure 2 show occupationâ€“gender skews remain across scales (e.g., â€œnurseâ€, â€œsecretaryâ€ favored for female images).
  - Performance disparity shrinks: e.g., GeoDE regional disparity decreases for all sizes when trained with 100B data (Table 5, lower half: disparities fall from 4.7â†’4.4 for `ViT-B`, 3.2â†’2.8 for `ViT-L`, 3.6â†’2.7 for `ViT-H`).

These are not merely incremental metric bumps; they reframe what â€œscaling helpsâ€ means for VLMs: less gain on headline Western benchmarks, more gain on longâ€‘tail inclusion and crossâ€‘group parity.

## 5. Experimental Analysis
- Evaluation design and metrics
  - All classification and retrieval numbers are reported as error percentages (lower is better) unless otherwise noted; representation bias is a disparity measure (Section headers of Tables 2â€“3; footnote in Appendix B).
  - Compute control: every configuration sees 100B examples total, enabling a fair comparison of â€œmore unique data onceâ€ vs â€œless unique data many timesâ€ (Section 3.2).

- Main quantitative results
  - Westernâ€‘centric saturation (Section 4.1; Table 2)
    - Quote: 
      > Wilcoxonâ€™s signed rank test gives a pâ€‘value of 0.9, indicating differences are not significant.
    - Examples (10Bâ†’100B):
      - `ViT-L/16` ImageNet 0â€‘shot error: 29.7%â†’28.5% (âˆ’1.2).
      - `ViT-H/14` COCO T2I@1 error: 60.3%â†’59.3% (âˆ’1.0).
    - Scaling law fits report similar asymptotic limits across scales (Tables 2, 95% CIs not significantly different; p=0.09).

  - Cultural diversity gains (Section 4.2; Table 3)
    - Quote:
      > Scaling training data from 10B to 100B yields substantial gains on Dollar Street 10â€‘shot, where ViTâ€‘L and ViTâ€‘H see absolute improvements of 5.8% and 5.4% respectively.
    - Additional examples:
      - `ViT-H/14` GeoDEâ€‘Country 10â€‘shot error: 50.2%â†’47.6% (âˆ’2.6).
      - `ViT-L/16` GLDv2 0â€‘shot error: 46.4%â†’45.7% (âˆ’0.7), a smaller but consistent improvement.

  - Multilinguality (Section 4.3; Figure 3; Appendix B)
    - Figure 3 shows larger decreases in error for lowâ€‘resource languages at 100B than for highâ€‘resource. The gap widens with model size (bars annotated with improvements; e.g., Î”â‰ˆ2â€“3 points for lowâ€‘resource vs â‰ˆ1 point for highâ€‘resource at `ViT-H`).

  - Fairness (Section 4.4; Tables 4â€“5; Figure 2)
    - Representation bias: 
      > Values â‰ˆ85%â€”preference for â€œMaleâ€ remainsâ€”do not improve at 100B (Table 4).
    - Association bias:
      > Heatmaps (Figure 2) for five occupation pairs across three model sizes and three data scales show persistent genderâ€‘occupation stereotypes.
    - Performance disparity improvement:
      - GeoDE regional disparity reduces across all model sizes at 100B (Table 5). 
      - Dollar Street income disparity slightly improves for `ViT-B` (32.5â†’29.0), stays similar for `ViT-H` (32.2â†’32.1), and increases slightly for `ViT-L` (29.7â†’30.4).

  - Transfer to generative models (Section 4.5; Table 6; Appendix C)
    - Aggregated results for `ViT-L/16` encoders in `PaliGemma` (frozen vs unfrozen):
      - Frozen averages: 73.6 (1B), 72.7 (10B), 73.9 (100B).
      - Unfrozen averages: 75.1 (1B), 73.7 (10B), 75.3 (100B).
    - Quote:
      > When taking noise level into consideration, no consistent performance gains across downstream tasks are observed as pretraining data scale increases (Table 6).

  - Quality filtering (Section 5.1; Figure 4; Appendix D)
    - Western metrics: CLIPâ€‘filtered outperforms baseline (e.g., average Western 0â€‘shot classification error is lower across seenâ€‘example checkpoints; Appendix D).
    - Cultural/fairness metrics: CLIP and Classifier filters hurt performance (Figure 4 middle/right).

  - Language rebalancing (Section 5.2; Figure 5; Table 11)
    - After upsampling 7 lowâ€‘resource languages to 1% each, lowâ€‘resource retrieval error drops notably (Figure 5 topâ€‘left; e.g., `ViT-L/16`, 100B seen: 75.01%â†’70.10%).
    - Sideâ€‘effects: small degradations on highâ€‘resource and Western metrics but overall multilingual average still improves (Figure 5; Table 11).

  - Qualitative attention maps (Section 5.3; Tables 1 and 7)
    - At 100B, attention focuses more precisely on culturally salient regions (e.g., the dome shape of an igloo; detailed patterns in â€œIgorot Danceâ€), illustrating learned representations beyond Western concepts.

- Do the experiments support the claims?
  - Yes for the central thesis: tables and statistical tests directly show saturation on classic benchmarks (Section 4.1) and significant gains in cultural diversity and lowâ€‘resource languages (Section 4.2; Figure 3).
  - The filtering and rebalancing analyses are ablations that clarify mechanisms: filtering skews the data distribution (hurting diversity), while rebalancing directly benefits targeted languages (Sections 5.1â€“5.2).
  - Bias findings are mixed: parity improves (Table 5), but inherent gender biases persist (Table 4; Figure 2), aligning with the nuance in the conclusions (Section 4.4; Section 7).

## 6. Limitations and Trade-offs
- Assumptions and design choices
  - Minimal filtering is intentional to preserve diversity, but it also retains noise (Section 6 â€œDiscussion: Data Filteringâ€). This likely explains why classic benchmarks do not improve.
  - Language identification uses the `content-language` meta tag; it can be missing or inaccurate, potentially affecting multilingual statistics (Section 3.1).
- Unaddressed scenarios / scope limits
  - Inclusivity is broader than the chosen metrics; only 36 languages in `Crossmodalâ€‘3600`, and fairness is limited to gender and regional/income disparity (Section 6 â€œLimitationsâ€).
  - Cultural metrics use specific datasets (Dollar Street, GeoDE, GLDv2); other forms of cultural knowledge (e.g., festivals, artifacts beyond landmarks/household items) are not separately benchmarked.
- Computational and data constraints
  - While compute is â€œmatchedâ€ across scales in terms of seen examples, training on 100B unique pairs (1 epoch) requires enormous data infrastructure and may behave differently than training on smaller data for many epochs (Section 3.2). This matters for practitioners who may not afford data collection or streaming at this scale.
- Tradeâ€‘offs evidenced by experiments
  - Filtering vs inclusivity: CLIP filtering helps Western benchmarks but reduces cultural diversity and fairness metrics (Figure 4; Appendix D). 
  - Rebalancing vs Western performance: upsampling lowâ€‘resource languages helps those languages but slightly hurts Western and some highâ€‘resource metrics (Figure 5; Table 11).
- Open questions
  - Can we design filtering that preserves or even enhances cultural diversity and multilinguality? The paper explicitly calls for new filtering strategies with this goal (Section 6).
  - How do results change at higher image resolutions or longer text inputs than 224Ã—224 and 64 tokens?

## 7. Implications and Future Directions
- How this work changes the fieldâ€™s perspective
  - It reframes â€œbenefit from more raw dataâ€ for VLMs: at 100B, the primary returns are not on saturated Westernâ€‘centric leaderboards but on inclusivityâ€”cultural coverage, lowâ€‘resource languages, and reduced regional performance disparity (Figure 1; Sections 4.2â€“4.4; Table 5).
  - It cautions against default CLIPâ€‘based filtering if the goal includes inclusivity; filtering can systematically strip away longâ€‘tail cultural content (Section 5.1; Figure 4).
- Followâ€‘up research directions
  - Diversityâ€‘preserving filtering and data governance:
    - Learn data selection that explicitly optimizes for cultural and linguistic coverage alongside quality.
    - Develop auditing tools that quantify coverage before training (expanding beyond the current fairness metrics).
  - Mixture design and curriculum:
    - Automated language and cultureâ€‘aware mixing ratios that balance Western performance with inclusivity.
    - Adaptive curriculum over data scales so that the model sees rare concepts enough times without overwhelming repeats of common ones.
  - Model/architecture innovations:
    - Explore whether generative preâ€‘training or hybrid losses at 100B can translate inclusivity gains into stronger downstream generative performance (since PaliGemma transfer gains were inconsistent; Table 6).
    - Investigate higherâ€‘resolution preâ€‘training and longer text contexts for better OCR and fineâ€‘grained cultural cues.
- Practical applications and use cases
  - Global search and retrieval systems that must work across languages and cultures.
  - Publicâ€‘sector and NGO tools where equitable performance (reduced subgroup disparity) mattersâ€”e.g., geolocalization for disaster response or cultural heritage identification (Table 5, GeoDE disparity improvements).
  - Multimodal assistants intended for worldwide audiences; the results argue for 100Bâ€‘scale pretraining if inclusivity is a requirement (Section 7).

> Bottom line (Section 7): while 100B raw web data may not move the needle on classic Western benchmarks, it is â€œvital for building truly inclusive multimodal systems.â€ The studyâ€™s careful computeâ€‘matched design and broad evaluation provide strong evidence for this claim, and its analyses of filtering and rebalancing show actionable levers to further improve inclusivity.

# FAST AND SIMPLEX: 2-SIMPLICIAL ATTENTION IN TRI-TON

**ArXiv:** [2507.02754](https://arxiv.org/abs/2507.02754)

## ðŸŽ¯ Pitch

This paper introduces 2-simplicial attention, a novel higher-order generalization of standard Transformer attention that allows each token to attend to pairs of other tokens via a trilinear function. By providing an efficient Triton-based implementation and rigorous scaling law analysis, the authors demonstrate that interleaving 2-simplicial layers enables models to achieve superior token efficiency and improved performance on reasoning, math, and coding tasksâ€”crucially, with a steeper scaling law exponent under a fixed token budget. This breakthrough addresses the pressing bottleneck of limited high-quality training data in large language models, paving the way for more resource-efficient and capable AI systems.

---

## 1. Executive Summary
This paper introduces 2â€‘simplicial attention, a higherâ€‘order generalization of standard attention that lets each token attend to pairs of other tokens via a trilinear scoring function. It provides a practical, highâ€‘throughput Triton implementation and shows that, at a fixed token budget, models with interleaved 2â€‘simplicial layers achieve better token efficiency and a steeper scalingâ€‘law exponent on reasoning, math, and coding benchmarks compared to dotâ€‘product Transformers.

## 2. Context and Motivation
- Problem addressed
  - Modern large language models (LLMs) are increasingly limited by the availability of highâ€‘quality training tokens rather than compute. Standard scaling laws recommend scaling parameters and tokens together, but this assumes infinite data. Section 1â€“3 emphasize the need for tokenâ€‘efficient architectures that improve loss with fewer tokens.
  - Prior practice shows that most architectural or optimizer tweaks shift the loss offset but typically do not change the scaling exponent; see Section 1 with references to Kaplan et al. (2020), Hoffmann et al. (2022), Hestness et al. (2017), and the summary in Everett (2025).

- Why it matters
  - Token scarcity is now a practical bottleneck. If an architecture improves the exponent in the lossâ€“vsâ€“parameters power law at fixed tokens, one can get better quality without acquiring proportionally more data (Sections 1 and 3).

- Prior approaches and gaps
  - Linearâ€‘time attention (e.g., kernelized attention, stateâ€‘space models like Mamba) improves complexity but often trails quality (Section 2).
  - Higherâ€‘order attention ideas exist (2â€‘simplicial attention, triangular attention in Edge Transformer or AlphaFold), but a scalable, generalâ€‘purpose implementation and endâ€‘toâ€‘end scaling analysis for LLM preâ€‘training remained lacking (Section 2).

- Positioning
  - The paper revisits 2â€‘simplicial attention (Clift et al., 2019) and contributes:
    - An efficient slidingâ€‘window design with a Triton kernel (Sections 6â€“7).
    - A rotationâ€‘invariant trilinear form compatible with positional encodings and a simple expressivity theorem (Section 5, Theorem 5.1).
    - Empirical evidence that interleaving 2â€‘simplicial layers yields better token efficiency and a different (steeper) scaling exponent than standard attention (Sections 8 and Tables 2â€“3).

## 3. Technical Approach
At a high level, 2â€‘simplicial attention lets each query position i attend not to single keys j, but to pairs (j, k). Concretely:

- Baseline: standard attention (Section 4)
  - For a sequence `X âˆˆ R^{nÃ—d}`, compute query/key/value: `Q = X W_Q`, `K = X W_K`, `V = X W_V`.
  - Logits are dot products (Equation 2): `A = Q K^T / sqrt(d)`, softmax along each row (Equation 3), then the output is a weighted sum of values (Equation 4).

- 2â€‘simplicial attention (Section 4)
  - Add a second key/value stream: `Kâ€² = X W_Kâ€²`, `Vâ€² = X W_Vâ€²`.
  - Use a trilinear score tying one query to a pair of keys (Equation 5):
    > A^{(2s)}_{i j k} = (1/âˆšd) Î£_{l=1..d} Q_{i l} K_{j l} Kâ€²_{k l}
  - Apply a softmax jointly over the pair indices (Equation 6).
  - Aggregate values via elementâ€‘wise (Hadamard) product of the paired values (Equation 7):
    > vÌƒ^{(2s)}(i) = Î£_{j,k} S^{(2s)}_{i j k} Â· (v_j â—¦ vâ€²_k)
  - Intuition: the model can directly capture triangular or threeâ€‘way relations among tokens (i, j, k), which are hard to represent with only pairwise dot products in a single layer.

- Rotary encodings and rotation invariance (Section 5)
  - A stumbling block: the trilinear form in Equation 5 is not invariant to the same orthogonal rotation applied to all three vectors, which complicates using rotary positional embeddings (RoPE).
  - Solution: use a determinantâ€‘based trilinear form that is rotationâ€‘invariant. For 3â€‘D chunks of `q, k, kâ€²`, define (Equations 8â€“9):
    > fÌ‚3(a,b,c) = det([a; b; c])  
    > A^{(det)}_{i j1 j2} = Î£_{l=1..p} det([q_i^{(l)}, k_{j1}^{(l)}, kâ€²_{j2}^{(l)}])
  - This preserves innerâ€‘productâ€‘like invariances under rotations, making it compatible with RoPE semantics.

- Expressivity result (Appendix A, Theorem 5.1)
  - With a single attention head of dimension `d = 7` using the determinantâ€‘style logits (Equation 9), there is a construction whose output at position i is 1 iff there exists a pair (j1, j2) such that `(x_i + x_{j1} + x_{j2}) â‰¡ 0 (mod M)`. This â€œMatch3â€â€‘style capability formalizes a class of tripleâ€‘matching problems solvable in one layer.

- Making it practical: slidingâ€‘window 2â€‘simplicial attention (Section 6)
  - Full 2â€‘simplicial attention is O(n^3). The paper constrains attention to a local rectangle `[w1 Ã— w2]` in the two key axes around each query i (Figure 2, left).
  - Complexity comparison (Section 6):
    > Dotâ€‘product causal attention: O(A) = 2 n^2  
    > 2â€‘simplicial (windowed): O(A^{(2s)}) = 6 n w1 w2
  - The implementation evaluates window configurations and chooses `(w1, w2) = (512, 32)` to balance quality and latency (Table 1).

- System and kernel design (Section 7)
  - Kernel core ideas:
    - Use â€œonline softmaxâ€ Ã  la FlashAttention for numerical stability and IOâ€‘awareness.
    - Convert the 3â€‘tensor QÂ·KÂ·Kâ€² into a sequence of dense GEMMs by first elementâ€‘wise multiplying one pair (e.g., `QÂ·K1` or `VÂ·Vâ€²`) and then applying a matrix multiply with the third factor (Figure 2, right).
    - Overlap CUDAâ€‘core (elementâ€‘wise) and Tensorâ€‘core (GEMM) work; Triton implementation reaches ~520 TFLOPS (Figure 3).
  - Backward pass (Equations 10â€“16) is decomposed into two kernels (Section 7) to avoid excessive atomics; for small `w2`, a twoâ€‘stage algorithm (Algorithm 2) computes `dQ` with `dKâ€², dVâ€²` without atomics by alternating even/odd tiles.

- Model architecture choices (Sections 6 and 8)
  - Interleave slidingâ€‘window 2â€‘simplicial layers: every fourth layer is 2â€‘simplicial to spread compute evenly across pipeline stages.
  - Use high Grouped Query Attention ratio (`GQA`=64) to tile heads efficiently and avoid expensive elementâ€‘wise masks (Section 6).
  - Experiments use Mixtureâ€‘ofâ€‘Experts (MoE) models with â€œactive parametersâ€ (parameters used per token during routing) substantially smaller than total parameters (Section 8).

## 4. Key Insights and Innovations
- Higherâ€‘order attention that changes scaling exponents (Sections 3, 8; Tables 2â€“3)
  - Novelty: While many changes shift the loss intercept, the paper shows a changed exponent Î± in the tokenâ€‘fixed scaling law `L(N) â‰ˆ E' + A / N^Î±` (Equations 17â€“20).
  - Significance: A higher Î± means that increasing model size yields faster improvement at fixed token countâ€”directly addressing token scarcity.

- Practical 2â€‘simplicial attention kernel (Section 7; Figure 3; Listings Bâ€“C)
  - Novelty: A Triton kernel that fuses 2â€‘simplicial online softmax with 2D tiling, reaching performance competitive with optimized FlashAttention v3 implementations at long sequence lengths.
  - Significance: Transforms a theoretically attractive but cubicâ€‘cost mechanism into a usable building block by leveraging sliding windows and hardwareâ€‘aware tiling.

- Rotationâ€‘invariant trilinear form and expressivity (Section 5; Appendix A)
  - Novelty: Determinantâ€‘based trilinear logits (Equations 8â€“9) compatible with rotational invariances introduced by RoPE, alongside a simple expressivity theorem (Theorem 5.1).
  - Significance: Bridges a key gap between theory and practical positional encoding, and shows oneâ€‘layer solvability of tripleâ€‘matching patterns.

- Systems design for throughput (Sections 6â€“7; Table 1; Algorithm 2)
  - Novelty: Interleaving 2â€‘simplicial layers with standard layers, aggressive GQA (64), and a twoâ€‘kernel backward pass with a twoâ€‘stage, noâ€‘atomics path for small `w2`.
  - Significance: Converts a complex nâ€‘way aggregation into a pipelineâ€‘friendly operator with predictable latency.

## 5. Experimental Analysis
- Setup (Section 8)
  - Models: Sparse MoE LLMs with three sizes, reported as active/total parameters: `1B/57B`, `2B/100B`, `3.5B/176B`.
  - Layering: Every fourth layer uses slidingâ€‘window 2â€‘simplicial attention; other layers use standard global attention.
  - Training: AdamW, peak LR `4e-3`, weight decay `0.0125`, 4k warmup, cosine decay to `0.01Ã—` peak (Section 8).
  - Token budget: Models in each pair are trained on the same number of tokens, enabling parameterâ€‘only scaling analysis (Section 8; Equations 17â€“20).
  - Metrics: Negative logâ€‘likelihood (NLL) on benchmarks that probe reasoning/math/coding quality in preâ€‘training:
    - GSM8K (5â€‘shot NLL), MMLU, MMLUâ€‘pro, MBPP (Section 8).
  - Baseline: Identically sized dotâ€‘product Transformers.

- Latency/throughput evidence (Sections 6â€“7)
  - Window search: Table 1 shows perâ€‘sequence latency for combinations of `w1` and `w2`, e.g., `(w1=512, w2=32)` at ~55.1 ms for 16k context.
  - FLOPs and runtime vs FlashAttention v3: Figure 3 compares theoretical FLOPs and measured ms; the proposed kernel tracks FA v3 closely at longer sequences.

- Main quantitative results (Table 2)
  - NLL (lower is better) summary; 2â€‘simplicial vs Transformer:
    - 1B active: mixed results; slight degradations or ties:
      > GSM8K: 0.3302 vs 0.3277 (Î” +0.79%)  
      > MMLU: 0.6423 vs 0.6411 (Î” +0.19%)  
      > MMLUâ€‘pro: 0.8718 vs 0.8718 (Î” âˆ’0.01%)  
      > MBPP: 0.2714 vs 0.2690 (Î” +0.88%)
    - 2B active: consistent improvements:
      > GSM8K: 0.2942 vs 0.2987 (Î” âˆ’1.51%)  
      > MMLU: 0.5862 vs 0.5932 (Î” âˆ’1.19%)  
      > MMLUâ€‘pro: 0.8135 vs 0.8193 (Î” âˆ’0.71%)  
      > MBPP: 0.2411 vs 0.2435 (Î” âˆ’1.0%)
    - 3.5B active: larger gains, especially on reasoningâ€‘heavy sets:
      > GSM8K: 0.2718 vs 0.2781 (Î” âˆ’2.27%)  
      > MMLU: 0.5484 vs 0.5543 (Î” âˆ’1.06%)  
      > MMLUâ€‘pro: 0.7689 vs 0.7858 (Î” âˆ’2.15%)  
      > MBPP: 0.2193 vs 0.2203 (Î” âˆ’0.45%)

- Scaling analysis (Tables 3â€“4; Equations 17â€“20)
  - With tokens fixed, fit `L(N) â‰ˆ Eâ€² + A / N^Î±`. Rewriting as `âˆ’log L â‰ˆ Î± log N + Î²` (Equation 20) yields slope Î± and intercept Î².
  - Table 3 shows Î± increases for 2â€‘simplicial attention, e.g.:
    > GSM8K: Î± 0.1420 â†’ 0.1683 (+18.5%)  
    > MMLU: Î± 0.1256 â†’ 0.1364 (+8.5%)  
    > MMLUâ€‘pro: Î± 0.0901 â†’ 0.1083 (+20.2%)  
    > MBPP: Î± 0.1720 â†’ 0.1837 (+6.8%)
  - Goodnessâ€‘ofâ€‘fit is strong for both models (Table 4), with RÂ² mostly â‰¥ 0.997 and small residuals, indicating the threeâ€‘point fit is consistent.

- Do the experiments support the claims?
  - Evidence aligns with two main claims:
    - Tokenâ€‘efficiency: At fixed tokens, larger activeâ€‘parameter models with interleaved 2â€‘simplicial layers yield consistently lower NLL than dotâ€‘product baselines for 2B and 3.5B models (Table 2).
    - Changed exponent: The fitted Î± is higher across all four benchmarks (Table 3), implying more favorable parameter scaling at fixed tokens. This is exactly the desired property in dataâ€‘limited regimes.
  - Caveats:
    - Gains do not appear at the smallest 1B active scale (Table 2), suggesting a scale threshold for benefit.
    - Metrics are NLL rather than task accuracy; while NLL is a strong preâ€‘training indicator, it is not a full downstream evaluation.

- Ablations and robustness
  - Latency ablation across window choices is provided (Table 1).
  - No ablation on percentage of 2â€‘simplicial layers, window size vs quality, or comparison of determinantâ€‘based logits vs simple trilinear logits in training (Section 5 notes the simpler Equation 5 is used for backpropagation derivations).

## 6. Limitations and Trade-offs
- Computational structure and locality (Sections 6â€“7)
  - Even with windows, 2â€‘simplicial cost is `O(n w1 w2)` and requires sophisticated tiling to be fast; windows reduce global receptive field for these layers. The model mitigates this by interleaving with global attention layers, but the optimal ratio is unexplored.
  - The kernel is in Triton and optimized for prototyping; the paper notes it is â€œstill far away from being used in productionâ€ and would benefit from lowerâ€‘level implementations (Section 9).

- Theoryâ€“practice gap (Section 5; Appendix A)
  - The rotationâ€‘invariant determinant form and expressivity theorem are not the exact form used in the main experiments (which use the simpler trilinear Equation 5 for derivations/implementation). The incremental quality gained by determinant logits vs simple trilinear logits under RoPE is not empirically isolated.

- Scope of evaluation (Section 8)
  - Only MoE models are reported; behavior for dense transformers is not shown.
  - Training data composition and total token counts are not detailed, limiting reproducibility and interpretation of absolute NLLs.
  - Reported metrics are NLL on few reasoning/coding sets; broader downstream evaluations (accuracy, robustness, calibration, longâ€‘context tasks) are not included.

- Scalingâ€‘law estimation
  - Powerâ€‘law fits use only three model sizes; while RÂ² is high (Table 4), more points across a wider scale would solidify the exponent estimates.

## 7. Implications and Future Directions
- Field impact
  - Demonstrates that architectural changes can alter scaling exponents at fixed tokens for knowledge/reasoning tasks (Table 3), contradicting the prevailing view that most changes only shift the loss offset. This opens a path to tokenâ€‘efficient scaling when data is scarce (Sections 1 and 3).

- Practical applications
  - Preâ€‘training regimes constrained by data budgets (e.g., domainâ€‘specific corpora) may benefit from interleaving 2â€‘simplicial layers to reach better quality without proportionally more tokens.
  - Reasoningâ€‘heavy domains (math, code, logic) appear to benefit most (Table 2 shows the largest relative NLL gains on GSM8K and MMLUâ€‘pro at 3.5B active).

- Followâ€‘up research
  - Kernel and systems coâ€‘design: Implement 2â€‘simplicial kernels in CUTLASS or vendor libraries; explore fused scheduling across attention types; extend to other accelerators (Section 9).
  - Architectural ablations: Vary window sizes, proportion/order of 2â€‘simplicial layers, and GQA ratios; test dense models and larger scales.
  - Positional encoding study: Empirically compare determinantâ€‘based rotationâ€‘invariant logits vs simple trilinear logits with RoPE, and test other relative position encodings.
  - Generalizations: Explore kâ€‘simplicial attention (k>2) with sparse tiling patterns; combine with looped Transformers to trade depth for higherâ€‘order interactions.
  - Evaluation: Move beyond NLL to accuracy and robustness on a wide suite (reasoning chains, longâ€‘context tasks, code execution correctness).

> Core takeaways grounded in the paperâ€™s evidence:  
> - Equation (1) motivates the search for better token efficiency via architectures that can change the exponent Î±.  
> - Equations (5)â€“(7) define a tractable 2â€‘simplicial operator; Sections 6â€“7 turn it into a highâ€‘throughput kernel with `O(n w1 w2)` cost.  
> - Table 2 shows consistent NLL improvements at 2B and 3.5B active parameters on GSM8K, MMLU, MMLUâ€‘pro, and MBPP.  
> - Table 3 shows Î± increases of 6.8%â€“20.2% across benchmarks, with strong fits (Table 4), supporting the claim that 2â€‘simplicial attention improves the scaling exponent under fixed tokens.

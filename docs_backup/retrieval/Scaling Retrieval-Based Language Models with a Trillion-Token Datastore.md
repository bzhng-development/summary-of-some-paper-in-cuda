# Scaling Retrieval-Based Language Models with a Trillion-Token Datastore

**ArXiv:** [2407.12854](https://arxiv.org/abs/2407.12854)
**Authors:** Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, Pang Wei Koh
**Institutions:** University of Washington, Allen Institute for AI

## ğŸ¯ Pitch

This paper introduces a transformative third scaling dimension for large language models by integrating vast external retrieval datastores, specifically MASSIVEDS, enhancing their performance on knowledge-intensive tasks without the limitations of parameter scaling. By demonstrating how small models paired with extensive datastores can outperform larger, model-only counterparts, this approach promises a more efficient compute-performance trade-off, marking a significant leap in factuality and domain adaptation while optimizing resource allocation in AI development.

---

## 1. Executive Summary
This paper shows that a third scaling dimension for large language modelsâ€”how much external text a model can retrieve at inference timeâ€”improves performance steadily without saturation. It builds a 1.4-trillionâ€‘token, multiâ€‘domain retrieval datastore (`MASSIVEDS`) and an efficient, provably equivalent pipeline to study datastore size, demonstrating that small models plus large datastores can beat larger â€œLMâ€‘onlyâ€ models on knowledge-heavy tasks, and that retrieval achieves better computeâ€“performance tradeâ€‘offs than pretraining alone (Figures 1, 3, 4; Â§3â€“Â§4).

## 2. Context and Motivation
- Gap addressed
  - Most scaling laws optimize two dimensions: number of parameters and amount of pretraining data (e.g., Kaplan et al. 2020; Hoffmann et al. 2022). This work adds a third: the size of a retrieval `datastore` (the external corpus the model can search and copy into its context at inference). See Â§1 and Figure 1.
  - Prior retrieval systems usually used small, singleâ€‘domain stores (e.g., Wikipedia with a few billion tokens) and custom architectures (e.g., RETRO) with proprietary corpora, leaving open how retrieval scaling behaves with modern â€œretrieve-in-contextâ€ setups on diverse downstream tasks (Table 1; Â§2).

- Why it matters
  - Retrieval can improve factuality, domain adaptation, and reduce parametric memorization. Indexing a datastore is much cheaper than pretraining on the same text (Â§4.3; Appendix B.4), so adding a larger datastore may be a more computeâ€‘efficient way to add knowledge.

- Prior approaches and limitations
  - RETRO (1.7T tokens) evaluated language modeling with a proprietary datastore, but used small taskâ€‘specific stores for downstream evaluation (Table 1).
  - SPHERE (90B tokens; open) did not consistently beat smaller, in-domain stores on downstream tasks.
  - Many works focus on single domains or lack open resources for trillionâ€‘token retrieval (Â§2; Table 1).

- Positioning
  - This paper introduces the largest open retrieval datastore to date, `MASSIVEDS` (1.4T tokens across eight domains; Table 2; Â§3.1), and an efficiencyâ€‘oriented pipeline (Â§3.2; Figure 2) that makes trillionâ€‘token retrieval studies feasible and repeatable. It then analyzes scaling on language modeling and multiple downstream tasks (Â§4â€“Â§5), including computeâ€‘optimal curves (Figure 4).

## 3. Technical Approach
This is a retrieveâ€‘inâ€‘context approach (RICâ€‘LM): at inference, the system retrieves documents and concatenates them to the prompt (no model architecture changes). Key components:

- What the system is
  - `Datastore`: a very large corpus chunked into fixed-length passages that can be retrieved at inference time. MASSIVEDS includes general web and domainâ€‘specific sources (Table 2; Â§3.1).
  - `Retriever`: a model that maps text to vectors and finds nearest neighbors; here `CONTRIEVERâ€‘MSMARCO` (177M parameters) is used by default (Â§4.1; Appendix E.1).
  - `RICâ€‘LM`: a standard LM (e.g., `LLaMAâ€‘2/3`, `Pythia`, `OLMo`) that reads the concatenation of retrieved documents plus the task prompt (Â§4.1).

- Datastore composition (Table 2; Â§3.1)
  - 1.4T tokens (LLaMAâ€‘2 tokenizer), mixing:
    - General web: Common Crawl (2019â€“2023) and C4 (â‰ˆ1.19T tokens).
    - Domains: Books (26.3B), STEM (97.7B, including arXiv and peS2o), Encyclopedia (31.9B, incl. Wikipedia), StackExchange (20.2B), Code (52.8B), Math (14.1B), Biomedical (6.5B).

- Efficient scaling pipeline (Figure 2; Â§3.2; Appendix A)
  - Challenge: naively rebuilding indices for every datastore variant (size, seed, filters) is prohibitively expensive at trillion scale (Figure 2, top).
  - Strategy: do the most expensive steps once (indexing and initial retrieval), then apply experimental variants only to the much smaller set of retrieved candidates (Figure 2, bottom).
    1) Distributed indexing: split data into shards; embed every document once; store in a flat innerâ€‘product index (FAISS `IndexFlatIP`) (Appendix A.1).
    2) Distributed retrieval: for each query, get topâ€‘`K` candidates per shard, then merge by score (Appendix A.2â€“A.3). A lemma proves this is equivalent to retrieving over a single unsharded index (Lemma A.1).
    3) Postâ€‘hoc filtering over retrieved candidates only:
       - Deduplication (nearâ€‘duplicate removal) using 13â€‘gram Jaccard â‰¥80% (Appendix A.4.1; B.1).
       - Decontamination (to avoid test leakage) using 13â€‘gram Jaccard and/or longestâ€‘overlap thresholds; stricter for language modeling (Â§5.3; Figure 8; Appendix A.4.2; B.1).
       - Optional reranking with a stronger model (crossâ€‘encoder `MiniLMâ€‘L12â€‘v2`), or oracle lexical reranker for an upper bound (Â§5.2; Appendix A.4.3).
    4) Subsampling to simulate smaller datastores: sample each of the `K` candidates independently with probability `p`, then keep the topâ€‘`k` by original retrieval score (Appendix A.5). A lemma shows this is equivalentâ€”with very high probabilityâ€”to building a smaller datastore and rerunning retrieval (Lemma A.3). Failure probability (not enough remaining docs) is exponentially small in `K`; with `K=1000`, `k=3`, even `p=0.01` succeeds â‰¥0.997 (Table 4).
    5) Evaluation: concatenate final topâ€‘`k` docs (default `k=3`) before the few-shot prompt (Â§4.1; B.3).

  - Commutativity and correctness: the paper proves which operations commute and when the reâ€‘ordering is equivalent to the naive pipeline (Lemmas A.1â€“A.4; Proposition A.1).

- Retrieval and prompting details (Â§4.1; B.2â€“B.3)
  - Chunking at retrieval granularity: 256 words.
  - For language modeling perplexity, use 1,024â€‘token windows with a 512â€‘token stride; the first 512 tokens form the retrieval query; the next 512 tokens are the target (Appendix B.2).
  - For downstream tasks, prepend retrieved docs, then the fewâ€‘shot examples, then the question (Appendix B.3). Default `k=3`, reverse order so higherâ€‘ranked docs are closer to the question (Â§4.1).

- Compute accounting for scaling (Appendix B.4; Â§4.3)
  - FLOPs approximations:
    - Pretraining: `FLOPspretrain â‰ˆ 6 * N_LM * D_pretrain`.
    - Datastore construction (embedding): `FLOPsdatastore â‰ˆ 2 * N_retriever * D_datastore`.
  - Indexing with a flat index adds no extra construction FLOPs (Â§4.3).
  - Computeâ€‘optimal curves use intermediate checkpoints from `Pythia` and `OLMo` to approximate different `D_pretrain` (Â§4.3; Appendix B.4).

## 4. Key Insights and Innovations
- Datastore size is a real scaling axis with monotonic gains
  - Insight: Increasing the datastore size consistently lowers perplexity and boosts accuracy on knowledgeâ€‘intensive tasks, with no obvious saturation up to 1.4T tokens (Figure 3aâ€“f; Â§4.2).
  - Significance: A smaller LM + large datastore can outperform a larger LM without retrieval on tasks like TriviaQA and NQ (Figure 3câ€“d).

- An efficient, provably equivalent scaling pipeline
  - Innovation: Retrieve a large candidate set once (`K â‰« k`) and apply subsampling, deduplication, decontamination, and reranking only on those candidates. Theoretical results show equivalence to naive construction with high probability (Appendix A; Figure 2).
  - Impact: Reduces compute by more than an order of magnitude (Â§3.2), making trillionâ€‘token retrieval studies feasible and reproducible.

- Computeâ€‘optimal scaling with retrieval
  - Finding: At equal training compute, retrievalâ€‘augmented systems achieve better Paretoâ€‘optimal performance on downstream tasks than LMâ€‘only baselines (Figure 4; Â§4.3).
  - Reason: Indexing is much cheaper than pretraining the LM on the same data (Appendix B.4), so shifting â€œknowledge storageâ€ into the datastore is more computeâ€‘efficientâ€”provided the LM can use retrieved text (Â§4.3).

- Multiâ€‘domain datastore that generalizes across tasks
  - Innovation: `MASSIVEDS` covers eight domains (Table 2) and is openâ€‘sourced (abstract; Â§6). Experiments show it matches or outperforms singleâ€‘domain datastores across tasks (Table 3; Â§5.1).
  - Mechanism: The retriever automatically pulls from relevant subâ€‘domains; e.g., more Wikipedia for `Natural Questions`, more scientific papers for `MedQA` (Figure 5).

## 5. Experimental Analysis
- Setup overview (Â§4.1; B.2â€“B.3)
  - Models: `LLaMAâ€‘2 (7B, 13B)`, `LLaMAâ€‘3 (8B)`, `Pythia (1B, 2.8B, 6.9B, 12B)`, `OLMoâ€‘1.7 (1B, 7B)`.
  - Retriever: `CONTRIEVERâ€‘MSMARCO` by default; ablations with `DRAGON` and `GTRâ€‘Base` show similar performance (Appendix E.1, Table 6).
  - Datastore sizes simulated by subsampling probabilities `p = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]`, with three random seeds (Appendix B.1).

- Datasets and metrics (Â§4.1; B.2â€“B.3)
  - Language modeling: RedPajama (multiâ€‘domain web) and S2ORC (scientific papers); metric: perplexity (PPL).
  - Downstream:
    - `TriviaQA` and `Natural Questions`: exact match accuracy.
    - `MMLU` and `MedQA`: accuracy (multipleâ€‘choice).
    - All downstream tasks evaluated 5â€‘shot with retrieved docs prepended (Appendix B.3).

- Main quantitative results
  - Language modeling improves monotonically with datastore size (Figure 3aâ€“b; Â§4.2):
    - Example with `LLaMAâ€‘2 7B`: RedPajama PPL drops below the LMâ€‘only baseline; Table 3 reports 4.09 (LMâ€‘only) â†’ 3.50 with MASSIVEDS. On S2ORC: 7.18 â†’ 6.57 (Table 3).
    - Gains persist even after aggressive decontamination (Figure 7, left): removal of exact lexical overlap reduces the benefit but does not eliminate it, indicating semantic help from retrieved docs.
  - Knowledgeâ€‘intensive QA gains (Figure 3câ€“d):
    - `TriviaQA` with `LLaMAâ€‘2 7B`: LMâ€‘only 64.1% vs 77.0% with MASSIVEDS (Table 3).
    - `Natural Questions` with the same model: 26.6% â†’ 34.6% (Table 3).
    - Benefits grow with datastore size; small models with retrieval beat larger LMâ€‘only models (Figure 3câ€“d).
  - Reasoningâ€‘heavy tasks are mixed (Figure 3eâ€“f; Â§4.2):
    - `MMLU`: retrieval helps steadily but doesnâ€™t flip ordering (smaller models donâ€™t surpass larger ones). For `LLaMAâ€‘2 7B`, LMâ€‘only 45.8% â†’ 49.3% with MASSIVEDS (Table 3).
    - `MedQA`: limited gains and mostly for weaker models (Figure 3f; Table 3).
  - Computeâ€‘optimal curves (Figure 4; Â§4.3):
    - For `TriviaQA` and `NQ`, `Pythia` (â‰¤300B pretraining tokens) and `OLMo` (â‰¤2â€“3T) show similar Pareto trajectories when retrieval is usedâ€”suggesting simple factual extraction is learned early (Â§4.3, Finding 4).
    - For `MMLU/MedQA`, `OLMo` benefits from retrieval, `Pythia` mostly doesnâ€™t (right half of Figure 4; Â§4.3, Finding 5).

- Reranking and retrieval quality (Â§5.2; Figure 6)
  - Replacing no reranker with a crossâ€‘encoder boosts both `TriviaQA` and `NQ`, but a large gap remains to the â€œlexical oracleâ€ (which knows the answer and ranks by overlap). This indicates ample headroom from better retrievers/rerankers.

- Singleâ€‘ vs multiâ€‘domain datastores (Â§5.1; Table 3; Figure 5)
  - `MASSIVEDS` outperforms or matches singleâ€‘domain stores across tasks:
    - On `TriviaQA`, it beats Wikipedia and other domain stores (77.0% vs next best 72.9% with RedPajama web; Table 3).
    - On `MMLU`, `MASSIVEDS` ties the best singleâ€‘domain score (49.3% vs 48.3%â€“48.3% ranges in Table 3).
  - Domainâ€‘adaptive retrieval (Figure 5): Topâ€‘1 retrieved docs for `NQ` skew to Wikipedia; for `MedQA`, they skew to scientific papers, even though the underlying datastore is broad.

- Data filtering and decontamination (Â§5.3; Figure 7; Appendix E.2)
  - Decontamination strongly affects PPL but not NQ accuracy (Figure 7): suggests PPL gains partly stem from lexical overlaps, but retrieval still helps after strict filtering.
  - Global deduplication mitigates saturation on NQ as datastore grows (Appendix E.2, Figure 13e). Dolmaâ€‘style quality filters have small effect here (Figure 13c,f), likely because inputs were already filtered (Appendix E.2).

- Additional observations and ablations
  - Removing â€œshort chunksâ€ (<13 words) avoids unhelpful lexical matches and improves NQ at large scales (Appendix E.2; Figures 14â€“15).
  - LLaMAâ€‘3 8B shows worse PPL than LLaMAâ€‘2 7B on RedPajama (Figure 3a; Appendix D), plausibly due to domain mismatch or postâ€‘training that prioritizes instructionâ€‘following over PPL on this corpus.

- Do the experiments support the claims?
  - Yes for knowledge recall: strong, consistent, monotonic gains and superiority at equal compute (Figures 3â€“4; Table 3).
  - Mixed for reasoning-heavy tasks: benefits depend on LM capability and datastore domain coverage (Figure 4 right; Â§4.2; Â§6).

## 6. Limitations and Trade-offs
- Assumptions and dependence on components (Â§6; Â§5.2â€“Â§5.3)
  - The LM must be capable enough to use retrieved evidence (Figure 4 right); weaker models may not convert retrieval into reasoning gains.
  - Results hinge on retriever quality; notable headroom remains between crossâ€‘encoder reranking and the lexical oracle (Figure 6).
  - Postâ€‘hoc decontamination is applied to retrieved candidates; while provably equivalent to global decontamination under their setup (Lemma A.2), it relies on retrieving a large enough `K`.

- Coverage and data quality (Â§6)
  - `MASSIVEDS` is broad but may still lack the specialized content needed for some reasoning tasks (e.g., textbooks for `MMLU`, biomedical knowledge for `MedQA`; Â§4.2, Finding 5).

- Compute and latency tradeâ€‘offs (Â§4.3, â€œDiscussion on inference costâ€)
  - Training compute improves with retrieval, but inference can be costlier: longer prompts (retrieved docs) and retrieval latency. However, switching from a larger to a smaller LM partly offsets this.

- Methodological constraints (Â§4.3)
  - Computeâ€‘optimal curves use intermediate checkpoints; the training schedules are not reâ€‘tuned per token budget, so some points may be suboptimal (Appendix B.4).

- Reproducibility and scope
  - Full scaling with many retrievers would require reâ€‘indexing; this study primarily uses `CONTRIEVER` (Appendix E.1).
  - Downstream evaluations focus mainly on QA and shortâ€‘form answers (Â§6), not longâ€‘form generation or complex math proofs.

## 7. Implications and Future Directions
- How this changes the landscape
  - Retrieval datastore size should be treated as a firstâ€‘class scaling axis alongside parameters and pretraining tokens. For knowledgeâ€‘intensive tasks, investing compute in a large datastore plus a capable retriever can be more efficient than further pretraining (Figures 1, 4; Â§4.3).

- Practical applications
  - Domain adaptation without retraining: swap or extend datastores for new domains (legal, medical, code) while keeping the LM fixed.
  - Compliance and attribution: retrieval enables citing sources and controlling data provenance (highlighted in Â§1â€“Â§2).
  - Costâ€‘sensitive deployment: run smaller LMs with large datastores for competitive performance on factual QA.

- Research directions
  - Better retrievers and rerankers: the gap to the lexical oracle (Figure 6) suggests substantial unrealized gains.
  - Datastore curation: add targeted highâ€‘quality sources (e.g., textbooks, curated biomedical corpora) for reasoningâ€‘heavy tasks (Â§4.2, Finding 5).
  - Inference efficiency: optimize retrieval latency and context usage (e.g., compression, selective augmentation; see cited works in Â§4.1 and Â§4.3).
  - Endâ€‘toâ€‘end scaling studies: jointly choose LM size, pretraining tokens, and datastore scale to meet compute and latency budgets, and extend computeâ€‘optimal analysis to inference cost (Â§4.3).
  - Broader evaluation: longâ€‘form generation, factâ€‘checking with citations, and mathematical reasoning at scale (Â§6).

> Key takeaway (Figure 4; Â§4.3): For the same training compute, retrievalâ€‘augmented systems reach better Pareto points than LMâ€‘only models, and the retrieval scaling curve shows no sign of saturating within a 1.4Tâ€‘token datastore.

> Resource: MASSIVEDS (raw passages, embeddings, and index) and the full scaling pipeline are openâ€‘sourced: https://github.com/RulinShao/retrieval-scaling (abstract; Â§6).

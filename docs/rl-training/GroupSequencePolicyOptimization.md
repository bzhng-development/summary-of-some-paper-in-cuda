# Group Sequence Policy Optimization

**ArXiv:** [2507.18071](https://arxiv.org/abs/2507.18071)

## ğŸ¯ Pitch

Group Sequence Policy Optimization (GSPO) introduces a novel reinforcement learning algorithm that shifts from unstable token-level updates to robust sequence-level updates for tuning large language models. By aligning importance sampling to the actual unit of rewardâ€”entire sequencesâ€”GSPO dramatically stabilizes training, especially for Mixture-of-Experts models, and delivers improved training efficiency and performance over prior methods. This breakthrough resolves longstanding scalability bottlenecks in RL for LLMs, paving the way for reliably training much larger, more capable language models and accelerating advancement in complex reasoning tasks.

---

## 1. Executive Summary
Group Sequence Policy Optimization (GSPO) is a reinforcement learning (RL) algorithm for fineâ€‘tuning large language models that replaces tokenâ€‘level updates with sequenceâ€‘level updates. By defining importance weights on entire responses and clipping at the sequence level, GSPO stabilizes trainingâ€”especially for Mixtureâ€‘ofâ€‘Experts (MoE) modelsâ€”while improving sample efficiency and benchmark performance over GRPO (Group Relative Policy Optimization) (see Â§4.1â€“4.2, Â§5.1, Fig. 1).

## 2. Context and Motivation
- Problem addressed
  - Scaling RL for large language models (LLMs) requires stable, robust training when responses are long and models are huge or sparse (MoE) (Â§1). Existing strong baselines, notably GRPO, frequently become unstable and can catastrophically collapse (Â§1, Â§3).
  - Collapse here means model quality degrades sharply and cannot be restored by resuming from checkpoints or retuning hyperparameters. As Â§3 emphasizes: 
    > â€œWe have empirically observed that this can lead to model collapse that is often irreversible.â€
- Why it matters
  - RL is a key path to growing reasoning capabilities (competitionâ€‘level math, programming) by encouraging long, deep chains of thought (Â§1). If training is unstable, it blocks scaling these benefits to larger models and tasks.
- Prior approaches and their limitations
  - PPO (Proximal Policy Optimization): Uses a separate value network to estimate perâ€‘token advantages and clips tokenâ€‘level importance ratios (Eq. 1). In practice, the value model doubles memory/compute and is hard to make reliable for long responses (Â§2).
  - GRPO: Avoids a value model by using groupâ€‘relative advantages (normalize rewards within a set of G responses for one prompt) but still optimizes perâ€‘token using tokenâ€‘level importance ratios (Eqs. 2â€“3). Â§3 argues this misapplies importance samplingâ€”using a single nextâ€‘token sample per time step as if it corrects distribution mismatchâ€”creating highâ€‘variance gradients that accumulate over long sequences and are amplified by clipping.
- Positioning
  - GSPO reframes the optimization unit to match the reward unit: sequences. It defines importance weights on sequence likelihood and performs sequenceâ€‘level clipping (Eq. 5â€“7). This aligns with the core principle of importance sampling (Eq. 4) and stabilizes MoE RL without extra tricks (Â§4.1â€“4.2, Â§5.3).

## 3. Technical Approach
At a high level: for each prompt `x`, generate a group of `G` responses `yâ‚,â€¦,y_G` using the old policy `Ï€_Î¸_old`. Compute a scalar reward `r(x,y)` for each response using a verifier (in [0,1]). Normalize these rewards within the group to form advantages. Then, update the new policy `Ï€_Î¸` using a sequenceâ€‘level importance ratio with clipping.

Stepâ€‘byâ€‘step:
1. Grouped rollouts and rewards (Â§2, Â§4.1)
   - For each query `x`, sample `G` responses from the current dataâ€‘collection policy `Ï€_Î¸_old`.
   - Score each response with a reward model/verifier `r(x,y) âˆˆ [0,1]`.
   - Compute groupâ€‘relative advantage (Eq. 6):
     - Subtract the mean reward of the G responses and divide by their standard deviation.
     - Intuition: this focuses optimization on which responses are better within the group, avoiding a separate value estimator (as in GRPO).

2. Sequenceâ€‘level importance ratio with length normalization (Â§4.1; Eqs. 5â€“7)
   - Define sequence likelihood as the joint probability of all tokens in a response: `Ï€_Î¸(y|x) = âˆ_t Ï€_Î¸(y_t | x, y_<t)`.
   - Define the sequenceâ€‘level importance ratio for response `y_i`:
     - `s_i(Î¸) = (Ï€_Î¸(y_i|x) / Ï€_Î¸_old(y_i|x))^(1/|y_i|)`.
     - The exponent `1/|y_i|` is length normalization. Without it, a few token changes can cause large swings in the ratio, and different lengths would need different clipping ranges (Â§4.1).
   - Objective with sequenceâ€‘level clipping (Eq. 5):
     - For each `i`, use `min(s_i(Î¸)*A_i, clip(s_i(Î¸), 1-Îµ, 1+Îµ)*A_i)` and average over the group.
     - This clips entire responses, not tokens.

3. Why sequenceâ€‘level? The importance sampling principle (Eq. 4; Â§3)
   - Importance sampling estimates expectations under a target distribution by reweighting samples from a behavior distribution.
   - In language generation, the natural unit is the whole sequence, because the reward is given per sequence.
   - Tokenâ€‘level weighting in GRPO uses one sample per nextâ€‘token distributionâ€”too few for the ratio to reliably correct the mismatchâ€”injecting variance that accumulates across tokens (Â§3).

4. Gradient behavior and stability (Â§4.2)
   - GSPO gradient (Eq. 10): every tokenâ€™s logâ€‘prob gradient in a response gets the same weight `s_i(Î¸)*A_i/|y_i|`. This removes intraâ€‘sequence tokenâ€‘level weighting noise.
   - GRPO gradient (Eq. 12): token `t` is weighted by its own tokenâ€‘level importance ratio `w_{i,t}` which varies across tokens, leading to unequal weights that can accumulate unpredictably.
   - Consequence: GSPOâ€™s equal weighting per response reduces variance and avoids unstable training dynamics (Â§4.2).

5. Optional tokenâ€‘granular advantages: GSPOâ€‘token (Â§4.3; Eqs. 13â€“17)
   - When finer credit assignment is needed (e.g., multiâ€‘turn RL), GSPOâ€‘token allows perâ€‘token advantages `A_{i,t}` but keeps the sequenceâ€‘level importance ratio by â€œstopping the gradientâ€ through tokenâ€‘level probabilities:
     - `s_{i,t}(Î¸) = sg[s_i(Î¸)] * Ï€_Î¸(y_{i,t}|â€¦)/sg[Ï€_Î¸(y_{i,t}|â€¦)]`.
     - Numerically, `s_{i,t}(Î¸)` equals `s_i(Î¸)` for all tokens, so clipping/weights remain sequenceâ€‘level; gradients distribute across tokens according to `A_{i,t}` (Eq. 17).
   - If all `A_{i,t}` are equal to `A_i`, GSPOâ€‘token is identical to GSPO in value, clipping, and gradient (Â§4.3).

6. Practical training setup (Â§5.1)
   - Large rollout batches are split into miniâ€‘batches for efficiency, creating an offâ€‘policy gap between `Ï€_Î¸_old` (generator) and `Ï€_Î¸` (optimizer), hence the need for clipping (Â§3).
   - Example hyperparameters (for the headâ€‘toâ€‘head with GRPO):
     - GSPO clipping range: left 3eâ€‘4, right 4eâ€‘4 (Eq. 5).
     - GRPO clipping range: left 0.2, right 0.27 (Eq. 2).
     - Each rollout batch is split into four miniâ€‘batches (Â§5.1).
   - Note the magnitude difference in clipping ranges: a byâ€‘product of how ratios are defined (sequenceâ€‘ vs tokenâ€‘level), not a simple retuning (Â§4.1, Â§5.1).

7. Why GSPO helps MoE models (Â§5.3)
   - MoE instability under GRPO: after each gradient step, which experts the model routes tokens to can shift. With Qwen3â€‘30Bâ€‘A3Bâ€‘Base (48 layers), about 10% of experts change for the same sample across updates (Â§5.3). This makes tokenâ€‘level ratios `w_{i,t}` fluctuate dramatically.
   - Prior workaround: Routing Replayâ€”cache expert choices from `Ï€_Î¸_old` and force `Ï€_Î¸` to reuse them when computing ratiosâ€”adds memory/communication overhead and limits capacity (Â§5.3).
   - GSPO focuses on sequence likelihood, which is much less sensitive to perâ€‘token routing flips; it converges without Routing Replay (Fig. 1; Â§5.3).

8. Infrastructure simplification (Â§5.4)
   - Because GSPO uses sequenceâ€‘level likelihoods, it is more tolerant to numerical precision differences between training and inference engines. Â§5.4 notes it may be possible to use likelihoods returned by the inference engine directly, avoiding recomputation.

## 4. Key Insights and Innovations
- Importance ratios must match the reward unit (Â§3, Â§4.1)
  - Novelty: Define and clip importance ratios at the sequence level to align with sequenceâ€‘level rewards (Eqs. 5â€“7). This embodies the core importance sampling principle (Eq. 4) within LLM RL.
  - Significance: Removes a major source of variance and instability in GRPOâ€™s tokenâ€‘level weighting, especially for long sequences and MoE routing volatility (Â§4.2, Â§5.3).
- Lengthâ€‘normalized sequence ratios (Â§4.1)
  - Novelty: Raise the ratio to the power `1/|y|` to normalize for response length.
  - Significance: Prevents a few tokens from causing outsized ratio fluctuations and allows a single clipping range to work across lengths, lowering variance and operational complexity (Â§4.1).
- Sequenceâ€‘level clipping of entire responses (Â§4.1)
  - Novelty: Clip complete response updates instead of perâ€‘token updates.
  - Significance: Excludes overly offâ€‘policy sequences cleanly and consistently with how rewards are assigned, improving sample exploitation and stability (Fig. 2; Â§5.2).
- GSPOâ€‘token for fineâ€‘grained credit assignment with sequenceâ€‘level stability (Â§4.3)
  - Novelty: A stopâ€‘gradient construction that retains sequenceâ€‘level ratios while allowing tokenâ€‘wise advantages; provably reduces to GSPO when perâ€‘token advantages are uniform (Eqs. 13â€“17).
  - Significance: Extends GSPO to settings like multiâ€‘turn RL without sacrificing the core stability benefit.
- Stabilizing MoE RL without Routing Replay (Â§5.3)
  - Fundamental change in practice: 
    > â€œGSPO eliminates the dependency on Routing Replay and is fully capable of computing the importance ratios s_i(Î¸) conventionally, converging normally, and optimizing stably.â€ (Â§5.3; Fig. 1)
  - Significance: Simplifies infrastructure and removes capacityâ€‘limiting workarounds in large MoE training.

## 5. Experimental Analysis
- Setup (Â§5.1)
  - Model: Coldâ€‘start fineâ€‘tune from `Qwen3â€‘30Bâ€‘A3Bâ€‘Base`.
  - Training: Each rollout batch split into 4 miniâ€‘batches; GRPO requires Routing Replay to converge on MoE; GSPO does not.
  - Metrics and benchmarks:
    - Training reward (verifier score).
    - AIMEâ€™24: average Pass@1 over 32 samples.
    - LiveCodeBench (202410â€“202502): average Pass@1 over 8 samples.
    - CodeForces: Elo Rating.
  - Clipping ranges: GSPO 3eâ€‘4/4eâ€‘4 (left/right), GRPO 0.2/0.27 (Â§5.1).
- Main results (Fig. 1)
  - Training stability: GSPO shows smooth, steady improvement throughout training.
  - Efficiency: At similar compute and query budgets, GSPO achieves higher training reward and higher scores on AIMEâ€™24, LiveCodeBench, and CodeForces than GRPO (which is run with Routing Replay).
  - Scaling: GSPO continues to improve with more compute, periodic query refresh, and longer generations.
- Clipping behavior (Fig. 2; Â§5.2)
  - Empirical observation:
    > â€œWe observe a difference of two orders of magnitude in the fractions of clipped tokens between GSPO and GRPOâ€¦â€
  - Reported averages: GRPO â‰ˆ 0.0013 clipped fraction vs GSPO â‰ˆ 0.15.
  - Interpretation: Despite clipping far more tokens (because whole responses are clipped), GSPO trains more efficiently. This suggests GRPOâ€™s tokenâ€‘level gradients are noisy/inefficient, while GSPOâ€™s sequenceâ€‘level signal is cleaner (Â§5.2).
- MoE stability (Fig. 3; Â§5.3)
  - Without Routing Replay, GRPO fails to converge on MoE; with Routing Replay it converges but adds overhead and constrains capacity.
  - GSPO converges without Routing Replay and avoids the expertâ€‘activation volatility issue because it does not rely on perâ€‘token likelihood stability (Â§5.3).
- Broader deployment signal (Â§5.1, Â§6)
  - The method has been applied to train recent Qwen3 models, indicating practical readiness; however, detailed external benchmarks for those models are not enumerated here.
- Are the experiments convincing?
  - Strengths:
    - Headâ€‘toâ€‘head training curves across multiple benchmarks (Fig. 1) and infrastructure studies (Fig. 2â€“3) directly target the claimed failure modes: instability, sample efficiency, and MoE routing volatility.
    - Concrete hyperparameters for clipping show that GSPOâ€™s ratio scale differs materially from GRPO (Â§5.1), which aligns with the lengthâ€‘normalized sequence definition (Â§4.1).
  - Gaps:
    - Numerical summaries beyond plots are limited; exact gains are not tabulated.
    - Sensitivity to group size `G`, advantage normalization choice, and Îµ ranges is not ablated.
    - Comparisons are primarily to GRPO; PPO or other RLHF/RLAIF baselines are not included in this paperâ€™s experiments.
    - Details on the verifier(s) used for rewards are abstracted (only the [0,1] range is specified).

## 6. Limitations and Trade-offs
- Sequenceâ€‘level focus may blunt tokenâ€‘level credit assignment
  - GSPO addresses instability by equalizing token weights within a response (Eq. 10). This is ideal for sequenceâ€‘level rewards, but tasks that truly need precise temporal credit assignment could benefit from GSPOâ€‘token (Â§4.3). The paper does not present empirical results for GSPOâ€‘token.
- Heavy clipping of entire sequences
  - Fig. 2 shows GSPO discards a large fraction of tokens via sequence clipping (~0.15). While training remains efficient, this could translate to wasted generation compute; the paper argues the net effect is positive but does not quantify computeâ€‘efficiency tradeâ€‘offs.
- Limited ablations
  - No reported sensitivity studies on:
    - Group size `G` and the statistics used for advantage normalization.
    - Length normalization choice and its exponent.
    - KL regularization strength (omitted from equations â€œfor brevityâ€ Â§2, but often important in practice).
- Scope of evaluation
  - Experiments focus on a single MoE base model and three reasoning/coding benchmarks. Broader tasks (dialogue, safety RL, preference RL) are not covered in this paper.
- Theoretical coverage
  - While the gradient comparison is clear (Â§4.2), formal variance or convergence analyses are not provided; claims are primarily empirical.

## 7. Implications and Future Directions
- Impact on the field
  - GSPO reframes the de facto RL objective for LLMs from tokenâ€‘ to sequenceâ€‘levelâ€”all the way through importance weighting, clipping, and gradient aggregation. This is a fundamental shift that aligns the optimization unit with the reward unit (Â§3, Â§4.1).
  - For MoE models, removing Routing Replay simplifies training pipelines and unlocks true capacity usage (Â§5.3).
- Followâ€‘up research enabled
  - Algorithmic analysis:
    - Variance/bias characterization of lengthâ€‘normalized sequence ratios and sequenceâ€‘level clipping.
    - Adaptive clipping policies driven by measured offâ€‘policy distance.
  - Practical RL design:
    - Systematic studies of group size `G`, reward normalization strategies, and verifier design.
    - Extensions to multiâ€‘turn RL and tool use with GSPOâ€‘token, including perâ€‘turn advantages and partial rollouts.
  - Infrastructure:
    - Trainingâ€“inference disaggregation: directly using inferenceâ€‘engine likelihoods (Â§5.4) to reduce recomputation and memory.
    - Efficient handling of high clipping fractions (e.g., earlyâ€‘reject during generation when ratios exceed bounds).
- Applications
  - Stable largeâ€‘scale RL for math/coding assistants and longâ€‘reasoning agents.
  - Training large MoE models without routing constraints or custom replay mechanisms.
  - Industrial RL pipelines where inference engines differ numerically from training engines and recomputation is costly (Â§5.4).

> Overall, GSPOâ€™s central ideaâ€”sequenceâ€‘level importance weighting and clipping that matches how rewards are assignedâ€”addresses a core instability in stateâ€‘ofâ€‘theâ€‘art LLM RL. The experiments (Figs. 1â€“3; Â§5.1â€“5.3) show stability and efficiency gains, particularly for MoE, and point to simpler, more scalable RL infrastructure.

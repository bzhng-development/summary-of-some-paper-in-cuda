# RLHF Workflow: From Reward Modeling to Online RLHF

**ArXiv:** [2405.07863](https://arxiv.org/abs/2405.07863)
**Authors:** Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang
**Institutions:** Siebel School of Computing and Data Science, University of Illinois (inferred)

## ðŸŽ¯ Pitch

This paper introduces a reproducible workflow for online Reinforcement Learning from Human Feedback (RLHF) that replaces costly human labeling with a high-quality proxy preference model, enhancing the conversation quality of large language models without sacrificing reasoning or knowledge performance. By implementing a grounded exploration-exploitation scheme, it democratizes advanced RLHF techniques, allowing resource-constrained teams to achieve state-of-the-art results without extensive human input, thus significantly broadening access to cutting-edge AI alignment methods.

---

## 1. Executive Summary (2-3 sentences)
This paper delivers an endâ€‘toâ€‘end, reproducible workflow for online, iterative Reinforcement Learning from Human Feedback (RLHF) that replaces costly human raters with a highâ€‘quality proxy preference model and uses a theoretically grounded explorationâ€“exploitation scheme. Practically, the recipe improves an 8B-parameter LLMâ€™s conversation quality over strong baselines and keeps (or improves) scores on reasoning and knowledge benchmarks, while exposing concrete tactics to curb reward-driven verbosity (Sections 2â€“4; Tables 1â€“4; Figures 1, 3â€“4, 8â€“9).

## 2. Context and Motivation
- Problem addressed
  - Most open-source RLHF is still â€œofflineâ€: models are trained once on a fixed, pre-collected set of preferences and are not updated with new, on-policy data during training. This causes out-of-distribution (OOD) issues when the policy drifts far from the data distribution (Section 1.1).
  - Deep-RLâ€“based RLHF (e.g., PPO) is resourceâ€‘intensive and fragile to hyperparameters and implementation details, making it hard to reproduce in the open-source community (Section 1.1).
  - Online learning with live human feedback is ideal but expensive. The paper fills this gap by building a proxy preference model from diverse open datasets and using it to power an online, iterative RLHF loop (Sections 1.3 and 2).

- Why it matters
  - Real-world impact: Online preference learning has been key to performance in leading systems (e.g., Claude, LLaMAâ€‘2), but practical open recipes have been lacking. The paperâ€™s workflow enables resourceâ€‘constrained teams to reproduce the benefits of online RLHF without a human-labelling pipeline (Sections 1.2 and 1.3; Figure 1).
  - Theoretical significance: The paper connects iterative preference learning to a KLâ€‘regularized objective and provides a formal explorationâ€“exploitation formulation with finiteâ€‘sample guarantees (Equations 2â€“3; Algorithm 1; Theorem 1 in Section 3.2).

- Prior approaches and their shortcomings
  - PPO-style RLHF: powerful, but unstable and expensive; requires multiple large models in memory (actor, critic, reward, reference), tough for open projects (Section 1.1).
  - Offline direct preference learning (e.g., `DPO`): simpler and more stable than PPO but still limited by fixed datasets; struggles when the optimized policy diverges (large density ratios are reported during RLHF, indicating substantial distribution shift; Section 1.1).
  - Prior online efforts existed mainly in proprietary or DRLâ€‘heavy settings (Claude, LLaMAâ€‘2); open-source iterative DPO with clear guidance remained under-explored (Section 1.2).

- Positioning
  - The paper sits at the intersection of theory and practice:
    - It adopts the KLâ€‘regularized RLHF formulation with a Bradleyâ€“Terry (BT) preference model abstraction (Definitions 1â€“2; Equations 1â€“3).
    - It proposes a practical online direct-preference workflow with an exploration â€œenhancerâ€ and a high-quality, open preference model to approximate human raters (Algorithms 1â€“2; Sections 2â€“3).
    - It provides a full, replicable recipeâ€”data curation, reward/preference modeling, on-policy sampling, and iterative DPO trainingâ€”plus ablations on reward length bias (Sections 2â€“4; Table 4; Figure 3).

## 3. Technical Approach
The workflow has three major parts: (A) build a strong proxy preference model; (B) run an online, iterative preference-learning loop with exploration; (C) apply practical engineering choices that make the loop efficient and stable.

A. Build the proxy preference model (Section 2)
- Terminology
  - `Preference oracle`: a function that, given a prompt `x` and two responses `a1, a2`, returns which one is preferred (Definition 1).
  - `Bradleyâ€“Terry (BT) model`: a canonical model for pairwise preferences where the probability that `a1` is preferred over `a2` depends on a latent scalar reward `r*(x,a)` via a sigmoid of the reward difference (Definition 2; Equation 1).
- Two model families are constructed on top of `LLaMAâ€‘3â€‘8B-Instruct`:
  1) BT reward model: replaces the final layer with a scalar â€œreward headâ€ and trains by maximum likelihood on pairwise comparisons (negative log-sigmoid of reward differences), i.e.,
     > `LRM(Î¸) = âˆ’ E_{(x,aw,al)} log Ïƒ(rÎ¸(x,aw) âˆ’ rÎ¸(x,al))` (Section 2.1).
     - Training config: 1 epoch, global batch 512, lr `2e-6`, cosine schedule, warmup `0.03` (Section 2.1).
  2) Pairwise preference model (â€œLLM-as-classifierâ€): formats `(x, a1, a2)` as an instruction with label `A` or `B`, fine-tuned to predict the preferred side (Section 2.1).
     - Inference computes `pA/(pA+pB)` as the preference probability.
     - Training config: packed blocks of length `3072`, global batch `128`, lr `5e-6`, cosine lr, warmup `0.03`, 1 epoch.
- Data mixtures (Table 5; Section B.1)
  - `mix1`: HH-RLHF + SHP + UltraFeedback + Summarization.
  - `mix2` (larger and more diverse): adds safety, math, and coding preferences (e.g., UltraInteract, CodeUltraFeedback). Filtering removes noisy samples and low-margin pairs (Section B.1).
- Why two model types?
  - BT rewards are efficient for ranking many candidate responses (`O(n)` scoring).
  - Pairwise models can better capture complex preferences in reasoning tasks (Table 1 shows stronger â€œReasoningâ€ accuracy).

B. Iterative online preference optimization (Section 3)
- Objective background
  - The RLHF target balances reward and staying close to the initial SFT policy `Ï€0`:
    > `J(Ï€) = E_{x ~ d0} [ E_{a ~ Ï€} r*(x,a) âˆ’ Î· D_KL(Ï€(Â·|x) || Ï€0(Â·|x)) ]` (Equation 2),
    with an intractable closed-form solution that reweights `Ï€0` by the exponentiated reward (Equation 3).
- Core idea: Learn from on-policy preferences with exploration (Algorithm 1, Figure 1).
  - At each iteration `t`:
    1) Main agent policy `Ï€1_t` is the exploitation choiceâ€”best under the current reward estimate (Equation 7).
    2) Enhancer policy `Ï€2_t` is the exploration choiceâ€”chosen to maximize an uncertainty measure `Î“` relative to `Ï€1_t`, while keeping a moderate KL divergence from `Ï€1_t` (Equation 8).
    3) Collect `m` new preference data points by sampling `(a1, a2)` from `(Ï€1_t, Ï€2_t)` on prompts `x ~ d0`, then query preferences from the proxy model (Algorithm 1, Step 5).
  - Intuition: The enhancer perturbation seeks high-uncertainty response regions so each batch contributes new information; the KL constraint prevents degenerate exploration (Section 3.2).
  - Theorem (informal): With suitable `m` and hyperparameters, after `OÌƒ(d_e)` iterations the KLâ€‘regularized value of some iterate approaches the optimum to accuracy `Îµ`, where `d_e` is the complexity of the function class (Theorem 1; Section 3.2).

C. Practical recipe (Section 3.3; Algorithm 2; Figure 4)
- Replace the theoretical oracle with stable, lowâ€‘overhead direct preference optimization:
  - Train `Ï€_t` using `DPO` on all data so far (`Doff âˆª D1:tâˆ’1`), with `Ï€0` as the fixed reference (Algorithm 2, Step 3).
  - Why DPO? It optimizes a surrogate derived from the KLâ€‘regularized objective (Equation 5), is stable, and avoids multi-model PPO overhead (Section 1.1).
- Exploration without computing uncertainty explicitly:
  - Use ensemble-style variation through sampling and selection:
    - For each prompt, sample multiple responses and rank them using the proxy reward. Construct a training pair from the best and the worst candidatesâ€”this stretches the â€œpreference margin,â€ promoting informative learning signals (Algorithm 2, Steps 4â€“5; Figure 4).
    - Temperature mixing: sample half the candidates with temperature `1.0` and half with `0.7` to diversify candidates (Algorithm 2, Step 4).
    - Best-ofâ€‘`n` / worst-ofâ€‘`n`: in practice the paper uses this as `Ï€1_t` (best) and `Ï€2_t` (worst) induced by `Ï€_MLE_t`, which encourages large, informative differences while keeping policies related (Section 3.3).
- Training and generation specifics:
  - DPO details: 2 epochs per iteration, global batch 128, cosine schedule with peak lr `5e-7`, warmup `0.03`, KL coefficient `Î·=0.1`; they warmâ€‘start each iteration from the previous model while keeping `Ï€0` as the reference (Section 3.3).
  - Data generation: 60k prompts total, 3 iterations with 20k prompts each; candidates are generated with vLLM, max generation length 2048, temperatures 1.0 and 0.7 (Section 3.3). Note: Appendix mentions â€œ20K Ã— 16 responses per iteration,â€ while Section 3.3 also describes best-ofâ€‘8/worst-ofâ€‘8â€”this indicates `n` between 8 and 16 depending on the iteration or run configuration.

Clarifying uncommon terms used:
- `DPO` (Direct Preference Optimization): an algorithm that trains a policy directly on pairwise preferences by maximizing the log odds that preferred responses are more likely than dispreferred ones under the policy relative to a fixed reference (`Ï€0`), effectively baking in KL regularization (Equation 5, Section 1.1).
- `Rejection sampling` / `best-ofâ€‘n`: generate `n` samples and choose the one with the highest reward; here extended to also use the worst candidate to form a strong contrast (Section 3.3).
- `Preference model`: a classifier over pairs (`x, a1, a2`) that predicts which `a` is preferred; different from a scalar reward model (Section 2.1).

## 4. Key Insights and Innovations
1) A reproducible online iterative preference-learning workflow that avoids PPO (Sections 3.2â€“3.3; Algorithm 2; Figure 4)
- Whatâ€™s new: Moves the â€œiterativeâ€ part of RLHF into a DPO-based pipeline with an explicit exploration mechanism via best/worst-ofâ€‘`n` sampling and temperature mixing, rather than relying on PPO.
- Why it matters: Delivers stability and lower compute/memory footprint while retaining the well-known benefits of online data collection (Section 1.2), making the approach usable by open-source teams.

2) A strong, open proxy for human feedback using diverse datasets and two modeling strategies (Section 2; Table 1; Table 5)
- Whatâ€™s new: Trains both a scalar BT reward model and a pairwise preference model from a carefully filtered and diverse set of open preference datasets (â€œmix2â€: safety, math, coding included; Table 5).
- Why it matters: The proxy is accurate on RewardBench and supports on-policy learning without human labellers. The pairwise preference model notably excels on reasoning (â€œReasoningâ€ accuracy 94.7 vs 86.4 for BT; Table 1).

3) Theoretical framing with an â€œenhancerâ€ policy and finiteâ€‘sample guarantees (Section 3.2; Algorithm 1; Theorem 1)
- Whatâ€™s new: The main agent exploits the current reward estimate, while the enhancer explores under a KLâ€‘bounded uncertainty criterion, bringing classical exploration ideas into preference optimization.
- Why it matters: Gives conceptual clarity and a path to principled exploration beyond simple heuristics. Although uncertainty is approximated pragmatically, the framework justifies exploration choices.

4) Diagnosing and mitigating length bias in reward models and iterative RLHF (Figure 3; Table 4)
- Whatâ€™s new: Direct analysis of rewardâ€“length correlations (Figure 3) and a simple lengthâ€‘penalized reward `re(x,a)= rÌ‚(x,a) âˆ’ Î»|a|` for filtering (Equation 9).
- Why it matters: RLHF often amplifies verbosity; the length penalty improves lengthâ€‘controlled win rate on AlpacaEvalâ€‘2 (from 31.3 to 38.1) and shortens responses substantially (average length: 656 â†’ 382 characters; Table 4), while maintaining or improving several academic metrics.

## 5. Experimental Analysis
- Evaluation design (Sections 2 and 4; Appendix B.2; Tables 1â€“4)
  - Preference model quality: RewardBench across â€œChat,â€ â€œChat-Hard,â€ â€œSafety,â€ and â€œReasoningâ€ (Table 1).
  - Conversational ability: AlpacaEvalâ€‘2 (length-control win rate), MTâ€‘Bench (GPTâ€‘judged score), Chatâ€‘Arenaâ€‘Hard (win rate vs GPTâ€‘4 judge) (Table 2; Appendix B.2).
  - Reasoning and knowledge: GSMâ€‘8K (math), MMLU (general knowledge), HumanEval/MBPP (code), TruthfulQA (truthfulness), ARC (reasoning) (Table 3; Appendix B.2).
  - Iterative dynamics: progress over three iterations (Figure 8).
  - Reward bias: rewardâ€“length correlation heatmaps (Figure 3).
  - Ablations: length penalty and impact of different reward models (UltraRMâ€‘13B vs theirs) (Table 4).

- Key quantitative findings
  - Proxy preference model strength (Table 1):
    > With `mix2`, the BT reward model reaches â€œChat 99.4, Chatâ€‘Hard 65.1, Safety 87.8, Reasoning 86.4,â€ and the pairwise preference model reaches â€œChat 98.3, Chatâ€‘Hard 65.8, Safety 89.7, Reasoning 94.7.â€
    - Takeaway: the pairwise model is notably better on reasoning tasks, justifying its use when reasoning quality matters.
  - Online iterative RLHF vs baselines (Table 2):
    > The 8B model with iterative DPO scores â€œLC AlpacaEvalâ€‘2 31.3, MTâ€‘Bench 8.46, Chatâ€‘Arenaâ€‘Hard 29.1.â€
    - Comparisons:
      - vs its own DPO baseline (offline): â€œ22.5, 8.17, 22.4â€ â‡’ consistent gains across all three.
      - vs LLaMAâ€‘3â€‘8Bâ€‘Instruct: â€œ22.9, 8.16, 20.6â€ â‡’ substantial improvements.
      - vs several 7Bâ€“45B open models: competitive or superior on conversational metrics; smaller than LLaMAâ€‘3â€‘70Bâ€‘Instruct (as expected) but much better than GPTâ€‘3.5â€‘turbo in Chatâ€‘Arenaâ€‘Hard (29.1 vs 18.9).
  - Academic benchmarks (Table 3):
    > Iterative DPO: â€œGSMâ€‘8K 80.7, MMLU 65.3, HumanEval 64.6, TruthfulQA 60.4, ARC 64.3, MBPP 60.8.â€
    - Outcome: no major alignment tax; in several cases, it improves over the SFT baseline (e.g., GSMâ€‘8K 80.7 vs 74.2; TruthfulQA 60.4 vs 53.4).
  - Iteration-by-iteration gains (Figure 8):
    > Steady increases across iterations for MTâ€‘Bench and both AlpacaEvalâ€‘2 variants; Chatâ€‘Arenaâ€‘Hard also increases monotonically.
  - Length bias and mitigation (Figure 3; Table 4):
    > Rewardâ€“length correlation: UltraRMâ€‘13B shows stronger positive correlation (mean â‰ˆ 0.19) than the paperâ€™s BT reward (mean â‰ˆ 0.06) (Figure 3).
    > With length penalty `Î»=0.001`, LC AlpacaEvalâ€‘2 improves from 31.3 â†’ 38.1, and average response length drops from 656 â†’ 382 characters (Table 4). Some trade-offs appear (Chatâ€‘Arenaâ€‘Hard declines).

- Do the experiments support the claims?
  - Yes, for three reasons:
    - The online loop provides consistent, acrossâ€‘theâ€‘board improvements in conversational metrics relative to the same model trained offline with DPO (Table 2; Figure 8).
    - The proxy preference model is demonstrably competent, especially on reasoning, supporting its role as an effective labeler (Table 1).
    - The lengthâ€‘bias analysis identifies a real failure mode and shows a practical mitigation that improves a lengthâ€‘controlled metric without eroding academic performance (Figure 3; Table 4).

- Robustness and ablations
  - Reward model choice matters: training with UltraRMâ€‘13B leads to longer outputs and generally worse academic scores than the paperâ€™s BT reward; this aligns with UltraRMâ€™s stronger length bias and weaker reasoning accuracy (Table 1 vs Table 4).
  - Length penalty helps where verbosity is penalized (LC AlpacaEvalâ€‘2), but can hurt benchmarks favoring detailed answers (Chatâ€‘Arenaâ€‘Hard) (Table 4).
  - The paper also cautions that benchmark scores can be sensitive to evaluation configuration and warns against over-interpreting leaderboard wins (Remark 1, Section 4.2).

## 6. Limitations and Trade-offs
- Reliance on a proxy preference model
  - Assumption: proxy preferences approximate human preferences well enough to guide online learning (Section 1.3; Table 1).
  - Risk: biases in the proxy (e.g., verbosity, safety strictness, reasoning blind spots) can shape the final policy. The length-bias analysis (Figure 3) shows this is a real concern; mitigation requires careful tuning (Table 4).

- Exploration is heuristic in practice
  - The theoretical uncertaintyâ€‘guided enhancer (Equation 8) is approximated by best/worstâ€‘ofâ€‘`n` sampling with temperature mixing (Section 3.3). This is effective and simple but not the same as optimizing a principled uncertainty measure.

- Data generation cost
  - Online loops require generating many candidates per prompt and ranking them, which is inferenceâ€‘heavy even without PPOâ€™s training overhead (Section 3.3). The paper mitigates this with vLLM and modest `n`, but scaling to larger models or more iterations increases cost.

- Some configuration ambiguity
  - The number of samples per prompt appears as `n=8` in Section 3.3, while Appendix mentions â€œ20K Ã— 16 responses per iteration.â€ This does not undermine results but suggests that exact `n` varied across runs; reproducibility is still strong given shared code and details.

- Benchmark generalization and â€œalignment taxâ€
  - Although academic metrics do not degrade overall (Table 3), the paper acknowledges risks of benchmark overfitting and configuration sensitivity for automatic judges (Remark 1, Section 4.2).

## 7. Implications and Future Directions
- How this changes the field
  - Provides a credible, open, and efficient alternative to PPOâ€‘based RLHF for achieving the benefits of online, onâ€‘policy preference learning (Algorithms 1â€“2; Section 3.3).
  - Demonstrates that a wellâ€‘trained proxy preference model, built from diverse open datasets, can meaningfully replace human raters for iterative alignment without collapsing reasoning performance (Sections 2â€“4; Tables 1â€“3).

- Enabled followâ€‘up research
  - Better proxies and multiâ€‘objective reward heads: explore multiâ€‘head rewards (e.g., helpfulness, honesty, safety, conciseness) and dynamic activation policies (Section 5).
  - Principled exploration: replace best/worstâ€‘ofâ€‘`n` with explicit uncertainty estimation or optimistic objectives adapted from RL theory (Section 3.2; related works cited in Section 3.3).
  - Controlling verbosity beyond penalties: integrate length- or brevity-aware objectives into DPO variants, or postâ€‘training re-ranking with calibrated judges (Table 4; Figure 3).
  - Reasoningâ€‘aware aligners: couple the preference model with program-of-thought or verifier signals for math/code, given the strong gains of the pairwise preference model on reasoning (Table 1).

- Practical applications
  - Open-source assistants: iterative improvement of chatbots with minimal labeling budgets.
  - Domain customization: plug domainâ€‘specific preference datasets (e.g., legal, medical, coding) into the proxy model, then run the online loop to adapt style and safety constraints.
  - Continual alignment: periodically reâ€‘run the online iterations as prompts and usage evolve, giving a maintainable path to keep models aligned over time.

> Bottom line: By (i) training a strong open preference model, (ii) instantiating an online explorationâ€“exploitation loop with DPO, and (iii) engineering a reproducible recipe that diagnoses and reduces rewardâ€‘length bias, this paper makes online RLHF both attainable and effective for the broader community (Figures 1, 3â€“4, 8; Tables 1â€“4).

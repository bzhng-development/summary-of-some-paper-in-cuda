# MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning

**ArXiv:** [2409.20566](https://arxiv.org/abs/2409.20566)

## ğŸ¯ Pitch

MM1.5 introduces a new family of multimodal large language models (MLLMs) that achieve breakthroughs in text-rich image understanding, visual referring and grounding, and multi-image reasoningâ€”three historically challenging areas for vision-language models. The paper's core innovation is a rigorous, data-centric three-stage training regimen, including high-resolution continual pre-training and dynamic image splitting, which allows even small models (1Bâ€“3B) to excel at fine-grained, grounded, and multi-image tasks. These advances unlock new practical capabilities such as robust document and UI comprehension, precise visual grounding, and scalable video and multi-image analysis, positioning MM1.5 as a versatile foundation for the next generation of multimodal AI applications.

---

## 1. Executive Summary (2â€“3 sentences)
MM1.5 is a family of multimodal large language models (MLLMs) that significantly improves three hard capabilities in visionâ€“language systems: understanding textâ€‘rich images (OCRâ€‘heavy), visual referring/grounding (point/box-conditioned reasoning and boxâ€‘grounded output), and multiâ€‘image reasoning. It achieves these through a dataâ€‘centric threeâ€‘stage training recipe (largeâ€‘scale preâ€‘training, highâ€‘resolution continual preâ€‘training, and carefully balanced supervised fineâ€‘tuning), plus a dynamic highâ€‘resolution image encoding method (â€œAnyResâ€) and explicit coordinate handling; the approach scales from 1B to 30B parameters, with dense and mixtureâ€‘ofâ€‘experts (MoE) variants, and is further specialized into video and UI models (Sections 1, 3â€“4; Figs. 1â€“2).

## 2. Context and Motivation
- Problem/gap addressed
  - Existing MLLMs often struggle with three underdeveloped but practically crucial competencies:
    - Reading and reasoning over highâ€‘resolution, textâ€‘rich images (documents, charts, UI) where small text and layout matter.
    - Referring to specific regions in an image and grounding responses with coordinates (boxes/points) instead of loose natural language references.
    - Multiâ€‘image reasoning and multimodal inâ€‘context learning (ICL) at inference time.
  - Many open models focus mainly on supervised instruction tuning (SFT) and single images, which limits generalization and fineâ€‘grained grounding (Section 2).
- Why it matters
  - Real applications like document understanding, mobile UI agents, and multiâ€‘image reasoning (e.g., comparison, retrievalâ€‘augmented vision, or video via frame sampling) need precise reading, fineâ€‘grained spatial reasoning, and the ability to handle more than one image (Sections 1â€“2).
- Prior approaches and their limits
  - Highâ€‘resolution comprehension: previous â€œstatic tilingâ€ (e.g., fixed 2Ã—2 grid) wastes tokens or pads empty areas for unusual aspect ratios; other works do not fully study design choices (Section 3.5).
  - Visual referring/grounding: strong proprietary systems (e.g., GPTâ€‘4o) often rely on â€œsetâ€‘ofâ€‘mark (SoM) promptingâ€ (a separate markup to denote regions) rather than native coordinate tokens; most open models lack robust, integrated grounding (Section 2).
  - Training recipes: many open approaches emphasize SFT alone; the impact of preâ€‘training data mix, highâ€‘resolution continual preâ€‘training, or SFT category composition on crossâ€‘capabilities remains underâ€‘explored (Sections 1, 3.2â€“3.4).
- Positioning relative to the field
  - MM1.5 keeps the MM1 architecture (Section 3.1) to isolate the benefits of data and training strategies. It contributes a thorough empirical study and a matured recipe that delivers strong performance at small scales (1B/3B) and scales to 30B, with new, specialized video and UI variants (Sections 1, 4â€“6).

## 3. Technical Approach
This section decodes the full pipeline from model design to training and inference.

- Base architecture (Section 3.1; Fig. 1)
  - Vision encoder: a CLIPâ€‘style image encoder.
  - LLM: the same decoderâ€‘only language backbone as MM1.
  - Connector: `C-Abstractor` to map visual features to the LLM space.
  - Inputs/outputs:
    - Supports single or multiple images.
    - Accepts visual prompts (points and bounding boxes) through â€œcoordinate tokens.â€
    - Can output grounded responses by emitting bounding boxes inside text.
  - For all ablations, the 3B dense model is used unless noted (Section 3.1).

- Dynamic highâ€‘resolution image encoding (â€œAnyResâ€) (Sections 3.5â€“3.5.1; Fig. 11; Tables 1â€“3)
  - Problem: fixed 2Ã—2 tiling (static splitting) is inefficientâ€”small images are oversplit and long/tall images waste tiles with padding.
  - Solution: dynamic image splitting selects a grid `(n_h, n_w)` on a perâ€‘image basis:
    - Consider all grids whose number of tiles lies in `[n_min, n_max]` (e.g., up to 10 tiles).
    - If a grid covers the image without downscaling, choose the one that minimizes padding after longerâ€‘side resizing.
    - Otherwise choose the grid that minimizes resolution loss due to downscaling (Eq. (1), Fig. 11).
  - Globalâ€‘local format: always include a lowâ€‘resolution overview image in addition to tiles (â€œglobal viewâ€), placed after subâ€‘images so that, under the autoregressive mask, the overview can attend to all tiles (Table 3 row 4 vs. row 1).
  - Subâ€‘image position indicators (Table 3): tested two schemesâ€”`index` (triplet `(k, i, j)` for image ID and tile row/col) and `seps` (text separators like â€œ:â€, â€œ,â€, â€œ<n>â€ between image tokens). These are optional; average gains are small, but index tokens help grounding slightly.

- Threeâ€‘stage training recipe (Fig. 2; Sections 3.2â€“3.4 and 4)
  1) Largeâ€‘scale preâ€‘training
     - Data (Section 4): 
       - 2B imageâ€“text pairs (captioningâ€‘like).
       - 600M interleaved imageâ€“text documents (1B images total; sequences that mix images and text).
       - 2T textâ€‘only tokens (â€œHQâ€‘Text,â€ a curated higherâ€‘quality mix emphasizing general knowledge, math, and code; Section 3.4).
     - Crucial change vs. MM1: adjust the data ratio from `45:45:10` (image:interleaved:text) to `50:10:40` (Section 3.4; Fig. 10). This substantially downweights interleaved and upweights text, improving downstream knowledge and textâ€‘rich tasks after SFT.
     - Optimization (Section 4): 200k steps, sequence length 4096, same schedule as MM1.
  2) Highâ€‘resolution continual preâ€‘training (Section 3.3; Fig. 9)
     - Goal: before SFT, inject strong OCR/textâ€‘rich skills at high pixel density.
     - Data: 45M OCRâ€‘style examples (PDFA, IDL, RenderedText, DocStructâ€‘4M), sampled equally per batch.
     - Input resolution: highâ€‘res is criticalâ€”`1344Ã—1344` performs best; using `378Ã—378` can underperform skipping this stage entirely (Fig. 9a).
     - Synthetic captions: public synthetic caption sets (ShareGPT4Vâ€‘PT, LLaVAâ€‘Recapâ€‘3M) did not beat OCRâ€‘only continual preâ€‘training in this setup (Fig. 9b). However, an inâ€‘house 7M selfâ€‘training caption set shows consistent gains (Appendix A.1, Fig. 13).
     - Optimization: batch size 256, AdaFactor, peak LR `1e-5`, cosine decay, 30k steps.
  3) Supervised fineâ€‘tuning (SFT) with balanced mixtures (Section 3.2; Fig. 4)
     - Data is categorized to target capabilities: `general`, `text-rich`, `refer&ground`, `science`, `math`, `code`, plus `multi-image` and `text-only` (Section 3.1 â€œSFT data categorizationâ€, Fig. 4; Appendix A.2 Table 13).
     - Mixing ratios are chosen via extensive ablations (Sections 3.2.1â€“3.2.2; Figs. 5â€“8):
       - Within singleâ€‘image data, use the `general + textâ€‘rich` mix as a strong base, then add others using perâ€‘category ratios Î± relative to `general` per training batch:
         - `Î±_science = 0.1`, `Î±_math = 0.5`, `Î±_code = 0.2`.
         - `Î±_ref&ground = 2.0` to substantially boost grounding accuracy, accepting a small drop to other averages (Fig. 6d).
       - Across groups: set sampling weights `(w_single, w_multi, w_text) = (0.8, 0.1, 0.1)` (Fig. 7).
       - The final â€œAll Mixtureâ€ balances best overall capability (Fig. 8).
     - Optimization: batch size 256, LR `2e-5`, 23k steps, 1 epoch.

- Final dynamic resolution settings (Section 4; Section 3.5.1)
  - Training: `(n_min, n_max) = (4, 9)`, encoder resolution `672Ã—672`, `144` tokens per subâ€‘image, overview after tiles; dynamic splitting enabled only if a sample has fewer than 3 images.
  - Inference: can increase `n_max` (e.g., to 16) for higher effective resolution without retraining (Table 2 rows 1â†’3, 5â€“6).

- Mixtureâ€‘ofâ€‘Experts language backbones (Section 4 â€œMixtureâ€‘ofâ€‘Experts (MoE)â€)
  - Replace dense FFN layers in the LLM with `64` experts (every two layers), `topâ€‘2` gating, balance loss `0.01`, router zâ€‘loss `0.001`. Vision stack unchanged. MoE yields stronger multiâ€‘capability integration at 1B/3B parameter scales (Tables 4â€“9).

- Specialized variants (Sections 5â€“6)
  - `MM1.5-Video`: treat videos as multiâ€‘image inputs by uniformly sampling `N=24` frames, `144` tokens per frame, dynamic splitting disabled per frame due to token budget. Both trainingâ€‘free (reuse MM1.5 image model) and SFT versions (mix of ShareGPTVideo, VideoChat2, ActivityNetâ€‘QA) are built (Section 5; Table 10â€“11).
  - `MM1.5-UI`: further SFT on Ferretâ€‘UI mixture (801k samples) to target mobile UI understanding and interaction; retains coordinate grounding (Section 6; Fig. 12; Table 12).

## 4. Key Insights and Innovations
- Dataâ€‘centric, stageâ€‘wise recipe that measurably transfers to downstream capabilities
  - Innovation: explicit ablations that link data choices to specific capability gains across stages. Examples:
    - Highâ€‘res continual preâ€‘training on OCR data is pivotalâ€”`1344Ã—1344` outperforms lower resolutions and even beats skipping the stage (Fig. 9a).
    - SFT category ratios (`Î±` for science/math/code/ref&ground) and group weights `(w_single, w_multi, w_text)` are tuned to balance capabilities (Figs. 6â€“8).
    - Changing preâ€‘training mix from `45:45:10` to `50:10:40` and swapping to `HQâ€‘Text` improves textâ€‘rich (+0.85) and knowledge (+0.99) averages with a minor multiâ€‘image tradeâ€‘off (Fig. 10).
  - Significance: turns a common intuitionâ€”â€œdata quality and balance matterâ€â€”into a clear, reproducible recipe with quantified tradeâ€‘offs.

- Dynamic highâ€‘resolution image splitting with globalâ€‘local fusion
  - Whatâ€™s new: a principled grid selection (Eq. (1), Fig. 11) that minimizes padding or downscale loss, plus an overview image placed after tiles for better attention flow (Table 3).
  - Why it matters: improves textâ€‘rich benchmarks (DocVQA, InfoVQA) and adapts to unusual aspect ratios (Table 2). With 10 tiles at `672Ã—672` and `144` tokens per tile, textâ€‘rich and general averages improve (Table 1, row 7).

- Native visual referring and grounding with coordinate tokens
  - Difference from prior models: avoids SoM prompting and instead encodes/decodes boxes/points directly in text (Section 3.1; Fig. 1). This yields strong grounding scores (Table 7), while still preserving general abilities.

- Strong smallâ€‘scale models and MoE integration
  - Contribution: 1B/3B dense and MoE variants that, under the above recipe, outperform or match larger open baselines across many benchmarks (Tables 4â€“9). The 3Bâ€‘MoE rivals or surpasses 7B dense on several categories, showing MoE is an effective way to pack diverse capabilities at constant activated parameters.

- Unified path to video/UI without architecture changes
  - Insight: treating video frames as â€œmultiâ€‘imageâ€ is effective even trainingâ€‘free; further SFT yields SOTAâ€‘level results on public video QA and stateâ€‘ofâ€‘theâ€‘art on UI elementary tasks (Tables 10â€“12; Fig. 12).

## 5. Experimental Analysis
- Evaluation setup (Sections 4.1 and Appendix A.4; Table 14)
  - Benchmarks grouped by capability: `general`, `text-rich`, `knowledge`, `refer&ground`, `multi-image`, and `VL-ICL`.
  - Metrics vary by benchmark (e.g., accuracy, ANLS for document QA, Recall@IoU>0.5, GPTâ€‘assisted scores); zeroâ€‘shot and greedy decoding by default. Some competitors use beam search (Table 4 note).

- Main quantitative results
  - Across sizes, MM1.5 improves substantially over MM1 in nearly all categories:
    - Example at 30B (Tables 5â€“9):
      - MathVista +16.2 points (39.4â†’55.6), DocVQA +15.6 (75.8â†’91.4), InfoVQA +20.0 (47.3â†’67.3), MuirBench +21.5 (36.7â†’58.2).
  - Smallâ€‘scale leadership (Table 4):
    - At 1B, `MM1.5-1B` surpasses contemporaries like SPHINXâ€‘Tiny, DeepSeekâ€‘VL, TinyLLaVA on most reported benchmarks. Quote:
      > On DocVQA(test), `MM1.5-1B` reaches 81.0% vs. 70.0% for LLaVAOneVisionâ€‘0.5B and 53.0% for SPHINXâ€‘Tiny (Table 6).
  - 3B dense vs. popular 3â€“4B models (Tables 4â€“6, 8â€“9):
    - `MM1.5-3B` beats MiniCPMâ€‘V2 on many tasks, e.g., MathVista 44.4 vs. 38.7, DocVQA 87.7 vs. 71.9 (Table 6), and offers native grounding (Table 7).
    - Against Phiâ€‘3â€‘Visionâ€‘4B, `MM1.5-3B` lags on some knowledge tasks (AI2D/MMMU) but wins on textâ€‘rich (DocVQA 87.7 vs. 83.3; InfoVQA 58.5 vs. 49.0), grounding (Table 7), and multimodal ICL (56.3 vs. 19.5; Table 8).  
  - MoE gains (Tables 4â€“9):
    - `MM1.5-3B-MoE` often surpasses `MM1.5-7B` in knowledge/general/grounding/multiâ€‘image, trading a slight drop on textâ€‘rich.  
      > For example, VLâ€‘ICL average is 59.6 for 3Bâ€‘MoE vs. 56.0 for 7B dense (Table 8).
  - Referring & Grounding (Table 7):
    - `MM1.5-3B` achieves RefCOCO average 85.6, Flickr30k 85.9, and LVISâ€‘Ref 67.9; comparable to or better than many larger models; `MM1.5-30B` reaches LVISâ€‘Ref 84.9/61.4 (box/point) and Ferretâ€‘Bench 77.1.
  - Multiâ€‘image & ICL (Tables 8â€“9):
    - `MM1.5-30B` attains NLVR2 90.6 and MuirBench 58.2; in VLâ€‘ICL, `MM1.5-30B` reaches 77.6, exceeding several open competitors and approaching GPTâ€‘4V on some subtasks (Table 8).

- Ablations and what they prove
  - SFT category effects (Section 3.2.1; Fig. 5):
    - Adding `text-rich` boosts both textâ€‘rich and knowledge averages.
    - `Science` helps knowledge; `code` mildly helps textâ€‘rich; `refer&ground` is necessary for grounding but slightly regresses other categoriesâ€”hence the tuned ratio `Î±_ref&ground = 2.0`.
  - SFT mixing ratios (Section 3.2.2; Figs. 6â€“8):
    - `w_text=0.1` has minor effect on core capabilities but helps language generalization.
    - `w_multi=0.1` meaningfully lifts multiâ€‘image scores while slightly reducing base capabilities. The â€œAll Mixtureâ€ maximizes overall average (Fig. 8).
  - Continual preâ€‘training resolution (Section 3.3; Fig. 9a):
    - `1344Ã—1344` clearly best; `378Ã—378` can be worse than no continual preâ€‘training.
  - Continual preâ€‘training data (Fig. 9b; Appendix A.1):
    - OCRâ€‘only is strong; public synthetic captions did not add gains in this setup, but 7M highâ€‘quality selfâ€‘generated captions do scale improvements (Fig. 13).
  - Preâ€‘training mix (Section 3.4; Fig. 10):
    - Switching to HQâ€‘Text and `50:10:40` further increases textâ€‘rich (+0.85), knowledge (+0.99), and grounding (+~1.4), with a small multiâ€‘image decrease (âˆ’0.05).
  - Dynamic splitting (Section 3.5.1; Tables 1â€“3):
    - More tiles and higher perâ€‘tile resolution help textâ€‘rich (Table 1).
    - Increasing `n_max` particularly helps DocVQA/InfoVQA (Table 2). Training with higher `n_max` is better than only increasing it at inference.
    - Placing overview after tiles yields a small but consistent gain (Table 3).

- Specialized variants
  - Video (Section 5; Tables 10â€“11):
    - Trainingâ€‘free: `MM1.5-Video-3B` already surpasses several 7B trainingâ€‘free baselines on multipleâ€‘choice QA (e.g., NExTQA 72.8 vs. SlowFastâ€‘LLaVAâ€‘7B at 64.2; Table 10).
    - With video SFT: `MM1.5-Video-7B` achieves ActivityNetâ€‘QA 60.9 and top/runnerâ€‘up results across diverse datasets; on LLaVAâ€‘Hound, it is SOTA among reported entries (Table 11).
  - UI (Section 6; Table 12):
    - Even `MM1.5-UI-1B` surpasses the 13B Ferretâ€‘UI baseline on all four elementary UI tasks (Refâ€‘i/A and Grdâ€‘i/A), e.g., Refâ€‘i 90.0 vs. 80.5 and Grdâ€‘i 86.5 vs. 79.4.

- Do the experiments support the claims?
  - Yes: the combination of broad SOTAâ€‘level scores (Tables 4â€“9), targeted ablations (Figs. 5â€“10; Tables 1â€“3), and transfer to video/UI (Tables 10â€“12) directly tie the proposed recipe and AnyRes design to the reported capability gains. The paper also reports when tradeâ€‘offs occur (e.g., refer&ground slightly lowering other averages, reduced interleaved preâ€‘training lowering multiâ€‘image a bit).

## 6. Limitations and Trade-offs
- Data and compute intensity
  - Large preâ€‘training corpora (2B imageâ€“text, 600M interleaved, 2T text) and highâ€‘resolution continual preâ€‘training (45M OCR) are expensive (Sections 3.3â€“4). Dynamic splitting increases vision tokens for textâ€‘rich inputs (Table 1).
- Mixture sensitivity and tuning overhead
  - Capability balance depends on careful ratio tuning. For instance, raising multiâ€‘image SFT weight improves multiâ€‘image benchmarks but reduces the â€œbaseâ€ capability average (Fig. 7 right).
- Interleaved preâ€‘training tradeâ€‘off
  - Lowering interleaved proportion from 45% to 10% slightly drops multiâ€‘image performance (âˆ’0.05 average) even though textâ€‘rich and knowledge improve (Fig. 10).
- Grounding and coordinate transformations
  - Inferenceâ€‘time changes to `n_min` can hurt grounding due to mismatched localâ†’global coordinate conversion (Table 2 row 7), which constrains â€œjust scale up at inferenceâ€ tactics.
- Video framing constraints
  - Video variant disables dynamic splitting per frame and uses a fixed 24â€‘frame, 144â€‘token budget to fit context (Section 5). This may limit very long videos or fine temporal granularity.
- Generalization scope
  - Despite breadth, there remain untested areas (e.g., 3D, depth, fineâ€‘grained pixelâ€‘level segmentation). Some reported benchmarks are GPTâ€‘assessed (e.g., MMâ€‘Vet, LLaVAâ€‘Hound), which can introduce evaluation variance.

## 7. Implications and Future Directions
- Field impact
  - The work shifts attention from architecture novelty to a reproducible, dataâ€‘centric training recipe that demonstrably transfers to core multimodal abilities and smallâ€‘scale models. It elevates integrated grounding and multiâ€‘image reasoning to firstâ€‘class citizens in generalist MLLMs (Sections 1, 4).
- Followâ€‘up research enabled
  - Systematic study of highâ€‘quality synthetic captions at scale (Appendix A.1 shows early promise).
  - Joint optimization of interleaved preâ€‘training and dynamic splitting for longâ€‘context, multiâ€‘image chains (e.g., videos or multiâ€‘page documents).
  - Unifying image, video, and UI capabilities under one training schedule, possibly with curriculum over `n_max`, frame counts, and UIâ€‘specific tasks (Section 7, conclusion).
  - More robust grounding with consistent coordinate systems under varying tiling, and extensions to pixelâ€‘level grounding.
- Practical applications
  - Document understanding and enterprise OCR (charts, forms, infographics; Table 6).
  - Grounded assistants that can refer to and act on UI elements (Section 6; Fig. 12; Table 12).
  - Multiâ€‘image analytics (e.g., product comparisons, surveillance snapshots) and video QA/analysis pipelines that can start trainingâ€‘free, then gain with targeted SFT (Section 5; Tables 10â€“11).
  - Edge deployment scenarios: 1B/3B dense and MoE variants provide competitive capability for mobile and embedded settings (Tables 4â€“9).

In short, MM1.5 demonstrates that careful, highâ€‘resolution continual preâ€‘training plus principled SFT mixture designâ€”and a wellâ€‘engineered AnyRes pipelineâ€”unlock substantial, verifiable gains in OCRâ€‘heavy understanding, native grounding, and multiâ€‘image reasoning, from mobile scales to 30B models, with straightforward extension to video and UI domains.

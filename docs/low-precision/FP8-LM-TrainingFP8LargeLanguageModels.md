# FP8-LM: Training FP8 Large Language Models

**ArXiv:** [2310.18313](https://arxiv.org/abs/2310.18313)

## ðŸŽ¯ Pitch

This paper presents FP8-LM, the first end-to-end framework for training large language models (LLMs) entirely in 8-bit floating point precisionâ€”not just compute, but also optimizer states and inter-GPU communication. By innovatively addressing the numerical instabilities of FP8, this approach delivers up to 39% lower memory usage and 75% faster training versus standard BF16 methods, all while preserving model qualityâ€”making truly massive, cost-efficient LLMs accessible on contemporary GPU hardware.

---

## 1. Executive Summary
This paper introduces an end-to-end FP8 (8â€‘bit floating point) mixedâ€‘precision framework that trains large language models (LLMs) using 8â€‘bit numbers not only for computation but also for gradient communication and optimizer states. It achieves large savings in memory and communication while matching the accuracy of BF16 (16â€‘bit) training, and it accelerates training substantiallyâ€”up to 75% faster than BF16 for a 175Bâ€‘parameter GPT model on H100 GPUs (Table 5), with no changes to typical hyperparameters.

## 2. Context and Motivation
- Problem addressed
  - Training LLMs is extremely costly in compute, memory, and inter-GPU communication. Examples cited include GPTâ€‘3 (175B) and PaLM (540B) requiring thousands of accelerators and large energy footprints (Section 1).
  - Prior mixedâ€‘precision schemes (FP16/FP32, BF16/FP32) reduce cost but still leave significant savings unrealized because gradients, optimizer states, and communications remain high precision in mainstream systems (Section 1, 2).

- Why it matters
  - Reducing precision from 16/32â€‘bit to FP8 in the full training loop can theoretically yield 2Ã— compute speedup and 50â€“75% savings in memory and communication (Abstract; Appendix A.1). This enables training larger models or longer contexts on the same hardware (Figure 1; Section 3.2.2).

- Prior approaches and gaps
  - FP16/FP32 mixed precision suffered instability for very large models due to FP16â€™s limited dynamic range; BF16/FP32 became the standard for LLMs because BF16 has FP32â€‘like range (Section 2).
  - With Nvidia H100, FP8 became practical; however, Nvidiaâ€™s Transformer Engine (TE) uses FP8 mainly for matrix multiplications (GEMMs) while keeping gradients, master weights, and optimizer states in higher precision, so endâ€‘toâ€‘end gains are modest (Section 1; 2).

- Positioning
  - This work targets a full FP8 training stack. It designs:
    - FP8 gradients and FP8 allâ€‘reduce communication with automatic scaling (Section 2.1; Eqs. 1â€“6).
    - An FP8â€‘aware optimizer via precision decouplingâ€”8â€‘bit where safe, 16/32â€‘bit where needed (Section 2.2; Eqs. 7â€“8; Table 6).
    - FP8â€‘compatible distributed parallelism (tensor, pipeline, sequence) and a ZeROâ€‘style sharding that respects FP8 scaling (Section 2.3; Figure 2â€“3; Algorithm 1).
  - It demonstrates parity in model accuracy with BF16 for pretraining, SFT, and RLHF, while materially reducing memory and boosting speed (Figures 4â€“6; Tables 2â€“5).

## 3. Technical Approach
The framework integrates FP8 across compute, storage, and communication. Below, â€œFP8â€ refers to two standardized subâ€‘formatsâ€”E4M3 and E5M2â€”with different tradeoffs of range and precision (Appendix A.1). Because FP8 has a much narrower dynamic range and fewer mantissa bits than BF16/FP32 (Table 9), the framework relies on careful scaling and selective highâ€‘precision placement.

1) FP8 gradients and FP8 allâ€‘reduce communication (Section 2.1)
- Background definitions
  - `All-reduce`: a distributed operation that sums corresponding elements of tensors across GPUs and makes the sum available to all GPUs.
  - `Underflow/overflow`: values too small/large to be represented in a chosen format; with FP8 this is common if data is not scaled.
  - `Tensor scaling`: multiplying a tensor by a scalar so its values fall within the representable range; in FP8 one typically stores the scaled tensor in FP8 and the scale as a separate factor (Appendix A.2).

- The problem
  - Two standard ways to average gradients across N GPUs:
    - Pre-scaling: divide each local gradient `g_i` by `N` before summing: g = g1/N + ... + gN/N (Eq. 1). This causes underflow in FP8 when N is large.
    - Post-scaling: sum first and divide later: g = (g1 + ... + gN)/N (Eq. 2). This risks overflow during the sum.
  - NCCL (the common communication library) does not natively handle perâ€‘tensor scaling factors during allâ€‘reduce.

- The solution: automatic scaling + shared scaling per gradient tensor
  - Autoâ€‘scaling factor `Î¼` (Eq. 3): before quantizing to FP8, multiply gradients by a dynamic factor. At each step, measure how many values saturate to FP8â€™s max. If more than 0.001% saturate, halve `Î¼` next step to avoid overflow; if saturation stays below threshold, gradually double `Î¼` over ~1000 steps to reduce underflow.
  - Shared scaling across GPUs (Eqs. 4â€“6):
    - Each GPU has a local FP8 gradient `g_i'` with its local scale `s_i'`. GPUs first all-gather the perâ€‘tensor scale values (only scalars), compute the global minimum scale `s_g' = min(s_1', ..., s_N')` (Eq. 4), and reâ€‘quantize their gradients to use this shared scale before allâ€‘reduce (Eq. 5).
    - After summation, set the new scale to `s = N * s_g'` (Eq. 6), which is equivalent to averaging the unscaled gradients. This lets standard NCCL handle the allâ€‘reduce (no custom perâ€‘element scale handling), while communicating only one scalar per gradient tensor.

- Why it works
  - Using the global minimum scale avoids overflow across GPUs and keeps all shards on a common quantization grid, enabling a conventional allâ€‘reduce.
  - Autoâ€‘scaling balances the underflow/overflow tradeoff over time without manual tuning.

2) FP8 optimizer with precision decoupling (Section 2.2)
- Background definitions
  - `AdamW` keeps, per parameter, a highâ€‘precision copy of the parameter (`master weight`), the gradient, and two moving averages: the firstâ€‘order moment (like a running average of gradients) and the secondâ€‘order moment (running average of squared gradients).
  - In mainstream mixed precision, these states are typically FP32 for stability, costing 16 bytes/parameter (Eq. 7).

- Design choices and rationale
  - Keep what is precisionâ€‘sensitive highâ€‘precision; reduce where safe:
    - Master weights: FP16 with tensor scaling (or FP32) to reliably apply very small updates (Section 2.2). FP8 here degrades training (ablation in Table 6 & Figure 8).
    - Gradients: FP8 (already handled by the communication scheme in 2.1).
    - First moment (m): FP8 with scalingâ€”its direction matters more than its exact magnitude; tolerates quantization noise.
    - Second moment (v): FP16â€”squaring small gradients is vulnerable to underflow in FP8 and harms stability/accuracy (Figure 8; FP8 for v diverges).
  - This configuration uses 6 bytes per parameter (Eq. 8), a 2.6Ã— reduction vs. the 16 bytes/param baseline (Eq. 7).

3) FP8â€‘compatible distributed training (Section 2.3)
- Background definitions
  - `Tensor parallelism`: split large matrix multiplications within a layer across GPUs.
  - `Pipeline parallelism`: split layers across GPUs.
  - `Sequence parallelism`: split the sequence length dimension across GPUs to save activation memory.
  - `ZeRO`: shard optimizer states and gradients across devices to eliminate redundancy.

- FP8 integration
  - Tensor + sequence parallelism with FP8 activations and weights (Figure 2):
    - Convert activations and sharded weights to FP8 at the GEMM boundaries, so forward, backward, and gradient communication run in FP8.
    - Introduce FP8 conversion before the gather/reduce operator `g` that bridges sequence and tensor parallel regions (Figure 2), cutting activation allâ€‘gather/reduceâ€‘scatter communication roughly by a third (Table 7).
  - FP8â€‘aware ZeRO sharding (Figure 3; Algorithm 1):
    - Challenge: FP8 uses perâ€‘tensor scaling factors; if a tensor is split into chunks, managing scales per chunk complicates communication and correctness.
    - Solution: do not split tensors; instead, greedily assign whole tensors (FP8 array + its scale) to GPUs based on remaining memory (Algorithm 1). This keeps scale/tensor together, simplifies communication, and still balances memory (Table 8).

4) Experimental setup at a glance (Section 3.1; Table 1; Appendix A.3)
- Models: GPTâ€‘style decoderâ€‘only Transformers, sizes 125M, 7B, 13B, 175B; use RoPE positional embeddings and FlashAttention for efficiency.
- Data: A mixture of CommonCrawl, The Pile components, Wikipedia, code, books, etc. (Table 10; Appendix A.3). Instruction tuning uses ShareGPT; RLHF uses Anthropic Helpful/Harmless + OpenAssistant (Section 3.1.1).
- Hardware: Nvidia H100 80GB on Azure NDv5 (Section 3.1.2).
- Training settings: Cosine LR schedule; standard AdamW hyperparameters; sequence length 2048; batch sizes in Table 1. For 175B, they cap to 40B tokens (Table 1) to study system behavior while saving compute/emissions.

## 4. Key Insights and Innovations
1) Endâ€‘toâ€‘end FP8 beyond compute: gradients, communication, and optimizer (Sections 2.1â€“2.3)
- Whatâ€™s new vs prior FP8 (e.g., Transformer Engine)?
  - TE uses FP8 mainly within GEMM compute but retains FP16/FP32 for gradients, master weights, and communication. This work converts those remaining bottlenecks to FP8 where safe, with targeted highâ€‘precision where necessary (master weights and second moments).
- Why it matters
  - Produces substantial endâ€‘toâ€‘end gains: 28â€“39% memory reduction vs BF16 for GPTâ€‘7B/13B/175B (Table 5), 63â€“65% reduction in weightâ€‘related communication (Table 5), and up to 75% throughput gain vs BF16 on a 175B model (Table 5).

2) Automatic gradient scaling for FP8 allâ€‘reduce (Section 2.1; Eqs. 1â€“6; Figure 7)
- Novelty
  - A simple, hardwareâ€‘friendly mechanism that uses a perâ€‘tensor shared scale and a dynamic factor `Î¼`, avoiding NCCL changes and large overhead while keeping overflow/underflow in check.
- Evidence
  - Across Transformer blocks, autoâ€‘scaling lowers both underflow and overflow compared to preâ€‘ or postâ€‘scaling alone and improves signalâ€‘toâ€‘noise ratio (Figure 7aâ€“c).

3) Precision decoupling in the optimizer (Section 2.2; Table 6; Figure 8)
- Novelty
  - First moment in FP8 + second moment in FP16 + FP16 master weights (with scaling) strikes the right balance of stability and memory savings.
- Evidence
  - Ablations on GPTâ€‘125M show:
    - FP8 master weights degrade (compare â€œFP8 #3â€ with â€œFP8 #2a/#2bâ€ in Figure 8).
    - FP8 second moment diverges (the â€œFP8 #4â€ curve in Figure 8).
    - FP16 master weights with scaling match FP32/BF16 baselines (Figure 8 and Table 6).

4) FP8â€‘aware distributed parallelism and ZeRO (Section 2.3; Figure 2â€“3; Table 7â€“8)
- Distinguishing features
  - FP8 conversions are placed precisely at sequence/tensor parallel boundaries to reduce activation communication without altering model semantics (Figure 2).
  - Wholeâ€‘tensor FP8 ZeRO sharding preserves scale factors cleanly (Figure 3), improving memory balance and reducing peak usage (Table 8).
- Impact
  - 34% reduction in activationâ€‘related communication for GPTâ€‘13B and GPTâ€‘175B (Table 7).
  - Lower and more balanced memory use across GPUs (Table 8).

These are more than incremental tweaks: they are the missing glue that makes FP8 viable across the entire training stack.

## 5. Experimental Analysis
- Evaluation design
  - Accuracy: Compare FP8 vs BF16 pretraining loss for GPTâ€‘7B/13B/175B (Figure 4); evaluate zeroâ€‘shot downstream performance (Table 2). For SFT and RLHF, compare losses and standard alignment benchmarks (Figures 5â€“6; Tables 3â€“4).
  - Systems: Measure GPU memory, throughput, TFLOPS, MFU, and communication volumes under different parallelism settings and microâ€‘batches (Table 5). Also quantify activation communication (Table 7) and memory balancing in ZeRO (Table 8).
  - Baselines: BF16 training (Megatronâ€‘LM style) and Nvidia TEâ€™s FP8 compute mode (Table 5).

- Main quantitative results
  - Accuracy parity
    - Pretraining loss curves for FP8 and BF16 overlap across scales (Figure 4aâ€“c), with no hyperparameter changes.
    - Zeroâ€‘shot downstream accuracy is comparable. Example (Table 2, averages for 7B/13B):
      - GPTâ€‘7B: 58.4 (BF16) vs 58.0 (FP8).
      - GPTâ€‘13B: 61.0 (BF16) vs 60.4 (FP8).
    - SFT (Table 3): similar quality on AlpacaEval (66.15 vs 67.20 winâ€‘rate vs Davinciâ€‘003) and MTâ€‘Bench (5.75 vs 5.70), with FP8 using 14% less GPU memory and 27% higher throughput.
    - RLHF (Table 4): comparable AlpacaEval (72.05 vs 72.42) and MTâ€‘Bench (6.16 vs 6.04), while reducing weights memory by 32% and optimizer states by 62%.
  - System gains
    - Weightâ€‘related communication volume cut by 63â€“65% across model sizes (Table 5; â€œWeightâ€‘related Comm. Volumeâ€).
    - Memory savings versus BF16:
      - GPTâ€‘7B: 69.6 GB â†’ 49.4 GB (âˆ’29%); GPTâ€‘13B: 68.2 GB â†’ 48.9 GB (âˆ’28%); GPTâ€‘175B (microâ€‘batch 1): 66.1 GB â†’ 40.3 GB (âˆ’39%) (Table 5).
    - Speed
      - GPTâ€‘175B on 32 H100s: FP8 reaches 39.3 samples/s at microâ€‘batch 4 vs 22.4 for BF16 (+75%); and 39.3 vs 28.7 for TE (+37%) (Table 5).
      - Model FLOPs Utilization (MFU) improves to 34.2% for 175B with FP8 microâ€‘batch 4, outperforming TEâ€™s 24.9% (Table 5).

- Ablations and robustness checks
  - Gradient scaling strategies (Figure 7): autoâ€‘scaling improves SNR while controlling under/overflow compared to pre/postâ€‘scaling.
  - Optimizer precision (Table 6; Figure 8): highâ€‘precision for master weights and the second moment is necessary; otherwise training degrades or diverges.
  - FP8 parallelism reduces activation communication by about oneâ€‘third (Table 7).
  - FP8 ZeRO keeps memory well balanced (Table 8).

- Do the experiments support the claims?
  - Yes for system efficiency and shortâ€‘toâ€‘midâ€‘horizon training stability/accuracy. Parity is shown across multiple scales for losses and task metrics (Figure 4; Table 2) and across tuning scenarios (Tables 3â€“4).
  - Caveat: the 175B experiment is restricted to 40B tokens for cost reasons (Table 1), so very longâ€‘horizon stability (e.g., at trillionâ€‘token scales) is not directly demonstrated.

## 6. Limitations and Trade-offs
- Dependence on hardware and libraries
  - FP8 training assumes access to H100â€‘class GPUs that natively support FP8 (Appendix A.1). Earlier hardware will not realize these gains.
  - NCCL does not support perâ€‘tensor scaleâ€‘aware allâ€‘reduce; the method circumvents this via a shared scalar and reâ€‘quantization (Section 2.1). This is elegant but adds steps and depends on heuristic thresholds (e.g., 0.001% saturation).

- Precision choices are problemâ€‘specific
  - The chosen precision split (FP8 for gradients and first moment; FP16 for second moment and master weights) works well for the tested GPT setups (Section 2.2; Table 6), but other architectures/optimizers might require reâ€‘tuning.

- Training horizon and coverage
  - The 175B model is trained on 40B tokens (Table 1). While sufficient for system benchmarks and earlyâ€‘phase stability, it does not fully demonstrate endâ€‘toâ€‘end convergence behavior at massive token counts.

- Communication scaling nuances
  - Using the global minimum scale across GPUs (Eq. 4) guarantees safety but may be conservative if one shard has atypically large values, potentially increasing quantization noise elsewhere. Figure 7â€™s SNR results suggest the tradeâ€‘off is acceptable, but behavior under extreme heterogeneity is not deeply explored.

- ZeRO tensorâ€‘asâ€‘aâ€‘whole sharding
  - Keeping tensors whole simplifies scaling but constrains placement granularity. Algorithm 1â€™s greedy allocator (Section 2.3) balances memory well in reported setups (Table 8), yet more complex cluster topologies might require enhanced placement strategies.

## 7. Implications and Future Directions
- How this changes the landscape
  - Demonstrates that FP8 can be used safely across the entire LLM training stackâ€”not just compute kernelsâ€”without accuracy loss and with substantial efficiency gains (Figures 4â€“6; Tables 2â€“5). This can lower the barrier to training larger models and/or longer contexts on fixed budgets (Section 3.2.2 and Figure 1).

- What it enables next
  - Systems research: integrate tensorâ€‘scaleâ€‘aware primitives directly into collective communication libraries (future NCCL enhancements) to remove remaining overheads.
  - Algorithmic research: explore adaptive/learned scaling schedules beyond the simple threshold rule (Eq. 3), and extend precisionâ€‘decoupling ideas to other optimizers (e.g., Adafactor) and architectures (e.g., MoE, diffusion).
  - Even lower precision: given the stability shown with FP8 when carefully engineered, push to 4â€‘bit training for parts of the stack, building on the same scaling and decoupling principles.

- Practical applications
  - Pretraining: cut training time/cost for very large GPTâ€‘style models.
  - Fineâ€‘tuning and RLHF: reduce memory/compute when multiple models (policy, reference, reward) are loaded concurrently; the reported 32%â€“62% memory reductions in RLHF (Table 4) are particularly valuable for practitioners.
  - Longer contexts and larger batch sizes on the same hardware footprint (Section 3.2.2): e.g., 175B with 4096 tokens on 32Ã—H100 becomes feasible with this FP8 scheme, whereas TE fits only 2048 (Section 3.2.2).

> Net takeaway: With targeted scaling and precision decoupling, FP8 is not just a computeâ€‘kernel trickâ€”it is a viable, generalâ€‘purpose training precision for LLMs that meaningfully reduces cost while preserving quality.
